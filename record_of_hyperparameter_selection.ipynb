{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "!python train.py kfold baseline\n",
    "#d 0.25 p 20"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 17:34:54.023367: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 17:34:54.023406: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 17:34:54.023895: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 17:34:54.221029: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 6397518336.0000 - rmse: 79984.4844 - val_loss: 1246643072.0000 - val_rmse: 35307.8320\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1877726464.0000 - rmse: 43332.7422 - val_loss: 880853696.0000 - val_rmse: 29679.1797\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1623821312.0000 - rmse: 40296.6680 - val_loss: 823978752.0000 - val_rmse: 28705.0293\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1536930176.0000 - rmse: 39203.6992 - val_loss: 767314240.0000 - val_rmse: 27700.4375\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1396675968.0000 - rmse: 37372.1289 - val_loss: 785914176.0000 - val_rmse: 28034.1602\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1367948800.0000 - rmse: 36985.7930 - val_loss: 737412672.0000 - val_rmse: 27155.3438\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1333125888.0000 - rmse: 36511.9961 - val_loss: 838437824.0000 - val_rmse: 28955.7910\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1369092608.0000 - rmse: 37001.2500 - val_loss: 794497280.0000 - val_rmse: 28186.8281\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1219444480.0000 - rmse: 34920.5469 - val_loss: 950929408.0000 - val_rmse: 30837.1426\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1232772608.0000 - rmse: 35110.8633 - val_loss: 671192192.0000 - val_rmse: 25907.3770\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1119732736.0000 - rmse: 33462.4062 - val_loss: 608944640.0000 - val_rmse: 24676.8027\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1085880064.0000 - rmse: 32952.6953 - val_loss: 894492032.0000 - val_rmse: 29908.0566\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1140157312.0000 - rmse: 33766.2148 - val_loss: 704229120.0000 - val_rmse: 26537.3125\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1044232576.0000 - rmse: 32314.5879 - val_loss: 620344512.0000 - val_rmse: 24906.7168\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1011260992.0000 - rmse: 31800.3301 - val_loss: 687778176.0000 - val_rmse: 26225.5254\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 977960768.0000 - rmse: 31272.3652 - val_loss: 700035904.0000 - val_rmse: 26458.1914\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 970030912.0000 - rmse: 31145.3145 - val_loss: 574637248.0000 - val_rmse: 23971.5918\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 945877568.0000 - rmse: 30755.1230 - val_loss: 689006400.0000 - val_rmse: 26248.9297\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 899649984.0000 - rmse: 29994.1660 - val_loss: 722496832.0000 - val_rmse: 26879.3008\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1055686848.0000 - rmse: 32491.3359 - val_loss: 629337024.0000 - val_rmse: 25086.5898\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 839266048.0000 - rmse: 28970.0879 - val_loss: 511692256.0000 - val_rmse: 22620.6152\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 840001856.0000 - rmse: 28982.7852 - val_loss: 804341888.0000 - val_rmse: 28360.9219\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 831664768.0000 - rmse: 28838.5977 - val_loss: 470280512.0000 - val_rmse: 21685.9512\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 796269568.0000 - rmse: 28218.2480 - val_loss: 838766976.0000 - val_rmse: 28961.4746\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 869229696.0000 - rmse: 29482.7012 - val_loss: 459991488.0000 - val_rmse: 21447.4121\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 759538176.0000 - rmse: 27559.7188 - val_loss: 667604416.0000 - val_rmse: 25838.0430\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 779987072.0000 - rmse: 27928.2480 - val_loss: 645207552.0000 - val_rmse: 25400.9355\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 844344192.0000 - rmse: 29057.6016 - val_loss: 441115744.0000 - val_rmse: 21002.7559\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 764329280.0000 - rmse: 27646.5059 - val_loss: 605599808.0000 - val_rmse: 24608.9355\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 696686336.0000 - rmse: 26394.8145 - val_loss: 464367456.0000 - val_rmse: 21549.1875\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782017024.0000 - rmse: 27964.5664 - val_loss: 532318208.0000 - val_rmse: 23072.0215\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 712768128.0000 - rmse: 26697.7168 - val_loss: 414605088.0000 - val_rmse: 20361.8516\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 696064128.0000 - rmse: 26383.0273 - val_loss: 596450176.0000 - val_rmse: 24422.3281\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 738789248.0000 - rmse: 27180.6758 - val_loss: 448751968.0000 - val_rmse: 21183.7676\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 723544064.0000 - rmse: 26898.7734 - val_loss: 1054959296.0000 - val_rmse: 32480.1367\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704602304.0000 - rmse: 26544.3457 - val_loss: 383881824.0000 - val_rmse: 19592.9023\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 683381120.0000 - rmse: 26141.5586 - val_loss: 1653940992.0000 - val_rmse: 40668.6719\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 720082048.0000 - rmse: 26834.3438 - val_loss: 637910016.0000 - val_rmse: 25256.8809\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704804672.0000 - rmse: 26548.1562 - val_loss: 752521280.0000 - val_rmse: 27432.1211\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 671978880.0000 - rmse: 25922.5547 - val_loss: 876368448.0000 - val_rmse: 29603.5215\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 653034944.0000 - rmse: 25554.5488 - val_loss: 620830080.0000 - val_rmse: 24916.4629\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 716634560.0000 - rmse: 26770.0312 - val_loss: 480418240.0000 - val_rmse: 21918.4453\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 645985984.0000 - rmse: 25416.2539 - val_loss: 1528734208.0000 - val_rmse: 39099.0312\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641560320.0000 - rmse: 25329.0410 - val_loss: 626693888.0000 - val_rmse: 25033.8555\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604585600.0000 - rmse: 24588.3223 - val_loss: 525558880.0000 - val_rmse: 22925.0703\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620101312.0000 - rmse: 24901.8301 - val_loss: 735413504.0000 - val_rmse: 27118.5078\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 636660928.0000 - rmse: 25232.1406 - val_loss: 946393088.0000 - val_rmse: 30763.5020\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621322176.0000 - rmse: 24926.3340 - val_loss: 890048384.0000 - val_rmse: 29833.6777\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 602005184.0000 - rmse: 24535.7930 - val_loss: 691070464.0000 - val_rmse: 26288.2188\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512451680.0000 - rmse: 22637.3965 - val_loss: 1194856448.0000 - val_rmse: 34566.6953\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572217152.0000 - rmse: 23921.0605 - val_loss: 800252608.0000 - val_rmse: 28288.7363\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578306816.0000 - rmse: 24048.0098 - val_loss: 594454016.0000 - val_rmse: 24381.4277\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538896000.0000 - rmse: 23214.1328 - val_loss: 1579955200.0000 - val_rmse: 39748.6484\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601093376.0000 - rmse: 24517.2051 - val_loss: 616843968.0000 - val_rmse: 24836.3438\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597085120.0000 - rmse: 24435.3262 - val_loss: 825948416.0000 - val_rmse: 28739.3184\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539495424.0000 - rmse: 23227.0391 - val_loss: 617499072.0000 - val_rmse: 24849.5273\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568907008.0000 - rmse: 23851.7715 - val_loss: 1074164736.0000 - val_rmse: 32774.4531\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537347328.0000 - rmse: 23180.7539 - val_loss: 1070012096.0000 - val_rmse: 32711.0391\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564193344.0000 - rmse: 23752.7539 - val_loss: 605472256.0000 - val_rmse: 24606.3457\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530981280.0000 - rmse: 23043.0293 - val_loss: 509707712.0000 - val_rmse: 22576.7051\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548911872.0000 - rmse: 23428.8691 - val_loss: 532165344.0000 - val_rmse: 23068.7090\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469167744.0000 - rmse: 21660.2793 - val_loss: 781333888.0000 - val_rmse: 27952.3496\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550471808.0000 - rmse: 23462.1348 - val_loss: 408023840.0000 - val_rmse: 20199.5996\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457376640.0000 - rmse: 21386.3633 - val_loss: 1240632960.0000 - val_rmse: 35222.6172\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481066976.0000 - rmse: 21933.2383 - val_loss: 594050688.0000 - val_rmse: 24373.1543\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560506176.0000 - rmse: 23675.0117 - val_loss: 890729152.0000 - val_rmse: 29845.0859\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505831136.0000 - rmse: 22490.6895 - val_loss: 598131840.0000 - val_rmse: 24456.7305\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576682304.0000 - rmse: 24014.2090 - val_loss: 558353024.0000 - val_rmse: 23629.4941\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511581824.0000 - rmse: 22618.1719 - val_loss: 519300256.0000 - val_rmse: 22788.1582\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435267296.0000 - rmse: 20863.0586 - val_loss: 485272384.0000 - val_rmse: 22028.8984\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496960224.0000 - rmse: 22292.6035 - val_loss: 796687872.0000 - val_rmse: 28225.6602\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471118464.0000 - rmse: 21705.2637 - val_loss: 883845376.0000 - val_rmse: 29729.5371\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498836928.0000 - rmse: 22334.6582 - val_loss: 1354568704.0000 - val_rmse: 36804.4648\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494299072.0000 - rmse: 22232.8379 - val_loss: 735038080.0000 - val_rmse: 27111.5859\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474422400.0000 - rmse: 21781.2402 - val_loss: 1364392576.0000 - val_rmse: 36937.6836\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559772224.0000 - rmse: 23659.5039 - val_loss: 595341248.0000 - val_rmse: 24399.6133\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448054080.0000 - rmse: 21167.2871 - val_loss: 624157504.0000 - val_rmse: 24983.1445\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514452032.0000 - rmse: 22681.5352 - val_loss: 773353600.0000 - val_rmse: 27809.2363\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438871328.0000 - rmse: 20949.2539 - val_loss: 812146432.0000 - val_rmse: 28498.1816\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555510912.0000 - rmse: 23569.2773 - val_loss: 485470144.0000 - val_rmse: 22033.3867\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417497760.0000 - rmse: 20432.7598 - val_loss: 791182016.0000 - val_rmse: 28127.9570\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455665888.0000 - rmse: 21346.3320 - val_loss: 731578176.0000 - val_rmse: 27047.7012\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460397056.0000 - rmse: 21456.8652 - val_loss: 747766272.0000 - val_rmse: 27345.3145\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403935104.0000 - rmse: 20098.1367 - val_loss: 636250368.0000 - val_rmse: 25224.0039\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394894656.0000 - rmse: 19871.9551 - val_loss: 2124163200.0000 - val_rmse: 46088.6445\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437785312.0000 - rmse: 20923.3184 - val_loss: 718034496.0000 - val_rmse: 26796.1660\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424082816.0000 - rmse: 20593.2695 - val_loss: 900627072.0000 - val_rmse: 30010.4453\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378557088.0000 - rmse: 19456.5410 - val_loss: 1160362752.0000 - val_rmse: 34064.0938\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462110624.0000 - rmse: 21496.7578 - val_loss: 1018826624.0000 - val_rmse: 31919.0645\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422846752.0000 - rmse: 20563.2363 - val_loss: 551524224.0000 - val_rmse: 23484.5508\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453488320.0000 - rmse: 21295.2617 - val_loss: 758963648.0000 - val_rmse: 27549.2930\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434494976.0000 - rmse: 20844.5410 - val_loss: 1083975040.0000 - val_rmse: 32923.7773\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355152704.0000 - rmse: 18845.4941 - val_loss: 398688608.0000 - val_rmse: 19967.1875\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459807488.0000 - rmse: 21443.1211 - val_loss: 516470592.0000 - val_rmse: 22725.9902\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424793728.0000 - rmse: 20610.5215 - val_loss: 845529728.0000 - val_rmse: 29077.9922\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386234048.0000 - rmse: 19652.8359 - val_loss: 1322063616.0000 - val_rmse: 36360.1914\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418477824.0000 - rmse: 20456.7285 - val_loss: 1612198144.0000 - val_rmse: 40152.1875\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468195904.0000 - rmse: 21637.8340 - val_loss: 671944704.0000 - val_rmse: 25921.8965\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511719296.0000 - rmse: 22621.2109 - val_loss: 782919552.0000 - val_rmse: 27980.6992\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366317024.0000 - rmse: 19139.4082 - val_loss: 1119919872.0000 - val_rmse: 33465.2031\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462789504.0000 - rmse: 21512.5430 - val_loss: 444116768.0000 - val_rmse: 21074.0742\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385242272.0000 - rmse: 19627.5879 - val_loss: 1261148288.0000 - val_rmse: 35512.6484\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447977120.0000 - rmse: 21165.4707 - val_loss: 551993664.0000 - val_rmse: 23494.5449\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369146880.0000 - rmse: 19213.1934 - val_loss: 906309632.0000 - val_rmse: 30104.9746\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398777536.0000 - rmse: 19969.4141 - val_loss: 783949312.0000 - val_rmse: 27999.0957\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478494816.0000 - rmse: 21874.5234 - val_loss: 662550528.0000 - val_rmse: 25740.0566\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342021504.0000 - rmse: 18493.8203 - val_loss: 527508608.0000 - val_rmse: 22967.5547\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462901376.0000 - rmse: 21515.1387 - val_loss: 1082054528.0000 - val_rmse: 32894.5977\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365248320.0000 - rmse: 19111.4688 - val_loss: 796033216.0000 - val_rmse: 28214.0586\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354174656.0000 - rmse: 18819.5254 - val_loss: 979549184.0000 - val_rmse: 31297.7480\n",
      "Epoch 111/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443742144.0000 - rmse: 21065.1855 - val_loss: 656025024.0000 - val_rmse: 25612.9824\n",
      "Epoch 112/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331618176.0000 - rmse: 18210.3828 - val_loss: 1151047936.0000 - val_rmse: 33927.0977\n",
      "Epoch 113/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397268160.0000 - rmse: 19931.5879 - val_loss: 671997632.0000 - val_rmse: 25922.9180\n",
      "Epoch 114/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411535744.0000 - rmse: 20286.3418 - val_loss: 509541280.0000 - val_rmse: 22573.0195\n",
      "Epoch 115/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338975776.0000 - rmse: 18411.2949 - val_loss: 790049984.0000 - val_rmse: 28107.8281\n",
      "Epoch 116/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338434976.0000 - rmse: 18396.5996 - val_loss: 1318640768.0000 - val_rmse: 36313.0938\n",
      "Epoch 117/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425938272.0000 - rmse: 20638.2695 - val_loss: 522226848.0000 - val_rmse: 22852.2832\n",
      "Epoch 118/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392944704.0000 - rmse: 19822.8301 - val_loss: 610390656.0000 - val_rmse: 24706.0859\n",
      "Epoch 119/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448663264.0000 - rmse: 21181.6699 - val_loss: 522506592.0000 - val_rmse: 22858.4004\n",
      "Epoch 120/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327759712.0000 - rmse: 18104.1348 - val_loss: 660529664.0000 - val_rmse: 25700.7695\n",
      "Epoch 121/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403802240.0000 - rmse: 20094.8281 - val_loss: 573040768.0000 - val_rmse: 23938.2695\n",
      "Epoch 122/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364763136.0000 - rmse: 19098.7715 - val_loss: 532109632.0000 - val_rmse: 23067.5000\n",
      "Epoch 123/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387917728.0000 - rmse: 19695.6250 - val_loss: 707330112.0000 - val_rmse: 26595.6777\n",
      "Epoch 124/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381154336.0000 - rmse: 19523.1719 - val_loss: 928702144.0000 - val_rmse: 30474.6133\n",
      "Epoch 125/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420579584.0000 - rmse: 20508.0352 - val_loss: 590326784.0000 - val_rmse: 24296.6387\n",
      "Epoch 126/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395165376.0000 - rmse: 19878.7656 - val_loss: 364965984.0000 - val_rmse: 19104.0820\n",
      "Epoch 127/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356884256.0000 - rmse: 18891.3789 - val_loss: 478540672.0000 - val_rmse: 21875.5703\n",
      "Epoch 128/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444196192.0000 - rmse: 21075.9609 - val_loss: 611324032.0000 - val_rmse: 24724.9668\n",
      "Epoch 129/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282313888.0000 - rmse: 16802.1973 - val_loss: 551932352.0000 - val_rmse: 23493.2383\n",
      "Epoch 130/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357637216.0000 - rmse: 18911.2969 - val_loss: 656299968.0000 - val_rmse: 25618.3516\n",
      "Epoch 131/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360788064.0000 - rmse: 18994.4219 - val_loss: 678838208.0000 - val_rmse: 26054.5195\n",
      "Epoch 132/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394962432.0000 - rmse: 19873.6602 - val_loss: 591859264.0000 - val_rmse: 24328.1582\n",
      "Epoch 133/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345808544.0000 - rmse: 18595.9258 - val_loss: 499060352.0000 - val_rmse: 22339.6562\n",
      "Epoch 134/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384953408.0000 - rmse: 19620.2285 - val_loss: 666045952.0000 - val_rmse: 25807.8652\n",
      "Epoch 135/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353205312.0000 - rmse: 18793.7559 - val_loss: 315090912.0000 - val_rmse: 17750.7969\n",
      "Epoch 136/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326492192.0000 - rmse: 18069.0938 - val_loss: 562828672.0000 - val_rmse: 23724.0078\n",
      "Epoch 137/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461062848.0000 - rmse: 21472.3711 - val_loss: 580000768.0000 - val_rmse: 24083.2031\n",
      "Epoch 138/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306150560.0000 - rmse: 17497.1543 - val_loss: 558455104.0000 - val_rmse: 23631.6523\n",
      "Epoch 139/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335084480.0000 - rmse: 18305.3105 - val_loss: 1123463040.0000 - val_rmse: 33518.0977\n",
      "Epoch 140/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355464832.0000 - rmse: 18853.7715 - val_loss: 494041472.0000 - val_rmse: 22227.0430\n",
      "Epoch 141/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341609568.0000 - rmse: 18482.6816 - val_loss: 577596608.0000 - val_rmse: 24033.2363\n",
      "Epoch 142/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375415552.0000 - rmse: 19375.6406 - val_loss: 1105593728.0000 - val_rmse: 33250.4688\n",
      "Epoch 143/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385340896.0000 - rmse: 19630.0996 - val_loss: 585580352.0000 - val_rmse: 24198.7676\n",
      "Epoch 144/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317413248.0000 - rmse: 17816.0938 - val_loss: 1494239744.0000 - val_rmse: 38655.3984\n",
      "Epoch 145/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380731008.0000 - rmse: 19512.3281 - val_loss: 754535168.0000 - val_rmse: 27468.8008\n",
      "Epoch 146/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318296544.0000 - rmse: 17840.8652 - val_loss: 1227322112.0000 - val_rmse: 35033.1562\n",
      "Epoch 147/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317850176.0000 - rmse: 17828.3496 - val_loss: 770780224.0000 - val_rmse: 27762.9297\n",
      "Epoch 148/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451059264.0000 - rmse: 21238.1543 - val_loss: 468788960.0000 - val_rmse: 21651.5332\n",
      "Epoch 149/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336918112.0000 - rmse: 18355.3262 - val_loss: 574081984.0000 - val_rmse: 23960.0059\n",
      "104/104 [==============================] - 0s 716us/step - loss: 451018528.0000 - rmse: 21237.1934\n",
      "[451018528.0, 21237.193359375]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 4704061440.0000 - rmse: 68586.1641 - val_loss: 1192722944.0000 - val_rmse: 34535.8203\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1476537728.0000 - rmse: 38425.7422 - val_loss: 1074481792.0000 - val_rmse: 32779.2891\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1318533120.0000 - rmse: 36311.6133 - val_loss: 1005838336.0000 - val_rmse: 31714.9551\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1250923136.0000 - rmse: 35368.3906 - val_loss: 1021878080.0000 - val_rmse: 31966.8281\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1231650048.0000 - rmse: 35094.8711 - val_loss: 828464704.0000 - val_rmse: 28783.0625\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1133054464.0000 - rmse: 33660.8750 - val_loss: 826745792.0000 - val_rmse: 28753.1875\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1078888448.0000 - rmse: 32846.4375 - val_loss: 1408175616.0000 - val_rmse: 37525.6680\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1126365824.0000 - rmse: 33561.3750 - val_loss: 907634560.0000 - val_rmse: 30126.9727\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1046628800.0000 - rmse: 32351.6426 - val_loss: 785272320.0000 - val_rmse: 28022.7109\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1017202304.0000 - rmse: 31893.6094 - val_loss: 734898560.0000 - val_rmse: 27109.0117\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1025178048.0000 - rmse: 32018.4023 - val_loss: 917990976.0000 - val_rmse: 30298.3652\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 903429888.0000 - rmse: 30057.1113 - val_loss: 760352896.0000 - val_rmse: 27574.4980\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 998783552.0000 - rmse: 31603.5371 - val_loss: 682191872.0000 - val_rmse: 26118.8027\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 920948416.0000 - rmse: 30347.1328 - val_loss: 1220526080.0000 - val_rmse: 34936.0273\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 880678272.0000 - rmse: 29676.2246 - val_loss: 659109440.0000 - val_rmse: 25673.1270\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 916033664.0000 - rmse: 30266.0488 - val_loss: 2141046400.0000 - val_rmse: 46271.4414\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 882990272.0000 - rmse: 29715.1523 - val_loss: 719848000.0000 - val_rmse: 26829.9824\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 772212032.0000 - rmse: 27788.7031 - val_loss: 1786953984.0000 - val_rmse: 42272.3789\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 796304320.0000 - rmse: 28218.8633 - val_loss: 653857088.0000 - val_rmse: 25570.6289\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 911402496.0000 - rmse: 30189.4434 - val_loss: 646051456.0000 - val_rmse: 25417.5430\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 834606016.0000 - rmse: 28889.5469 - val_loss: 625911680.0000 - val_rmse: 25018.2266\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 761237568.0000 - rmse: 27590.5332 - val_loss: 636548352.0000 - val_rmse: 25229.9102\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 776716032.0000 - rmse: 27869.6250 - val_loss: 620012224.0000 - val_rmse: 24900.0449\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 801291072.0000 - rmse: 28307.0859 - val_loss: 708112768.0000 - val_rmse: 26610.3887\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 757971968.0000 - rmse: 27531.2910 - val_loss: 666806848.0000 - val_rmse: 25822.6035\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 748405952.0000 - rmse: 27357.0098 - val_loss: 642162752.0000 - val_rmse: 25340.9297\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622366976.0000 - rmse: 24947.2832 - val_loss: 639089280.0000 - val_rmse: 25280.2148\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661978176.0000 - rmse: 25728.9375 - val_loss: 1218696064.0000 - val_rmse: 34909.8281\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724227840.0000 - rmse: 26911.4824 - val_loss: 576168640.0000 - val_rmse: 24003.5137\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 670051840.0000 - rmse: 25885.3574 - val_loss: 1104579968.0000 - val_rmse: 33235.2227\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718194432.0000 - rmse: 26799.1465 - val_loss: 587753280.0000 - val_rmse: 24243.6230\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 667622528.0000 - rmse: 25838.3906 - val_loss: 672527104.0000 - val_rmse: 25933.1270\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640461056.0000 - rmse: 25307.3301 - val_loss: 603874944.0000 - val_rmse: 24573.8652\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680746944.0000 - rmse: 26091.1270 - val_loss: 644806784.0000 - val_rmse: 25393.0469\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 672137472.0000 - rmse: 25925.6133 - val_loss: 592099392.0000 - val_rmse: 24333.0918\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647030912.0000 - rmse: 25436.8027 - val_loss: 596670848.0000 - val_rmse: 24426.8477\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609499200.0000 - rmse: 24688.0352 - val_loss: 590247104.0000 - val_rmse: 24295.0020\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626189696.0000 - rmse: 25023.7832 - val_loss: 754513344.0000 - val_rmse: 27468.4062\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 682041216.0000 - rmse: 26115.9160 - val_loss: 954120640.0000 - val_rmse: 30888.8438\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 600865792.0000 - rmse: 24512.5645 - val_loss: 658894656.0000 - val_rmse: 25668.9414\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605045568.0000 - rmse: 24597.6719 - val_loss: 684087232.0000 - val_rmse: 26155.0605\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540499520.0000 - rmse: 23248.6465 - val_loss: 797117824.0000 - val_rmse: 28233.2754\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553408128.0000 - rmse: 23524.6289 - val_loss: 819040960.0000 - val_rmse: 28618.8926\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589506752.0000 - rmse: 24279.7598 - val_loss: 583450368.0000 - val_rmse: 24154.7168\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571781696.0000 - rmse: 23911.9570 - val_loss: 849208640.0000 - val_rmse: 29141.1816\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609146048.0000 - rmse: 24680.8848 - val_loss: 578154816.0000 - val_rmse: 24044.8496\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516643936.0000 - rmse: 22729.8008 - val_loss: 588100672.0000 - val_rmse: 24250.7871\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604696256.0000 - rmse: 24590.5703 - val_loss: 1301005312.0000 - val_rmse: 36069.4531\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536433184.0000 - rmse: 23161.0273 - val_loss: 591521152.0000 - val_rmse: 24321.2070\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590556544.0000 - rmse: 24301.3672 - val_loss: 632203072.0000 - val_rmse: 25143.6484\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512908800.0000 - rmse: 22647.4883 - val_loss: 613002688.0000 - val_rmse: 24758.8906\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491677984.0000 - rmse: 22173.8105 - val_loss: 544371072.0000 - val_rmse: 23331.7617\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495913920.0000 - rmse: 22269.1250 - val_loss: 695827840.0000 - val_rmse: 26378.5469\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515527872.0000 - rmse: 22705.2383 - val_loss: 575185216.0000 - val_rmse: 23983.0176\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504077632.0000 - rmse: 22451.6738 - val_loss: 687106368.0000 - val_rmse: 26212.7129\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449371712.0000 - rmse: 21198.3867 - val_loss: 563353408.0000 - val_rmse: 23735.0664\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466224768.0000 - rmse: 21592.2383 - val_loss: 578813440.0000 - val_rmse: 24058.5410\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460991040.0000 - rmse: 21470.7012 - val_loss: 540640960.0000 - val_rmse: 23251.6875\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489339392.0000 - rmse: 22121.0176 - val_loss: 783582464.0000 - val_rmse: 27992.5430\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401277504.0000 - rmse: 20031.9121 - val_loss: 552115648.0000 - val_rmse: 23497.1406\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470344704.0000 - rmse: 21687.4316 - val_loss: 565994624.0000 - val_rmse: 23790.6406\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435309280.0000 - rmse: 20864.0645 - val_loss: 572547968.0000 - val_rmse: 23927.9746\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431534464.0000 - rmse: 20773.4062 - val_loss: 578862208.0000 - val_rmse: 24059.5547\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398001920.0000 - rmse: 19949.9863 - val_loss: 526317632.0000 - val_rmse: 22941.6133\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457944320.0000 - rmse: 21399.6328 - val_loss: 576354048.0000 - val_rmse: 24007.3750\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452424928.0000 - rmse: 21270.2812 - val_loss: 724496384.0000 - val_rmse: 26916.4707\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443613664.0000 - rmse: 21062.1367 - val_loss: 569291520.0000 - val_rmse: 23859.8301\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502005408.0000 - rmse: 22405.4766 - val_loss: 531014688.0000 - val_rmse: 23043.7559\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467366400.0000 - rmse: 21618.6582 - val_loss: 579128256.0000 - val_rmse: 24065.0840\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426411776.0000 - rmse: 20649.7402 - val_loss: 545373632.0000 - val_rmse: 23353.2363\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439201984.0000 - rmse: 20957.1445 - val_loss: 542269888.0000 - val_rmse: 23286.6875\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477169600.0000 - rmse: 21844.2109 - val_loss: 615077824.0000 - val_rmse: 24800.7617\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497211648.0000 - rmse: 22298.2422 - val_loss: 591735104.0000 - val_rmse: 24325.6055\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522643232.0000 - rmse: 22861.3906 - val_loss: 543246080.0000 - val_rmse: 23307.6406\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384919744.0000 - rmse: 19619.3711 - val_loss: 738438528.0000 - val_rmse: 27174.2246\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441919680.0000 - rmse: 21021.8848 - val_loss: 565953408.0000 - val_rmse: 23789.7734\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469762976.0000 - rmse: 21674.0156 - val_loss: 617355328.0000 - val_rmse: 24846.6348\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402272768.0000 - rmse: 20056.7363 - val_loss: 568160512.0000 - val_rmse: 23836.1172\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441816608.0000 - rmse: 21019.4336 - val_loss: 561514240.0000 - val_rmse: 23696.2910\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380831040.0000 - rmse: 19514.8926 - val_loss: 525795744.0000 - val_rmse: 22930.2363\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426681344.0000 - rmse: 20656.2656 - val_loss: 623033152.0000 - val_rmse: 24960.6328\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359366944.0000 - rmse: 18956.9746 - val_loss: 540273088.0000 - val_rmse: 23243.7754\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428395296.0000 - rmse: 20697.7109 - val_loss: 561103680.0000 - val_rmse: 23687.6250\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434297824.0000 - rmse: 20839.8125 - val_loss: 607291392.0000 - val_rmse: 24643.2812\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391971040.0000 - rmse: 19798.2559 - val_loss: 692604928.0000 - val_rmse: 26317.3887\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380352864.0000 - rmse: 19502.6348 - val_loss: 670590720.0000 - val_rmse: 25895.7637\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393815328.0000 - rmse: 19844.7793 - val_loss: 578563264.0000 - val_rmse: 24053.3398\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347292576.0000 - rmse: 18635.7871 - val_loss: 535782880.0000 - val_rmse: 23146.9844\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382092672.0000 - rmse: 19547.1895 - val_loss: 958548672.0000 - val_rmse: 30960.4355\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361703968.0000 - rmse: 19018.5156 - val_loss: 582506112.0000 - val_rmse: 24135.1641\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335329152.0000 - rmse: 18311.9941 - val_loss: 694043456.0000 - val_rmse: 26344.7031\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390794144.0000 - rmse: 19768.5137 - val_loss: 732467584.0000 - val_rmse: 27064.1367\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341889632.0000 - rmse: 18490.2559 - val_loss: 568048256.0000 - val_rmse: 23833.7617\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352542304.0000 - rmse: 18776.1074 - val_loss: 828028160.0000 - val_rmse: 28775.4785\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350977952.0000 - rmse: 18734.4043 - val_loss: 528769888.0000 - val_rmse: 22994.9961\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402403648.0000 - rmse: 20060.0000 - val_loss: 979702912.0000 - val_rmse: 31300.2070\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348103936.0000 - rmse: 18657.5430 - val_loss: 610182400.0000 - val_rmse: 24701.8711\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395937920.0000 - rmse: 19898.1875 - val_loss: 529282560.0000 - val_rmse: 23006.1406\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347845920.0000 - rmse: 18650.6270 - val_loss: 589056640.0000 - val_rmse: 24270.4883\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411956640.0000 - rmse: 20296.7129 - val_loss: 580999104.0000 - val_rmse: 24103.9219\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373422496.0000 - rmse: 19324.1426 - val_loss: 837326528.0000 - val_rmse: 28936.5957\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424079296.0000 - rmse: 20593.1816 - val_loss: 504801120.0000 - val_rmse: 22467.7793\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374456512.0000 - rmse: 19350.8789 - val_loss: 728212544.0000 - val_rmse: 26985.4121\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389430336.0000 - rmse: 19733.9863 - val_loss: 540052224.0000 - val_rmse: 23239.0215\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338368000.0000 - rmse: 18394.7793 - val_loss: 580182016.0000 - val_rmse: 24086.9668\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335661760.0000 - rmse: 18321.0723 - val_loss: 571756032.0000 - val_rmse: 23911.4199\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359416512.0000 - rmse: 18958.2812 - val_loss: 579807616.0000 - val_rmse: 24079.1934\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346044192.0000 - rmse: 18602.2637 - val_loss: 614161408.0000 - val_rmse: 24782.2793\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335327424.0000 - rmse: 18311.9453 - val_loss: 550693696.0000 - val_rmse: 23466.8633\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368993312.0000 - rmse: 19209.1953 - val_loss: 557839936.0000 - val_rmse: 23618.6328\n",
      "Epoch 111/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337475840.0000 - rmse: 18370.5137 - val_loss: 517796096.0000 - val_rmse: 22755.1328\n",
      "Epoch 112/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333137248.0000 - rmse: 18252.0469 - val_loss: 526226336.0000 - val_rmse: 22939.6211\n",
      "Epoch 113/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348131424.0000 - rmse: 18658.2793 - val_loss: 788829440.0000 - val_rmse: 28086.1074\n",
      "Epoch 114/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354289920.0000 - rmse: 18822.5879 - val_loss: 510364992.0000 - val_rmse: 22591.2578\n",
      "Epoch 115/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316834112.0000 - rmse: 17799.8320 - val_loss: 551890304.0000 - val_rmse: 23492.3457\n",
      "Epoch 116/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359324320.0000 - rmse: 18955.8516 - val_loss: 500364032.0000 - val_rmse: 22368.8164\n",
      "Epoch 117/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377506176.0000 - rmse: 19429.5176 - val_loss: 646032000.0000 - val_rmse: 25417.1562\n",
      "Epoch 118/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422097408.0000 - rmse: 20545.0078 - val_loss: 498409536.0000 - val_rmse: 22325.0859\n",
      "Epoch 119/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325643488.0000 - rmse: 18045.5938 - val_loss: 520826464.0000 - val_rmse: 22821.6230\n",
      "Epoch 120/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324183232.0000 - rmse: 18005.0879 - val_loss: 745039168.0000 - val_rmse: 27295.4043\n",
      "Epoch 121/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329642176.0000 - rmse: 18156.0488 - val_loss: 571548224.0000 - val_rmse: 23907.0723\n",
      "Epoch 122/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355495520.0000 - rmse: 18854.5859 - val_loss: 549298304.0000 - val_rmse: 23437.1133\n",
      "Epoch 123/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315021440.0000 - rmse: 17748.8418 - val_loss: 530582528.0000 - val_rmse: 23034.3750\n",
      "Epoch 124/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348537184.0000 - rmse: 18669.1484 - val_loss: 981541376.0000 - val_rmse: 31329.5605\n",
      "Epoch 125/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337452608.0000 - rmse: 18369.8789 - val_loss: 551674368.0000 - val_rmse: 23487.7461\n",
      "Epoch 126/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327958336.0000 - rmse: 18109.6191 - val_loss: 563310400.0000 - val_rmse: 23734.1602\n",
      "Epoch 127/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299870656.0000 - rmse: 17316.7734 - val_loss: 616927488.0000 - val_rmse: 24838.0234\n",
      "Epoch 128/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345595456.0000 - rmse: 18590.1953 - val_loss: 618742208.0000 - val_rmse: 24874.5273\n",
      "Epoch 129/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304634528.0000 - rmse: 17453.7812 - val_loss: 632879232.0000 - val_rmse: 25157.0918\n",
      "Epoch 130/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309254368.0000 - rmse: 17585.6289 - val_loss: 550562432.0000 - val_rmse: 23464.0664\n",
      "Epoch 131/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315601344.0000 - rmse: 17765.1719 - val_loss: 918748480.0000 - val_rmse: 30310.8633\n",
      "Epoch 132/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330272512.0000 - rmse: 18173.3965 - val_loss: 598667968.0000 - val_rmse: 24467.6914\n",
      "Epoch 133/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293878432.0000 - rmse: 17142.8789 - val_loss: 1319498496.0000 - val_rmse: 36324.9023\n",
      "Epoch 134/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355964160.0000 - rmse: 18867.0098 - val_loss: 502297760.0000 - val_rmse: 22411.9980\n",
      "Epoch 135/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354002720.0000 - rmse: 18814.9609 - val_loss: 634617920.0000 - val_rmse: 25191.6230\n",
      "Epoch 136/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293984704.0000 - rmse: 17145.9824 - val_loss: 617762624.0000 - val_rmse: 24854.8301\n",
      "Epoch 137/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323373056.0000 - rmse: 17982.5742 - val_loss: 572056576.0000 - val_rmse: 23917.7012\n",
      "Epoch 138/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395851488.0000 - rmse: 19896.0156 - val_loss: 605763520.0000 - val_rmse: 24612.2617\n",
      "Epoch 139/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330814656.0000 - rmse: 18188.3105 - val_loss: 574266816.0000 - val_rmse: 23963.8633\n",
      "Epoch 140/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332814144.0000 - rmse: 18243.1934 - val_loss: 587957184.0000 - val_rmse: 24247.8262\n",
      "Epoch 141/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335220192.0000 - rmse: 18309.0176 - val_loss: 760015168.0000 - val_rmse: 27568.3711\n",
      "Epoch 142/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301958592.0000 - rmse: 17376.9512 - val_loss: 568353856.0000 - val_rmse: 23840.1719\n",
      "Epoch 143/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329114912.0000 - rmse: 18141.5234 - val_loss: 805629312.0000 - val_rmse: 28383.6094\n",
      "Epoch 144/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321078112.0000 - rmse: 17918.6523 - val_loss: 588541440.0000 - val_rmse: 24259.8711\n",
      "Epoch 145/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319644256.0000 - rmse: 17878.5957 - val_loss: 644159360.0000 - val_rmse: 25380.2949\n",
      "Epoch 146/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323252160.0000 - rmse: 17979.2129 - val_loss: 689644608.0000 - val_rmse: 26261.0859\n",
      "Epoch 147/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329289664.0000 - rmse: 18146.3379 - val_loss: 567485312.0000 - val_rmse: 23821.9473\n",
      "Epoch 148/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318790368.0000 - rmse: 17854.6992 - val_loss: 504131296.0000 - val_rmse: 22452.8672\n",
      "Epoch 149/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288353696.0000 - rmse: 16980.9785 - val_loss: 500120640.0000 - val_rmse: 22363.3750\n",
      "Epoch 150/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283008256.0000 - rmse: 16822.8477 - val_loss: 797406720.0000 - val_rmse: 28238.3906\n",
      "Epoch 151/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302919680.0000 - rmse: 17404.5879 - val_loss: 538768000.0000 - val_rmse: 23211.3770\n",
      "Epoch 152/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327693376.0000 - rmse: 18102.3008 - val_loss: 593501120.0000 - val_rmse: 24361.8770\n",
      "Epoch 153/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309862880.0000 - rmse: 17602.9199 - val_loss: 590113024.0000 - val_rmse: 24292.2422\n",
      "Epoch 154/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329853248.0000 - rmse: 18161.8633 - val_loss: 644757056.0000 - val_rmse: 25392.0645\n",
      "Epoch 155/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277797728.0000 - rmse: 16667.2617 - val_loss: 645294080.0000 - val_rmse: 25402.6367\n",
      "Epoch 156/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336768448.0000 - rmse: 18351.2500 - val_loss: 532984160.0000 - val_rmse: 23086.4492\n",
      "Epoch 157/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281827680.0000 - rmse: 16787.7227 - val_loss: 501521760.0000 - val_rmse: 22394.6797\n",
      "Epoch 158/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467866752.0000 - rmse: 21630.2285 - val_loss: 608817216.0000 - val_rmse: 24674.2207\n",
      "Epoch 159/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305640096.0000 - rmse: 17482.5645 - val_loss: 525448896.0000 - val_rmse: 22922.6699\n",
      "Epoch 160/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294832832.0000 - rmse: 17170.6953 - val_loss: 465988512.0000 - val_rmse: 21586.7656\n",
      "Epoch 161/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302985536.0000 - rmse: 17406.4785 - val_loss: 518819424.0000 - val_rmse: 22777.6074\n",
      "Epoch 162/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330370080.0000 - rmse: 18176.0840 - val_loss: 547041600.0000 - val_rmse: 23388.9199\n",
      "Epoch 163/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302486016.0000 - rmse: 17392.1250 - val_loss: 634737984.0000 - val_rmse: 25194.0059\n",
      "Epoch 164/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301247840.0000 - rmse: 17356.4902 - val_loss: 571949440.0000 - val_rmse: 23915.4629\n",
      "Epoch 165/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377257056.0000 - rmse: 19423.1055 - val_loss: 511735104.0000 - val_rmse: 22621.5605\n",
      "Epoch 166/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418467456.0000 - rmse: 20456.4766 - val_loss: 520831552.0000 - val_rmse: 22821.7324\n",
      "Epoch 167/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300496160.0000 - rmse: 17334.8242 - val_loss: 582934912.0000 - val_rmse: 24144.0410\n",
      "Epoch 168/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258269648.0000 - rmse: 16070.7686 - val_loss: 558145664.0000 - val_rmse: 23625.1035\n",
      "Epoch 169/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377945856.0000 - rmse: 19440.8262 - val_loss: 544009344.0000 - val_rmse: 23324.0039\n",
      "Epoch 170/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305414112.0000 - rmse: 17476.0996 - val_loss: 586711104.0000 - val_rmse: 24222.1191\n",
      "Epoch 171/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330629632.0000 - rmse: 18183.2227 - val_loss: 736858688.0000 - val_rmse: 27145.1406\n",
      "Epoch 172/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283964832.0000 - rmse: 16851.2539 - val_loss: 842984512.0000 - val_rmse: 29034.1953\n",
      "Epoch 173/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298876128.0000 - rmse: 17288.0332 - val_loss: 523430688.0000 - val_rmse: 22878.6035\n",
      "Epoch 174/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307682048.0000 - rmse: 17540.8652 - val_loss: 578380096.0000 - val_rmse: 24049.5332\n",
      "Epoch 175/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312434048.0000 - rmse: 17675.8008 - val_loss: 575961984.0000 - val_rmse: 23999.2051\n",
      "Epoch 176/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352024480.0000 - rmse: 18762.3125 - val_loss: 537337728.0000 - val_rmse: 23180.5449\n",
      "Epoch 177/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298914528.0000 - rmse: 17289.1426 - val_loss: 557253696.0000 - val_rmse: 23606.2188\n",
      "Epoch 178/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264732432.0000 - rmse: 16270.5967 - val_loss: 1215084416.0000 - val_rmse: 34858.0625\n",
      "Epoch 179/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276260256.0000 - rmse: 16621.0742 - val_loss: 553769728.0000 - val_rmse: 23532.3105\n",
      "Epoch 180/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286958688.0000 - rmse: 16939.8535 - val_loss: 489892832.0000 - val_rmse: 22133.5215\n",
      "Epoch 181/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318465728.0000 - rmse: 17845.6055 - val_loss: 483980576.0000 - val_rmse: 21999.5566\n",
      "Epoch 182/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285603232.0000 - rmse: 16899.7969 - val_loss: 521932640.0000 - val_rmse: 22845.8438\n",
      "Epoch 183/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283945984.0000 - rmse: 16850.6953 - val_loss: 885470400.0000 - val_rmse: 29756.8555\n",
      "Epoch 184/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295467840.0000 - rmse: 17189.1758 - val_loss: 487204096.0000 - val_rmse: 22072.6992\n",
      "Epoch 185/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312902752.0000 - rmse: 17689.0566 - val_loss: 586555520.0000 - val_rmse: 24218.9062\n",
      "Epoch 186/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272688768.0000 - rmse: 16513.2871 - val_loss: 545196352.0000 - val_rmse: 23349.4395\n",
      "Epoch 187/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264360944.0000 - rmse: 16259.1787 - val_loss: 602133440.0000 - val_rmse: 24538.4082\n",
      "Epoch 188/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309158912.0000 - rmse: 17582.9141 - val_loss: 627169216.0000 - val_rmse: 25043.3438\n",
      "104/104 [==============================] - 0s 708us/step - loss: 727217088.0000 - rmse: 26966.9629\n",
      "[727217088.0, 26966.962890625]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 6636771840.0000 - rmse: 81466.3828 - val_loss: 1522537728.0000 - val_rmse: 39019.7109\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1570502144.0000 - rmse: 39629.5625 - val_loss: 1356750848.0000 - val_rmse: 36834.0977\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1393336704.0000 - rmse: 37327.4258 - val_loss: 1081269504.0000 - val_rmse: 32882.6641\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1257675648.0000 - rmse: 35463.7227 - val_loss: 1011977728.0000 - val_rmse: 31811.5977\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1202531584.0000 - rmse: 34677.5391 - val_loss: 1017335552.0000 - val_rmse: 31895.6973\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1143916032.0000 - rmse: 33821.8281 - val_loss: 951491712.0000 - val_rmse: 30846.2598\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1030311936.0000 - rmse: 32098.4727 - val_loss: 991546176.0000 - val_rmse: 31488.8262\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1115315200.0000 - rmse: 33396.3359 - val_loss: 988851968.0000 - val_rmse: 31446.0195\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1064800768.0000 - rmse: 32631.2852 - val_loss: 944184896.0000 - val_rmse: 30727.5918\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 998843776.0000 - rmse: 31604.4902 - val_loss: 1095426816.0000 - val_rmse: 33097.2344\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1029696000.0000 - rmse: 32088.8750 - val_loss: 905534848.0000 - val_rmse: 30092.1055\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1007642816.0000 - rmse: 31743.3867 - val_loss: 890672192.0000 - val_rmse: 29844.1309\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 987232512.0000 - rmse: 31420.2559 - val_loss: 897177984.0000 - val_rmse: 29952.9297\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 949784960.0000 - rmse: 30818.5820 - val_loss: 923088128.0000 - val_rmse: 30382.3652\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 940594176.0000 - rmse: 30669.1074 - val_loss: 1099119872.0000 - val_rmse: 33152.9766\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 946226240.0000 - rmse: 30760.7910 - val_loss: 860495616.0000 - val_rmse: 29334.2051\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 970853248.0000 - rmse: 31158.5176 - val_loss: 891661120.0000 - val_rmse: 29860.6953\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 905139200.0000 - rmse: 30085.5312 - val_loss: 1350082304.0000 - val_rmse: 36743.4648\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 916893952.0000 - rmse: 30280.2559 - val_loss: 871737280.0000 - val_rmse: 29525.1973\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 916950592.0000 - rmse: 30281.1895 - val_loss: 856809024.0000 - val_rmse: 29271.2988\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 918750528.0000 - rmse: 30310.8984 - val_loss: 1115697152.0000 - val_rmse: 33402.0547\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 846767424.0000 - rmse: 29099.2656 - val_loss: 938385088.0000 - val_rmse: 30633.0723\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 917775104.0000 - rmse: 30294.8027 - val_loss: 890580800.0000 - val_rmse: 29842.5977\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 919780992.0000 - rmse: 30327.8906 - val_loss: 903254912.0000 - val_rmse: 30054.1973\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 930836800.0000 - rmse: 30509.6172 - val_loss: 914977216.0000 - val_rmse: 30248.5898\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 907691200.0000 - rmse: 30127.9141 - val_loss: 899505792.0000 - val_rmse: 29991.7617\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 816718592.0000 - rmse: 28578.2891 - val_loss: 921716864.0000 - val_rmse: 30359.7891\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 784834560.0000 - rmse: 28014.8984 - val_loss: 1096816256.0000 - val_rmse: 33118.2148\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 805086400.0000 - rmse: 28374.0430 - val_loss: 1009127104.0000 - val_rmse: 31766.7617\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 812986368.0000 - rmse: 28512.9160 - val_loss: 1184772736.0000 - val_rmse: 34420.5273\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 748069568.0000 - rmse: 27350.8613 - val_loss: 942242304.0000 - val_rmse: 30695.9648\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 757044224.0000 - rmse: 27514.4336 - val_loss: 1305635968.0000 - val_rmse: 36133.5859\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 758505344.0000 - rmse: 27540.9766 - val_loss: 922137600.0000 - val_rmse: 30366.7168\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 741587520.0000 - rmse: 27232.1016 - val_loss: 1197456000.0000 - val_rmse: 34604.2773\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 768298816.0000 - rmse: 27718.2031 - val_loss: 991185152.0000 - val_rmse: 31483.0898\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 675923904.0000 - rmse: 25998.5371 - val_loss: 1131411584.0000 - val_rmse: 33636.4609\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 717084416.0000 - rmse: 26778.4316 - val_loss: 952442240.0000 - val_rmse: 30861.6602\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724831680.0000 - rmse: 26922.6953 - val_loss: 981951936.0000 - val_rmse: 31336.1133\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 755473408.0000 - rmse: 27485.8770 - val_loss: 809196096.0000 - val_rmse: 28446.3691\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 740349568.0000 - rmse: 27209.3652 - val_loss: 904220480.0000 - val_rmse: 30070.2598\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 700730368.0000 - rmse: 26471.3105 - val_loss: 930505280.0000 - val_rmse: 30504.1855\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 716711296.0000 - rmse: 26771.4648 - val_loss: 896395328.0000 - val_rmse: 29939.8613\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 716963456.0000 - rmse: 26776.1738 - val_loss: 1032204800.0000 - val_rmse: 32127.9434\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 672290880.0000 - rmse: 25928.5723 - val_loss: 1033914688.0000 - val_rmse: 32154.5430\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661745728.0000 - rmse: 25724.4180 - val_loss: 917781760.0000 - val_rmse: 30294.9141\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687368512.0000 - rmse: 26217.7109 - val_loss: 1001430080.0000 - val_rmse: 31645.3789\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628458176.0000 - rmse: 25069.0684 - val_loss: 1009736832.0000 - val_rmse: 31776.3574\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 688615936.0000 - rmse: 26241.4922 - val_loss: 917571328.0000 - val_rmse: 30291.4395\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687150464.0000 - rmse: 26213.5547 - val_loss: 1568252288.0000 - val_rmse: 39601.1641\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 696993344.0000 - rmse: 26400.6309 - val_loss: 1080052096.0000 - val_rmse: 32864.1445\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601150784.0000 - rmse: 24518.3770 - val_loss: 933570048.0000 - val_rmse: 30554.3789\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624653376.0000 - rmse: 24993.0645 - val_loss: 1217814400.0000 - val_rmse: 34897.1992\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651444672.0000 - rmse: 25523.4121 - val_loss: 1048796480.0000 - val_rmse: 32385.1250\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620456960.0000 - rmse: 24908.9727 - val_loss: 1130048512.0000 - val_rmse: 33616.1953\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620562816.0000 - rmse: 24911.0977 - val_loss: 860747904.0000 - val_rmse: 29338.5039\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574881600.0000 - rmse: 23976.6855 - val_loss: 803551040.0000 - val_rmse: 28346.9727\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599910592.0000 - rmse: 24493.0723 - val_loss: 963811264.0000 - val_rmse: 31045.3105\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573191168.0000 - rmse: 23941.4121 - val_loss: 776069184.0000 - val_rmse: 27858.0176\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557433344.0000 - rmse: 23610.0254 - val_loss: 1074049920.0000 - val_rmse: 32772.6992\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578275904.0000 - rmse: 24047.3652 - val_loss: 725371456.0000 - val_rmse: 26932.7207\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526988160.0000 - rmse: 22956.2227 - val_loss: 841533888.0000 - val_rmse: 29009.2012\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547565056.0000 - rmse: 23400.1074 - val_loss: 739134720.0000 - val_rmse: 27187.0312\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540482880.0000 - rmse: 23248.2871 - val_loss: 1043496256.0000 - val_rmse: 32303.1914\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574565888.0000 - rmse: 23970.1035 - val_loss: 609540992.0000 - val_rmse: 24688.8848\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591916928.0000 - rmse: 24329.3438 - val_loss: 812488896.0000 - val_rmse: 28504.1914\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556168640.0000 - rmse: 23583.2285 - val_loss: 708567552.0000 - val_rmse: 26618.9316\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571745280.0000 - rmse: 23911.1953 - val_loss: 622843392.0000 - val_rmse: 24956.8301\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485274816.0000 - rmse: 22028.9531 - val_loss: 963145856.0000 - val_rmse: 31034.5918\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533506144.0000 - rmse: 23097.7520 - val_loss: 937218304.0000 - val_rmse: 30614.0195\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504457568.0000 - rmse: 22460.1309 - val_loss: 1222453504.0000 - val_rmse: 34963.6016\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499237952.0000 - rmse: 22343.6328 - val_loss: 911089536.0000 - val_rmse: 30184.2559\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527542912.0000 - rmse: 22968.3008 - val_loss: 885268096.0000 - val_rmse: 29753.4531\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495979104.0000 - rmse: 22270.5859 - val_loss: 759603392.0000 - val_rmse: 27560.9023\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486863776.0000 - rmse: 22064.9883 - val_loss: 669886016.0000 - val_rmse: 25882.1562\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482491584.0000 - rmse: 21965.6895 - val_loss: 720557952.0000 - val_rmse: 26843.2090\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479435936.0000 - rmse: 21896.0234 - val_loss: 666683648.0000 - val_rmse: 25820.2168\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438308608.0000 - rmse: 20935.8203 - val_loss: 589920128.0000 - val_rmse: 24288.2695\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478441184.0000 - rmse: 21873.2969 - val_loss: 622932736.0000 - val_rmse: 24958.6172\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490188832.0000 - rmse: 22140.2070 - val_loss: 951067200.0000 - val_rmse: 30839.3770\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478763488.0000 - rmse: 21880.6621 - val_loss: 569554816.0000 - val_rmse: 23865.3457\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477786432.0000 - rmse: 21858.3242 - val_loss: 671576768.0000 - val_rmse: 25914.7969\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483786208.0000 - rmse: 21995.1406 - val_loss: 647912640.0000 - val_rmse: 25454.1250\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504144096.0000 - rmse: 22453.1523 - val_loss: 657430400.0000 - val_rmse: 25640.4023\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418862784.0000 - rmse: 20466.1348 - val_loss: 615049728.0000 - val_rmse: 24800.1953\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390927296.0000 - rmse: 19771.8809 - val_loss: 798117056.0000 - val_rmse: 28250.9648\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429118720.0000 - rmse: 20715.1816 - val_loss: 566690112.0000 - val_rmse: 23805.2539\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399988512.0000 - rmse: 19999.7109 - val_loss: 441827232.0000 - val_rmse: 21019.6836\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475609760.0000 - rmse: 21808.4785 - val_loss: 746643776.0000 - val_rmse: 27324.7832\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382759136.0000 - rmse: 19564.2305 - val_loss: 536233376.0000 - val_rmse: 23156.7129\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488154464.0000 - rmse: 22094.2188 - val_loss: 857814848.0000 - val_rmse: 29288.4766\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487921248.0000 - rmse: 22088.9395 - val_loss: 852782656.0000 - val_rmse: 29202.4414\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381523712.0000 - rmse: 19532.6289 - val_loss: 628870528.0000 - val_rmse: 25077.2871\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387753504.0000 - rmse: 19691.4551 - val_loss: 573107392.0000 - val_rmse: 23939.6602\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443134048.0000 - rmse: 21050.7480 - val_loss: 625020096.0000 - val_rmse: 25000.3984\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445333312.0000 - rmse: 21102.9219 - val_loss: 699896640.0000 - val_rmse: 26455.5586\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395573024.0000 - rmse: 19889.0156 - val_loss: 521597152.0000 - val_rmse: 22838.5000\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448604384.0000 - rmse: 21180.2812 - val_loss: 885305088.0000 - val_rmse: 29754.0762\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408242848.0000 - rmse: 20205.0195 - val_loss: 1114713344.0000 - val_rmse: 33387.3242\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409495616.0000 - rmse: 20235.9961 - val_loss: 645734848.0000 - val_rmse: 25411.3125\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373375744.0000 - rmse: 19322.9297 - val_loss: 793524032.0000 - val_rmse: 28169.5586\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444493600.0000 - rmse: 21083.0137 - val_loss: 680890752.0000 - val_rmse: 26093.8828\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385330528.0000 - rmse: 19629.8359 - val_loss: 528236608.0000 - val_rmse: 22983.3965\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460588384.0000 - rmse: 21461.3223 - val_loss: 530835328.0000 - val_rmse: 23039.8633\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393519520.0000 - rmse: 19837.3262 - val_loss: 820718528.0000 - val_rmse: 28648.1836\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331002560.0000 - rmse: 18193.4746 - val_loss: 567605568.0000 - val_rmse: 23824.4746\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398436448.0000 - rmse: 19960.8711 - val_loss: 882755328.0000 - val_rmse: 29711.1973\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393363136.0000 - rmse: 19833.3828 - val_loss: 799569280.0000 - val_rmse: 28276.6543\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446470016.0000 - rmse: 21129.8340 - val_loss: 844407360.0000 - val_rmse: 29058.6875\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345769376.0000 - rmse: 18594.8730 - val_loss: 634540928.0000 - val_rmse: 25190.0938\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383485696.0000 - rmse: 19582.7891 - val_loss: 487371808.0000 - val_rmse: 22076.4961\n",
      "Epoch 111/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342629664.0000 - rmse: 18510.2578 - val_loss: 523116640.0000 - val_rmse: 22871.7422\n",
      "Epoch 112/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376878560.0000 - rmse: 19413.3574 - val_loss: 478038048.0000 - val_rmse: 21864.0781\n",
      "Epoch 113/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421842624.0000 - rmse: 20538.8066 - val_loss: 556947776.0000 - val_rmse: 23599.7402\n",
      "Epoch 114/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376868832.0000 - rmse: 19413.1094 - val_loss: 700330112.0000 - val_rmse: 26463.7500\n",
      "Epoch 115/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315307424.0000 - rmse: 17756.8965 - val_loss: 725141952.0000 - val_rmse: 26928.4551\n",
      "Epoch 116/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370675392.0000 - rmse: 19252.9297 - val_loss: 526487040.0000 - val_rmse: 22945.3047\n",
      "Epoch 117/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356915712.0000 - rmse: 18892.2129 - val_loss: 560175040.0000 - val_rmse: 23668.0176\n",
      "Epoch 118/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364580608.0000 - rmse: 19093.9922 - val_loss: 771679936.0000 - val_rmse: 27779.1270\n",
      "Epoch 119/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409703360.0000 - rmse: 20241.1289 - val_loss: 2655827200.0000 - val_rmse: 51534.7148\n",
      "Epoch 120/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335788320.0000 - rmse: 18324.5254 - val_loss: 952674240.0000 - val_rmse: 30865.4180\n",
      "Epoch 121/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396869856.0000 - rmse: 19921.5918 - val_loss: 936169472.0000 - val_rmse: 30596.8828\n",
      "Epoch 122/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310978624.0000 - rmse: 17634.5840 - val_loss: 555184832.0000 - val_rmse: 23562.3613\n",
      "Epoch 123/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322491392.0000 - rmse: 17958.0430 - val_loss: 605235008.0000 - val_rmse: 24601.5215\n",
      "Epoch 124/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323491136.0000 - rmse: 17985.8555 - val_loss: 773612736.0000 - val_rmse: 27813.8926\n",
      "Epoch 125/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355751200.0000 - rmse: 18861.3633 - val_loss: 529294848.0000 - val_rmse: 23006.4062\n",
      "Epoch 126/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341356224.0000 - rmse: 18475.8281 - val_loss: 810697728.0000 - val_rmse: 28472.7500\n",
      "Epoch 127/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365419072.0000 - rmse: 19115.9355 - val_loss: 743396224.0000 - val_rmse: 27265.2930\n",
      "Epoch 128/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350988224.0000 - rmse: 18734.6797 - val_loss: 870773952.0000 - val_rmse: 29508.8770\n",
      "Epoch 129/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365456096.0000 - rmse: 19116.9043 - val_loss: 612787712.0000 - val_rmse: 24754.5488\n",
      "Epoch 130/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370639424.0000 - rmse: 19251.9941 - val_loss: 677892032.0000 - val_rmse: 26036.3594\n",
      "Epoch 131/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404725472.0000 - rmse: 20117.7891 - val_loss: 692379968.0000 - val_rmse: 26313.1113\n",
      "Epoch 132/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314161120.0000 - rmse: 17724.5879 - val_loss: 730158848.0000 - val_rmse: 27021.4512\n",
      "Epoch 133/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368544608.0000 - rmse: 19197.5117 - val_loss: 795199424.0000 - val_rmse: 28199.2773\n",
      "Epoch 134/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332504928.0000 - rmse: 18234.7168 - val_loss: 1848931712.0000 - val_rmse: 42999.2070\n",
      "Epoch 135/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339228992.0000 - rmse: 18418.1660 - val_loss: 630537216.0000 - val_rmse: 25110.4980\n",
      "Epoch 136/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323770208.0000 - rmse: 17993.6133 - val_loss: 1040070784.0000 - val_rmse: 32250.1270\n",
      "Epoch 137/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481763328.0000 - rmse: 21949.1055 - val_loss: 1044650752.0000 - val_rmse: 32321.0547\n",
      "Epoch 138/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327904224.0000 - rmse: 18108.1211 - val_loss: 671486464.0000 - val_rmse: 25913.0527\n",
      "Epoch 139/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312192576.0000 - rmse: 17668.9688 - val_loss: 1159358208.0000 - val_rmse: 34049.3477\n",
      "Epoch 140/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344872992.0000 - rmse: 18570.7539 - val_loss: 861784704.0000 - val_rmse: 29356.1699\n",
      "Epoch 141/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301443904.0000 - rmse: 17362.1367 - val_loss: 535516352.0000 - val_rmse: 23141.2246\n",
      "Epoch 142/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370012768.0000 - rmse: 19235.7148 - val_loss: 828874624.0000 - val_rmse: 28790.1816\n",
      "Epoch 143/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419655744.0000 - rmse: 20485.4980 - val_loss: 691472704.0000 - val_rmse: 26295.8691\n",
      "Epoch 144/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287088000.0000 - rmse: 16943.6680 - val_loss: 1247069440.0000 - val_rmse: 35313.8711\n",
      "Epoch 145/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295107808.0000 - rmse: 17178.6992 - val_loss: 961839808.0000 - val_rmse: 31013.5430\n",
      "Epoch 146/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356195488.0000 - rmse: 18873.1387 - val_loss: 1596372096.0000 - val_rmse: 39954.6250\n",
      "Epoch 147/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332381568.0000 - rmse: 18231.3320 - val_loss: 952781952.0000 - val_rmse: 30867.1660\n",
      "Epoch 148/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284481856.0000 - rmse: 16866.5879 - val_loss: 616287872.0000 - val_rmse: 24825.1445\n",
      "Epoch 149/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331920416.0000 - rmse: 18218.6797 - val_loss: 539607552.0000 - val_rmse: 23229.4512\n",
      "Epoch 150/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306836352.0000 - rmse: 17516.7441 - val_loss: 609202752.0000 - val_rmse: 24682.0312\n",
      "Epoch 151/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290623488.0000 - rmse: 17047.6777 - val_loss: 1391865344.0000 - val_rmse: 37307.7070\n",
      "Epoch 152/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312719552.0000 - rmse: 17683.8750 - val_loss: 1103901440.0000 - val_rmse: 33225.0117\n",
      "Epoch 153/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340378080.0000 - rmse: 18449.3359 - val_loss: 1099882624.0000 - val_rmse: 33164.4766\n",
      "Epoch 154/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281569440.0000 - rmse: 16780.0273 - val_loss: 1041838528.0000 - val_rmse: 32277.5234\n",
      "Epoch 155/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330168352.0000 - rmse: 18170.5352 - val_loss: 887197952.0000 - val_rmse: 29785.8672\n",
      "Epoch 156/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276741856.0000 - rmse: 16635.5586 - val_loss: 1296109184.0000 - val_rmse: 36001.5117\n",
      "Epoch 157/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391444256.0000 - rmse: 19784.9492 - val_loss: 968024320.0000 - val_rmse: 31113.0879\n",
      "Epoch 158/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281157536.0000 - rmse: 16767.7500 - val_loss: 671985728.0000 - val_rmse: 25922.6855\n",
      "Epoch 159/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301354560.0000 - rmse: 17359.5645 - val_loss: 516765792.0000 - val_rmse: 22732.4805\n",
      "Epoch 160/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272247168.0000 - rmse: 16499.9121 - val_loss: 1176772608.0000 - val_rmse: 34304.1172\n",
      "Epoch 161/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353835552.0000 - rmse: 18810.5156 - val_loss: 545049856.0000 - val_rmse: 23346.3008\n",
      "Epoch 162/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274733280.0000 - rmse: 16575.0762 - val_loss: 504550976.0000 - val_rmse: 22462.2109\n",
      "Epoch 163/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368829632.0000 - rmse: 19204.9355 - val_loss: 514691360.0000 - val_rmse: 22686.8086\n",
      "Epoch 164/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262825024.0000 - rmse: 16211.8760 - val_loss: 1110820096.0000 - val_rmse: 33328.9648\n",
      "Epoch 165/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258941280.0000 - rmse: 16091.6484 - val_loss: 746841024.0000 - val_rmse: 27328.3887\n",
      "Epoch 166/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333766688.0000 - rmse: 18269.2812 - val_loss: 453168960.0000 - val_rmse: 21287.7617\n",
      "Epoch 167/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294010656.0000 - rmse: 17146.7363 - val_loss: 624488960.0000 - val_rmse: 24989.7754\n",
      "Epoch 168/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285331776.0000 - rmse: 16891.7617 - val_loss: 524346208.0000 - val_rmse: 22898.6055\n",
      "Epoch 169/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271063200.0000 - rmse: 16463.9941 - val_loss: 694475968.0000 - val_rmse: 26352.9102\n",
      "Epoch 170/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310144576.0000 - rmse: 17610.9199 - val_loss: 686671872.0000 - val_rmse: 26204.4219\n",
      "Epoch 171/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312092096.0000 - rmse: 17666.1250 - val_loss: 440540160.0000 - val_rmse: 20989.0469\n",
      "Epoch 172/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313548928.0000 - rmse: 17707.3105 - val_loss: 448387552.0000 - val_rmse: 21175.1602\n",
      "Epoch 173/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273726624.0000 - rmse: 16544.6836 - val_loss: 441137856.0000 - val_rmse: 21003.2773\n",
      "Epoch 174/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248489472.0000 - rmse: 15763.5459 - val_loss: 694396096.0000 - val_rmse: 26351.3945\n",
      "Epoch 175/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235077536.0000 - rmse: 15332.2373 - val_loss: 646974912.0000 - val_rmse: 25435.7012\n",
      "Epoch 176/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335934912.0000 - rmse: 18328.5254 - val_loss: 418297248.0000 - val_rmse: 20452.3125\n",
      "Epoch 177/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278890208.0000 - rmse: 16700.0039 - val_loss: 818069184.0000 - val_rmse: 28601.9043\n",
      "Epoch 178/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337052896.0000 - rmse: 18358.9980 - val_loss: 770064960.0000 - val_rmse: 27750.0430\n",
      "Epoch 179/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283227072.0000 - rmse: 16829.3496 - val_loss: 760930496.0000 - val_rmse: 27584.9668\n",
      "Epoch 180/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290597216.0000 - rmse: 17046.9082 - val_loss: 928805824.0000 - val_rmse: 30476.3164\n",
      "Epoch 181/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342181888.0000 - rmse: 18498.1562 - val_loss: 476484192.0000 - val_rmse: 21828.5156\n",
      "Epoch 182/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257694096.0000 - rmse: 16052.8477 - val_loss: 529383680.0000 - val_rmse: 23008.3340\n",
      "Epoch 183/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253097296.0000 - rmse: 15909.0293 - val_loss: 997264704.0000 - val_rmse: 31579.4961\n",
      "Epoch 184/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280715392.0000 - rmse: 16754.5625 - val_loss: 1106691072.0000 - val_rmse: 33266.9648\n",
      "Epoch 185/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278896384.0000 - rmse: 16700.1875 - val_loss: 470496736.0000 - val_rmse: 21690.9336\n",
      "Epoch 186/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270008800.0000 - rmse: 16431.9414 - val_loss: 438905472.0000 - val_rmse: 20950.0684\n",
      "Epoch 187/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279667040.0000 - rmse: 16723.2461 - val_loss: 643230272.0000 - val_rmse: 25361.9844\n",
      "Epoch 188/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265320128.0000 - rmse: 16288.6475 - val_loss: 779317248.0000 - val_rmse: 27916.2539\n",
      "Epoch 189/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337878240.0000 - rmse: 18381.4629 - val_loss: 803417728.0000 - val_rmse: 28344.6230\n",
      "Epoch 190/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277210560.0000 - rmse: 16649.6367 - val_loss: 1124631680.0000 - val_rmse: 33535.5234\n",
      "Epoch 191/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304192256.0000 - rmse: 17441.1035 - val_loss: 1202231552.0000 - val_rmse: 34673.2109\n",
      "Epoch 192/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286943328.0000 - rmse: 16939.3984 - val_loss: 865723904.0000 - val_rmse: 29423.1855\n",
      "Epoch 193/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271834240.0000 - rmse: 16487.3906 - val_loss: 664225920.0000 - val_rmse: 25772.5781\n",
      "Epoch 194/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273645344.0000 - rmse: 16542.2266 - val_loss: 562662912.0000 - val_rmse: 23720.5156\n",
      "Epoch 195/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337511200.0000 - rmse: 18371.4766 - val_loss: 604028608.0000 - val_rmse: 24576.9922\n",
      "104/104 [==============================] - 0s 735us/step - loss: 877049792.0000 - rmse: 29615.0234\n",
      "[877049792.0, 29615.0234375]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 5315383296.0000 - rmse: 72906.6719 - val_loss: 1260556160.0000 - val_rmse: 35504.3125\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1719255296.0000 - rmse: 41463.9023 - val_loss: 1022953408.0000 - val_rmse: 31983.6426\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1547340800.0000 - rmse: 39336.2539 - val_loss: 897567360.0000 - val_rmse: 29959.4277\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1453990528.0000 - rmse: 38131.2266 - val_loss: 898500416.0000 - val_rmse: 29974.9961\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1364114560.0000 - rmse: 36933.9219 - val_loss: 829779776.0000 - val_rmse: 28805.8984\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1335072896.0000 - rmse: 36538.6484 - val_loss: 2191940096.0000 - val_rmse: 46818.1602\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1263561984.0000 - rmse: 35546.6172 - val_loss: 931892416.0000 - val_rmse: 30526.9121\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1216654592.0000 - rmse: 34880.5742 - val_loss: 1025615744.0000 - val_rmse: 32025.2363\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1152843392.0000 - rmse: 33953.5469 - val_loss: 866107776.0000 - val_rmse: 29429.7090\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1121530368.0000 - rmse: 33489.2578 - val_loss: 952743552.0000 - val_rmse: 30866.5449\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1183555968.0000 - rmse: 34402.8477 - val_loss: 866163200.0000 - val_rmse: 29430.6504\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1143614720.0000 - rmse: 33817.3750 - val_loss: 936246144.0000 - val_rmse: 30598.1387\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1111483776.0000 - rmse: 33338.9219 - val_loss: 1467603712.0000 - val_rmse: 38309.3164\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1112801536.0000 - rmse: 33358.6797 - val_loss: 871311872.0000 - val_rmse: 29517.9922\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1078555392.0000 - rmse: 32841.3672 - val_loss: 810394944.0000 - val_rmse: 28467.4375\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1094968832.0000 - rmse: 33090.3125 - val_loss: 905921280.0000 - val_rmse: 30098.5254\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1032286144.0000 - rmse: 32129.2109 - val_loss: 1012816384.0000 - val_rmse: 31824.7754\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 940436544.0000 - rmse: 30666.5371 - val_loss: 740669760.0000 - val_rmse: 27215.2480\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 986987584.0000 - rmse: 31416.3594 - val_loss: 678509952.0000 - val_rmse: 26048.2227\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 854385600.0000 - rmse: 29229.8750 - val_loss: 730608576.0000 - val_rmse: 27029.7695\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 925205248.0000 - rmse: 30417.1875 - val_loss: 726669760.0000 - val_rmse: 26956.8125\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 826747200.0000 - rmse: 28753.2109 - val_loss: 742484992.0000 - val_rmse: 27248.5781\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 841591168.0000 - rmse: 29010.1914 - val_loss: 693779072.0000 - val_rmse: 26339.6855\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 874560384.0000 - rmse: 29572.9668 - val_loss: 969231360.0000 - val_rmse: 31132.4805\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782406336.0000 - rmse: 27971.5273 - val_loss: 1121043712.0000 - val_rmse: 33481.9922\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 747913536.0000 - rmse: 27348.0059 - val_loss: 1685077376.0000 - val_rmse: 41049.6953\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 673642112.0000 - rmse: 25954.6172 - val_loss: 1079840256.0000 - val_rmse: 32860.9219\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 883003840.0000 - rmse: 29715.3789 - val_loss: 878825088.0000 - val_rmse: 29644.9844\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 857232000.0000 - rmse: 29278.5254 - val_loss: 3110076672.0000 - val_rmse: 55768.0625\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 761319168.0000 - rmse: 27592.0117 - val_loss: 1973328384.0000 - val_rmse: 44422.1602\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 849454912.0000 - rmse: 29145.4102 - val_loss: 1541770112.0000 - val_rmse: 39265.3789\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680301440.0000 - rmse: 26082.5879 - val_loss: 686292416.0000 - val_rmse: 26197.1816\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 771776384.0000 - rmse: 27780.8633 - val_loss: 851072064.0000 - val_rmse: 29173.1387\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 852597568.0000 - rmse: 29199.2734 - val_loss: 832382656.0000 - val_rmse: 28851.0430\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 841214464.0000 - rmse: 29003.6953 - val_loss: 3095958528.0000 - val_rmse: 55641.3398\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751948480.0000 - rmse: 27421.6797 - val_loss: 1927510528.0000 - val_rmse: 43903.4219\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 764755072.0000 - rmse: 27654.2051 - val_loss: 844141312.0000 - val_rmse: 29054.1074\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 769537408.0000 - rmse: 27740.5371 - val_loss: 1498718080.0000 - val_rmse: 38713.2812\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 636927808.0000 - rmse: 25237.4258 - val_loss: 1328568832.0000 - val_rmse: 36449.5391\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 657880704.0000 - rmse: 25649.1855 - val_loss: 2138247680.0000 - val_rmse: 46241.1914\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 768410560.0000 - rmse: 27720.2188 - val_loss: 1271844736.0000 - val_rmse: 35662.9336\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721238656.0000 - rmse: 26855.8867 - val_loss: 923241344.0000 - val_rmse: 30384.8867\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 727199936.0000 - rmse: 26966.6445 - val_loss: 1933593344.0000 - val_rmse: 43972.6445\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 681578240.0000 - rmse: 26107.0527 - val_loss: 1242733824.0000 - val_rmse: 35252.4297\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 602624192.0000 - rmse: 24548.4043 - val_loss: 3376683264.0000 - val_rmse: 58109.2344\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526727744.0000 - rmse: 22950.5488 - val_loss: 721793024.0000 - val_rmse: 26866.2031\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 635087616.0000 - rmse: 25200.9453 - val_loss: 1207060352.0000 - val_rmse: 34742.7734\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 613559232.0000 - rmse: 24770.1270 - val_loss: 1100727296.0000 - val_rmse: 33177.2070\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544651136.0000 - rmse: 23337.7598 - val_loss: 1142148608.0000 - val_rmse: 33795.6875\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505041312.0000 - rmse: 22473.1250 - val_loss: 712884160.0000 - val_rmse: 26699.8887\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558046080.0000 - rmse: 23622.9980 - val_loss: 1005305664.0000 - val_rmse: 31706.5547\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532429088.0000 - rmse: 23074.4258 - val_loss: 843782720.0000 - val_rmse: 29047.9355\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570830976.0000 - rmse: 23892.0664 - val_loss: 690142528.0000 - val_rmse: 26270.5625\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594812480.0000 - rmse: 24388.7773 - val_loss: 1323974528.0000 - val_rmse: 36386.4609\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597015104.0000 - rmse: 24433.8906 - val_loss: 1027379584.0000 - val_rmse: 32052.7617\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540984448.0000 - rmse: 23259.0723 - val_loss: 679048256.0000 - val_rmse: 26058.5547\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624306176.0000 - rmse: 24986.1172 - val_loss: 801065536.0000 - val_rmse: 28303.1016\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540501184.0000 - rmse: 23248.6816 - val_loss: 907393344.0000 - val_rmse: 30122.9707\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497093216.0000 - rmse: 22295.5879 - val_loss: 1100437888.0000 - val_rmse: 33172.8477\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 735202816.0000 - rmse: 27114.6230 - val_loss: 1231472000.0000 - val_rmse: 35092.3359\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510895104.0000 - rmse: 22602.9883 - val_loss: 1932642688.0000 - val_rmse: 43961.8320\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529495520.0000 - rmse: 23010.7695 - val_loss: 1513490048.0000 - val_rmse: 38903.5977\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584937344.0000 - rmse: 24185.4785 - val_loss: 1088099968.0000 - val_rmse: 32986.3594\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517738240.0000 - rmse: 22753.8613 - val_loss: 1371290240.0000 - val_rmse: 37030.9375\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485251424.0000 - rmse: 22028.4238 - val_loss: 900033280.0000 - val_rmse: 30000.5547\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462196032.0000 - rmse: 21498.7441 - val_loss: 865197760.0000 - val_rmse: 29414.2441\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525668000.0000 - rmse: 22927.4512 - val_loss: 1674692736.0000 - val_rmse: 40923.0117\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568108288.0000 - rmse: 23835.0215 - val_loss: 1050648832.0000 - val_rmse: 32413.7129\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514421952.0000 - rmse: 22680.8691 - val_loss: 1230363392.0000 - val_rmse: 35076.5312\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534307616.0000 - rmse: 23115.0957 - val_loss: 787416896.0000 - val_rmse: 28060.9492\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531631104.0000 - rmse: 23057.1250 - val_loss: 1369319168.0000 - val_rmse: 37004.3125\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515537888.0000 - rmse: 22705.4570 - val_loss: 1299890816.0000 - val_rmse: 36054.0000\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525019776.0000 - rmse: 22913.3105 - val_loss: 1306540544.0000 - val_rmse: 36146.1016\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482819168.0000 - rmse: 21973.1465 - val_loss: 1245220992.0000 - val_rmse: 35287.6875\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556505600.0000 - rmse: 23590.3691 - val_loss: 819241792.0000 - val_rmse: 28622.3984\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448622656.0000 - rmse: 21180.7129 - val_loss: 805273152.0000 - val_rmse: 28377.3340\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464894368.0000 - rmse: 21561.4082 - val_loss: 2245332480.0000 - val_rmse: 47384.9375\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467617536.0000 - rmse: 21624.4648 - val_loss: 1538643456.0000 - val_rmse: 39225.5469\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566545792.0000 - rmse: 23802.2207 - val_loss: 1661352320.0000 - val_rmse: 40759.6914\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451125536.0000 - rmse: 21239.7168 - val_loss: 1911718528.0000 - val_rmse: 43723.2031\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438419008.0000 - rmse: 20938.4570 - val_loss: 918800320.0000 - val_rmse: 30311.7188\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381831168.0000 - rmse: 19540.5000 - val_loss: 797757824.0000 - val_rmse: 28244.6074\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437129312.0000 - rmse: 20907.6348 - val_loss: 1582249472.0000 - val_rmse: 39777.5000\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 608496448.0000 - rmse: 24667.7188 - val_loss: 849175872.0000 - val_rmse: 29140.6230\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444234816.0000 - rmse: 21076.8770 - val_loss: 1637382272.0000 - val_rmse: 40464.5820\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483188736.0000 - rmse: 21981.5527 - val_loss: 1019901120.0000 - val_rmse: 31935.8906\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447401664.0000 - rmse: 21151.8711 - val_loss: 1312694528.0000 - val_rmse: 36231.1250\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484164352.0000 - rmse: 22003.7344 - val_loss: 1265806848.0000 - val_rmse: 35578.1758\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467702816.0000 - rmse: 21626.4375 - val_loss: 1329760384.0000 - val_rmse: 36465.8789\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375130496.0000 - rmse: 19368.2852 - val_loss: 1178753664.0000 - val_rmse: 34332.9805\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424772928.0000 - rmse: 20610.0195 - val_loss: 1657405824.0000 - val_rmse: 40711.2500\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476067296.0000 - rmse: 21818.9648 - val_loss: 2162415616.0000 - val_rmse: 46501.7812\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543774784.0000 - rmse: 23318.9785 - val_loss: 824083392.0000 - val_rmse: 28706.8496\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411392704.0000 - rmse: 20282.8164 - val_loss: 1744839936.0000 - val_rmse: 41771.2812\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388580256.0000 - rmse: 19712.4375 - val_loss: 1047519424.0000 - val_rmse: 32365.4023\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376150528.0000 - rmse: 19394.5996 - val_loss: 2622719488.0000 - val_rmse: 51212.4922\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476419968.0000 - rmse: 21827.0430 - val_loss: 1224447488.0000 - val_rmse: 34992.1055\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407386336.0000 - rmse: 20183.8125 - val_loss: 962837312.0000 - val_rmse: 31029.6191\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389864960.0000 - rmse: 19744.9980 - val_loss: 1763928704.0000 - val_rmse: 41999.1523\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506903968.0000 - rmse: 22514.5254 - val_loss: 1651539840.0000 - val_rmse: 40639.1406\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406743712.0000 - rmse: 20167.8867 - val_loss: 753625472.0000 - val_rmse: 27452.2383\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420071968.0000 - rmse: 20495.6543 - val_loss: 1159004032.0000 - val_rmse: 34044.1484\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436079264.0000 - rmse: 20882.5098 - val_loss: 1007020608.0000 - val_rmse: 31733.5879\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399675744.0000 - rmse: 19991.8906 - val_loss: 2001757312.0000 - val_rmse: 44741.0039\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521360960.0000 - rmse: 22833.3301 - val_loss: 924039616.0000 - val_rmse: 30398.0195\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365293408.0000 - rmse: 19112.6504 - val_loss: 882225728.0000 - val_rmse: 29702.2832\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416010784.0000 - rmse: 20396.3418 - val_loss: 909948032.0000 - val_rmse: 30165.3457\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342704960.0000 - rmse: 18512.2930 - val_loss: 1225272192.0000 - val_rmse: 35003.8867\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478213088.0000 - rmse: 21868.0840 - val_loss: 854210688.0000 - val_rmse: 29226.8789\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472302656.0000 - rmse: 21732.5234 - val_loss: 1475150208.0000 - val_rmse: 38407.6836\n",
      "Epoch 111/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428430368.0000 - rmse: 20698.5586 - val_loss: 691825984.0000 - val_rmse: 26302.5840\n",
      "Epoch 112/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492179264.0000 - rmse: 22185.1113 - val_loss: 911216000.0000 - val_rmse: 30186.3516\n",
      "Epoch 113/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388491488.0000 - rmse: 19710.1855 - val_loss: 1790116352.0000 - val_rmse: 42309.7617\n",
      "Epoch 114/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452651040.0000 - rmse: 21275.5977 - val_loss: 833019648.0000 - val_rmse: 28862.0801\n",
      "Epoch 115/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395308672.0000 - rmse: 19882.3691 - val_loss: 901418048.0000 - val_rmse: 30023.6230\n",
      "Epoch 116/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404251840.0000 - rmse: 20106.0137 - val_loss: 914798848.0000 - val_rmse: 30245.6426\n",
      "Epoch 117/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357621248.0000 - rmse: 18910.8730 - val_loss: 1272036864.0000 - val_rmse: 35665.6250\n",
      "Epoch 118/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301518912.0000 - rmse: 17364.2988 - val_loss: 1337796864.0000 - val_rmse: 36575.9023\n",
      "Epoch 119/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339354656.0000 - rmse: 18421.5801 - val_loss: 839534080.0000 - val_rmse: 28974.7148\n",
      "Epoch 120/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314369920.0000 - rmse: 17730.4785 - val_loss: 702657216.0000 - val_rmse: 26507.6816\n",
      "Epoch 121/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354780576.0000 - rmse: 18835.6191 - val_loss: 873725248.0000 - val_rmse: 29558.8438\n",
      "Epoch 122/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373780224.0000 - rmse: 19333.3965 - val_loss: 1518924800.0000 - val_rmse: 38973.3867\n",
      "Epoch 123/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376867744.0000 - rmse: 19413.0801 - val_loss: 1411664384.0000 - val_rmse: 37572.1211\n",
      "Epoch 124/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411869120.0000 - rmse: 20294.5566 - val_loss: 1887944064.0000 - val_rmse: 43450.4766\n",
      "Epoch 125/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408014592.0000 - rmse: 20199.3711 - val_loss: 1305366656.0000 - val_rmse: 36129.8555\n",
      "Epoch 126/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366933408.0000 - rmse: 19155.5039 - val_loss: 2696581120.0000 - val_rmse: 51928.6172\n",
      "Epoch 127/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349609664.0000 - rmse: 18697.8516 - val_loss: 644732480.0000 - val_rmse: 25391.5820\n",
      "Epoch 128/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376218112.0000 - rmse: 19396.3398 - val_loss: 731796480.0000 - val_rmse: 27051.7363\n",
      "Epoch 129/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403111392.0000 - rmse: 20077.6328 - val_loss: 1140591744.0000 - val_rmse: 33772.6484\n",
      "Epoch 130/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363475520.0000 - rmse: 19065.0332 - val_loss: 937006912.0000 - val_rmse: 30610.5684\n",
      "Epoch 131/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356414240.0000 - rmse: 18878.9355 - val_loss: 893607104.0000 - val_rmse: 29893.2617\n",
      "Epoch 132/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315742528.0000 - rmse: 17769.1445 - val_loss: 1095979264.0000 - val_rmse: 33105.5742\n",
      "Epoch 133/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319757312.0000 - rmse: 17881.7578 - val_loss: 906788800.0000 - val_rmse: 30112.9316\n",
      "Epoch 134/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315527456.0000 - rmse: 17763.0898 - val_loss: 1138596352.0000 - val_rmse: 33743.0938\n",
      "Epoch 135/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332427296.0000 - rmse: 18232.5879 - val_loss: 667869568.0000 - val_rmse: 25843.1719\n",
      "Epoch 136/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343624000.0000 - rmse: 18537.0977 - val_loss: 686672960.0000 - val_rmse: 26204.4434\n",
      "Epoch 137/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320298048.0000 - rmse: 17896.8691 - val_loss: 1514701056.0000 - val_rmse: 38919.1602\n",
      "Epoch 138/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403710688.0000 - rmse: 20092.5527 - val_loss: 1561193472.0000 - val_rmse: 39511.9414\n",
      "104/104 [==============================] - 0s 698us/step - loss: 422049120.0000 - rmse: 20543.8340\n",
      "[422049120.0, 20543.833984375]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 4651596800.0000 - rmse: 68202.6172 - val_loss: 1250460800.0000 - val_rmse: 35361.8555\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1777275520.0000 - rmse: 42157.7461 - val_loss: 988051136.0000 - val_rmse: 31433.2812\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1576856960.0000 - rmse: 39709.6602 - val_loss: 916857408.0000 - val_rmse: 30279.6543\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1438185984.0000 - rmse: 37923.4219 - val_loss: 1074732800.0000 - val_rmse: 32783.1172\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1428034688.0000 - rmse: 37789.3477 - val_loss: 1123903104.0000 - val_rmse: 33524.6641\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1278399488.0000 - rmse: 35754.7109 - val_loss: 912073984.0000 - val_rmse: 30200.5625\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1227064960.0000 - rmse: 35029.4883 - val_loss: 1211990400.0000 - val_rmse: 34813.6523\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1302883200.0000 - rmse: 36095.4727 - val_loss: 947907584.0000 - val_rmse: 30788.1074\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1193320576.0000 - rmse: 34544.4727 - val_loss: 833290432.0000 - val_rmse: 28866.7695\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1184930432.0000 - rmse: 34422.8203 - val_loss: 829118720.0000 - val_rmse: 28794.4219\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1139644928.0000 - rmse: 33758.6289 - val_loss: 765235840.0000 - val_rmse: 27662.8965\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1105730688.0000 - rmse: 33252.5273 - val_loss: 796765056.0000 - val_rmse: 28227.0273\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 995052416.0000 - rmse: 31544.4512 - val_loss: 1077142144.0000 - val_rmse: 32819.8438\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 971518528.0000 - rmse: 31169.1914 - val_loss: 879699904.0000 - val_rmse: 29659.7363\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 998202496.0000 - rmse: 31594.3438 - val_loss: 846926144.0000 - val_rmse: 29101.9961\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 988768128.0000 - rmse: 31444.6836 - val_loss: 1448764672.0000 - val_rmse: 38062.6406\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 967539072.0000 - rmse: 31105.2910 - val_loss: 746191488.0000 - val_rmse: 27316.5059\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1048538048.0000 - rmse: 32381.1367 - val_loss: 732886848.0000 - val_rmse: 27071.8828\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 971289344.0000 - rmse: 31165.5156 - val_loss: 835715456.0000 - val_rmse: 28908.7441\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 884073536.0000 - rmse: 29733.3750 - val_loss: 799017280.0000 - val_rmse: 28266.8926\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 852507968.0000 - rmse: 29197.7383 - val_loss: 757296448.0000 - val_rmse: 27519.0195\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 844188800.0000 - rmse: 29054.9277 - val_loss: 795696000.0000 - val_rmse: 28208.0840\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 826382912.0000 - rmse: 28746.8770 - val_loss: 689060480.0000 - val_rmse: 26249.9609\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 827821120.0000 - rmse: 28771.8789 - val_loss: 644883136.0000 - val_rmse: 25394.5488\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751438720.0000 - rmse: 27412.3828 - val_loss: 1036523072.0000 - val_rmse: 32195.0781\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 884860544.0000 - rmse: 29746.6055 - val_loss: 796242688.0000 - val_rmse: 28217.7715\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 767376192.0000 - rmse: 27701.5566 - val_loss: 890966464.0000 - val_rmse: 29849.0605\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 836640960.0000 - rmse: 28924.7461 - val_loss: 964078720.0000 - val_rmse: 31049.6172\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676501440.0000 - rmse: 26009.6406 - val_loss: 712935808.0000 - val_rmse: 26700.8574\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718552064.0000 - rmse: 26805.8223 - val_loss: 686658368.0000 - val_rmse: 26204.1680\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 722820096.0000 - rmse: 26885.3145 - val_loss: 705745088.0000 - val_rmse: 26565.8613\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729525120.0000 - rmse: 27009.7207 - val_loss: 887163136.0000 - val_rmse: 29785.2832\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 635368448.0000 - rmse: 25206.5156 - val_loss: 854723008.0000 - val_rmse: 29235.6465\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729454208.0000 - rmse: 27008.4082 - val_loss: 892500800.0000 - val_rmse: 29874.7520\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 664290112.0000 - rmse: 25773.8262 - val_loss: 633321728.0000 - val_rmse: 25165.8828\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598060672.0000 - rmse: 24455.2793 - val_loss: 642265088.0000 - val_rmse: 25342.9473\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604033920.0000 - rmse: 24577.1016 - val_loss: 790304320.0000 - val_rmse: 28112.3516\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 674815936.0000 - rmse: 25977.2188 - val_loss: 744253248.0000 - val_rmse: 27281.0059\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584641216.0000 - rmse: 24179.3555 - val_loss: 910137216.0000 - val_rmse: 30168.4805\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 803992320.0000 - rmse: 28354.7578 - val_loss: 687639424.0000 - val_rmse: 26222.8789\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 662526080.0000 - rmse: 25739.5820 - val_loss: 764478784.0000 - val_rmse: 27649.2090\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 696263744.0000 - rmse: 26386.8105 - val_loss: 856118976.0000 - val_rmse: 29259.5117\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575674816.0000 - rmse: 23993.2246 - val_loss: 671929472.0000 - val_rmse: 25921.6016\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706287616.0000 - rmse: 26576.0723 - val_loss: 642607872.0000 - val_rmse: 25349.7109\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621775424.0000 - rmse: 24935.4258 - val_loss: 676565568.0000 - val_rmse: 26010.8750\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604590272.0000 - rmse: 24588.4180 - val_loss: 601899264.0000 - val_rmse: 24533.6348\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557314112.0000 - rmse: 23607.5020 - val_loss: 745891328.0000 - val_rmse: 27311.0117\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594149632.0000 - rmse: 24375.1855 - val_loss: 2347185152.0000 - val_rmse: 48447.7539\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522448736.0000 - rmse: 22857.1348 - val_loss: 723288448.0000 - val_rmse: 26894.0234\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547854848.0000 - rmse: 23406.2988 - val_loss: 845118912.0000 - val_rmse: 29070.9277\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611817408.0000 - rmse: 24734.9414 - val_loss: 689645952.0000 - val_rmse: 26261.1094\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556512128.0000 - rmse: 23590.5078 - val_loss: 768049664.0000 - val_rmse: 27713.7090\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567117056.0000 - rmse: 23814.2188 - val_loss: 1734712960.0000 - val_rmse: 41649.8867\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594928064.0000 - rmse: 24391.1465 - val_loss: 623368320.0000 - val_rmse: 24967.3457\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548142144.0000 - rmse: 23412.4355 - val_loss: 916348416.0000 - val_rmse: 30271.2480\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596632832.0000 - rmse: 24426.0684 - val_loss: 801657984.0000 - val_rmse: 28313.5645\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514565280.0000 - rmse: 22684.0312 - val_loss: 598282944.0000 - val_rmse: 24459.8223\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506335456.0000 - rmse: 22501.8984 - val_loss: 1102407808.0000 - val_rmse: 33202.5273\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550779264.0000 - rmse: 23468.6875 - val_loss: 815945536.0000 - val_rmse: 28564.7598\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530650080.0000 - rmse: 23035.8418 - val_loss: 721611072.0000 - val_rmse: 26862.8203\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525727520.0000 - rmse: 22928.7480 - val_loss: 766891264.0000 - val_rmse: 27692.7988\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524545088.0000 - rmse: 22902.9473 - val_loss: 1033580672.0000 - val_rmse: 32149.3496\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558536576.0000 - rmse: 23633.3789 - val_loss: 1268111488.0000 - val_rmse: 35610.5547\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575967936.0000 - rmse: 23999.3320 - val_loss: 605348928.0000 - val_rmse: 24603.8379\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 528513824.0000 - rmse: 22989.4277 - val_loss: 712240576.0000 - val_rmse: 26687.8359\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514145536.0000 - rmse: 22674.7773 - val_loss: 1186764544.0000 - val_rmse: 34449.4492\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551055744.0000 - rmse: 23474.5762 - val_loss: 1074421760.0000 - val_rmse: 32778.3750\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502247520.0000 - rmse: 22410.8789 - val_loss: 794459648.0000 - val_rmse: 28186.1602\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518172992.0000 - rmse: 22763.4121 - val_loss: 710531392.0000 - val_rmse: 26655.7949\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529686624.0000 - rmse: 23014.9219 - val_loss: 770441088.0000 - val_rmse: 27756.8203\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439828096.0000 - rmse: 20972.0781 - val_loss: 586357696.0000 - val_rmse: 24214.8223\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471568256.0000 - rmse: 21715.6230 - val_loss: 719235904.0000 - val_rmse: 26818.5742\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473246496.0000 - rmse: 21754.2285 - val_loss: 733196160.0000 - val_rmse: 27077.5957\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491782048.0000 - rmse: 22176.1582 - val_loss: 678754432.0000 - val_rmse: 26052.9141\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455110816.0000 - rmse: 21333.3262 - val_loss: 608661632.0000 - val_rmse: 24671.0684\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450098944.0000 - rmse: 21215.5352 - val_loss: 772638208.0000 - val_rmse: 27796.3711\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465817376.0000 - rmse: 21582.8008 - val_loss: 688661376.0000 - val_rmse: 26242.3555\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426263040.0000 - rmse: 20646.1367 - val_loss: 630207232.0000 - val_rmse: 25103.9258\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474684256.0000 - rmse: 21787.2480 - val_loss: 568552064.0000 - val_rmse: 23844.3281\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407045920.0000 - rmse: 20175.3789 - val_loss: 606638656.0000 - val_rmse: 24630.0352\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462166016.0000 - rmse: 21498.0469 - val_loss: 592961216.0000 - val_rmse: 24350.7930\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468443648.0000 - rmse: 21643.5586 - val_loss: 1298764800.0000 - val_rmse: 36038.3789\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438968064.0000 - rmse: 20951.5645 - val_loss: 658221056.0000 - val_rmse: 25655.8184\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461682624.0000 - rmse: 21486.7988 - val_loss: 628504704.0000 - val_rmse: 25069.9961\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491825152.0000 - rmse: 22177.1309 - val_loss: 746461696.0000 - val_rmse: 27321.4512\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472162144.0000 - rmse: 21729.2910 - val_loss: 998757568.0000 - val_rmse: 31603.1250\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401926016.0000 - rmse: 20048.0918 - val_loss: 624081600.0000 - val_rmse: 24981.6230\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421237184.0000 - rmse: 20524.0625 - val_loss: 668007616.0000 - val_rmse: 25845.8438\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471157376.0000 - rmse: 21706.1602 - val_loss: 953632000.0000 - val_rmse: 30880.9297\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451368416.0000 - rmse: 21245.4316 - val_loss: 695531776.0000 - val_rmse: 26372.9355\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414027040.0000 - rmse: 20347.6543 - val_loss: 829235456.0000 - val_rmse: 28796.4492\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464364000.0000 - rmse: 21549.1074 - val_loss: 517668352.0000 - val_rmse: 22752.3242\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427637664.0000 - rmse: 20679.4004 - val_loss: 713872000.0000 - val_rmse: 26718.3828\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455815552.0000 - rmse: 21349.8359 - val_loss: 613584640.0000 - val_rmse: 24770.6406\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389669984.0000 - rmse: 19740.0605 - val_loss: 1119974400.0000 - val_rmse: 33466.0195\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487285184.0000 - rmse: 22074.5352 - val_loss: 769284992.0000 - val_rmse: 27735.9863\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421159808.0000 - rmse: 20522.1777 - val_loss: 707001152.0000 - val_rmse: 26589.4941\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405831232.0000 - rmse: 20145.2520 - val_loss: 661060928.0000 - val_rmse: 25711.1035\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409385920.0000 - rmse: 20233.2871 - val_loss: 651756224.0000 - val_rmse: 25529.5156\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419559232.0000 - rmse: 20483.1445 - val_loss: 675154176.0000 - val_rmse: 25983.7285\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386262496.0000 - rmse: 19653.5625 - val_loss: 691313472.0000 - val_rmse: 26292.8379\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346748000.0000 - rmse: 18621.1699 - val_loss: 1093116672.0000 - val_rmse: 33062.3125\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515288480.0000 - rmse: 22699.9648 - val_loss: 692322816.0000 - val_rmse: 26312.0273\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347741632.0000 - rmse: 18647.8301 - val_loss: 557929920.0000 - val_rmse: 23620.5391\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447497408.0000 - rmse: 21154.1328 - val_loss: 578788544.0000 - val_rmse: 24058.0215\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427574208.0000 - rmse: 20677.8672 - val_loss: 937860096.0000 - val_rmse: 30624.5000\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398496672.0000 - rmse: 19962.3789 - val_loss: 659766656.0000 - val_rmse: 25685.9199\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380395040.0000 - rmse: 19503.7188 - val_loss: 661159296.0000 - val_rmse: 25713.0176\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453122752.0000 - rmse: 21286.6797 - val_loss: 568989504.0000 - val_rmse: 23853.5000\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432219680.0000 - rmse: 20789.8945 - val_loss: 718626240.0000 - val_rmse: 26807.2031\n",
      "Epoch 111/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364506464.0000 - rmse: 19092.0508 - val_loss: 682362432.0000 - val_rmse: 26122.0684\n",
      "Epoch 112/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375693824.0000 - rmse: 19382.8223 - val_loss: 674251584.0000 - val_rmse: 25966.3535\n",
      "Epoch 113/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349821536.0000 - rmse: 18703.5137 - val_loss: 595687104.0000 - val_rmse: 24406.6992\n",
      "Epoch 114/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376052384.0000 - rmse: 19392.0684 - val_loss: 684982528.0000 - val_rmse: 26172.1699\n",
      "Epoch 115/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420394912.0000 - rmse: 20503.5312 - val_loss: 986924160.0000 - val_rmse: 31415.3477\n",
      "Epoch 116/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397144160.0000 - rmse: 19928.4746 - val_loss: 628264704.0000 - val_rmse: 25065.2090\n",
      "Epoch 117/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362718880.0000 - rmse: 19045.1777 - val_loss: 685686784.0000 - val_rmse: 26185.6211\n",
      "Epoch 118/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334711040.0000 - rmse: 18295.1094 - val_loss: 614689600.0000 - val_rmse: 24792.9336\n",
      "Epoch 119/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362984192.0000 - rmse: 19052.1426 - val_loss: 554252224.0000 - val_rmse: 23542.5605\n",
      "Epoch 120/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356047616.0000 - rmse: 18869.2227 - val_loss: 725410624.0000 - val_rmse: 26933.4473\n",
      "Epoch 121/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353520896.0000 - rmse: 18802.1465 - val_loss: 626181952.0000 - val_rmse: 25023.6270\n",
      "Epoch 122/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363199424.0000 - rmse: 19057.7891 - val_loss: 670062144.0000 - val_rmse: 25885.5566\n",
      "Epoch 123/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400857344.0000 - rmse: 20021.4199 - val_loss: 850568320.0000 - val_rmse: 29164.5039\n",
      "Epoch 124/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412340256.0000 - rmse: 20306.1621 - val_loss: 630427840.0000 - val_rmse: 25108.3203\n",
      "Epoch 125/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361630528.0000 - rmse: 19016.5840 - val_loss: 720010944.0000 - val_rmse: 26833.0195\n",
      "Epoch 126/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361041824.0000 - rmse: 19001.0996 - val_loss: 819447360.0000 - val_rmse: 28625.9883\n",
      "Epoch 127/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373342336.0000 - rmse: 19322.0684 - val_loss: 626856896.0000 - val_rmse: 25037.1094\n",
      "Epoch 128/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362203392.0000 - rmse: 19031.6387 - val_loss: 595951872.0000 - val_rmse: 24412.1250\n",
      "Epoch 129/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367083232.0000 - rmse: 19159.4141 - val_loss: 524437440.0000 - val_rmse: 22900.5977\n",
      "Epoch 130/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309440352.0000 - rmse: 17590.9141 - val_loss: 716329600.0000 - val_rmse: 26764.3340\n",
      "Epoch 131/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342840352.0000 - rmse: 18515.9473 - val_loss: 917385536.0000 - val_rmse: 30288.3730\n",
      "Epoch 132/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307964992.0000 - rmse: 17548.9297 - val_loss: 1144312704.0000 - val_rmse: 33827.6914\n",
      "Epoch 133/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342976512.0000 - rmse: 18519.6230 - val_loss: 588451008.0000 - val_rmse: 24258.0098\n",
      "Epoch 134/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343785952.0000 - rmse: 18541.4648 - val_loss: 579172736.0000 - val_rmse: 24066.0059\n",
      "Epoch 135/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384851488.0000 - rmse: 19617.6328 - val_loss: 583378368.0000 - val_rmse: 24153.2266\n",
      "Epoch 136/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347228096.0000 - rmse: 18634.0566 - val_loss: 669801536.0000 - val_rmse: 25880.5215\n",
      "Epoch 137/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365599488.0000 - rmse: 19120.6543 - val_loss: 627344384.0000 - val_rmse: 25046.8418\n",
      "Epoch 138/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382594752.0000 - rmse: 19560.0273 - val_loss: 590503808.0000 - val_rmse: 24300.2812\n",
      "Epoch 139/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329870464.0000 - rmse: 18162.3340 - val_loss: 801960576.0000 - val_rmse: 28318.9082\n",
      "Epoch 140/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357554848.0000 - rmse: 18909.1191 - val_loss: 557519424.0000 - val_rmse: 23611.8457\n",
      "Epoch 141/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355416992.0000 - rmse: 18852.5059 - val_loss: 579156736.0000 - val_rmse: 24065.6719\n",
      "Epoch 142/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332595552.0000 - rmse: 18237.2012 - val_loss: 565872832.0000 - val_rmse: 23788.0801\n",
      "Epoch 143/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329619936.0000 - rmse: 18155.4375 - val_loss: 710568192.0000 - val_rmse: 26656.4844\n",
      "Epoch 144/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332079744.0000 - rmse: 18223.0527 - val_loss: 695926656.0000 - val_rmse: 26380.4199\n",
      "Epoch 145/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340333376.0000 - rmse: 18448.1250 - val_loss: 1008269696.0000 - val_rmse: 31753.2617\n",
      "Epoch 146/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293505440.0000 - rmse: 17131.9980 - val_loss: 789192128.0000 - val_rmse: 28092.5605\n",
      "Epoch 147/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439034240.0000 - rmse: 20953.1426 - val_loss: 670021376.0000 - val_rmse: 25884.7715\n",
      "Epoch 148/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324890048.0000 - rmse: 18024.7031 - val_loss: 724667648.0000 - val_rmse: 26919.6523\n",
      "Epoch 149/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314294592.0000 - rmse: 17728.3535 - val_loss: 603084544.0000 - val_rmse: 24557.7773\n",
      "Epoch 150/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334903968.0000 - rmse: 18300.3789 - val_loss: 631042688.0000 - val_rmse: 25120.5625\n",
      "Epoch 151/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341834208.0000 - rmse: 18488.7578 - val_loss: 501533568.0000 - val_rmse: 22394.9434\n",
      "Epoch 152/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297797408.0000 - rmse: 17256.8066 - val_loss: 572470976.0000 - val_rmse: 23926.3613\n",
      "Epoch 153/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284243520.0000 - rmse: 16859.5195 - val_loss: 539008576.0000 - val_rmse: 23216.5547\n",
      "Epoch 154/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321291840.0000 - rmse: 17924.6152 - val_loss: 638766656.0000 - val_rmse: 25273.8340\n",
      "Epoch 155/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284452128.0000 - rmse: 16865.7051 - val_loss: 601956288.0000 - val_rmse: 24534.7949\n",
      "Epoch 156/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326931552.0000 - rmse: 18081.2461 - val_loss: 708297280.0000 - val_rmse: 26613.8535\n",
      "Epoch 157/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308330016.0000 - rmse: 17559.3262 - val_loss: 581478912.0000 - val_rmse: 24113.8730\n",
      "Epoch 158/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325868928.0000 - rmse: 18051.8398 - val_loss: 623108352.0000 - val_rmse: 24962.1367\n",
      "Epoch 159/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336923584.0000 - rmse: 18355.4766 - val_loss: 683941120.0000 - val_rmse: 26152.2676\n",
      "Epoch 160/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342882720.0000 - rmse: 18517.0898 - val_loss: 658910656.0000 - val_rmse: 25669.2539\n",
      "Epoch 161/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348972000.0000 - rmse: 18680.7910 - val_loss: 610174464.0000 - val_rmse: 24701.7090\n",
      "Epoch 162/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327729984.0000 - rmse: 18103.3125 - val_loss: 581670656.0000 - val_rmse: 24117.8477\n",
      "Epoch 163/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355175808.0000 - rmse: 18846.1074 - val_loss: 624881088.0000 - val_rmse: 24997.6211\n",
      "Epoch 164/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269346912.0000 - rmse: 16411.7891 - val_loss: 607485760.0000 - val_rmse: 24647.2227\n",
      "Epoch 165/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302643616.0000 - rmse: 17396.6523 - val_loss: 609376512.0000 - val_rmse: 24685.5508\n",
      "Epoch 166/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323756960.0000 - rmse: 17993.2461 - val_loss: 726590208.0000 - val_rmse: 26955.3359\n",
      "Epoch 167/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276559264.0000 - rmse: 16630.0684 - val_loss: 585927296.0000 - val_rmse: 24205.9336\n",
      "Epoch 168/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320873664.0000 - rmse: 17912.9434 - val_loss: 619517824.0000 - val_rmse: 24890.1152\n",
      "Epoch 169/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309173888.0000 - rmse: 17583.3379 - val_loss: 613374336.0000 - val_rmse: 24766.3945\n",
      "Epoch 170/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296402528.0000 - rmse: 17216.3438 - val_loss: 607437952.0000 - val_rmse: 24646.2539\n",
      "Epoch 171/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342885728.0000 - rmse: 18517.1719 - val_loss: 616167680.0000 - val_rmse: 24822.7246\n",
      "Epoch 172/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303760608.0000 - rmse: 17428.7266 - val_loss: 809972096.0000 - val_rmse: 28460.0078\n",
      "Epoch 173/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290180128.0000 - rmse: 17034.6738 - val_loss: 697691840.0000 - val_rmse: 26413.8574\n",
      "Epoch 174/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345687776.0000 - rmse: 18592.6777 - val_loss: 840790464.0000 - val_rmse: 28996.3867\n",
      "Epoch 175/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306113536.0000 - rmse: 17496.0996 - val_loss: 580061760.0000 - val_rmse: 24084.4707\n",
      "Epoch 176/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305162368.0000 - rmse: 17468.8945 - val_loss: 742023104.0000 - val_rmse: 27240.1016\n",
      "Epoch 177/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374446208.0000 - rmse: 19350.6094 - val_loss: 704574976.0000 - val_rmse: 26543.8301\n",
      "Epoch 178/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360479488.0000 - rmse: 18986.2949 - val_loss: 1087821440.0000 - val_rmse: 32982.1367\n",
      "Epoch 179/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328442048.0000 - rmse: 18122.9688 - val_loss: 717657664.0000 - val_rmse: 26789.1328\n",
      "Epoch 180/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314113760.0000 - rmse: 17723.2520 - val_loss: 750539264.0000 - val_rmse: 27395.9688\n",
      "Epoch 181/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280775296.0000 - rmse: 16756.3477 - val_loss: 1008061056.0000 - val_rmse: 31749.9766\n",
      "Epoch 182/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329856576.0000 - rmse: 18161.9531 - val_loss: 876270208.0000 - val_rmse: 29601.8594\n",
      "Epoch 183/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273618368.0000 - rmse: 16541.4121 - val_loss: 744464448.0000 - val_rmse: 27284.8750\n",
      "Epoch 184/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321976128.0000 - rmse: 17943.6914 - val_loss: 715377728.0000 - val_rmse: 26746.5469\n",
      "104/104 [==============================] - 0s 683us/step - loss: 311159104.0000 - rmse: 17639.7012\n",
      "[311159104.0, 17639.701171875]\n",
      "[21237.193359375, 26966.962890625, 29615.0234375, 20543.833984375, 17639.701171875]\n",
      "23200.54296875\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "!python train.py kfold baseline\n",
    "#d 0.25 p 25"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 17:56:23.973625: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 17:56:23.973659: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 17:56:23.974074: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 17:56:24.204937: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 7144590848.0000 - rmse: 84525.6797 - val_loss: 1283876608.0000 - val_rmse: 35831.2227\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1831790592.0000 - rmse: 42799.4219 - val_loss: 908755328.0000 - val_rmse: 30145.5684\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1576736640.0000 - rmse: 39708.1445 - val_loss: 789518720.0000 - val_rmse: 28098.3750\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1504629760.0000 - rmse: 38789.5586 - val_loss: 727280640.0000 - val_rmse: 26968.1406\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1476020608.0000 - rmse: 38419.0117 - val_loss: 725643712.0000 - val_rmse: 26937.7754\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1417188352.0000 - rmse: 37645.5625 - val_loss: 761424320.0000 - val_rmse: 27593.9180\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1410202368.0000 - rmse: 37552.6602 - val_loss: 683338688.0000 - val_rmse: 26140.7480\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1344834432.0000 - rmse: 36671.9844 - val_loss: 700219328.0000 - val_rmse: 26461.6582\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1315217408.0000 - rmse: 36265.9258 - val_loss: 685179968.0000 - val_rmse: 26175.9395\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1280325504.0000 - rmse: 35781.6367 - val_loss: 728001280.0000 - val_rmse: 26981.4980\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1208150400.0000 - rmse: 34758.4570 - val_loss: 635470464.0000 - val_rmse: 25208.5391\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1305558272.0000 - rmse: 36132.5117 - val_loss: 623623296.0000 - val_rmse: 24972.4473\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1221379968.0000 - rmse: 34948.2461 - val_loss: 1475504896.0000 - val_rmse: 38412.3008\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1072915264.0000 - rmse: 32755.3848 - val_loss: 594134912.0000 - val_rmse: 24374.8828\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1156011136.0000 - rmse: 34000.1641 - val_loss: 596542144.0000 - val_rmse: 24424.2129\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1037460736.0000 - rmse: 32209.6367 - val_loss: 2133212288.0000 - val_rmse: 46186.7109\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1063036416.0000 - rmse: 32604.2383 - val_loss: 598706688.0000 - val_rmse: 24468.4844\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 995675264.0000 - rmse: 31554.3223 - val_loss: 620870336.0000 - val_rmse: 24917.2695\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 982000640.0000 - rmse: 31336.8867 - val_loss: 560734720.0000 - val_rmse: 23679.8379\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 876770112.0000 - rmse: 29610.3047 - val_loss: 558668672.0000 - val_rmse: 23636.1738\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 967915328.0000 - rmse: 31111.3379 - val_loss: 515108128.0000 - val_rmse: 22695.9941\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 907300608.0000 - rmse: 30121.4316 - val_loss: 631476480.0000 - val_rmse: 25129.1953\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 881341952.0000 - rmse: 29687.4043 - val_loss: 933663424.0000 - val_rmse: 30555.9062\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 932648192.0000 - rmse: 30539.2891 - val_loss: 525110656.0000 - val_rmse: 22915.2930\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 901750464.0000 - rmse: 30029.1582 - val_loss: 548304768.0000 - val_rmse: 23415.9082\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 924519296.0000 - rmse: 30405.9082 - val_loss: 600217216.0000 - val_rmse: 24499.3301\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 776581632.0000 - rmse: 27867.2148 - val_loss: 578638656.0000 - val_rmse: 24054.9102\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 847323456.0000 - rmse: 29108.8203 - val_loss: 507224416.0000 - val_rmse: 22521.6426\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 824150784.0000 - rmse: 28708.0273 - val_loss: 548243456.0000 - val_rmse: 23414.5977\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 851193344.0000 - rmse: 29175.2188 - val_loss: 471599392.0000 - val_rmse: 21716.3398\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721808128.0000 - rmse: 26866.4863 - val_loss: 680675200.0000 - val_rmse: 26089.7520\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 728787008.0000 - rmse: 26996.0547 - val_loss: 623860288.0000 - val_rmse: 24977.1953\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730875264.0000 - rmse: 27034.7031 - val_loss: 1764876288.0000 - val_rmse: 42010.4297\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704375744.0000 - rmse: 26540.0781 - val_loss: 982880512.0000 - val_rmse: 31350.9258\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 745543808.0000 - rmse: 27304.6465 - val_loss: 524353856.0000 - val_rmse: 22898.7715\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 705279360.0000 - rmse: 26557.0957 - val_loss: 929872768.0000 - val_rmse: 30493.8125\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630864256.0000 - rmse: 25117.0098 - val_loss: 1336434560.0000 - val_rmse: 36557.2773\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 664782400.0000 - rmse: 25783.3750 - val_loss: 335961984.0000 - val_rmse: 18329.2637\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 619504192.0000 - rmse: 24889.8418 - val_loss: 475010496.0000 - val_rmse: 21794.7363\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 639209280.0000 - rmse: 25282.5879 - val_loss: 459043904.0000 - val_rmse: 21425.3105\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647660288.0000 - rmse: 25449.1699 - val_loss: 609045504.0000 - val_rmse: 24678.8477\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 595778368.0000 - rmse: 24408.5723 - val_loss: 647400320.0000 - val_rmse: 25444.0625\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 638544384.0000 - rmse: 25269.4355 - val_loss: 703177856.0000 - val_rmse: 26517.5000\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586898560.0000 - rmse: 24225.9863 - val_loss: 605379584.0000 - val_rmse: 24604.4629\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545750080.0000 - rmse: 23361.2949 - val_loss: 588479616.0000 - val_rmse: 24258.5996\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579520768.0000 - rmse: 24073.2383 - val_loss: 1481748736.0000 - val_rmse: 38493.4883\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598493440.0000 - rmse: 24464.1250 - val_loss: 734437952.0000 - val_rmse: 27100.5156\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535871072.0000 - rmse: 23148.8887 - val_loss: 453066592.0000 - val_rmse: 21285.3594\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559823680.0000 - rmse: 23660.5938 - val_loss: 698147136.0000 - val_rmse: 26422.4746\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 663453760.0000 - rmse: 25757.5957 - val_loss: 897819328.0000 - val_rmse: 29963.6328\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542933312.0000 - rmse: 23300.9258 - val_loss: 858115008.0000 - val_rmse: 29293.5977\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585380800.0000 - rmse: 24194.6445 - val_loss: 441579680.0000 - val_rmse: 21013.7969\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580638528.0000 - rmse: 24096.4414 - val_loss: 778893632.0000 - val_rmse: 27908.6660\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543368960.0000 - rmse: 23310.2754 - val_loss: 712910656.0000 - val_rmse: 26700.3848\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541383296.0000 - rmse: 23267.6426 - val_loss: 544615488.0000 - val_rmse: 23336.9980\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584935104.0000 - rmse: 24185.4316 - val_loss: 496168256.0000 - val_rmse: 22274.8340\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511262688.0000 - rmse: 22611.1172 - val_loss: 997347264.0000 - val_rmse: 31580.8047\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467908032.0000 - rmse: 21631.1816 - val_loss: 1610454912.0000 - val_rmse: 40130.4727\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510309568.0000 - rmse: 22590.0312 - val_loss: 482375232.0000 - val_rmse: 21963.0430\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497334368.0000 - rmse: 22300.9922 - val_loss: 589185088.0000 - val_rmse: 24273.1348\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471761920.0000 - rmse: 21720.0801 - val_loss: 1327782272.0000 - val_rmse: 36438.7461\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475721024.0000 - rmse: 21811.0293 - val_loss: 627532800.0000 - val_rmse: 25050.6055\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491406560.0000 - rmse: 22167.6895 - val_loss: 710545344.0000 - val_rmse: 26656.0566\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504413984.0000 - rmse: 22459.1582 - val_loss: 848155712.0000 - val_rmse: 29123.1133\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433000832.0000 - rmse: 20808.6719 - val_loss: 694423488.0000 - val_rmse: 26351.9160\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510168512.0000 - rmse: 22586.9082 - val_loss: 549131456.0000 - val_rmse: 23433.5508\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497019360.0000 - rmse: 22293.9316 - val_loss: 1002905152.0000 - val_rmse: 31668.6777\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458113088.0000 - rmse: 21403.5742 - val_loss: 779312320.0000 - val_rmse: 27916.1641\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539346560.0000 - rmse: 23223.8340 - val_loss: 603343872.0000 - val_rmse: 24563.0586\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506440768.0000 - rmse: 22504.2383 - val_loss: 889265920.0000 - val_rmse: 29820.5625\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473371552.0000 - rmse: 21757.0996 - val_loss: 461017408.0000 - val_rmse: 21471.3145\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473802848.0000 - rmse: 21767.0117 - val_loss: 375349184.0000 - val_rmse: 19373.9297\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517614976.0000 - rmse: 22751.1504 - val_loss: 1191132032.0000 - val_rmse: 34512.7812\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481931264.0000 - rmse: 21952.9316 - val_loss: 656218240.0000 - val_rmse: 25616.7539\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498634144.0000 - rmse: 22330.1152 - val_loss: 445531392.0000 - val_rmse: 21107.6133\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500908288.0000 - rmse: 22380.9805 - val_loss: 671042688.0000 - val_rmse: 25904.4922\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553944128.0000 - rmse: 23536.0156 - val_loss: 510500512.0000 - val_rmse: 22594.2578\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437136032.0000 - rmse: 20907.7969 - val_loss: 746486912.0000 - val_rmse: 27321.9121\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485382368.0000 - rmse: 22031.3945 - val_loss: 634073216.0000 - val_rmse: 25180.8105\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407044704.0000 - rmse: 20175.3496 - val_loss: 646303808.0000 - val_rmse: 25422.5039\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436676992.0000 - rmse: 20896.8164 - val_loss: 654362624.0000 - val_rmse: 25580.5117\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485304416.0000 - rmse: 22029.6230 - val_loss: 367187968.0000 - val_rmse: 19162.1484\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452038528.0000 - rmse: 21261.1973 - val_loss: 465592576.0000 - val_rmse: 21577.5938\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500359200.0000 - rmse: 22368.7090 - val_loss: 1146044160.0000 - val_rmse: 33853.2734\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405032544.0000 - rmse: 20125.4180 - val_loss: 508265280.0000 - val_rmse: 22544.7383\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479316672.0000 - rmse: 21893.3008 - val_loss: 392380384.0000 - val_rmse: 19808.5938\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489681728.0000 - rmse: 22128.7520 - val_loss: 1213484416.0000 - val_rmse: 34835.1016\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583275136.0000 - rmse: 24151.0879 - val_loss: 545512512.0000 - val_rmse: 23356.2090\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413931392.0000 - rmse: 20345.3047 - val_loss: 824479040.0000 - val_rmse: 28713.7402\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451524608.0000 - rmse: 21249.1094 - val_loss: 508235104.0000 - val_rmse: 22544.0684\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461476960.0000 - rmse: 21482.0117 - val_loss: 377421952.0000 - val_rmse: 19427.3496\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478241280.0000 - rmse: 21868.7285 - val_loss: 1766089856.0000 - val_rmse: 42024.8711\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452414560.0000 - rmse: 21270.0391 - val_loss: 478972544.0000 - val_rmse: 21885.4395\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404766624.0000 - rmse: 20118.8105 - val_loss: 485828064.0000 - val_rmse: 22041.5078\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402371488.0000 - rmse: 20059.1973 - val_loss: 704485568.0000 - val_rmse: 26542.1465\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437982656.0000 - rmse: 20928.0352 - val_loss: 632118528.0000 - val_rmse: 25141.9668\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377679392.0000 - rmse: 19433.9746 - val_loss: 490865248.0000 - val_rmse: 22155.4785\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433282336.0000 - rmse: 20815.4336 - val_loss: 399977696.0000 - val_rmse: 19999.4434\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427016512.0000 - rmse: 20664.3770 - val_loss: 570384512.0000 - val_rmse: 23882.7227\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452491424.0000 - rmse: 21271.8438 - val_loss: 559970048.0000 - val_rmse: 23663.6855\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439210848.0000 - rmse: 20957.3555 - val_loss: 1379435520.0000 - val_rmse: 37140.7500\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459024704.0000 - rmse: 21424.8613 - val_loss: 455736544.0000 - val_rmse: 21347.9844\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392705664.0000 - rmse: 19816.8008 - val_loss: 538323712.0000 - val_rmse: 23201.8047\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434246432.0000 - rmse: 20838.5801 - val_loss: 453025856.0000 - val_rmse: 21284.4023\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376140832.0000 - rmse: 19394.3496 - val_loss: 478379616.0000 - val_rmse: 21871.8906\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414812160.0000 - rmse: 20366.9375 - val_loss: 517781312.0000 - val_rmse: 22754.8086\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415398816.0000 - rmse: 20381.3320 - val_loss: 374671680.0000 - val_rmse: 19356.4375\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352350432.0000 - rmse: 18770.9980 - val_loss: 658667776.0000 - val_rmse: 25664.5234\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419997600.0000 - rmse: 20493.8418 - val_loss: 663875904.0000 - val_rmse: 25765.7891\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366606496.0000 - rmse: 19146.9688 - val_loss: 396424416.0000 - val_rmse: 19910.4102\n",
      "Epoch 111/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354093120.0000 - rmse: 18817.3613 - val_loss: 630430976.0000 - val_rmse: 25108.3828\n",
      "Epoch 112/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471561696.0000 - rmse: 21715.4707 - val_loss: 411759648.0000 - val_rmse: 20291.8613\n",
      "Epoch 113/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368968672.0000 - rmse: 19208.5547 - val_loss: 373480160.0000 - val_rmse: 19325.6348\n",
      "Epoch 114/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331137984.0000 - rmse: 18197.1953 - val_loss: 346366144.0000 - val_rmse: 18610.9141\n",
      "Epoch 115/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372768032.0000 - rmse: 19307.2012 - val_loss: 575296512.0000 - val_rmse: 23985.3379\n",
      "Epoch 116/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352136224.0000 - rmse: 18765.2930 - val_loss: 467704640.0000 - val_rmse: 21626.4785\n",
      "Epoch 117/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402153824.0000 - rmse: 20053.7715 - val_loss: 586183744.0000 - val_rmse: 24211.2285\n",
      "Epoch 118/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363286240.0000 - rmse: 19060.0684 - val_loss: 643249024.0000 - val_rmse: 25362.3535\n",
      "Epoch 119/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481238496.0000 - rmse: 21937.1484 - val_loss: 539038592.0000 - val_rmse: 23217.2051\n",
      "Epoch 120/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338493440.0000 - rmse: 18398.1895 - val_loss: 348816224.0000 - val_rmse: 18676.6191\n",
      "Epoch 121/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380503136.0000 - rmse: 19506.4883 - val_loss: 407560544.0000 - val_rmse: 20188.1250\n",
      "Epoch 122/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346959072.0000 - rmse: 18626.8359 - val_loss: 437004256.0000 - val_rmse: 20904.6445\n",
      "Epoch 123/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426104608.0000 - rmse: 20642.2988 - val_loss: 392694336.0000 - val_rmse: 19816.5156\n",
      "Epoch 124/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390436736.0000 - rmse: 19759.4688 - val_loss: 395252640.0000 - val_rmse: 19880.9609\n",
      "Epoch 125/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367269632.0000 - rmse: 19164.2773 - val_loss: 404872576.0000 - val_rmse: 20121.4434\n",
      "Epoch 126/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361342464.0000 - rmse: 19009.0078 - val_loss: 562164736.0000 - val_rmse: 23710.0098\n",
      "Epoch 127/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357209856.0000 - rmse: 18899.9961 - val_loss: 458249696.0000 - val_rmse: 21406.7656\n",
      "Epoch 128/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358331424.0000 - rmse: 18929.6426 - val_loss: 533327616.0000 - val_rmse: 23093.8848\n",
      "Epoch 129/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426422304.0000 - rmse: 20649.9941 - val_loss: 746027392.0000 - val_rmse: 27313.5020\n",
      "Epoch 130/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385344736.0000 - rmse: 19630.1973 - val_loss: 480460480.0000 - val_rmse: 21919.4062\n",
      "Epoch 131/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373007552.0000 - rmse: 19313.4023 - val_loss: 429070240.0000 - val_rmse: 20714.0098\n",
      "Epoch 132/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382429504.0000 - rmse: 19555.8027 - val_loss: 477536032.0000 - val_rmse: 21852.5957\n",
      "Epoch 133/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318962240.0000 - rmse: 17859.5117 - val_loss: 311952960.0000 - val_rmse: 17662.1875\n",
      "Epoch 134/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378172640.0000 - rmse: 19446.6602 - val_loss: 316116128.0000 - val_rmse: 17779.6543\n",
      "Epoch 135/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399962016.0000 - rmse: 19999.0469 - val_loss: 437575264.0000 - val_rmse: 20918.2988\n",
      "Epoch 136/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363124064.0000 - rmse: 19055.8145 - val_loss: 548808960.0000 - val_rmse: 23426.6699\n",
      "Epoch 137/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361672160.0000 - rmse: 19017.6797 - val_loss: 653597952.0000 - val_rmse: 25565.5625\n",
      "Epoch 138/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399316544.0000 - rmse: 19982.9062 - val_loss: 528198080.0000 - val_rmse: 22982.5586\n",
      "Epoch 139/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369220352.0000 - rmse: 19215.1074 - val_loss: 1097749888.0000 - val_rmse: 33132.3086\n",
      "Epoch 140/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347540576.0000 - rmse: 18642.4375 - val_loss: 590475200.0000 - val_rmse: 24299.6934\n",
      "Epoch 141/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366242944.0000 - rmse: 19137.4707 - val_loss: 365751520.0000 - val_rmse: 19124.6289\n",
      "Epoch 142/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416909472.0000 - rmse: 20418.3594 - val_loss: 413148032.0000 - val_rmse: 20326.0410\n",
      "Epoch 143/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335841568.0000 - rmse: 18325.9805 - val_loss: 431804832.0000 - val_rmse: 20779.9121\n",
      "Epoch 144/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381670784.0000 - rmse: 19536.3965 - val_loss: 443278752.0000 - val_rmse: 21054.1855\n",
      "Epoch 145/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404376736.0000 - rmse: 20109.1172 - val_loss: 564192128.0000 - val_rmse: 23752.7266\n",
      "Epoch 146/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358148352.0000 - rmse: 18924.8047 - val_loss: 378660576.0000 - val_rmse: 19459.2012\n",
      "Epoch 147/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361805408.0000 - rmse: 19021.1816 - val_loss: 1417002112.0000 - val_rmse: 37643.0898\n",
      "Epoch 148/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514151296.0000 - rmse: 22674.9023 - val_loss: 364265888.0000 - val_rmse: 19085.7500\n",
      "Epoch 149/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413659840.0000 - rmse: 20338.6270 - val_loss: 417330336.0000 - val_rmse: 20428.6621\n",
      "Epoch 150/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334010816.0000 - rmse: 18275.9609 - val_loss: 529968032.0000 - val_rmse: 23021.0332\n",
      "Epoch 151/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336406304.0000 - rmse: 18341.3789 - val_loss: 503483744.0000 - val_rmse: 22438.4395\n",
      "Epoch 152/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355599968.0000 - rmse: 18857.3574 - val_loss: 479075360.0000 - val_rmse: 21887.7910\n",
      "Epoch 153/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381399904.0000 - rmse: 19529.4629 - val_loss: 377881504.0000 - val_rmse: 19439.1719\n",
      "Epoch 154/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349856416.0000 - rmse: 18704.4473 - val_loss: 464241568.0000 - val_rmse: 21546.2637\n",
      "Epoch 155/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329621504.0000 - rmse: 18155.4785 - val_loss: 448185920.0000 - val_rmse: 21170.3984\n",
      "Epoch 156/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354199680.0000 - rmse: 18820.1914 - val_loss: 517059296.0000 - val_rmse: 22738.9336\n",
      "Epoch 157/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352581184.0000 - rmse: 18777.1426 - val_loss: 372475488.0000 - val_rmse: 19299.6250\n",
      "Epoch 158/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332610752.0000 - rmse: 18237.6172 - val_loss: 383899456.0000 - val_rmse: 19593.3516\n",
      "104/104 [==============================] - 0s 758us/step - loss: 494463168.0000 - rmse: 22236.5254\n",
      "[494463168.0, 22236.525390625]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 5661883392.0000 - rmse: 75245.4844 - val_loss: 1185057920.0000 - val_rmse: 34424.6719\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1570843648.0000 - rmse: 39633.8711 - val_loss: 1000130432.0000 - val_rmse: 31624.8379\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1337530752.0000 - rmse: 36572.2656 - val_loss: 1041778048.0000 - val_rmse: 32276.5859\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1271421184.0000 - rmse: 35656.9922 - val_loss: 1105271552.0000 - val_rmse: 33245.6250\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1196293888.0000 - rmse: 34587.4805 - val_loss: 948920320.0000 - val_rmse: 30804.5508\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1215909760.0000 - rmse: 34869.8984 - val_loss: 834188608.0000 - val_rmse: 28882.3242\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1198312576.0000 - rmse: 34616.6523 - val_loss: 830791872.0000 - val_rmse: 28823.4609\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1076103808.0000 - rmse: 32804.0195 - val_loss: 1070799872.0000 - val_rmse: 32723.0781\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1121415424.0000 - rmse: 33487.5391 - val_loss: 798009408.0000 - val_rmse: 28249.0605\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1027557184.0000 - rmse: 32055.5332 - val_loss: 767421824.0000 - val_rmse: 27702.3789\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1071546176.0000 - rmse: 32734.4805 - val_loss: 785893504.0000 - val_rmse: 28033.7930\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 997929472.0000 - rmse: 31590.0215 - val_loss: 838668672.0000 - val_rmse: 28959.7773\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 926301248.0000 - rmse: 30435.1973 - val_loss: 969531072.0000 - val_rmse: 31137.2930\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 992222080.0000 - rmse: 31499.5566 - val_loss: 736927680.0000 - val_rmse: 27146.4121\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 852954176.0000 - rmse: 29205.3770 - val_loss: 715631488.0000 - val_rmse: 26751.2891\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 931581184.0000 - rmse: 30521.8145 - val_loss: 1270613248.0000 - val_rmse: 35645.6641\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 817193920.0000 - rmse: 28586.6035 - val_loss: 720797568.0000 - val_rmse: 26847.6738\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 921780800.0000 - rmse: 30360.8438 - val_loss: 798283392.0000 - val_rmse: 28253.9102\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 849006080.0000 - rmse: 29137.7090 - val_loss: 659104256.0000 - val_rmse: 25673.0254\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 768202240.0000 - rmse: 27716.4609 - val_loss: 1196390016.0000 - val_rmse: 34588.8711\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 737515712.0000 - rmse: 27157.2402 - val_loss: 611720384.0000 - val_rmse: 24732.9824\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 720358208.0000 - rmse: 26839.4902 - val_loss: 645910464.0000 - val_rmse: 25414.7695\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 732312576.0000 - rmse: 27061.2754 - val_loss: 788870720.0000 - val_rmse: 28086.8418\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 644803968.0000 - rmse: 25392.9902 - val_loss: 608615552.0000 - val_rmse: 24670.1348\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 710630976.0000 - rmse: 26657.6621 - val_loss: 895986304.0000 - val_rmse: 29933.0312\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 688337152.0000 - rmse: 26236.1797 - val_loss: 572294080.0000 - val_rmse: 23922.6680\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684147712.0000 - rmse: 26156.2168 - val_loss: 790267072.0000 - val_rmse: 28111.6895\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 664900352.0000 - rmse: 25785.6621 - val_loss: 648975040.0000 - val_rmse: 25474.9883\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611886720.0000 - rmse: 24736.3438 - val_loss: 611086464.0000 - val_rmse: 24720.1641\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 638921408.0000 - rmse: 25276.8945 - val_loss: 687369408.0000 - val_rmse: 26217.7305\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606920832.0000 - rmse: 24635.7637 - val_loss: 635085824.0000 - val_rmse: 25200.9102\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605109248.0000 - rmse: 24598.9668 - val_loss: 617357376.0000 - val_rmse: 24846.6758\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573143104.0000 - rmse: 23940.4062 - val_loss: 520711200.0000 - val_rmse: 22819.0977\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536411616.0000 - rmse: 23160.5605 - val_loss: 549007040.0000 - val_rmse: 23430.8984\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520874240.0000 - rmse: 22822.6680 - val_loss: 574726208.0000 - val_rmse: 23973.4473\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521352896.0000 - rmse: 22833.1504 - val_loss: 625629184.0000 - val_rmse: 25012.5801\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563987200.0000 - rmse: 23748.4141 - val_loss: 799897216.0000 - val_rmse: 28282.4551\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463727712.0000 - rmse: 21534.3359 - val_loss: 604415488.0000 - val_rmse: 24584.8613\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563098688.0000 - rmse: 23729.6992 - val_loss: 598419840.0000 - val_rmse: 24462.6211\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526372800.0000 - rmse: 22942.8164 - val_loss: 625991104.0000 - val_rmse: 25019.8125\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546862656.0000 - rmse: 23385.0918 - val_loss: 674923072.0000 - val_rmse: 25979.2812\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526170624.0000 - rmse: 22938.4082 - val_loss: 542211200.0000 - val_rmse: 23285.4277\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539877888.0000 - rmse: 23235.2715 - val_loss: 559589056.0000 - val_rmse: 23655.6328\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566709504.0000 - rmse: 23805.6621 - val_loss: 687744064.0000 - val_rmse: 26224.8730\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544675648.0000 - rmse: 23338.2871 - val_loss: 532194432.0000 - val_rmse: 23069.3398\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493299136.0000 - rmse: 22210.3379 - val_loss: 617010240.0000 - val_rmse: 24839.6914\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458366304.0000 - rmse: 21409.4902 - val_loss: 619142720.0000 - val_rmse: 24882.5781\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484043360.0000 - rmse: 22000.9824 - val_loss: 570497600.0000 - val_rmse: 23885.0918\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437420640.0000 - rmse: 20914.6035 - val_loss: 611818880.0000 - val_rmse: 24734.9727\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434130208.0000 - rmse: 20835.7910 - val_loss: 865533504.0000 - val_rmse: 29419.9512\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536979968.0000 - rmse: 23172.8262 - val_loss: 1416824192.0000 - val_rmse: 37640.7266\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502601504.0000 - rmse: 22418.7754 - val_loss: 720319360.0000 - val_rmse: 26838.7656\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451061760.0000 - rmse: 21238.2129 - val_loss: 593956672.0000 - val_rmse: 24371.2266\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499536128.0000 - rmse: 22350.3047 - val_loss: 636726720.0000 - val_rmse: 25233.4453\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430823744.0000 - rmse: 20756.2930 - val_loss: 575946880.0000 - val_rmse: 23998.8926\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480450080.0000 - rmse: 21919.1699 - val_loss: 646206656.0000 - val_rmse: 25420.5938\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443517376.0000 - rmse: 21059.8516 - val_loss: 621171648.0000 - val_rmse: 24923.3145\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468518688.0000 - rmse: 21645.2910 - val_loss: 734134080.0000 - val_rmse: 27094.9062\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386685376.0000 - rmse: 19664.3164 - val_loss: 603024384.0000 - val_rmse: 24556.5527\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387183392.0000 - rmse: 19676.9766 - val_loss: 666446528.0000 - val_rmse: 25815.6250\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462609632.0000 - rmse: 21508.3613 - val_loss: 722905344.0000 - val_rmse: 26886.8984\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463806368.0000 - rmse: 21536.1641 - val_loss: 559404480.0000 - val_rmse: 23651.7324\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441182784.0000 - rmse: 21004.3496 - val_loss: 603941440.0000 - val_rmse: 24575.2188\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385886528.0000 - rmse: 19643.9941 - val_loss: 588932608.0000 - val_rmse: 24267.9316\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442572448.0000 - rmse: 21037.4043 - val_loss: 583677184.0000 - val_rmse: 24159.4102\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429365536.0000 - rmse: 20721.1348 - val_loss: 612489664.0000 - val_rmse: 24748.5273\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399414304.0000 - rmse: 19985.3516 - val_loss: 568463296.0000 - val_rmse: 23842.4668\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412410848.0000 - rmse: 20307.9004 - val_loss: 683596992.0000 - val_rmse: 26145.6875\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404040960.0000 - rmse: 20100.7695 - val_loss: 588056832.0000 - val_rmse: 24249.8809\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386298624.0000 - rmse: 19654.4785 - val_loss: 698033152.0000 - val_rmse: 26420.3164\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410779072.0000 - rmse: 20267.6855 - val_loss: 658190976.0000 - val_rmse: 25655.2305\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421679872.0000 - rmse: 20534.8457 - val_loss: 556271424.0000 - val_rmse: 23585.4062\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435923136.0000 - rmse: 20878.7715 - val_loss: 555498560.0000 - val_rmse: 23569.0176\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394017248.0000 - rmse: 19849.8652 - val_loss: 590970368.0000 - val_rmse: 24309.8809\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421315296.0000 - rmse: 20525.9648 - val_loss: 619263808.0000 - val_rmse: 24885.0098\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383213824.0000 - rmse: 19575.8457 - val_loss: 602327808.0000 - val_rmse: 24542.3672\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366932448.0000 - rmse: 19155.4785 - val_loss: 791872576.0000 - val_rmse: 28140.2305\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365089408.0000 - rmse: 19107.3105 - val_loss: 790934976.0000 - val_rmse: 28123.5645\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396726240.0000 - rmse: 19917.9863 - val_loss: 571505728.0000 - val_rmse: 23906.1855\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376275328.0000 - rmse: 19397.8164 - val_loss: 658562368.0000 - val_rmse: 25662.4668\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370929920.0000 - rmse: 19259.5391 - val_loss: 631457024.0000 - val_rmse: 25128.8066\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396353248.0000 - rmse: 19908.6211 - val_loss: 564452352.0000 - val_rmse: 23758.2051\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352701632.0000 - rmse: 18780.3516 - val_loss: 566480128.0000 - val_rmse: 23800.8438\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383293056.0000 - rmse: 19577.8691 - val_loss: 592994112.0000 - val_rmse: 24351.4688\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355037088.0000 - rmse: 18842.4258 - val_loss: 634090176.0000 - val_rmse: 25181.1465\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391824512.0000 - rmse: 19794.5566 - val_loss: 573518912.0000 - val_rmse: 23948.2559\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356177312.0000 - rmse: 18872.6602 - val_loss: 538065792.0000 - val_rmse: 23196.2441\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358741792.0000 - rmse: 18940.4785 - val_loss: 535232992.0000 - val_rmse: 23135.1035\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402580544.0000 - rmse: 20064.4082 - val_loss: 694625728.0000 - val_rmse: 26355.7520\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361926176.0000 - rmse: 19024.3555 - val_loss: 681728256.0000 - val_rmse: 26109.9258\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434621120.0000 - rmse: 20847.5664 - val_loss: 560622656.0000 - val_rmse: 23677.4707\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367066016.0000 - rmse: 19158.9668 - val_loss: 669922112.0000 - val_rmse: 25882.8535\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368442048.0000 - rmse: 19194.8418 - val_loss: 570790272.0000 - val_rmse: 23891.2168\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384449056.0000 - rmse: 19607.3711 - val_loss: 690447872.0000 - val_rmse: 26276.3730\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357191776.0000 - rmse: 18899.5156 - val_loss: 624944064.0000 - val_rmse: 24998.8809\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369078912.0000 - rmse: 19211.4258 - val_loss: 582459008.0000 - val_rmse: 24134.1875\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318844864.0000 - rmse: 17856.2246 - val_loss: 736266304.0000 - val_rmse: 27134.2266\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358124352.0000 - rmse: 18924.1719 - val_loss: 530258400.0000 - val_rmse: 23027.3398\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336616800.0000 - rmse: 18347.1191 - val_loss: 616437312.0000 - val_rmse: 24828.1562\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369384288.0000 - rmse: 19219.3691 - val_loss: 542874624.0000 - val_rmse: 23299.6680\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365949952.0000 - rmse: 19129.8164 - val_loss: 545472320.0000 - val_rmse: 23355.3477\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321342976.0000 - rmse: 17926.0391 - val_loss: 820932736.0000 - val_rmse: 28651.9238\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330615008.0000 - rmse: 18182.8184 - val_loss: 641201920.0000 - val_rmse: 25321.9648\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320226048.0000 - rmse: 17894.8574 - val_loss: 617160128.0000 - val_rmse: 24842.7070\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414190304.0000 - rmse: 20351.6660 - val_loss: 597895936.0000 - val_rmse: 24451.9102\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305715904.0000 - rmse: 17484.7324 - val_loss: 498384288.0000 - val_rmse: 22324.5215\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340801728.0000 - rmse: 18460.8145 - val_loss: 835141248.0000 - val_rmse: 28898.8086\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386646592.0000 - rmse: 19663.3301 - val_loss: 551469696.0000 - val_rmse: 23483.3887\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340142592.0000 - rmse: 18442.9531 - val_loss: 662609024.0000 - val_rmse: 25741.1914\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358329376.0000 - rmse: 18929.5898 - val_loss: 669402496.0000 - val_rmse: 25872.8125\n",
      "Epoch 111/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362721056.0000 - rmse: 19045.2363 - val_loss: 582738944.0000 - val_rmse: 24139.9844\n",
      "Epoch 112/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309129440.0000 - rmse: 17582.0742 - val_loss: 523881152.0000 - val_rmse: 22888.4492\n",
      "Epoch 113/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375927232.0000 - rmse: 19388.8418 - val_loss: 602467136.0000 - val_rmse: 24545.2051\n",
      "Epoch 114/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351045952.0000 - rmse: 18736.2188 - val_loss: 585851072.0000 - val_rmse: 24204.3574\n",
      "Epoch 115/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344442720.0000 - rmse: 18559.1660 - val_loss: 529989600.0000 - val_rmse: 23021.5020\n",
      "Epoch 116/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339410016.0000 - rmse: 18423.0801 - val_loss: 541678656.0000 - val_rmse: 23273.9902\n",
      "Epoch 117/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345663744.0000 - rmse: 18592.0312 - val_loss: 724307776.0000 - val_rmse: 26912.9648\n",
      "Epoch 118/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365459680.0000 - rmse: 19116.9980 - val_loss: 644324800.0000 - val_rmse: 25383.5547\n",
      "Epoch 119/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386257152.0000 - rmse: 19653.4219 - val_loss: 558345920.0000 - val_rmse: 23629.3438\n",
      "Epoch 120/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344639872.0000 - rmse: 18564.4766 - val_loss: 524181952.0000 - val_rmse: 22895.0176\n",
      "Epoch 121/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347075232.0000 - rmse: 18629.9512 - val_loss: 519868672.0000 - val_rmse: 22800.6270\n",
      "Epoch 122/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331372032.0000 - rmse: 18203.6270 - val_loss: 521911904.0000 - val_rmse: 22845.3906\n",
      "Epoch 123/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344348608.0000 - rmse: 18556.6309 - val_loss: 557237952.0000 - val_rmse: 23605.8887\n",
      "Epoch 124/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348809568.0000 - rmse: 18676.4414 - val_loss: 468372000.0000 - val_rmse: 21641.9023\n",
      "Epoch 125/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311241440.0000 - rmse: 17642.0352 - val_loss: 495985664.0000 - val_rmse: 22270.7344\n",
      "Epoch 126/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306548256.0000 - rmse: 17508.5176 - val_loss: 519325728.0000 - val_rmse: 22788.7168\n",
      "Epoch 127/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333159360.0000 - rmse: 18252.6504 - val_loss: 1059888448.0000 - val_rmse: 32555.9277\n",
      "Epoch 128/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352890016.0000 - rmse: 18785.3652 - val_loss: 491379712.0000 - val_rmse: 22167.0840\n",
      "Epoch 129/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333859136.0000 - rmse: 18271.8105 - val_loss: 868755776.0000 - val_rmse: 29474.6641\n",
      "Epoch 130/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406728256.0000 - rmse: 20167.5039 - val_loss: 524275360.0000 - val_rmse: 22897.0586\n",
      "Epoch 131/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289528384.0000 - rmse: 17015.5312 - val_loss: 658035776.0000 - val_rmse: 25652.2051\n",
      "Epoch 132/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333363520.0000 - rmse: 18258.2422 - val_loss: 466151168.0000 - val_rmse: 21590.5332\n",
      "Epoch 133/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299841024.0000 - rmse: 17315.9160 - val_loss: 507197600.0000 - val_rmse: 22521.0469\n",
      "Epoch 134/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300893568.0000 - rmse: 17346.2812 - val_loss: 583145920.0000 - val_rmse: 24148.4141\n",
      "Epoch 135/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303283040.0000 - rmse: 17415.0195 - val_loss: 478494816.0000 - val_rmse: 21874.5234\n",
      "Epoch 136/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334187552.0000 - rmse: 18280.7949 - val_loss: 451874656.0000 - val_rmse: 21257.3418\n",
      "Epoch 137/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279862592.0000 - rmse: 16729.0938 - val_loss: 479499584.0000 - val_rmse: 21897.4766\n",
      "Epoch 138/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398324320.0000 - rmse: 19958.0605 - val_loss: 492064320.0000 - val_rmse: 22182.5215\n",
      "Epoch 139/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345279168.0000 - rmse: 18581.6875 - val_loss: 459536768.0000 - val_rmse: 21436.8066\n",
      "Epoch 140/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329882368.0000 - rmse: 18162.6621 - val_loss: 514553472.0000 - val_rmse: 22683.7695\n",
      "Epoch 141/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327562400.0000 - rmse: 18098.6836 - val_loss: 450401344.0000 - val_rmse: 21222.6602\n",
      "Epoch 142/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318548608.0000 - rmse: 17847.9258 - val_loss: 471865216.0000 - val_rmse: 21722.4551\n",
      "Epoch 143/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275167392.0000 - rmse: 16588.1680 - val_loss: 737346368.0000 - val_rmse: 27154.1230\n",
      "Epoch 144/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271699200.0000 - rmse: 16483.2969 - val_loss: 762219520.0000 - val_rmse: 27608.3242\n",
      "Epoch 145/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309055712.0000 - rmse: 17579.9785 - val_loss: 487517248.0000 - val_rmse: 22079.7910\n",
      "Epoch 146/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332242144.0000 - rmse: 18227.5078 - val_loss: 531093440.0000 - val_rmse: 23045.4609\n",
      "Epoch 147/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337524736.0000 - rmse: 18371.8457 - val_loss: 549711552.0000 - val_rmse: 23445.9277\n",
      "Epoch 148/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315252768.0000 - rmse: 17755.3535 - val_loss: 1090123904.0000 - val_rmse: 33017.0234\n",
      "Epoch 149/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315096064.0000 - rmse: 17750.9434 - val_loss: 502397536.0000 - val_rmse: 22414.2246\n",
      "Epoch 150/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305463104.0000 - rmse: 17477.5000 - val_loss: 504166944.0000 - val_rmse: 22453.6602\n",
      "Epoch 151/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272838784.0000 - rmse: 16517.8281 - val_loss: 512928288.0000 - val_rmse: 22647.9180\n",
      "Epoch 152/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304340160.0000 - rmse: 17445.3457 - val_loss: 489250560.0000 - val_rmse: 22119.0078\n",
      "Epoch 153/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362088640.0000 - rmse: 19028.6250 - val_loss: 519676128.0000 - val_rmse: 22796.4062\n",
      "Epoch 154/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303503552.0000 - rmse: 17421.3516 - val_loss: 497691296.0000 - val_rmse: 22308.9941\n",
      "Epoch 155/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323080192.0000 - rmse: 17974.4297 - val_loss: 552323968.0000 - val_rmse: 23501.5723\n",
      "Epoch 156/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362412736.0000 - rmse: 19037.1367 - val_loss: 508641312.0000 - val_rmse: 22553.0762\n",
      "Epoch 157/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348023616.0000 - rmse: 18655.3887 - val_loss: 574055552.0000 - val_rmse: 23959.4570\n",
      "Epoch 158/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295397504.0000 - rmse: 17187.1289 - val_loss: 467231232.0000 - val_rmse: 21615.5293\n",
      "Epoch 159/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321782688.0000 - rmse: 17938.2988 - val_loss: 452829856.0000 - val_rmse: 21279.7969\n",
      "Epoch 160/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291671488.0000 - rmse: 17078.3887 - val_loss: 476123168.0000 - val_rmse: 21820.2422\n",
      "Epoch 161/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337010880.0000 - rmse: 18357.8535 - val_loss: 620340160.0000 - val_rmse: 24906.6270\n",
      "Epoch 162/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281376416.0000 - rmse: 16774.2773 - val_loss: 531675424.0000 - val_rmse: 23058.0859\n",
      "Epoch 163/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308204384.0000 - rmse: 17555.7480 - val_loss: 511778752.0000 - val_rmse: 22622.5254\n",
      "Epoch 164/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301759904.0000 - rmse: 17371.2344 - val_loss: 640491072.0000 - val_rmse: 25307.9219\n",
      "Epoch 165/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352444640.0000 - rmse: 18773.5059 - val_loss: 571145408.0000 - val_rmse: 23898.6465\n",
      "Epoch 166/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322301568.0000 - rmse: 17952.7520 - val_loss: 687236736.0000 - val_rmse: 26215.1992\n",
      "Epoch 167/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297609984.0000 - rmse: 17251.3750 - val_loss: 501516480.0000 - val_rmse: 22394.5605\n",
      "Epoch 168/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316665664.0000 - rmse: 17795.0996 - val_loss: 602771712.0000 - val_rmse: 24551.4082\n",
      "Epoch 169/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319078688.0000 - rmse: 17862.7715 - val_loss: 533528320.0000 - val_rmse: 23098.2285\n",
      "104/104 [==============================] - 0s 695us/step - loss: 888013248.0000 - rmse: 29799.5488\n",
      "[888013248.0, 29799.548828125]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 5785091584.0000 - rmse: 76059.7891 - val_loss: 1514159232.0000 - val_rmse: 38912.1992\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1548443520.0000 - rmse: 39350.2656 - val_loss: 1135615744.0000 - val_rmse: 33698.8984\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1279502208.0000 - rmse: 35770.1289 - val_loss: 1016221184.0000 - val_rmse: 31878.2227\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1228775552.0000 - rmse: 35053.8945 - val_loss: 958062080.0000 - val_rmse: 30952.5781\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1111421056.0000 - rmse: 33337.9805 - val_loss: 1035771200.0000 - val_rmse: 32183.4004\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1133389184.0000 - rmse: 33665.8477 - val_loss: 898094272.0000 - val_rmse: 29968.2207\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1086905344.0000 - rmse: 32968.2461 - val_loss: 884661376.0000 - val_rmse: 29743.2578\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1119112576.0000 - rmse: 33453.1406 - val_loss: 922263296.0000 - val_rmse: 30368.7891\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1072079744.0000 - rmse: 32742.6289 - val_loss: 1302097664.0000 - val_rmse: 36084.5898\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1056860608.0000 - rmse: 32509.3926 - val_loss: 901340800.0000 - val_rmse: 30022.3379\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1030989248.0000 - rmse: 32109.0215 - val_loss: 875502592.0000 - val_rmse: 29588.8926\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 955117248.0000 - rmse: 30904.9707 - val_loss: 952718656.0000 - val_rmse: 30866.1406\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 921635264.0000 - rmse: 30358.4473 - val_loss: 865941504.0000 - val_rmse: 29426.8848\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 943472384.0000 - rmse: 30715.9961 - val_loss: 815609344.0000 - val_rmse: 28558.8750\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 917703744.0000 - rmse: 30293.6250 - val_loss: 870429184.0000 - val_rmse: 29503.0371\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800078720.0000 - rmse: 28285.6621 - val_loss: 940960896.0000 - val_rmse: 30675.0859\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 845445632.0000 - rmse: 29076.5449 - val_loss: 1130511744.0000 - val_rmse: 33623.0820\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782524160.0000 - rmse: 27973.6309 - val_loss: 953183936.0000 - val_rmse: 30873.6777\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 867208704.0000 - rmse: 29448.4082 - val_loss: 921408384.0000 - val_rmse: 30354.7090\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 784859456.0000 - rmse: 28015.3438 - val_loss: 1034150080.0000 - val_rmse: 32158.2031\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 803959104.0000 - rmse: 28354.1719 - val_loss: 957908544.0000 - val_rmse: 30950.0977\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 778562176.0000 - rmse: 27902.7266 - val_loss: 956032448.0000 - val_rmse: 30919.7734\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 889334272.0000 - rmse: 29821.7090 - val_loss: 799429504.0000 - val_rmse: 28274.1836\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 681765952.0000 - rmse: 26110.6484 - val_loss: 856084224.0000 - val_rmse: 29258.9180\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 773425280.0000 - rmse: 27810.5234 - val_loss: 794373312.0000 - val_rmse: 28184.6289\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 790211328.0000 - rmse: 28110.6973 - val_loss: 825307968.0000 - val_rmse: 28728.1738\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 760019840.0000 - rmse: 27568.4570 - val_loss: 788268672.0000 - val_rmse: 28076.1211\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 710580992.0000 - rmse: 26656.7227 - val_loss: 1592792064.0000 - val_rmse: 39909.8008\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 717821312.0000 - rmse: 26792.1875 - val_loss: 769403392.0000 - val_rmse: 27738.1211\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 743200000.0000 - rmse: 27261.6953 - val_loss: 715058240.0000 - val_rmse: 26740.5723\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647902272.0000 - rmse: 25453.9238 - val_loss: 819069888.0000 - val_rmse: 28619.3965\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 667545472.0000 - rmse: 25836.9004 - val_loss: 522406784.0000 - val_rmse: 22856.2207\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 633629888.0000 - rmse: 25172.0039 - val_loss: 1297001984.0000 - val_rmse: 36013.9141\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 731080832.0000 - rmse: 27038.5059 - val_loss: 496431264.0000 - val_rmse: 22280.7363\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622818560.0000 - rmse: 24956.3340 - val_loss: 464084672.0000 - val_rmse: 21542.6230\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614900096.0000 - rmse: 24797.1797 - val_loss: 1014281024.0000 - val_rmse: 31847.7793\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634381888.0000 - rmse: 25186.9375 - val_loss: 643640768.0000 - val_rmse: 25370.0762\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706718336.0000 - rmse: 26584.1738 - val_loss: 605220736.0000 - val_rmse: 24601.2324\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574202560.0000 - rmse: 23962.5234 - val_loss: 479726240.0000 - val_rmse: 21902.6523\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692737280.0000 - rmse: 26319.9023 - val_loss: 956779392.0000 - val_rmse: 30931.8516\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583776384.0000 - rmse: 24161.4648 - val_loss: 830149504.0000 - val_rmse: 28812.3145\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 608855296.0000 - rmse: 24674.9941 - val_loss: 508467840.0000 - val_rmse: 22549.2324\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556381056.0000 - rmse: 23587.7305 - val_loss: 592304192.0000 - val_rmse: 24337.3008\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550877568.0000 - rmse: 23470.7793 - val_loss: 506176352.0000 - val_rmse: 22498.3633\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497718464.0000 - rmse: 22309.6055 - val_loss: 552750720.0000 - val_rmse: 23510.6484\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562130752.0000 - rmse: 23709.2949 - val_loss: 828011328.0000 - val_rmse: 28775.1855\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545973824.0000 - rmse: 23366.0820 - val_loss: 754167936.0000 - val_rmse: 27462.1172\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560297408.0000 - rmse: 23670.6016 - val_loss: 1175251328.0000 - val_rmse: 34281.9375\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567857728.0000 - rmse: 23829.7637 - val_loss: 935013248.0000 - val_rmse: 30577.9863\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495744096.0000 - rmse: 22265.3105 - val_loss: 554379648.0000 - val_rmse: 23545.2676\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431145088.0000 - rmse: 20764.0332 - val_loss: 716649664.0000 - val_rmse: 26770.3125\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507255104.0000 - rmse: 22522.3242 - val_loss: 600309120.0000 - val_rmse: 24501.2070\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487947328.0000 - rmse: 22089.5293 - val_loss: 423113344.0000 - val_rmse: 20569.7188\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510175648.0000 - rmse: 22587.0684 - val_loss: 926102912.0000 - val_rmse: 30431.9395\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517689312.0000 - rmse: 22752.7852 - val_loss: 492123904.0000 - val_rmse: 22183.8652\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426968512.0000 - rmse: 20663.2148 - val_loss: 834365056.0000 - val_rmse: 28885.3750\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450647616.0000 - rmse: 21228.4609 - val_loss: 599179264.0000 - val_rmse: 24478.1367\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479084352.0000 - rmse: 21887.9961 - val_loss: 490463680.0000 - val_rmse: 22146.4121\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406249216.0000 - rmse: 20155.6230 - val_loss: 383815392.0000 - val_rmse: 19591.2070\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487901600.0000 - rmse: 22088.4941 - val_loss: 401528128.0000 - val_rmse: 20038.1660\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555219456.0000 - rmse: 23563.0957 - val_loss: 376418176.0000 - val_rmse: 19401.5000\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445037184.0000 - rmse: 21095.9043 - val_loss: 366802880.0000 - val_rmse: 19152.0977\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441197344.0000 - rmse: 21004.6973 - val_loss: 895800832.0000 - val_rmse: 29929.9316\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496374816.0000 - rmse: 22279.4707 - val_loss: 364630368.0000 - val_rmse: 19095.2969\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438220064.0000 - rmse: 20933.7051 - val_loss: 534456320.0000 - val_rmse: 23118.3105\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428233056.0000 - rmse: 20693.7910 - val_loss: 409424992.0000 - val_rmse: 20234.2520\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466551872.0000 - rmse: 21599.8125 - val_loss: 441234400.0000 - val_rmse: 21005.5801\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462471392.0000 - rmse: 21505.1465 - val_loss: 446515296.0000 - val_rmse: 21130.9082\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451568768.0000 - rmse: 21250.1484 - val_loss: 410474880.0000 - val_rmse: 20260.1797\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510095296.0000 - rmse: 22585.2891 - val_loss: 370915264.0000 - val_rmse: 19259.1582\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372887104.0000 - rmse: 19310.2832 - val_loss: 387569440.0000 - val_rmse: 19686.7812\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438204992.0000 - rmse: 20933.3457 - val_loss: 388574272.0000 - val_rmse: 19712.2852\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443038880.0000 - rmse: 21048.4863 - val_loss: 448626624.0000 - val_rmse: 21180.8066\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461679328.0000 - rmse: 21486.7227 - val_loss: 439825408.0000 - val_rmse: 20972.0156\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451723840.0000 - rmse: 21253.7949 - val_loss: 447190368.0000 - val_rmse: 21146.8770\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422879648.0000 - rmse: 20564.0371 - val_loss: 545763648.0000 - val_rmse: 23361.5840\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368628800.0000 - rmse: 19199.7090 - val_loss: 572067200.0000 - val_rmse: 23917.9258\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472834144.0000 - rmse: 21744.7500 - val_loss: 468230400.0000 - val_rmse: 21638.6309\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380223552.0000 - rmse: 19499.3203 - val_loss: 605035776.0000 - val_rmse: 24597.4746\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437654752.0000 - rmse: 20920.1992 - val_loss: 455179232.0000 - val_rmse: 21334.9277\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407166368.0000 - rmse: 20178.3633 - val_loss: 605986048.0000 - val_rmse: 24616.7832\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340542720.0000 - rmse: 18453.7988 - val_loss: 433423712.0000 - val_rmse: 20818.8301\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435289856.0000 - rmse: 20863.6016 - val_loss: 466914112.0000 - val_rmse: 21608.1934\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359384576.0000 - rmse: 18957.4395 - val_loss: 625198464.0000 - val_rmse: 25003.9668\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452268128.0000 - rmse: 21266.5957 - val_loss: 598699840.0000 - val_rmse: 24468.3438\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359006880.0000 - rmse: 18947.4746 - val_loss: 505566912.0000 - val_rmse: 22484.8125\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468507328.0000 - rmse: 21645.0273 - val_loss: 519587040.0000 - val_rmse: 22794.4512\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393421216.0000 - rmse: 19834.8477 - val_loss: 557995264.0000 - val_rmse: 23621.9219\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370009952.0000 - rmse: 19235.6426 - val_loss: 573106944.0000 - val_rmse: 23939.6504\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386857632.0000 - rmse: 19668.6953 - val_loss: 580773760.0000 - val_rmse: 24099.2480\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359073184.0000 - rmse: 18949.2246 - val_loss: 436684704.0000 - val_rmse: 20897.0020\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347489568.0000 - rmse: 18641.0684 - val_loss: 480237184.0000 - val_rmse: 21914.3145\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354426176.0000 - rmse: 18826.2070 - val_loss: 655961280.0000 - val_rmse: 25611.7402\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472553088.0000 - rmse: 21738.2852 - val_loss: 524139936.0000 - val_rmse: 22894.0996\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521488672.0000 - rmse: 22836.1250 - val_loss: 392744800.0000 - val_rmse: 19817.7891\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339294304.0000 - rmse: 18419.9414 - val_loss: 402748096.0000 - val_rmse: 20068.5820\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394502560.0000 - rmse: 19862.0859 - val_loss: 388640928.0000 - val_rmse: 19713.9766\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384934080.0000 - rmse: 19619.7363 - val_loss: 594177280.0000 - val_rmse: 24375.7520\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345030560.0000 - rmse: 18574.9961 - val_loss: 459554464.0000 - val_rmse: 21437.2207\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456828992.0000 - rmse: 21373.5586 - val_loss: 490787520.0000 - val_rmse: 22153.7227\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348758528.0000 - rmse: 18675.0781 - val_loss: 648095488.0000 - val_rmse: 25457.7188\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306692896.0000 - rmse: 17512.6484 - val_loss: 430242592.0000 - val_rmse: 20742.2891\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354599872.0000 - rmse: 18830.8223 - val_loss: 421591520.0000 - val_rmse: 20532.6934\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371036576.0000 - rmse: 19262.3086 - val_loss: 992137600.0000 - val_rmse: 31498.2148\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337959552.0000 - rmse: 18383.6738 - val_loss: 464864352.0000 - val_rmse: 21560.7109\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339196640.0000 - rmse: 18417.2910 - val_loss: 625065728.0000 - val_rmse: 25001.3145\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330526432.0000 - rmse: 18180.3828 - val_loss: 444596928.0000 - val_rmse: 21085.4648\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342423936.0000 - rmse: 18504.6973 - val_loss: 385496416.0000 - val_rmse: 19634.0605\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357778560.0000 - rmse: 18915.0352 - val_loss: 369806176.0000 - val_rmse: 19230.3418\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321400768.0000 - rmse: 17927.6523 - val_loss: 385738336.0000 - val_rmse: 19640.2207\n",
      "Epoch 111/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375727552.0000 - rmse: 19383.6914 - val_loss: 517962784.0000 - val_rmse: 22758.7930\n",
      "Epoch 112/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320418496.0000 - rmse: 17900.2363 - val_loss: 505948992.0000 - val_rmse: 22493.3105\n",
      "Epoch 113/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333721472.0000 - rmse: 18268.0449 - val_loss: 382485344.0000 - val_rmse: 19557.2324\n",
      "Epoch 114/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350807456.0000 - rmse: 18729.8535 - val_loss: 540916352.0000 - val_rmse: 23257.6074\n",
      "Epoch 115/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301872672.0000 - rmse: 17374.4824 - val_loss: 452624192.0000 - val_rmse: 21274.9648\n",
      "Epoch 116/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385889856.0000 - rmse: 19644.0781 - val_loss: 385430112.0000 - val_rmse: 19632.3750\n",
      "Epoch 117/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330072544.0000 - rmse: 18167.8965 - val_loss: 434911008.0000 - val_rmse: 20854.5176\n",
      "Epoch 118/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320989184.0000 - rmse: 17916.1680 - val_loss: 387197408.0000 - val_rmse: 19677.3301\n",
      "Epoch 119/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385104192.0000 - rmse: 19624.0703 - val_loss: 474221600.0000 - val_rmse: 21776.6270\n",
      "Epoch 120/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299559840.0000 - rmse: 17307.7949 - val_loss: 648565312.0000 - val_rmse: 25466.9414\n",
      "Epoch 121/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305120608.0000 - rmse: 17467.6992 - val_loss: 541709312.0000 - val_rmse: 23274.6484\n",
      "Epoch 122/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381675776.0000 - rmse: 19536.5234 - val_loss: 673589568.0000 - val_rmse: 25953.6035\n",
      "Epoch 123/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342756192.0000 - rmse: 18513.6719 - val_loss: 486188512.0000 - val_rmse: 22049.6816\n",
      "Epoch 124/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330064096.0000 - rmse: 18167.6621 - val_loss: 470767776.0000 - val_rmse: 21697.1836\n",
      "Epoch 125/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307139968.0000 - rmse: 17525.4082 - val_loss: 444932800.0000 - val_rmse: 21093.4277\n",
      "Epoch 126/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310107648.0000 - rmse: 17609.8711 - val_loss: 477873024.0000 - val_rmse: 21860.3047\n",
      "Epoch 127/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383239360.0000 - rmse: 19576.4980 - val_loss: 407960352.0000 - val_rmse: 20198.0273\n",
      "Epoch 128/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338140192.0000 - rmse: 18388.5879 - val_loss: 484618016.0000 - val_rmse: 22014.0391\n",
      "Epoch 129/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301626944.0000 - rmse: 17367.4102 - val_loss: 409160064.0000 - val_rmse: 20227.7031\n",
      "Epoch 130/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338226304.0000 - rmse: 18390.9297 - val_loss: 551916928.0000 - val_rmse: 23492.9102\n",
      "Epoch 131/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339351840.0000 - rmse: 18421.5039 - val_loss: 431876576.0000 - val_rmse: 20781.6387\n",
      "Epoch 132/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345880576.0000 - rmse: 18597.8613 - val_loss: 990094976.0000 - val_rmse: 31465.7734\n",
      "Epoch 133/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315735296.0000 - rmse: 17768.9395 - val_loss: 414329440.0000 - val_rmse: 20355.0820\n",
      "Epoch 134/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325390848.0000 - rmse: 18038.5898 - val_loss: 400740640.0000 - val_rmse: 20018.5059\n",
      "Epoch 135/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326715968.0000 - rmse: 18075.2852 - val_loss: 510438560.0000 - val_rmse: 22592.8848\n",
      "Epoch 136/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314388000.0000 - rmse: 17730.9844 - val_loss: 670491456.0000 - val_rmse: 25893.8477\n",
      "Epoch 137/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315583808.0000 - rmse: 17764.6758 - val_loss: 461093216.0000 - val_rmse: 21473.0781\n",
      "Epoch 138/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309206624.0000 - rmse: 17584.2715 - val_loss: 461452416.0000 - val_rmse: 21481.4434\n",
      "Epoch 139/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304894720.0000 - rmse: 17461.2324 - val_loss: 443955808.0000 - val_rmse: 21070.2559\n",
      "Epoch 140/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306724544.0000 - rmse: 17513.5508 - val_loss: 475876960.0000 - val_rmse: 21814.6016\n",
      "Epoch 141/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255157968.0000 - rmse: 15973.6602 - val_loss: 435559008.0000 - val_rmse: 20870.0488\n",
      "Epoch 142/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275653856.0000 - rmse: 16602.8242 - val_loss: 431960000.0000 - val_rmse: 20783.6445\n",
      "Epoch 143/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299574208.0000 - rmse: 17308.2109 - val_loss: 433429280.0000 - val_rmse: 20818.9648\n",
      "Epoch 144/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280683456.0000 - rmse: 16753.6094 - val_loss: 457827008.0000 - val_rmse: 21396.8887\n",
      "Epoch 145/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294326272.0000 - rmse: 17155.9395 - val_loss: 501661856.0000 - val_rmse: 22397.8086\n",
      "Epoch 146/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318084320.0000 - rmse: 17834.9160 - val_loss: 575403712.0000 - val_rmse: 23987.5723\n",
      "Epoch 147/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286213152.0000 - rmse: 16917.8320 - val_loss: 544201152.0000 - val_rmse: 23328.1172\n",
      "Epoch 148/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302237344.0000 - rmse: 17384.9727 - val_loss: 495212320.0000 - val_rmse: 22253.3652\n",
      "Epoch 149/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337243104.0000 - rmse: 18364.1777 - val_loss: 437280000.0000 - val_rmse: 20911.2402\n",
      "Epoch 150/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276081248.0000 - rmse: 16615.6895 - val_loss: 437953824.0000 - val_rmse: 20927.3457\n",
      "Epoch 151/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312085248.0000 - rmse: 17665.9336 - val_loss: 427854944.0000 - val_rmse: 20684.6523\n",
      "Epoch 152/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336059104.0000 - rmse: 18331.9141 - val_loss: 502226976.0000 - val_rmse: 22410.4199\n",
      "Epoch 153/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282350752.0000 - rmse: 16803.2930 - val_loss: 708312640.0000 - val_rmse: 26614.1426\n",
      "Epoch 154/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331611456.0000 - rmse: 18210.2012 - val_loss: 399256224.0000 - val_rmse: 19981.3945\n",
      "Epoch 155/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291288096.0000 - rmse: 17067.1621 - val_loss: 512296416.0000 - val_rmse: 22633.9629\n",
      "Epoch 156/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352879712.0000 - rmse: 18785.0898 - val_loss: 512856352.0000 - val_rmse: 22646.3281\n",
      "Epoch 157/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302358848.0000 - rmse: 17388.4648 - val_loss: 503325952.0000 - val_rmse: 22434.9258\n",
      "Epoch 158/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298364096.0000 - rmse: 17273.2168 - val_loss: 452112416.0000 - val_rmse: 21262.9316\n",
      "Epoch 159/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281822688.0000 - rmse: 16787.5723 - val_loss: 524108640.0000 - val_rmse: 22893.4180\n",
      "Epoch 160/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292407328.0000 - rmse: 17099.9199 - val_loss: 469575136.0000 - val_rmse: 21669.6816\n",
      "Epoch 161/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260459824.0000 - rmse: 16138.7656 - val_loss: 477409312.0000 - val_rmse: 21849.6953\n",
      "Epoch 162/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330398080.0000 - rmse: 18176.8516 - val_loss: 827409344.0000 - val_rmse: 28764.7246\n",
      "Epoch 163/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282641024.0000 - rmse: 16811.9297 - val_loss: 411346208.0000 - val_rmse: 20281.6699\n",
      "Epoch 164/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357535296.0000 - rmse: 18908.5996 - val_loss: 425482432.0000 - val_rmse: 20627.2246\n",
      "Epoch 165/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281777792.0000 - rmse: 16786.2344 - val_loss: 427768256.0000 - val_rmse: 20682.5547\n",
      "Epoch 166/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275469184.0000 - rmse: 16597.2598 - val_loss: 419003872.0000 - val_rmse: 20469.5840\n",
      "104/104 [==============================] - 0s 693us/step - loss: 763985344.0000 - rmse: 27640.2832\n",
      "[763985344.0, 27640.283203125]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 7504805376.0000 - rmse: 86630.2812 - val_loss: 1381328128.0000 - val_rmse: 37166.2227\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1799491200.0000 - rmse: 42420.4102 - val_loss: 1098427648.0000 - val_rmse: 33142.5352\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1596323200.0000 - rmse: 39954.0117 - val_loss: 1137769728.0000 - val_rmse: 33730.8438\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1466503552.0000 - rmse: 38294.9531 - val_loss: 992388800.0000 - val_rmse: 31502.2031\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1377689344.0000 - rmse: 37117.2383 - val_loss: 869488896.0000 - val_rmse: 29487.0977\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1382568704.0000 - rmse: 37182.9102 - val_loss: 878154432.0000 - val_rmse: 29633.6699\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1294788736.0000 - rmse: 35983.1719 - val_loss: 868215680.0000 - val_rmse: 29465.5000\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1256675712.0000 - rmse: 35449.6211 - val_loss: 943624768.0000 - val_rmse: 30718.4766\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1222130560.0000 - rmse: 34958.9844 - val_loss: 789607616.0000 - val_rmse: 28099.9570\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1169287296.0000 - rmse: 34194.8438 - val_loss: 786690496.0000 - val_rmse: 28048.0039\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1162047232.0000 - rmse: 34088.8125 - val_loss: 842632832.0000 - val_rmse: 29028.1367\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1092532736.0000 - rmse: 33053.4844 - val_loss: 881233920.0000 - val_rmse: 29685.5840\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1057828160.0000 - rmse: 32524.2695 - val_loss: 1575803776.0000 - val_rmse: 39696.3945\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1084699904.0000 - rmse: 32934.7812 - val_loss: 1066371904.0000 - val_rmse: 32655.3477\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 990276032.0000 - rmse: 31468.6504 - val_loss: 1004326208.0000 - val_rmse: 31691.1055\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 945826816.0000 - rmse: 30754.2969 - val_loss: 1444348928.0000 - val_rmse: 38004.5898\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 924173888.0000 - rmse: 30400.2266 - val_loss: 847970368.0000 - val_rmse: 29119.9316\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 910889472.0000 - rmse: 30180.9453 - val_loss: 732204352.0000 - val_rmse: 27059.2754\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 871693888.0000 - rmse: 29524.4629 - val_loss: 913742528.0000 - val_rmse: 30228.1738\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 892924992.0000 - rmse: 29881.8496 - val_loss: 1123131264.0000 - val_rmse: 33513.1523\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 872671040.0000 - rmse: 29541.0059 - val_loss: 777725952.0000 - val_rmse: 27887.7383\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 844624384.0000 - rmse: 29062.4180 - val_loss: 1392281728.0000 - val_rmse: 37313.2930\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 868974784.0000 - rmse: 29478.3789 - val_loss: 815544448.0000 - val_rmse: 28557.7383\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 741592192.0000 - rmse: 27232.1875 - val_loss: 1172855552.0000 - val_rmse: 34246.9805\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 842325888.0000 - rmse: 29022.8516 - val_loss: 822665152.0000 - val_rmse: 28682.1387\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 840126016.0000 - rmse: 28984.9277 - val_loss: 811515968.0000 - val_rmse: 28487.1191\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 765494784.0000 - rmse: 27667.5762 - val_loss: 824019776.0000 - val_rmse: 28705.7441\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 770973952.0000 - rmse: 27766.4180 - val_loss: 971317248.0000 - val_rmse: 31165.9629\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 836571520.0000 - rmse: 28923.5469 - val_loss: 796118400.0000 - val_rmse: 28215.5703\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 705893888.0000 - rmse: 26568.6621 - val_loss: 929580096.0000 - val_rmse: 30489.0156\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 752514304.0000 - rmse: 27431.9922 - val_loss: 640507712.0000 - val_rmse: 25308.2520\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 699701376.0000 - rmse: 26451.8691 - val_loss: 910657920.0000 - val_rmse: 30177.1074\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 669488448.0000 - rmse: 25874.4746 - val_loss: 587985216.0000 - val_rmse: 24248.4062\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 732458432.0000 - rmse: 27063.9688 - val_loss: 731928576.0000 - val_rmse: 27054.1777\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 655164288.0000 - rmse: 25596.1777 - val_loss: 654818432.0000 - val_rmse: 25589.4199\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 734357376.0000 - rmse: 27099.0293 - val_loss: 976896448.0000 - val_rmse: 31255.3418\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621732800.0000 - rmse: 24934.5703 - val_loss: 1176363264.0000 - val_rmse: 34298.1523\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 663648192.0000 - rmse: 25761.3711 - val_loss: 1002660288.0000 - val_rmse: 31664.8105\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585761728.0000 - rmse: 24202.5156 - val_loss: 1315457280.0000 - val_rmse: 36269.2344\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577745152.0000 - rmse: 24036.3301 - val_loss: 717070592.0000 - val_rmse: 26778.1719\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543633024.0000 - rmse: 23315.9375 - val_loss: 622193280.0000 - val_rmse: 24943.8027\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676129344.0000 - rmse: 26002.4844 - val_loss: 881373824.0000 - val_rmse: 29687.9414\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556575168.0000 - rmse: 23591.8457 - val_loss: 1908901248.0000 - val_rmse: 43690.9727\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592409024.0000 - rmse: 24339.4531 - val_loss: 624538880.0000 - val_rmse: 24990.7754\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607297536.0000 - rmse: 24643.4082 - val_loss: 867277376.0000 - val_rmse: 29449.5723\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548449024.0000 - rmse: 23418.9883 - val_loss: 651521088.0000 - val_rmse: 25524.9102\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555576576.0000 - rmse: 23570.6699 - val_loss: 2620993792.0000 - val_rmse: 51195.6406\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562714368.0000 - rmse: 23721.5996 - val_loss: 757157696.0000 - val_rmse: 27516.4980\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562133120.0000 - rmse: 23709.3457 - val_loss: 684843520.0000 - val_rmse: 26169.5156\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524302976.0000 - rmse: 22897.6641 - val_loss: 1205940992.0000 - val_rmse: 34726.6602\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491006368.0000 - rmse: 22158.6641 - val_loss: 861264256.0000 - val_rmse: 29347.3047\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604088896.0000 - rmse: 24578.2207 - val_loss: 837997824.0000 - val_rmse: 28948.1914\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547263488.0000 - rmse: 23393.6641 - val_loss: 870973696.0000 - val_rmse: 29512.2637\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536469888.0000 - rmse: 23161.8203 - val_loss: 1406236928.0000 - val_rmse: 37499.8242\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495231584.0000 - rmse: 22253.7988 - val_loss: 771664960.0000 - val_rmse: 27778.8555\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497047392.0000 - rmse: 22294.5586 - val_loss: 595623424.0000 - val_rmse: 24405.3965\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472100256.0000 - rmse: 21727.8691 - val_loss: 1449960320.0000 - val_rmse: 38078.3438\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568497664.0000 - rmse: 23843.1895 - val_loss: 781054976.0000 - val_rmse: 27947.3594\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536125408.0000 - rmse: 23154.3828 - val_loss: 1125347200.0000 - val_rmse: 33546.1953\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542620480.0000 - rmse: 23294.2148 - val_loss: 712368384.0000 - val_rmse: 26690.2305\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559443328.0000 - rmse: 23652.5527 - val_loss: 1093916416.0000 - val_rmse: 33074.4062\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461226848.0000 - rmse: 21476.1914 - val_loss: 729149696.0000 - val_rmse: 27002.7715\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466081024.0000 - rmse: 21588.9102 - val_loss: 696920320.0000 - val_rmse: 26399.2461\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425300096.0000 - rmse: 20622.8027 - val_loss: 696777920.0000 - val_rmse: 26396.5488\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503448576.0000 - rmse: 22437.6602 - val_loss: 900587200.0000 - val_rmse: 30009.7832\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552769088.0000 - rmse: 23511.0410 - val_loss: 1085203840.0000 - val_rmse: 32942.4297\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502164928.0000 - rmse: 22409.0371 - val_loss: 794481920.0000 - val_rmse: 28186.5547\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493190944.0000 - rmse: 22207.9023 - val_loss: 800130944.0000 - val_rmse: 28286.5859\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430633216.0000 - rmse: 20751.7031 - val_loss: 860718464.0000 - val_rmse: 29338.0039\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429060800.0000 - rmse: 20713.7812 - val_loss: 929442240.0000 - val_rmse: 30486.7559\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490361632.0000 - rmse: 22144.1094 - val_loss: 710420224.0000 - val_rmse: 26653.7090\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459602176.0000 - rmse: 21438.3340 - val_loss: 1020704832.0000 - val_rmse: 31948.4688\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466683648.0000 - rmse: 21602.8613 - val_loss: 1012899648.0000 - val_rmse: 31826.0820\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471171104.0000 - rmse: 21706.4746 - val_loss: 687388544.0000 - val_rmse: 26218.0957\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511112384.0000 - rmse: 22607.7949 - val_loss: 773274624.0000 - val_rmse: 27807.8125\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458620864.0000 - rmse: 21415.4355 - val_loss: 879053248.0000 - val_rmse: 29648.8320\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459972480.0000 - rmse: 21446.9688 - val_loss: 665884288.0000 - val_rmse: 25804.7305\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414437280.0000 - rmse: 20357.7305 - val_loss: 737022464.0000 - val_rmse: 27148.1582\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402954656.0000 - rmse: 20073.7285 - val_loss: 1241543936.0000 - val_rmse: 35235.5508\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394795936.0000 - rmse: 19869.4707 - val_loss: 814175040.0000 - val_rmse: 28533.7520\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473241664.0000 - rmse: 21754.1172 - val_loss: 800101440.0000 - val_rmse: 28286.0625\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440392448.0000 - rmse: 20985.5273 - val_loss: 877629952.0000 - val_rmse: 29624.8184\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423858240.0000 - rmse: 20587.8145 - val_loss: 1516515328.0000 - val_rmse: 38942.4609\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467676512.0000 - rmse: 21625.8301 - val_loss: 1102196480.0000 - val_rmse: 33199.3438\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475360960.0000 - rmse: 21802.7734 - val_loss: 937644928.0000 - val_rmse: 30620.9883\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383717760.0000 - rmse: 19588.7129 - val_loss: 1015730112.0000 - val_rmse: 31870.5195\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462075360.0000 - rmse: 21495.9375 - val_loss: 1696110592.0000 - val_rmse: 41183.8633\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429460672.0000 - rmse: 20723.4297 - val_loss: 886224512.0000 - val_rmse: 29769.5234\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381616096.0000 - rmse: 19534.9961 - val_loss: 881214208.0000 - val_rmse: 29685.2500\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373644384.0000 - rmse: 19329.8828 - val_loss: 691419392.0000 - val_rmse: 26294.8555\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413053056.0000 - rmse: 20323.7051 - val_loss: 1038943936.0000 - val_rmse: 32232.6523\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416178080.0000 - rmse: 20400.4434 - val_loss: 799996352.0000 - val_rmse: 28284.2070\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362904736.0000 - rmse: 19050.0566 - val_loss: 1573211520.0000 - val_rmse: 39663.7305\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448261760.0000 - rmse: 21172.1895 - val_loss: 975548992.0000 - val_rmse: 31233.7793\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488958848.0000 - rmse: 22112.4102 - val_loss: 1000530304.0000 - val_rmse: 31631.1582\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403081440.0000 - rmse: 20076.8867 - val_loss: 915737216.0000 - val_rmse: 30261.1504\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374036736.0000 - rmse: 19340.0273 - val_loss: 1831105408.0000 - val_rmse: 42791.4141\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373200128.0000 - rmse: 19318.3867 - val_loss: 1124533504.0000 - val_rmse: 33534.0664\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447434432.0000 - rmse: 21152.6445 - val_loss: 1278849664.0000 - val_rmse: 35761.0039\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330523456.0000 - rmse: 18180.3027 - val_loss: 856692032.0000 - val_rmse: 29269.3027\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395168352.0000 - rmse: 19878.8398 - val_loss: 825017152.0000 - val_rmse: 28723.1094\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392924352.0000 - rmse: 19822.3203 - val_loss: 933883328.0000 - val_rmse: 30559.5020\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369884704.0000 - rmse: 19232.3867 - val_loss: 786091008.0000 - val_rmse: 28037.3125\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393205696.0000 - rmse: 19829.4141 - val_loss: 1029310336.0000 - val_rmse: 32082.8672\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435808608.0000 - rmse: 20876.0293 - val_loss: 840087488.0000 - val_rmse: 28984.2637\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360292512.0000 - rmse: 18981.3711 - val_loss: 1190553344.0000 - val_rmse: 34504.3945\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351744192.0000 - rmse: 18754.8438 - val_loss: 1080957568.0000 - val_rmse: 32877.9180\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430746432.0000 - rmse: 20754.4316 - val_loss: 1727153024.0000 - val_rmse: 41559.0273\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404287840.0000 - rmse: 20106.9102 - val_loss: 647252224.0000 - val_rmse: 25441.1523\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411661504.0000 - rmse: 20289.4414 - val_loss: 758378560.0000 - val_rmse: 27538.6719\n",
      "Epoch 111/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355760032.0000 - rmse: 18861.5996 - val_loss: 816509696.0000 - val_rmse: 28574.6328\n",
      "Epoch 112/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434472224.0000 - rmse: 20843.9961 - val_loss: 884354560.0000 - val_rmse: 29738.0996\n",
      "Epoch 113/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346599520.0000 - rmse: 18617.1816 - val_loss: 810258944.0000 - val_rmse: 28465.0469\n",
      "Epoch 114/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362147680.0000 - rmse: 19030.1758 - val_loss: 1216287488.0000 - val_rmse: 34875.3125\n",
      "Epoch 115/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366775264.0000 - rmse: 19151.3750 - val_loss: 724782720.0000 - val_rmse: 26921.7871\n",
      "Epoch 116/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344237088.0000 - rmse: 18553.6250 - val_loss: 769822272.0000 - val_rmse: 27745.6719\n",
      "Epoch 117/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359836576.0000 - rmse: 18969.3574 - val_loss: 654588800.0000 - val_rmse: 25584.9336\n",
      "Epoch 118/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362140736.0000 - rmse: 19029.9941 - val_loss: 1151788928.0000 - val_rmse: 33938.0156\n",
      "Epoch 119/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315427232.0000 - rmse: 17760.2695 - val_loss: 812331904.0000 - val_rmse: 28501.4336\n",
      "Epoch 120/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334666176.0000 - rmse: 18293.8828 - val_loss: 1051545856.0000 - val_rmse: 32427.5469\n",
      "Epoch 121/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362637568.0000 - rmse: 19043.0449 - val_loss: 818343488.0000 - val_rmse: 28606.7031\n",
      "Epoch 122/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345610528.0000 - rmse: 18590.6016 - val_loss: 1243216896.0000 - val_rmse: 35259.2812\n",
      "Epoch 123/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322770272.0000 - rmse: 17965.8066 - val_loss: 1132721024.0000 - val_rmse: 33655.9219\n",
      "Epoch 124/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384128032.0000 - rmse: 19599.1816 - val_loss: 910575872.0000 - val_rmse: 30175.7461\n",
      "Epoch 125/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327010752.0000 - rmse: 18083.4375 - val_loss: 1384221568.0000 - val_rmse: 37205.1289\n",
      "Epoch 126/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403238272.0000 - rmse: 20080.7930 - val_loss: 698390976.0000 - val_rmse: 26427.0859\n",
      "Epoch 127/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386938848.0000 - rmse: 19670.7598 - val_loss: 743174208.0000 - val_rmse: 27261.2188\n",
      "Epoch 128/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344887808.0000 - rmse: 18571.1543 - val_loss: 634028928.0000 - val_rmse: 25179.9297\n",
      "Epoch 129/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319751008.0000 - rmse: 17881.5820 - val_loss: 700937600.0000 - val_rmse: 26475.2266\n",
      "Epoch 130/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405294944.0000 - rmse: 20131.9375 - val_loss: 1347494528.0000 - val_rmse: 36708.2344\n",
      "Epoch 131/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322699616.0000 - rmse: 17963.8398 - val_loss: 798071616.0000 - val_rmse: 28250.1621\n",
      "Epoch 132/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360331616.0000 - rmse: 18982.4023 - val_loss: 1307516928.0000 - val_rmse: 36159.6055\n",
      "Epoch 133/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300598048.0000 - rmse: 17337.7637 - val_loss: 695716928.0000 - val_rmse: 26376.4434\n",
      "Epoch 134/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354198624.0000 - rmse: 18820.1641 - val_loss: 932661248.0000 - val_rmse: 30539.5020\n",
      "Epoch 135/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313667328.0000 - rmse: 17710.6543 - val_loss: 643632960.0000 - val_rmse: 25369.9199\n",
      "Epoch 136/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344504960.0000 - rmse: 18560.8438 - val_loss: 736010112.0000 - val_rmse: 27129.5039\n",
      "Epoch 137/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355370208.0000 - rmse: 18851.2637 - val_loss: 775388928.0000 - val_rmse: 27845.8047\n",
      "Epoch 138/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294894528.0000 - rmse: 17172.4941 - val_loss: 633624064.0000 - val_rmse: 25171.8887\n",
      "Epoch 139/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286079168.0000 - rmse: 16913.8730 - val_loss: 1032232448.0000 - val_rmse: 32128.3730\n",
      "Epoch 140/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330904288.0000 - rmse: 18190.7734 - val_loss: 771518272.0000 - val_rmse: 27776.2148\n",
      "Epoch 141/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328892160.0000 - rmse: 18135.3828 - val_loss: 715569152.0000 - val_rmse: 26750.1230\n",
      "Epoch 142/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290705312.0000 - rmse: 17050.0820 - val_loss: 836499008.0000 - val_rmse: 28922.2910\n",
      "Epoch 143/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325434560.0000 - rmse: 18039.8027 - val_loss: 935327040.0000 - val_rmse: 30583.1172\n",
      "Epoch 144/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362587840.0000 - rmse: 19041.7363 - val_loss: 1001829760.0000 - val_rmse: 31651.6953\n",
      "Epoch 145/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292956768.0000 - rmse: 17115.9785 - val_loss: 760194560.0000 - val_rmse: 27571.6270\n",
      "Epoch 146/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331088032.0000 - rmse: 18195.8223 - val_loss: 656004352.0000 - val_rmse: 25612.5781\n",
      "Epoch 147/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357142880.0000 - rmse: 18898.2207 - val_loss: 1084666880.0000 - val_rmse: 32934.2773\n",
      "Epoch 148/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299926976.0000 - rmse: 17318.3984 - val_loss: 767024384.0000 - val_rmse: 27695.2031\n",
      "Epoch 149/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298931744.0000 - rmse: 17289.6387 - val_loss: 529365408.0000 - val_rmse: 23007.9414\n",
      "Epoch 150/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324750624.0000 - rmse: 18020.8359 - val_loss: 731393664.0000 - val_rmse: 27044.2891\n",
      "Epoch 151/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273532992.0000 - rmse: 16538.8301 - val_loss: 762294336.0000 - val_rmse: 27609.6777\n",
      "Epoch 152/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302618176.0000 - rmse: 17395.9219 - val_loss: 1095851520.0000 - val_rmse: 33103.6445\n",
      "Epoch 153/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322711840.0000 - rmse: 17964.1797 - val_loss: 671804992.0000 - val_rmse: 25919.2012\n",
      "Epoch 154/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302523232.0000 - rmse: 17393.1934 - val_loss: 1285741824.0000 - val_rmse: 35857.2422\n",
      "Epoch 155/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291624192.0000 - rmse: 17077.0059 - val_loss: 832512064.0000 - val_rmse: 28853.2832\n",
      "Epoch 156/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343756800.0000 - rmse: 18540.6797 - val_loss: 585772608.0000 - val_rmse: 24202.7383\n",
      "Epoch 157/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321268192.0000 - rmse: 17923.9531 - val_loss: 656755456.0000 - val_rmse: 25627.2363\n",
      "Epoch 158/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310289024.0000 - rmse: 17615.0215 - val_loss: 783591936.0000 - val_rmse: 27992.7129\n",
      "Epoch 159/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298775552.0000 - rmse: 17285.1211 - val_loss: 673587968.0000 - val_rmse: 25953.5703\n",
      "Epoch 160/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277493760.0000 - rmse: 16658.1445 - val_loss: 651009152.0000 - val_rmse: 25514.8789\n",
      "Epoch 161/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314225472.0000 - rmse: 17726.4023 - val_loss: 605950528.0000 - val_rmse: 24616.0625\n",
      "Epoch 162/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322439168.0000 - rmse: 17956.5898 - val_loss: 586468800.0000 - val_rmse: 24217.1172\n",
      "Epoch 163/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289598496.0000 - rmse: 17017.5918 - val_loss: 828064768.0000 - val_rmse: 28776.1152\n",
      "Epoch 164/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289690464.0000 - rmse: 17020.2949 - val_loss: 1483218560.0000 - val_rmse: 38512.5781\n",
      "Epoch 165/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318405472.0000 - rmse: 17843.9160 - val_loss: 660732352.0000 - val_rmse: 25704.7129\n",
      "Epoch 166/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306347648.0000 - rmse: 17502.7871 - val_loss: 1131275136.0000 - val_rmse: 33634.4336\n",
      "Epoch 167/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319591296.0000 - rmse: 17877.1152 - val_loss: 617776704.0000 - val_rmse: 24855.1133\n",
      "Epoch 168/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363017952.0000 - rmse: 19053.0273 - val_loss: 1255140864.0000 - val_rmse: 35427.9648\n",
      "Epoch 169/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294267616.0000 - rmse: 17154.2285 - val_loss: 1151120768.0000 - val_rmse: 33928.1680\n",
      "Epoch 170/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323340896.0000 - rmse: 17981.6777 - val_loss: 597602048.0000 - val_rmse: 24445.8984\n",
      "Epoch 171/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298218912.0000 - rmse: 17269.0137 - val_loss: 822535808.0000 - val_rmse: 28679.8848\n",
      "Epoch 172/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350732512.0000 - rmse: 18727.8535 - val_loss: 802110848.0000 - val_rmse: 28321.5625\n",
      "Epoch 173/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330397504.0000 - rmse: 18176.8379 - val_loss: 888984896.0000 - val_rmse: 29815.8496\n",
      "Epoch 174/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296315872.0000 - rmse: 17213.8262 - val_loss: 568121216.0000 - val_rmse: 23835.2891\n",
      "Epoch 175/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325343584.0000 - rmse: 18037.2812 - val_loss: 658976000.0000 - val_rmse: 25670.5273\n",
      "Epoch 176/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302892192.0000 - rmse: 17403.7949 - val_loss: 1091026432.0000 - val_rmse: 33030.6875\n",
      "104/104 [==============================] - 0s 700us/step - loss: 969479616.0000 - rmse: 31136.4668\n",
      "[969479616.0, 31136.466796875]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 6375616512.0000 - rmse: 79847.4609 - val_loss: 1362484352.0000 - val_rmse: 36911.8438\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1883834240.0000 - rmse: 43403.1602 - val_loss: 1089444736.0000 - val_rmse: 33006.7383\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1632291840.0000 - rmse: 40401.6328 - val_loss: 991689664.0000 - val_rmse: 31491.1055\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1465607296.0000 - rmse: 38283.2500 - val_loss: 925921664.0000 - val_rmse: 30428.9609\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1412621184.0000 - rmse: 37584.8516 - val_loss: 948324224.0000 - val_rmse: 30794.8730\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1380904320.0000 - rmse: 37160.5195 - val_loss: 1001843968.0000 - val_rmse: 31651.9180\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1280866944.0000 - rmse: 35789.2031 - val_loss: 1003615168.0000 - val_rmse: 31679.8867\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1245782912.0000 - rmse: 35295.6484 - val_loss: 847901632.0000 - val_rmse: 29118.7500\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1182286080.0000 - rmse: 34384.3867 - val_loss: 1166638080.0000 - val_rmse: 34156.0859\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1126629760.0000 - rmse: 33565.3047 - val_loss: 887166080.0000 - val_rmse: 29785.3340\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1060482688.0000 - rmse: 32565.0508 - val_loss: 937823488.0000 - val_rmse: 30623.9023\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1127224576.0000 - rmse: 33574.1641 - val_loss: 843702592.0000 - val_rmse: 29046.5586\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1007305856.0000 - rmse: 31738.0820 - val_loss: 872796864.0000 - val_rmse: 29543.1348\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 996242624.0000 - rmse: 31563.3086 - val_loss: 826795264.0000 - val_rmse: 28754.0469\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1030574528.0000 - rmse: 32102.5625 - val_loss: 824946432.0000 - val_rmse: 28721.8809\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1061431488.0000 - rmse: 32579.6172 - val_loss: 1241608576.0000 - val_rmse: 35236.4648\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 992844608.0000 - rmse: 31509.4375 - val_loss: 783082496.0000 - val_rmse: 27983.6113\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 961945600.0000 - rmse: 31015.2480 - val_loss: 819811648.0000 - val_rmse: 28632.3535\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 891424192.0000 - rmse: 29856.7285 - val_loss: 1644457984.0000 - val_rmse: 40551.9180\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 911970240.0000 - rmse: 30198.8457 - val_loss: 776830400.0000 - val_rmse: 27871.6777\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 850723456.0000 - rmse: 29167.1621 - val_loss: 2327230720.0000 - val_rmse: 48241.3789\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 845775744.0000 - rmse: 29082.2246 - val_loss: 1022227392.0000 - val_rmse: 31972.2910\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 890894848.0000 - rmse: 29847.8613 - val_loss: 808060672.0000 - val_rmse: 28426.4082\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 826792256.0000 - rmse: 28753.9961 - val_loss: 971672512.0000 - val_rmse: 31171.6621\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 898882240.0000 - rmse: 29981.3652 - val_loss: 690753728.0000 - val_rmse: 26282.1934\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 786067584.0000 - rmse: 28036.8965 - val_loss: 1333624832.0000 - val_rmse: 36518.8281\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 787280064.0000 - rmse: 28058.5117 - val_loss: 706072512.0000 - val_rmse: 26572.0234\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 769767680.0000 - rmse: 27744.6855 - val_loss: 1029222464.0000 - val_rmse: 32081.4980\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 803339968.0000 - rmse: 28343.2520 - val_loss: 638680704.0000 - val_rmse: 25272.1328\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 756044608.0000 - rmse: 27496.2656 - val_loss: 687582016.0000 - val_rmse: 26221.7852\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 771772352.0000 - rmse: 27780.7910 - val_loss: 770130048.0000 - val_rmse: 27751.2168\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 711010688.0000 - rmse: 26664.7832 - val_loss: 1489254016.0000 - val_rmse: 38590.8555\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665634432.0000 - rmse: 25799.8926 - val_loss: 1129421056.0000 - val_rmse: 33606.8594\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 657833088.0000 - rmse: 25648.2578 - val_loss: 1402820608.0000 - val_rmse: 37454.2461\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 758855488.0000 - rmse: 27547.3301 - val_loss: 616305472.0000 - val_rmse: 24825.5000\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 791348096.0000 - rmse: 28130.9102 - val_loss: 845188480.0000 - val_rmse: 29072.1230\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704878208.0000 - rmse: 26549.5410 - val_loss: 821987072.0000 - val_rmse: 28670.3164\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800038464.0000 - rmse: 28284.9512 - val_loss: 822810944.0000 - val_rmse: 28684.6816\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618201280.0000 - rmse: 24863.6543 - val_loss: 997252864.0000 - val_rmse: 31579.3086\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628634112.0000 - rmse: 25072.5762 - val_loss: 840422336.0000 - val_rmse: 28990.0391\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632097024.0000 - rmse: 25141.5391 - val_loss: 1131174016.0000 - val_rmse: 33632.9297\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 639464576.0000 - rmse: 25287.6348 - val_loss: 757022976.0000 - val_rmse: 27514.0508\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537886784.0000 - rmse: 23192.3867 - val_loss: 1240517120.0000 - val_rmse: 35220.9766\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 645721344.0000 - rmse: 25411.0469 - val_loss: 1079876480.0000 - val_rmse: 32861.4727\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606269696.0000 - rmse: 24622.5410 - val_loss: 750948288.0000 - val_rmse: 27403.4355\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567912000.0000 - rmse: 23830.9043 - val_loss: 921078848.0000 - val_rmse: 30349.2812\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572594176.0000 - rmse: 23928.9395 - val_loss: 830546432.0000 - val_rmse: 28819.2031\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641363648.0000 - rmse: 25325.1582 - val_loss: 1004201024.0000 - val_rmse: 31689.1309\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534428768.0000 - rmse: 23117.7148 - val_loss: 1416013312.0000 - val_rmse: 37629.9531\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567005632.0000 - rmse: 23811.8789 - val_loss: 713143488.0000 - val_rmse: 26704.7461\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450045376.0000 - rmse: 21214.2734 - val_loss: 1134551936.0000 - val_rmse: 33683.1094\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508235328.0000 - rmse: 22544.0742 - val_loss: 2021772416.0000 - val_rmse: 44964.1250\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 608594752.0000 - rmse: 24669.7109 - val_loss: 670528000.0000 - val_rmse: 25894.5547\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542196672.0000 - rmse: 23285.1172 - val_loss: 867181248.0000 - val_rmse: 29447.9414\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491865664.0000 - rmse: 22178.0449 - val_loss: 865575168.0000 - val_rmse: 29420.6582\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501797504.0000 - rmse: 22400.8379 - val_loss: 976676288.0000 - val_rmse: 31251.8203\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549466240.0000 - rmse: 23440.6953 - val_loss: 782891008.0000 - val_rmse: 27980.1895\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496708736.0000 - rmse: 22286.9629 - val_loss: 785184896.0000 - val_rmse: 28021.1504\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618301888.0000 - rmse: 24865.6738 - val_loss: 826721024.0000 - val_rmse: 28752.7559\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549493696.0000 - rmse: 23441.2812 - val_loss: 1179476608.0000 - val_rmse: 34343.5078\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579109248.0000 - rmse: 24064.6875 - val_loss: 943676864.0000 - val_rmse: 30719.3242\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547234688.0000 - rmse: 23393.0469 - val_loss: 1020014080.0000 - val_rmse: 31937.6602\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529840320.0000 - rmse: 23018.2578 - val_loss: 1034032768.0000 - val_rmse: 32156.3809\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501718720.0000 - rmse: 22399.0781 - val_loss: 770529984.0000 - val_rmse: 27758.4199\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515207072.0000 - rmse: 22698.1719 - val_loss: 685246528.0000 - val_rmse: 26177.2129\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565979712.0000 - rmse: 23790.3262 - val_loss: 819419712.0000 - val_rmse: 28625.5078\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478651648.0000 - rmse: 21878.1094 - val_loss: 958875456.0000 - val_rmse: 30965.7129\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424119424.0000 - rmse: 20594.1582 - val_loss: 955346752.0000 - val_rmse: 30908.6816\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494494688.0000 - rmse: 22237.2363 - val_loss: 1112756864.0000 - val_rmse: 33358.0117\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475000544.0000 - rmse: 21794.5059 - val_loss: 789258048.0000 - val_rmse: 28093.7363\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473913152.0000 - rmse: 21769.5449 - val_loss: 1242048384.0000 - val_rmse: 35242.7070\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439233152.0000 - rmse: 20957.8887 - val_loss: 834854400.0000 - val_rmse: 28893.8457\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493803104.0000 - rmse: 22221.6816 - val_loss: 1105518208.0000 - val_rmse: 33249.3320\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536031776.0000 - rmse: 23152.3594 - val_loss: 700029120.0000 - val_rmse: 26458.0605\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373286720.0000 - rmse: 19320.6289 - val_loss: 780072128.0000 - val_rmse: 27929.7715\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441832704.0000 - rmse: 21019.8164 - val_loss: 822451904.0000 - val_rmse: 28678.4219\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381358944.0000 - rmse: 19528.4102 - val_loss: 818058112.0000 - val_rmse: 28601.7148\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424354976.0000 - rmse: 20599.8770 - val_loss: 784363968.0000 - val_rmse: 28006.4980\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407208672.0000 - rmse: 20179.4121 - val_loss: 725507648.0000 - val_rmse: 26935.2480\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397525312.0000 - rmse: 19938.0352 - val_loss: 731107264.0000 - val_rmse: 27038.9941\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431029728.0000 - rmse: 20761.2539 - val_loss: 788363648.0000 - val_rmse: 28077.8145\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443142912.0000 - rmse: 21050.9590 - val_loss: 773192320.0000 - val_rmse: 27806.3359\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429534592.0000 - rmse: 20725.2148 - val_loss: 703749248.0000 - val_rmse: 26528.2715\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405714112.0000 - rmse: 20142.3438 - val_loss: 1429867392.0000 - val_rmse: 37813.5859\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411224000.0000 - rmse: 20278.6562 - val_loss: 960191104.0000 - val_rmse: 30986.9492\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375585216.0000 - rmse: 19380.0195 - val_loss: 679718016.0000 - val_rmse: 26071.4004\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377273376.0000 - rmse: 19423.5254 - val_loss: 763389696.0000 - val_rmse: 27629.5059\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404483680.0000 - rmse: 20111.7773 - val_loss: 726518528.0000 - val_rmse: 26954.0059\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470498976.0000 - rmse: 21690.9863 - val_loss: 992364736.0000 - val_rmse: 31501.8203\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416216416.0000 - rmse: 20401.3828 - val_loss: 846223168.0000 - val_rmse: 29089.9121\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390747904.0000 - rmse: 19767.3438 - val_loss: 1330294656.0000 - val_rmse: 36473.2031\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411173728.0000 - rmse: 20277.4180 - val_loss: 1268257792.0000 - val_rmse: 35612.6055\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376325920.0000 - rmse: 19399.1191 - val_loss: 1116257920.0000 - val_rmse: 33410.4453\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418407616.0000 - rmse: 20455.0137 - val_loss: 892584256.0000 - val_rmse: 29876.1484\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405256096.0000 - rmse: 20130.9707 - val_loss: 934847680.0000 - val_rmse: 30575.2773\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386740032.0000 - rmse: 19665.7070 - val_loss: 1024882112.0000 - val_rmse: 32013.7793\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398257472.0000 - rmse: 19956.3887 - val_loss: 745510848.0000 - val_rmse: 27304.0430\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402863424.0000 - rmse: 20071.4570 - val_loss: 675561856.0000 - val_rmse: 25991.5723\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358438464.0000 - rmse: 18932.4707 - val_loss: 796871552.0000 - val_rmse: 28228.9121\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342409184.0000 - rmse: 18504.3008 - val_loss: 819620608.0000 - val_rmse: 28629.0176\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415589536.0000 - rmse: 20386.0117 - val_loss: 757680576.0000 - val_rmse: 27525.9961\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406525344.0000 - rmse: 20162.4727 - val_loss: 678079616.0000 - val_rmse: 26039.9590\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393540576.0000 - rmse: 19837.8555 - val_loss: 684685696.0000 - val_rmse: 26166.4980\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358313888.0000 - rmse: 18929.1797 - val_loss: 866355392.0000 - val_rmse: 29433.9160\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384271808.0000 - rmse: 19602.8516 - val_loss: 858836352.0000 - val_rmse: 29305.9102\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341281024.0000 - rmse: 18473.7930 - val_loss: 749740992.0000 - val_rmse: 27381.3965\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348476480.0000 - rmse: 18667.5234 - val_loss: 778521152.0000 - val_rmse: 27901.9883\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376299616.0000 - rmse: 19398.4414 - val_loss: 742576064.0000 - val_rmse: 27250.2480\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348778144.0000 - rmse: 18675.6016 - val_loss: 1285497856.0000 - val_rmse: 35853.8398\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356821440.0000 - rmse: 18889.7148 - val_loss: 717596736.0000 - val_rmse: 26787.9941\n",
      "Epoch 111/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356989440.0000 - rmse: 18894.1641 - val_loss: 910812672.0000 - val_rmse: 30179.6738\n",
      "Epoch 112/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373180480.0000 - rmse: 19317.8789 - val_loss: 885048512.0000 - val_rmse: 29749.7637\n",
      "Epoch 113/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338214240.0000 - rmse: 18390.5996 - val_loss: 804193088.0000 - val_rmse: 28358.2969\n",
      "Epoch 114/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375170464.0000 - rmse: 19369.3164 - val_loss: 704634688.0000 - val_rmse: 26544.9531\n",
      "Epoch 115/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379612672.0000 - rmse: 19483.6504 - val_loss: 1241273728.0000 - val_rmse: 35231.7148\n",
      "Epoch 116/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340760896.0000 - rmse: 18459.7090 - val_loss: 868931264.0000 - val_rmse: 29477.6348\n",
      "Epoch 117/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320847008.0000 - rmse: 17912.2012 - val_loss: 812977344.0000 - val_rmse: 28512.7559\n",
      "Epoch 118/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354159424.0000 - rmse: 18819.1230 - val_loss: 1032692672.0000 - val_rmse: 32135.5332\n",
      "Epoch 119/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410582432.0000 - rmse: 20262.8301 - val_loss: 778070720.0000 - val_rmse: 27893.9199\n",
      "Epoch 120/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344274176.0000 - rmse: 18554.6250 - val_loss: 1099015040.0000 - val_rmse: 33151.3945\n",
      "Epoch 121/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350315424.0000 - rmse: 18716.7129 - val_loss: 928264768.0000 - val_rmse: 30467.4355\n",
      "Epoch 122/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336741472.0000 - rmse: 18350.5156 - val_loss: 847117696.0000 - val_rmse: 29105.2852\n",
      "Epoch 123/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336402432.0000 - rmse: 18341.2754 - val_loss: 1085110784.0000 - val_rmse: 32941.0156\n",
      "Epoch 124/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313304608.0000 - rmse: 17700.4082 - val_loss: 814400448.0000 - val_rmse: 28537.6992\n",
      "Epoch 125/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318394208.0000 - rmse: 17843.6016 - val_loss: 872260096.0000 - val_rmse: 29534.0488\n",
      "Epoch 126/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276477216.0000 - rmse: 16627.6035 - val_loss: 686270080.0000 - val_rmse: 26196.7578\n",
      "Epoch 127/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338115136.0000 - rmse: 18387.9062 - val_loss: 710954048.0000 - val_rmse: 26663.7207\n",
      "Epoch 128/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333224384.0000 - rmse: 18254.4336 - val_loss: 770428224.0000 - val_rmse: 27756.5879\n",
      "Epoch 129/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365073440.0000 - rmse: 19106.8926 - val_loss: 730457920.0000 - val_rmse: 27026.9844\n",
      "Epoch 130/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317082016.0000 - rmse: 17806.7969 - val_loss: 763051712.0000 - val_rmse: 27623.3887\n",
      "Epoch 131/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329042720.0000 - rmse: 18139.5332 - val_loss: 829360640.0000 - val_rmse: 28798.6230\n",
      "Epoch 132/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281239808.0000 - rmse: 16770.2031 - val_loss: 724239296.0000 - val_rmse: 26911.6914\n",
      "Epoch 133/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338612448.0000 - rmse: 18401.4238 - val_loss: 853940672.0000 - val_rmse: 29222.2598\n",
      "Epoch 134/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328346048.0000 - rmse: 18120.3184 - val_loss: 1022130432.0000 - val_rmse: 31970.7734\n",
      "Epoch 135/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314071392.0000 - rmse: 17722.0566 - val_loss: 840836096.0000 - val_rmse: 28997.1719\n",
      "Epoch 136/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332385088.0000 - rmse: 18231.4297 - val_loss: 743935616.0000 - val_rmse: 27275.1816\n",
      "Epoch 137/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337558208.0000 - rmse: 18372.7559 - val_loss: 728089088.0000 - val_rmse: 26983.1270\n",
      "Epoch 138/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296047616.0000 - rmse: 17206.0332 - val_loss: 761549184.0000 - val_rmse: 27596.1797\n",
      "Epoch 139/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306906560.0000 - rmse: 17518.7461 - val_loss: 1235087488.0000 - val_rmse: 35143.8086\n",
      "Epoch 140/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319559264.0000 - rmse: 17876.2188 - val_loss: 913038400.0000 - val_rmse: 30216.5254\n",
      "Epoch 141/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318532000.0000 - rmse: 17847.4629 - val_loss: 767401024.0000 - val_rmse: 27702.0020\n",
      "Epoch 142/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314036768.0000 - rmse: 17721.0801 - val_loss: 937010496.0000 - val_rmse: 30610.6250\n",
      "Epoch 143/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382933152.0000 - rmse: 19568.6777 - val_loss: 854405888.0000 - val_rmse: 29230.2207\n",
      "Epoch 144/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307176096.0000 - rmse: 17526.4375 - val_loss: 1307187200.0000 - val_rmse: 36155.0430\n",
      "Epoch 145/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304063840.0000 - rmse: 17437.4238 - val_loss: 1346140288.0000 - val_rmse: 36689.7812\n",
      "Epoch 146/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308569568.0000 - rmse: 17566.1465 - val_loss: 1017141824.0000 - val_rmse: 31892.6602\n",
      "Epoch 147/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321702496.0000 - rmse: 17936.0645 - val_loss: 761919936.0000 - val_rmse: 27602.8965\n",
      "Epoch 148/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304445344.0000 - rmse: 17448.3613 - val_loss: 770020992.0000 - val_rmse: 27749.2500\n",
      "Epoch 149/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291192256.0000 - rmse: 17064.3535 - val_loss: 1158693120.0000 - val_rmse: 34039.5820\n",
      "Epoch 150/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285230912.0000 - rmse: 16888.7793 - val_loss: 2068925568.0000 - val_rmse: 45485.4453\n",
      "Epoch 151/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311601504.0000 - rmse: 17652.2383 - val_loss: 734437952.0000 - val_rmse: 27100.5137\n",
      "104/104 [==============================] - 0s 725us/step - loss: 413741536.0000 - rmse: 20340.6367\n",
      "[413741536.0, 20340.63671875]\n",
      "[22236.525390625, 29799.548828125, 27640.283203125, 31136.466796875, 20340.63671875]\n",
      "26230.6921875\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "!python train.py kfold baseline\n",
    "#d 0.25 p 15"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 18:02:29.311794: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 18:02:29.311830: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 18:02:29.312257: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 18:02:29.534915: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 6465809408.0000 - rmse: 80410.2578 - val_loss: 1580840832.0000 - val_rmse: 39759.7891\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1841531520.0000 - rmse: 42913.0703 - val_loss: 975959040.0000 - val_rmse: 31240.3438\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1621909632.0000 - rmse: 40272.9375 - val_loss: 929561152.0000 - val_rmse: 30488.7051\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1547761792.0000 - rmse: 39341.6055 - val_loss: 1337695232.0000 - val_rmse: 36574.5156\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1490894208.0000 - rmse: 38612.0977 - val_loss: 731979840.0000 - val_rmse: 27055.1250\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1439637504.0000 - rmse: 37942.5547 - val_loss: 718922560.0000 - val_rmse: 26812.7305\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1407230336.0000 - rmse: 37513.0703 - val_loss: 715725056.0000 - val_rmse: 26753.0391\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1340489600.0000 - rmse: 36612.6953 - val_loss: 859472896.0000 - val_rmse: 29316.7676\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1358028160.0000 - rmse: 36851.4336 - val_loss: 730783104.0000 - val_rmse: 27033.0000\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1273724416.0000 - rmse: 35689.2773 - val_loss: 724983488.0000 - val_rmse: 26925.5176\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1290247040.0000 - rmse: 35920.0078 - val_loss: 737618112.0000 - val_rmse: 27159.1250\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1320962688.0000 - rmse: 36345.0508 - val_loss: 688595776.0000 - val_rmse: 26241.1094\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1193028352.0000 - rmse: 34540.2422 - val_loss: 712140032.0000 - val_rmse: 26685.9512\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1227885184.0000 - rmse: 35041.1914 - val_loss: 704870912.0000 - val_rmse: 26549.4043\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1156660864.0000 - rmse: 34009.7188 - val_loss: 825328256.0000 - val_rmse: 28728.5273\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1129438080.0000 - rmse: 33607.1133 - val_loss: 688842496.0000 - val_rmse: 26245.8086\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1119273344.0000 - rmse: 33455.5430 - val_loss: 693143296.0000 - val_rmse: 26327.6152\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1123143680.0000 - rmse: 33513.3359 - val_loss: 688127872.0000 - val_rmse: 26232.1914\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1092498944.0000 - rmse: 33052.9727 - val_loss: 754200192.0000 - val_rmse: 27462.7051\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1013851008.0000 - rmse: 31841.0273 - val_loss: 643049536.0000 - val_rmse: 25358.4199\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1049944192.0000 - rmse: 32402.8398 - val_loss: 783358080.0000 - val_rmse: 27988.5352\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1041418624.0000 - rmse: 32271.0156 - val_loss: 582633344.0000 - val_rmse: 24137.7988\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 857965824.0000 - rmse: 29291.0508 - val_loss: 635807680.0000 - val_rmse: 25215.2266\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 836593664.0000 - rmse: 28923.9297 - val_loss: 821796480.0000 - val_rmse: 28666.9922\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 956188544.0000 - rmse: 30922.2969 - val_loss: 510508608.0000 - val_rmse: 22594.4375\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 852097792.0000 - rmse: 29190.7148 - val_loss: 1429418112.0000 - val_rmse: 37807.6445\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 877157696.0000 - rmse: 29616.8477 - val_loss: 1850711424.0000 - val_rmse: 43019.8945\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 865636352.0000 - rmse: 29421.6992 - val_loss: 704293440.0000 - val_rmse: 26538.5254\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 792900736.0000 - rmse: 28158.4922 - val_loss: 865085696.0000 - val_rmse: 29412.3398\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 843544448.0000 - rmse: 29043.8340 - val_loss: 679263168.0000 - val_rmse: 26062.6777\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 875106432.0000 - rmse: 29582.1973 - val_loss: 606793984.0000 - val_rmse: 24633.1895\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 848976576.0000 - rmse: 29137.2031 - val_loss: 626131648.0000 - val_rmse: 25022.6230\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 815816064.0000 - rmse: 28562.4941 - val_loss: 983959552.0000 - val_rmse: 31368.1289\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 735450368.0000 - rmse: 27119.1875 - val_loss: 646325888.0000 - val_rmse: 25422.9395\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 760690560.0000 - rmse: 27580.6172 - val_loss: 795982720.0000 - val_rmse: 28213.1641\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642181952.0000 - rmse: 25341.3086 - val_loss: 711238208.0000 - val_rmse: 26669.0488\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 660950784.0000 - rmse: 25708.9629 - val_loss: 640433216.0000 - val_rmse: 25306.7812\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665072000.0000 - rmse: 25788.9902 - val_loss: 569448320.0000 - val_rmse: 23863.1172\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 728662656.0000 - rmse: 26993.7520 - val_loss: 2257422080.0000 - val_rmse: 47512.3359\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 678719680.0000 - rmse: 26052.2500 - val_loss: 388910816.0000 - val_rmse: 19720.8203\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594935104.0000 - rmse: 24391.2910 - val_loss: 724523904.0000 - val_rmse: 26916.9805\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 664624000.0000 - rmse: 25780.3027 - val_loss: 562270272.0000 - val_rmse: 23712.2383\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 671161920.0000 - rmse: 25906.7910 - val_loss: 540479872.0000 - val_rmse: 23248.2207\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782285376.0000 - rmse: 27969.3652 - val_loss: 457688672.0000 - val_rmse: 21393.6582\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551321344.0000 - rmse: 23480.2324 - val_loss: 634293376.0000 - val_rmse: 25185.1816\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569929024.0000 - rmse: 23873.1855 - val_loss: 690907584.0000 - val_rmse: 26285.1191\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565382016.0000 - rmse: 23777.7637 - val_loss: 656891712.0000 - val_rmse: 25629.8984\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592780992.0000 - rmse: 24347.0938 - val_loss: 1046773504.0000 - val_rmse: 32353.8789\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564174144.0000 - rmse: 23752.3496 - val_loss: 564750080.0000 - val_rmse: 23764.4707\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513700000.0000 - rmse: 22664.9492 - val_loss: 748538048.0000 - val_rmse: 27359.4238\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 686549696.0000 - rmse: 26202.0938 - val_loss: 473259072.0000 - val_rmse: 21754.5176\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563382720.0000 - rmse: 23735.6836 - val_loss: 647558208.0000 - val_rmse: 25447.1641\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541188736.0000 - rmse: 23263.4609 - val_loss: 960638080.0000 - val_rmse: 30994.1602\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582429952.0000 - rmse: 24133.5840 - val_loss: 288968256.0000 - val_rmse: 16999.0664\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527318368.0000 - rmse: 22963.4141 - val_loss: 340750016.0000 - val_rmse: 18459.4160\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 528015168.0000 - rmse: 22978.5801 - val_loss: 1085358720.0000 - val_rmse: 32944.7812\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596197568.0000 - rmse: 24417.1562 - val_loss: 738550912.0000 - val_rmse: 27176.2930\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446778080.0000 - rmse: 21137.1250 - val_loss: 578146624.0000 - val_rmse: 24044.6797\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553978368.0000 - rmse: 23536.7441 - val_loss: 785274880.0000 - val_rmse: 28022.7559\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574416320.0000 - rmse: 23966.9844 - val_loss: 401864224.0000 - val_rmse: 20046.5508\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544678016.0000 - rmse: 23338.3379 - val_loss: 436578432.0000 - val_rmse: 20894.4570\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491622880.0000 - rmse: 22172.5703 - val_loss: 843632704.0000 - val_rmse: 29045.3555\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490027776.0000 - rmse: 22136.5703 - val_loss: 648984832.0000 - val_rmse: 25475.1816\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457862944.0000 - rmse: 21397.7324 - val_loss: 1505268864.0000 - val_rmse: 38797.7930\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520840480.0000 - rmse: 22821.9277 - val_loss: 329398784.0000 - val_rmse: 18149.3457\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442513792.0000 - rmse: 21036.0098 - val_loss: 1175155840.0000 - val_rmse: 34280.5469\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407189728.0000 - rmse: 20178.9414 - val_loss: 632023488.0000 - val_rmse: 25140.0781\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453322880.0000 - rmse: 21291.3789 - val_loss: 397914784.0000 - val_rmse: 19947.7988\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483719488.0000 - rmse: 21993.6230 - val_loss: 435825952.0000 - val_rmse: 20876.4434\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496868384.0000 - rmse: 22290.5449 - val_loss: 535528736.0000 - val_rmse: 23141.4922\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485928000.0000 - rmse: 22043.7734 - val_loss: 359812096.0000 - val_rmse: 18968.7129\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525266880.0000 - rmse: 22918.6992 - val_loss: 816164160.0000 - val_rmse: 28568.5879\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409717120.0000 - rmse: 20241.4688 - val_loss: 786683584.0000 - val_rmse: 28047.8809\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460633280.0000 - rmse: 21462.3691 - val_loss: 595280448.0000 - val_rmse: 24398.3691\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485759936.0000 - rmse: 22039.9609 - val_loss: 438547104.0000 - val_rmse: 20941.5156\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431600928.0000 - rmse: 20775.0059 - val_loss: 954142528.0000 - val_rmse: 30889.1973\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443256032.0000 - rmse: 21053.6465 - val_loss: 647292608.0000 - val_rmse: 25441.9434\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454444608.0000 - rmse: 21317.7051 - val_loss: 476198240.0000 - val_rmse: 21821.9668\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411202976.0000 - rmse: 20278.1406 - val_loss: 420156864.0000 - val_rmse: 20497.7285\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411809856.0000 - rmse: 20293.0957 - val_loss: 555817728.0000 - val_rmse: 23575.7871\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358538176.0000 - rmse: 18935.1035 - val_loss: 1316097152.0000 - val_rmse: 36278.0508\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459759392.0000 - rmse: 21442.0000 - val_loss: 397394752.0000 - val_rmse: 19934.7617\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353379584.0000 - rmse: 18798.3926 - val_loss: 749424320.0000 - val_rmse: 27375.6152\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483076768.0000 - rmse: 21979.0059 - val_loss: 716799360.0000 - val_rmse: 26773.1074\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446094112.0000 - rmse: 21120.9395 - val_loss: 1058805248.0000 - val_rmse: 32539.2871\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436255456.0000 - rmse: 20886.7285 - val_loss: 994729536.0000 - val_rmse: 31539.3340\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464593984.0000 - rmse: 21554.4395 - val_loss: 889580864.0000 - val_rmse: 29825.8418\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437952160.0000 - rmse: 20927.3047 - val_loss: 1157091200.0000 - val_rmse: 34016.0430\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456249536.0000 - rmse: 21359.9980 - val_loss: 713036992.0000 - val_rmse: 26702.7520\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315366976.0000 - rmse: 17758.5723 - val_loss: 1798625920.0000 - val_rmse: 42410.2109\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412991488.0000 - rmse: 20322.1914 - val_loss: 370813696.0000 - val_rmse: 19256.5234\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407478528.0000 - rmse: 20186.0957 - val_loss: 368156224.0000 - val_rmse: 19187.3965\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399462752.0000 - rmse: 19986.5645 - val_loss: 776683648.0000 - val_rmse: 27869.0449\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392913920.0000 - rmse: 19822.0566 - val_loss: 637470208.0000 - val_rmse: 25248.1719\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408629120.0000 - rmse: 20214.5762 - val_loss: 871502528.0000 - val_rmse: 29521.2207\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324736352.0000 - rmse: 18020.4414 - val_loss: 1770645376.0000 - val_rmse: 42079.0391\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403426880.0000 - rmse: 20085.4863 - val_loss: 665891840.0000 - val_rmse: 25804.8770\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404779968.0000 - rmse: 20119.1445 - val_loss: 892924544.0000 - val_rmse: 29881.8438\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467433472.0000 - rmse: 21620.2090 - val_loss: 549500224.0000 - val_rmse: 23441.4199\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389879680.0000 - rmse: 19745.3691 - val_loss: 413406208.0000 - val_rmse: 20332.3926\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414981952.0000 - rmse: 20371.1035 - val_loss: 1156657024.0000 - val_rmse: 34009.6602\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374585376.0000 - rmse: 19354.2070 - val_loss: 928109632.0000 - val_rmse: 30464.8926\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374508736.0000 - rmse: 19352.2266 - val_loss: 696316672.0000 - val_rmse: 26387.8105\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355163680.0000 - rmse: 18845.7852 - val_loss: 682984832.0000 - val_rmse: 26133.9766\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378317536.0000 - rmse: 19450.3848 - val_loss: 957328384.0000 - val_rmse: 30940.7246\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402104128.0000 - rmse: 20052.5332 - val_loss: 680574592.0000 - val_rmse: 26087.8242\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334381472.0000 - rmse: 18286.0977 - val_loss: 498998752.0000 - val_rmse: 22338.2793\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376715584.0000 - rmse: 19409.1582 - val_loss: 403696096.0000 - val_rmse: 20092.1875\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376595360.0000 - rmse: 19406.0645 - val_loss: 466923648.0000 - val_rmse: 21608.4141\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357874848.0000 - rmse: 18917.5801 - val_loss: 615976000.0000 - val_rmse: 24818.8613\n",
      "104/104 [==============================] - 0s 732us/step - loss: 427671808.0000 - rmse: 20680.2266\n",
      "[427671808.0, 20680.2265625]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 5153211904.0000 - rmse: 71785.8750 - val_loss: 1224706304.0000 - val_rmse: 34995.8047\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1551827328.0000 - rmse: 39393.2383 - val_loss: 1001295168.0000 - val_rmse: 31643.2480\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1391689472.0000 - rmse: 37305.3555 - val_loss: 1008411392.0000 - val_rmse: 31755.4941\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1276010496.0000 - rmse: 35721.2891 - val_loss: 1182206208.0000 - val_rmse: 34383.2266\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1172141696.0000 - rmse: 34236.5547 - val_loss: 960983488.0000 - val_rmse: 30999.7344\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1161905664.0000 - rmse: 34086.7344 - val_loss: 998868096.0000 - val_rmse: 31604.8750\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1142981760.0000 - rmse: 33808.0117 - val_loss: 771569600.0000 - val_rmse: 27777.1426\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 991684352.0000 - rmse: 31491.0195 - val_loss: 793207872.0000 - val_rmse: 28163.9473\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1015131136.0000 - rmse: 31861.1230 - val_loss: 878243840.0000 - val_rmse: 29635.1797\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1009524160.0000 - rmse: 31773.0098 - val_loss: 714815552.0000 - val_rmse: 26736.0332\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 974861184.0000 - rmse: 31222.7676 - val_loss: 839016384.0000 - val_rmse: 28965.7793\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 904452736.0000 - rmse: 30074.1211 - val_loss: 1336599936.0000 - val_rmse: 36559.5391\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 847266496.0000 - rmse: 29107.8398 - val_loss: 674616576.0000 - val_rmse: 25973.3828\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 859792832.0000 - rmse: 29322.2246 - val_loss: 697532032.0000 - val_rmse: 26410.8320\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 770524864.0000 - rmse: 27758.3301 - val_loss: 643822336.0000 - val_rmse: 25373.6543\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 760171072.0000 - rmse: 27571.1992 - val_loss: 651504256.0000 - val_rmse: 25524.5820\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 866767168.0000 - rmse: 29440.9102 - val_loss: 565493056.0000 - val_rmse: 23780.0957\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733125824.0000 - rmse: 27076.2969 - val_loss: 791993472.0000 - val_rmse: 28142.3789\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 753534016.0000 - rmse: 27450.5742 - val_loss: 567658880.0000 - val_rmse: 23825.5938\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621028544.0000 - rmse: 24920.4434 - val_loss: 624310208.0000 - val_rmse: 24986.2012\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 694961152.0000 - rmse: 26362.1152 - val_loss: 547413632.0000 - val_rmse: 23396.8730\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 682739008.0000 - rmse: 26129.2734 - val_loss: 592538176.0000 - val_rmse: 24342.1074\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 657491904.0000 - rmse: 25641.6035 - val_loss: 531724992.0000 - val_rmse: 23059.1621\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623866304.0000 - rmse: 24977.3164 - val_loss: 809425856.0000 - val_rmse: 28450.4102\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615273728.0000 - rmse: 24804.7109 - val_loss: 466625312.0000 - val_rmse: 21601.5117\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618204224.0000 - rmse: 24863.7129 - val_loss: 632304768.0000 - val_rmse: 25145.6719\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547106304.0000 - rmse: 23390.3027 - val_loss: 506290272.0000 - val_rmse: 22500.8945\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577776320.0000 - rmse: 24036.9785 - val_loss: 523896864.0000 - val_rmse: 22888.7930\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626241664.0000 - rmse: 25024.8203 - val_loss: 1268413952.0000 - val_rmse: 35614.8008\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548997632.0000 - rmse: 23430.6973 - val_loss: 509003680.0000 - val_rmse: 22561.1074\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 600618752.0000 - rmse: 24507.5234 - val_loss: 488799520.0000 - val_rmse: 22108.8105\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481911776.0000 - rmse: 21952.4883 - val_loss: 518556768.0000 - val_rmse: 22771.8418\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453941888.0000 - rmse: 21305.9121 - val_loss: 576262464.0000 - val_rmse: 24005.4668\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 603278080.0000 - rmse: 24561.7188 - val_loss: 657746816.0000 - val_rmse: 25646.5742\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487466656.0000 - rmse: 22078.6465 - val_loss: 510743648.0000 - val_rmse: 22599.6387\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453841152.0000 - rmse: 21303.5488 - val_loss: 611375872.0000 - val_rmse: 24726.0156\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484888416.0000 - rmse: 22020.1797 - val_loss: 923158976.0000 - val_rmse: 30383.5312\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576445824.0000 - rmse: 24009.2852 - val_loss: 486919584.0000 - val_rmse: 22066.2539\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508321312.0000 - rmse: 22545.9805 - val_loss: 592339584.0000 - val_rmse: 24338.0273\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499910624.0000 - rmse: 22358.6797 - val_loss: 544631296.0000 - val_rmse: 23337.3359\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501005824.0000 - rmse: 22383.1582 - val_loss: 586047808.0000 - val_rmse: 24208.4238\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426803168.0000 - rmse: 20659.2148 - val_loss: 485236640.0000 - val_rmse: 22028.0879\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422039904.0000 - rmse: 20543.6074 - val_loss: 587305536.0000 - val_rmse: 24234.3867\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434198720.0000 - rmse: 20837.4355 - val_loss: 612980480.0000 - val_rmse: 24758.4414\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444077824.0000 - rmse: 21073.1523 - val_loss: 1338798336.0000 - val_rmse: 36589.5938\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432036288.0000 - rmse: 20785.4824 - val_loss: 539545600.0000 - val_rmse: 23228.1191\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429362080.0000 - rmse: 20721.0527 - val_loss: 640717632.0000 - val_rmse: 25312.4004\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420210368.0000 - rmse: 20499.0332 - val_loss: 509890816.0000 - val_rmse: 22580.7617\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502934368.0000 - rmse: 22426.1992 - val_loss: 535079648.0000 - val_rmse: 23131.7871\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447461536.0000 - rmse: 21153.2871 - val_loss: 472221120.0000 - val_rmse: 21730.6484\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428293952.0000 - rmse: 20695.2637 - val_loss: 479389792.0000 - val_rmse: 21894.9727\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434972416.0000 - rmse: 20855.9902 - val_loss: 515645088.0000 - val_rmse: 22707.8184\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438093344.0000 - rmse: 20930.6777 - val_loss: 742339200.0000 - val_rmse: 27245.9004\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417607008.0000 - rmse: 20435.4336 - val_loss: 479385984.0000 - val_rmse: 21894.8848\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435985664.0000 - rmse: 20880.2695 - val_loss: 580912576.0000 - val_rmse: 24102.1289\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397187744.0000 - rmse: 19929.5684 - val_loss: 500675552.0000 - val_rmse: 22375.7793\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431508448.0000 - rmse: 20772.7793 - val_loss: 526587072.0000 - val_rmse: 22947.4844\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380788544.0000 - rmse: 19513.8047 - val_loss: 531579072.0000 - val_rmse: 23055.9980\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382647840.0000 - rmse: 19561.3848 - val_loss: 610319360.0000 - val_rmse: 24704.6426\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341451040.0000 - rmse: 18478.3926 - val_loss: 577024704.0000 - val_rmse: 24021.3379\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372128896.0000 - rmse: 19290.6426 - val_loss: 516924064.0000 - val_rmse: 22735.9629\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395413632.0000 - rmse: 19885.0098 - val_loss: 604538432.0000 - val_rmse: 24587.3613\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362757952.0000 - rmse: 19046.2051 - val_loss: 534398560.0000 - val_rmse: 23117.0605\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487452224.0000 - rmse: 22078.3203 - val_loss: 495274496.0000 - val_rmse: 22254.7637\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502221984.0000 - rmse: 22410.3086 - val_loss: 535473312.0000 - val_rmse: 23140.2969\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347321504.0000 - rmse: 18636.5645 - val_loss: 1045519616.0000 - val_rmse: 32334.4922\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399846752.0000 - rmse: 19996.1680 - val_loss: 514556992.0000 - val_rmse: 22683.8496\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376253888.0000 - rmse: 19397.2617 - val_loss: 535501824.0000 - val_rmse: 23140.9121\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363581120.0000 - rmse: 19067.8008 - val_loss: 538827840.0000 - val_rmse: 23212.6660\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335923808.0000 - rmse: 18328.2207 - val_loss: 533995520.0000 - val_rmse: 23108.3438\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372492352.0000 - rmse: 19300.0586 - val_loss: 537773376.0000 - val_rmse: 23189.9395\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344264672.0000 - rmse: 18554.3691 - val_loss: 616817088.0000 - val_rmse: 24835.8027\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464578688.0000 - rmse: 21554.0859 - val_loss: 469645600.0000 - val_rmse: 21671.3066\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383113184.0000 - rmse: 19573.2754 - val_loss: 477729248.0000 - val_rmse: 21857.0176\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393486688.0000 - rmse: 19836.4961 - val_loss: 486545376.0000 - val_rmse: 22057.7734\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387535712.0000 - rmse: 19685.9258 - val_loss: 518571584.0000 - val_rmse: 22772.1660\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336632352.0000 - rmse: 18347.5410 - val_loss: 465298400.0000 - val_rmse: 21570.7754\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332771168.0000 - rmse: 18242.0156 - val_loss: 653333056.0000 - val_rmse: 25560.3809\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393390528.0000 - rmse: 19834.0742 - val_loss: 1159053568.0000 - val_rmse: 34044.8750\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391556896.0000 - rmse: 19787.7969 - val_loss: 569417280.0000 - val_rmse: 23862.4668\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385379680.0000 - rmse: 19631.0879 - val_loss: 462634048.0000 - val_rmse: 21508.9277\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373232576.0000 - rmse: 19319.2266 - val_loss: 470292640.0000 - val_rmse: 21686.2305\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327999872.0000 - rmse: 18110.7656 - val_loss: 875327168.0000 - val_rmse: 29585.9277\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346621376.0000 - rmse: 18617.7695 - val_loss: 464814976.0000 - val_rmse: 21559.5664\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389085376.0000 - rmse: 19725.2461 - val_loss: 689456448.0000 - val_rmse: 26257.5020\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334224064.0000 - rmse: 18281.7949 - val_loss: 901699904.0000 - val_rmse: 30028.3164\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407531712.0000 - rmse: 20187.4121 - val_loss: 422676352.0000 - val_rmse: 20559.0918\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318114272.0000 - rmse: 17835.7578 - val_loss: 503183424.0000 - val_rmse: 22431.7500\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300903264.0000 - rmse: 17346.5625 - val_loss: 555561408.0000 - val_rmse: 23570.3496\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362311936.0000 - rmse: 19034.4941 - val_loss: 549984384.0000 - val_rmse: 23451.7441\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300783584.0000 - rmse: 17343.1113 - val_loss: 432039328.0000 - val_rmse: 20785.5547\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369712864.0000 - rmse: 19227.9180 - val_loss: 737728640.0000 - val_rmse: 27161.1602\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311555232.0000 - rmse: 17650.9258 - val_loss: 499828736.0000 - val_rmse: 22356.8496\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280877824.0000 - rmse: 16759.4062 - val_loss: 552343232.0000 - val_rmse: 23501.9824\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337041536.0000 - rmse: 18358.6895 - val_loss: 442522336.0000 - val_rmse: 21036.2129\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329439456.0000 - rmse: 18150.4648 - val_loss: 523180640.0000 - val_rmse: 22873.1426\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345744448.0000 - rmse: 18594.2031 - val_loss: 523218944.0000 - val_rmse: 22873.9785\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350889408.0000 - rmse: 18732.0391 - val_loss: 495372320.0000 - val_rmse: 22256.9609\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326288704.0000 - rmse: 18063.4609 - val_loss: 534955200.0000 - val_rmse: 23129.0957\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331553824.0000 - rmse: 18208.6191 - val_loss: 472902432.0000 - val_rmse: 21746.3203\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312282784.0000 - rmse: 17671.5234 - val_loss: 486556736.0000 - val_rmse: 22058.0312\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301524160.0000 - rmse: 17364.4492 - val_loss: 430172288.0000 - val_rmse: 20740.5957\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319858368.0000 - rmse: 17884.5840 - val_loss: 540411136.0000 - val_rmse: 23246.7441\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317418912.0000 - rmse: 17816.2539 - val_loss: 413330432.0000 - val_rmse: 20330.5273\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331810816.0000 - rmse: 18215.6738 - val_loss: 410452320.0000 - val_rmse: 20259.6211\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314802624.0000 - rmse: 17742.6777 - val_loss: 535499808.0000 - val_rmse: 23140.8672\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272219456.0000 - rmse: 16499.0703 - val_loss: 427791104.0000 - val_rmse: 20683.1113\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304592768.0000 - rmse: 17452.5840 - val_loss: 461153504.0000 - val_rmse: 21474.4844\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314772064.0000 - rmse: 17741.8145 - val_loss: 507593472.0000 - val_rmse: 22529.8340\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333034912.0000 - rmse: 18249.2422 - val_loss: 485083104.0000 - val_rmse: 22024.5996\n",
      "Epoch 111/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324749600.0000 - rmse: 18020.8105 - val_loss: 482191584.0000 - val_rmse: 21958.8594\n",
      "Epoch 112/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322578624.0000 - rmse: 17960.4707 - val_loss: 712305984.0000 - val_rmse: 26689.0605\n",
      "Epoch 113/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306338048.0000 - rmse: 17502.5137 - val_loss: 486431488.0000 - val_rmse: 22055.1895\n",
      "Epoch 114/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326073472.0000 - rmse: 18057.5020 - val_loss: 633183872.0000 - val_rmse: 25163.1445\n",
      "Epoch 115/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323799168.0000 - rmse: 17994.4160 - val_loss: 431046304.0000 - val_rmse: 20761.6523\n",
      "Epoch 116/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324635584.0000 - rmse: 18017.6426 - val_loss: 512886016.0000 - val_rmse: 22646.9844\n",
      "Epoch 117/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326206592.0000 - rmse: 18061.1875 - val_loss: 415352800.0000 - val_rmse: 20380.2031\n",
      "Epoch 118/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254231472.0000 - rmse: 15944.6357 - val_loss: 489660800.0000 - val_rmse: 22128.2793\n",
      "Epoch 119/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271126752.0000 - rmse: 16465.9238 - val_loss: 492476256.0000 - val_rmse: 22191.8047\n",
      "Epoch 120/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272672352.0000 - rmse: 16512.7930 - val_loss: 536884032.0000 - val_rmse: 23170.7559\n",
      "Epoch 121/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305678464.0000 - rmse: 17483.6621 - val_loss: 492792800.0000 - val_rmse: 22198.9355\n",
      "Epoch 122/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291950592.0000 - rmse: 17086.5625 - val_loss: 470367456.0000 - val_rmse: 21687.9551\n",
      "Epoch 123/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323400096.0000 - rmse: 17983.3262 - val_loss: 464823808.0000 - val_rmse: 21559.7715\n",
      "Epoch 124/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291854912.0000 - rmse: 17083.7598 - val_loss: 466836256.0000 - val_rmse: 21606.3926\n",
      "Epoch 125/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309496128.0000 - rmse: 17592.5000 - val_loss: 422864448.0000 - val_rmse: 20563.6660\n",
      "Epoch 126/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275678560.0000 - rmse: 16603.5684 - val_loss: 688212096.0000 - val_rmse: 26233.7969\n",
      "Epoch 127/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287089664.0000 - rmse: 16943.7168 - val_loss: 459266176.0000 - val_rmse: 21430.4941\n",
      "Epoch 128/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298298624.0000 - rmse: 17271.3203 - val_loss: 494175712.0000 - val_rmse: 22230.0625\n",
      "Epoch 129/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290137760.0000 - rmse: 17033.4277 - val_loss: 517637792.0000 - val_rmse: 22751.6504\n",
      "Epoch 130/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297902688.0000 - rmse: 17259.8555 - val_loss: 532991488.0000 - val_rmse: 23086.6074\n",
      "Epoch 131/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287106784.0000 - rmse: 16944.2227 - val_loss: 477584992.0000 - val_rmse: 21853.7168\n",
      "Epoch 132/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307433536.0000 - rmse: 17533.7812 - val_loss: 432941664.0000 - val_rmse: 20807.2500\n",
      "Epoch 133/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281517728.0000 - rmse: 16778.4863 - val_loss: 420856704.0000 - val_rmse: 20514.7910\n",
      "Epoch 134/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308288384.0000 - rmse: 17558.1406 - val_loss: 474863584.0000 - val_rmse: 21791.3633\n",
      "Epoch 135/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296591776.0000 - rmse: 17221.8398 - val_loss: 512935424.0000 - val_rmse: 22648.0762\n",
      "Epoch 136/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353952544.0000 - rmse: 18813.6230 - val_loss: 663824576.0000 - val_rmse: 25764.7930\n",
      "Epoch 137/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294969472.0000 - rmse: 17174.6738 - val_loss: 475586976.0000 - val_rmse: 21807.9551\n",
      "Epoch 138/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283741152.0000 - rmse: 16844.6172 - val_loss: 532203360.0000 - val_rmse: 23069.5312\n",
      "104/104 [==============================] - 0s 717us/step - loss: 965317952.0000 - rmse: 31069.5645\n",
      "[965317952.0, 31069.564453125]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 7032689152.0000 - rmse: 83861.1328 - val_loss: 1519370240.0000 - val_rmse: 38979.1016\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1545579904.0000 - rmse: 39313.8633 - val_loss: 1198292352.0000 - val_rmse: 34616.3594\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1371560576.0000 - rmse: 37034.5859 - val_loss: 1046545664.0000 - val_rmse: 32350.3574\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1269588224.0000 - rmse: 35631.2812 - val_loss: 967492160.0000 - val_rmse: 31104.5352\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1186852992.0000 - rmse: 34450.7344 - val_loss: 939943936.0000 - val_rmse: 30658.5059\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1073412608.0000 - rmse: 32762.9766 - val_loss: 1214699264.0000 - val_rmse: 34852.5352\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1114157952.0000 - rmse: 33379.0039 - val_loss: 920642624.0000 - val_rmse: 30342.0938\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1069559104.0000 - rmse: 32704.1152 - val_loss: 894655232.0000 - val_rmse: 29910.7871\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1046851136.0000 - rmse: 32355.0781 - val_loss: 1369319936.0000 - val_rmse: 37004.3242\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 988142400.0000 - rmse: 31434.7324 - val_loss: 936357056.0000 - val_rmse: 30599.9512\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1045003456.0000 - rmse: 32326.5137 - val_loss: 1033037824.0000 - val_rmse: 32140.9062\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 934482432.0000 - rmse: 30569.3047 - val_loss: 1411894912.0000 - val_rmse: 37575.1914\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 949003648.0000 - rmse: 30805.9023 - val_loss: 914998272.0000 - val_rmse: 30248.9375\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 925643520.0000 - rmse: 30424.3906 - val_loss: 869383552.0000 - val_rmse: 29485.3105\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 893602304.0000 - rmse: 29893.1816 - val_loss: 1045691712.0000 - val_rmse: 32337.1562\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 952376448.0000 - rmse: 30860.5977 - val_loss: 887346368.0000 - val_rmse: 29788.3594\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 900722752.0000 - rmse: 30012.0430 - val_loss: 1133827328.0000 - val_rmse: 33672.3516\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 878089088.0000 - rmse: 29632.5684 - val_loss: 887413248.0000 - val_rmse: 29789.4824\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 866066752.0000 - rmse: 29429.0117 - val_loss: 1456033536.0000 - val_rmse: 38158.0078\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 946246976.0000 - rmse: 30761.1270 - val_loss: 873344064.0000 - val_rmse: 29552.3945\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 802444544.0000 - rmse: 28327.4512 - val_loss: 960335552.0000 - val_rmse: 30989.2812\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 772653056.0000 - rmse: 27796.6348 - val_loss: 965703616.0000 - val_rmse: 31075.7715\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 830756544.0000 - rmse: 28822.8477 - val_loss: 905144448.0000 - val_rmse: 30085.6172\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 811743936.0000 - rmse: 28491.1211 - val_loss: 885276160.0000 - val_rmse: 29753.5898\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 788980928.0000 - rmse: 28088.8027 - val_loss: 712171072.0000 - val_rmse: 26686.5332\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684190784.0000 - rmse: 26157.0391 - val_loss: 837204032.0000 - val_rmse: 28934.4785\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 746030016.0000 - rmse: 27313.5488 - val_loss: 921446400.0000 - val_rmse: 30355.3359\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 758548160.0000 - rmse: 27541.7539 - val_loss: 984827712.0000 - val_rmse: 31381.9648\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 702984832.0000 - rmse: 26513.8594 - val_loss: 798688704.0000 - val_rmse: 28261.0820\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 686703040.0000 - rmse: 26205.0195 - val_loss: 1065963712.0000 - val_rmse: 32649.0996\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 739309952.0000 - rmse: 27190.2539 - val_loss: 1553150208.0000 - val_rmse: 39410.0273\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 734760704.0000 - rmse: 27106.4688 - val_loss: 1352357120.0000 - val_rmse: 36774.4102\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622967488.0000 - rmse: 24959.3164 - val_loss: 677920960.0000 - val_rmse: 26036.9160\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 700062464.0000 - rmse: 26458.6934 - val_loss: 843147136.0000 - val_rmse: 29036.9961\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 670734144.0000 - rmse: 25898.5352 - val_loss: 1151318528.0000 - val_rmse: 33931.0859\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685416320.0000 - rmse: 26180.4570 - val_loss: 819335744.0000 - val_rmse: 28624.0410\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651130432.0000 - rmse: 25517.2559 - val_loss: 915989568.0000 - val_rmse: 30265.3203\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 678026240.0000 - rmse: 26038.9375 - val_loss: 908016832.0000 - val_rmse: 30133.3184\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609745472.0000 - rmse: 24693.0254 - val_loss: 552450496.0000 - val_rmse: 23504.2637\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606833344.0000 - rmse: 24633.9883 - val_loss: 779067904.0000 - val_rmse: 27911.7871\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582963136.0000 - rmse: 24144.6289 - val_loss: 848658368.0000 - val_rmse: 29131.7422\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 741155840.0000 - rmse: 27224.1777 - val_loss: 898123776.0000 - val_rmse: 29968.7129\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609715712.0000 - rmse: 24692.4219 - val_loss: 753138176.0000 - val_rmse: 27443.3613\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572982400.0000 - rmse: 23937.0508 - val_loss: 727528832.0000 - val_rmse: 26972.7402\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605869568.0000 - rmse: 24614.4141 - val_loss: 694342080.0000 - val_rmse: 26350.3711\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609096960.0000 - rmse: 24679.8887 - val_loss: 584967168.0000 - val_rmse: 24186.0938\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634401280.0000 - rmse: 25187.3223 - val_loss: 822974592.0000 - val_rmse: 28687.5332\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588044224.0000 - rmse: 24249.6230 - val_loss: 729101056.0000 - val_rmse: 27001.8711\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642005760.0000 - rmse: 25337.8320 - val_loss: 903861568.0000 - val_rmse: 30064.2910\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 627235712.0000 - rmse: 25044.6738 - val_loss: 594924224.0000 - val_rmse: 24391.0684\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559735552.0000 - rmse: 23658.7305 - val_loss: 978593600.0000 - val_rmse: 31282.4805\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646499840.0000 - rmse: 25426.3594 - val_loss: 501376800.0000 - val_rmse: 22391.4453\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640412032.0000 - rmse: 25306.3633 - val_loss: 660214592.0000 - val_rmse: 25694.6406\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599592128.0000 - rmse: 24486.5684 - val_loss: 902036288.0000 - val_rmse: 30033.9199\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579769920.0000 - rmse: 24078.4102 - val_loss: 502218336.0000 - val_rmse: 22410.2266\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624391680.0000 - rmse: 24987.8301 - val_loss: 613884544.0000 - val_rmse: 24776.6934\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530408640.0000 - rmse: 23030.5996 - val_loss: 547868608.0000 - val_rmse: 23406.5918\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594354112.0000 - rmse: 24379.3789 - val_loss: 478406144.0000 - val_rmse: 21872.4961\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500917120.0000 - rmse: 22381.1758 - val_loss: 591337792.0000 - val_rmse: 24317.4375\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610900160.0000 - rmse: 24716.3926 - val_loss: 523482368.0000 - val_rmse: 22879.7363\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569427392.0000 - rmse: 23862.6758 - val_loss: 573968192.0000 - val_rmse: 23957.6328\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592965184.0000 - rmse: 24350.8750 - val_loss: 601518720.0000 - val_rmse: 24525.8770\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587328960.0000 - rmse: 24234.8691 - val_loss: 588508416.0000 - val_rmse: 24259.1914\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559059264.0000 - rmse: 23644.4336 - val_loss: 465063200.0000 - val_rmse: 21565.3223\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535892896.0000 - rmse: 23149.3594 - val_loss: 713175168.0000 - val_rmse: 26705.3379\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594320576.0000 - rmse: 24378.6914 - val_loss: 751188992.0000 - val_rmse: 27407.8262\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513119680.0000 - rmse: 22652.1445 - val_loss: 670401728.0000 - val_rmse: 25892.1152\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537419264.0000 - rmse: 23182.3047 - val_loss: 854464256.0000 - val_rmse: 29231.2188\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486692768.0000 - rmse: 22061.1152 - val_loss: 618710784.0000 - val_rmse: 24873.8965\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560992960.0000 - rmse: 23685.2871 - val_loss: 589563264.0000 - val_rmse: 24280.9219\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523672448.0000 - rmse: 22883.8887 - val_loss: 431896256.0000 - val_rmse: 20782.1133\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475064672.0000 - rmse: 21795.9766 - val_loss: 1204506880.0000 - val_rmse: 34706.0039\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524879616.0000 - rmse: 22910.2500 - val_loss: 529631744.0000 - val_rmse: 23013.7285\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513015008.0000 - rmse: 22649.8301 - val_loss: 605702336.0000 - val_rmse: 24611.0195\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438478624.0000 - rmse: 20939.8809 - val_loss: 489090144.0000 - val_rmse: 22115.3828\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479484192.0000 - rmse: 21897.1250 - val_loss: 641207808.0000 - val_rmse: 25322.0781\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534493792.0000 - rmse: 23119.1211 - val_loss: 533968576.0000 - val_rmse: 23107.7598\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478786496.0000 - rmse: 21881.1875 - val_loss: 443923584.0000 - val_rmse: 21069.4922\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506387648.0000 - rmse: 22503.0566 - val_loss: 657467904.0000 - val_rmse: 25641.1348\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472885856.0000 - rmse: 21745.9395 - val_loss: 467944672.0000 - val_rmse: 21632.0273\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524723648.0000 - rmse: 22906.8457 - val_loss: 462075904.0000 - val_rmse: 21495.9492\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429017184.0000 - rmse: 20712.7285 - val_loss: 735682752.0000 - val_rmse: 27123.4707\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501617088.0000 - rmse: 22396.8086 - val_loss: 711274816.0000 - val_rmse: 26669.7363\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484690176.0000 - rmse: 22015.6777 - val_loss: 512576224.0000 - val_rmse: 22640.1465\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480070848.0000 - rmse: 21910.5176 - val_loss: 427450432.0000 - val_rmse: 20674.8730\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470561376.0000 - rmse: 21692.4238 - val_loss: 499233760.0000 - val_rmse: 22343.5371\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478401600.0000 - rmse: 21872.3906 - val_loss: 440372288.0000 - val_rmse: 20985.0488\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394343456.0000 - rmse: 19858.0801 - val_loss: 702428992.0000 - val_rmse: 26503.3730\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474376192.0000 - rmse: 21780.1777 - val_loss: 479330208.0000 - val_rmse: 21893.6094\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506204992.0000 - rmse: 22499.0000 - val_loss: 411399488.0000 - val_rmse: 20282.9805\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470580832.0000 - rmse: 21692.8730 - val_loss: 377055328.0000 - val_rmse: 19417.9102\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474997216.0000 - rmse: 21794.4277 - val_loss: 675916032.0000 - val_rmse: 25998.3848\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455082720.0000 - rmse: 21332.6680 - val_loss: 535777792.0000 - val_rmse: 23146.8750\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453694272.0000 - rmse: 21300.0996 - val_loss: 632311744.0000 - val_rmse: 25145.8105\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539477120.0000 - rmse: 23226.6445 - val_loss: 517329984.0000 - val_rmse: 22744.8887\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522052800.0000 - rmse: 22848.4727 - val_loss: 903397568.0000 - val_rmse: 30056.5723\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466680416.0000 - rmse: 21602.7871 - val_loss: 522630752.0000 - val_rmse: 22861.1172\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410361472.0000 - rmse: 20257.3809 - val_loss: 488738560.0000 - val_rmse: 22107.4316\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451662688.0000 - rmse: 21252.3574 - val_loss: 499755776.0000 - val_rmse: 22355.2168\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435282752.0000 - rmse: 20863.4297 - val_loss: 537237056.0000 - val_rmse: 23178.3750\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462693152.0000 - rmse: 21510.3008 - val_loss: 593350144.0000 - val_rmse: 24358.7793\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459137920.0000 - rmse: 21427.5020 - val_loss: 587005376.0000 - val_rmse: 24228.1934\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437847488.0000 - rmse: 20924.8047 - val_loss: 785819072.0000 - val_rmse: 28032.4648\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427453088.0000 - rmse: 20674.9375 - val_loss: 502750080.0000 - val_rmse: 22422.0879\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474782592.0000 - rmse: 21789.5039 - val_loss: 474954272.0000 - val_rmse: 21793.4434\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466072416.0000 - rmse: 21588.7109 - val_loss: 443815584.0000 - val_rmse: 21066.9297\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439433536.0000 - rmse: 20962.6699 - val_loss: 449235840.0000 - val_rmse: 21195.1836\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478874560.0000 - rmse: 21883.2012 - val_loss: 460600192.0000 - val_rmse: 21461.5957\n",
      "104/104 [==============================] - 0s 671us/step - loss: 976100928.0000 - rmse: 31242.6133\n",
      "[976100928.0, 31242.61328125]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 5379968000.0000 - rmse: 73348.2656 - val_loss: 1213277952.0000 - val_rmse: 34832.1406\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1771189632.0000 - rmse: 42085.5039 - val_loss: 1017365440.0000 - val_rmse: 31896.1660\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1533688832.0000 - rmse: 39162.3398 - val_loss: 1237064320.0000 - val_rmse: 35171.9258\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1458586624.0000 - rmse: 38191.4453 - val_loss: 885899328.0000 - val_rmse: 29764.0605\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1349682688.0000 - rmse: 36738.0273 - val_loss: 940318400.0000 - val_rmse: 30664.6113\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1336865024.0000 - rmse: 36563.1641 - val_loss: 1230174592.0000 - val_rmse: 35073.8438\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1324588160.0000 - rmse: 36394.8906 - val_loss: 885637568.0000 - val_rmse: 29759.6641\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1233023872.0000 - rmse: 35114.4414 - val_loss: 985583616.0000 - val_rmse: 31394.0059\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1237519104.0000 - rmse: 35178.3906 - val_loss: 850660736.0000 - val_rmse: 29166.0879\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1038704128.0000 - rmse: 32228.9336 - val_loss: 1539677824.0000 - val_rmse: 39238.7305\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1121175552.0000 - rmse: 33483.9609 - val_loss: 942415552.0000 - val_rmse: 30698.7871\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1073470592.0000 - rmse: 32763.8613 - val_loss: 937219520.0000 - val_rmse: 30614.0410\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1033822528.0000 - rmse: 32153.1113 - val_loss: 973805440.0000 - val_rmse: 31205.8555\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 999811584.0000 - rmse: 31619.7969 - val_loss: 1316574464.0000 - val_rmse: 36284.6328\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 982582656.0000 - rmse: 31346.1738 - val_loss: 885448192.0000 - val_rmse: 29756.4824\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1013689088.0000 - rmse: 31838.4844 - val_loss: 943001280.0000 - val_rmse: 30708.3262\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 897740672.0000 - rmse: 29962.3203 - val_loss: 918354112.0000 - val_rmse: 30304.3574\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 945433024.0000 - rmse: 30747.8926 - val_loss: 925219200.0000 - val_rmse: 30417.4160\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 883653376.0000 - rmse: 29726.3086 - val_loss: 1089593472.0000 - val_rmse: 33008.9922\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 967891840.0000 - rmse: 31110.9609 - val_loss: 945205056.0000 - val_rmse: 30744.1855\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 894773824.0000 - rmse: 29912.7695 - val_loss: 1087023488.0000 - val_rmse: 32970.0391\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800472576.0000 - rmse: 28292.6250 - val_loss: 992459136.0000 - val_rmse: 31503.3203\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 954136704.0000 - rmse: 30889.1035 - val_loss: 1053759360.0000 - val_rmse: 32461.6602\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 896959872.0000 - rmse: 29949.2891 - val_loss: 1170067968.0000 - val_rmse: 34206.2578\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 896528832.0000 - rmse: 29942.0918 - val_loss: 1120958080.0000 - val_rmse: 33480.7109\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 824402112.0000 - rmse: 28712.4043 - val_loss: 1602358400.0000 - val_rmse: 40029.4688\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 823469248.0000 - rmse: 28696.1543 - val_loss: 2276541184.0000 - val_rmse: 47713.1133\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 791551680.0000 - rmse: 28134.5254 - val_loss: 1440586496.0000 - val_rmse: 37955.0586\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 784324736.0000 - rmse: 28005.7969 - val_loss: 2024234752.0000 - val_rmse: 44991.4961\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 839953152.0000 - rmse: 28981.9453 - val_loss: 1435234304.0000 - val_rmse: 37884.4844\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733671104.0000 - rmse: 27086.3633 - val_loss: 1394800640.0000 - val_rmse: 37347.0312\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 796772416.0000 - rmse: 28227.1582 - val_loss: 1653380224.0000 - val_rmse: 40661.7773\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729870144.0000 - rmse: 27016.1094 - val_loss: 1116873088.0000 - val_rmse: 33419.6484\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622932672.0000 - rmse: 24958.6172 - val_loss: 1589191040.0000 - val_rmse: 39864.6602\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 672874624.0000 - rmse: 25939.8262 - val_loss: 1114507392.0000 - val_rmse: 33384.2383\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 829182528.0000 - rmse: 28795.5273 - val_loss: 1107545984.0000 - val_rmse: 33279.8125\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 658724224.0000 - rmse: 25665.6230 - val_loss: 2406440192.0000 - val_rmse: 49055.4805\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668389888.0000 - rmse: 25853.2383 - val_loss: 1153797504.0000 - val_rmse: 33967.5938\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 712587200.0000 - rmse: 26694.3281 - val_loss: 1952754432.0000 - val_rmse: 44189.9805\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 839740096.0000 - rmse: 28978.2695 - val_loss: 908260160.0000 - val_rmse: 30137.3555\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698522112.0000 - rmse: 26429.5684 - val_loss: 865764800.0000 - val_rmse: 29423.8809\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634126464.0000 - rmse: 25181.8672 - val_loss: 782545280.0000 - val_rmse: 27974.0117\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 793042432.0000 - rmse: 28161.0098 - val_loss: 1097524992.0000 - val_rmse: 33128.9141\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588499776.0000 - rmse: 24259.0137 - val_loss: 1992009984.0000 - val_rmse: 44631.9375\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530246624.0000 - rmse: 23027.0840 - val_loss: 1696337664.0000 - val_rmse: 41186.6211\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594192896.0000 - rmse: 24376.0723 - val_loss: 1481448704.0000 - val_rmse: 38489.5938\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 664763840.0000 - rmse: 25783.0137 - val_loss: 1339642752.0000 - val_rmse: 36601.1289\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561434368.0000 - rmse: 23694.6055 - val_loss: 1508334848.0000 - val_rmse: 38837.2852\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594989824.0000 - rmse: 24392.4121 - val_loss: 2820453888.0000 - val_rmse: 53107.9453\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 660009024.0000 - rmse: 25690.6406 - val_loss: 1260503040.0000 - val_rmse: 35503.5625\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620293440.0000 - rmse: 24905.6914 - val_loss: 1401968512.0000 - val_rmse: 37442.8711\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506279168.0000 - rmse: 22500.6484 - val_loss: 938646720.0000 - val_rmse: 30637.3418\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493197312.0000 - rmse: 22208.0449 - val_loss: 1232232960.0000 - val_rmse: 35103.1758\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486421824.0000 - rmse: 22054.9707 - val_loss: 1655385344.0000 - val_rmse: 40686.4258\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612107456.0000 - rmse: 24740.8047 - val_loss: 1077765376.0000 - val_rmse: 32829.3359\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452579776.0000 - rmse: 21273.9219 - val_loss: 898704896.0000 - val_rmse: 29978.4043\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586126528.0000 - rmse: 24210.0488 - val_loss: 744368128.0000 - val_rmse: 27283.1074\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449692480.0000 - rmse: 21205.9531 - val_loss: 1130201472.0000 - val_rmse: 33618.4688\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504266624.0000 - rmse: 22455.8809 - val_loss: 1561436288.0000 - val_rmse: 39515.0117\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523173696.0000 - rmse: 22872.9883 - val_loss: 1604189824.0000 - val_rmse: 40052.3359\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458845344.0000 - rmse: 21420.6758 - val_loss: 1534612608.0000 - val_rmse: 39174.1328\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 638123072.0000 - rmse: 25261.0977 - val_loss: 1215895296.0000 - val_rmse: 34869.6914\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487286816.0000 - rmse: 22074.5742 - val_loss: 1169930624.0000 - val_rmse: 34204.2500\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466242560.0000 - rmse: 21592.6504 - val_loss: 1348121344.0000 - val_rmse: 36716.7734\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530854240.0000 - rmse: 23040.2734 - val_loss: 1124710016.0000 - val_rmse: 33536.6953\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477896448.0000 - rmse: 21860.8438 - val_loss: 1239155712.0000 - val_rmse: 35201.6445\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656302272.0000 - rmse: 25618.3965 - val_loss: 996340800.0000 - val_rmse: 31564.8672\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435239200.0000 - rmse: 20862.3848 - val_loss: 2484788736.0000 - val_rmse: 49847.6562\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464706528.0000 - rmse: 21557.0527 - val_loss: 1038798592.0000 - val_rmse: 32230.3965\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442806272.0000 - rmse: 21042.9590 - val_loss: 1462311168.0000 - val_rmse: 38240.1758\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562763392.0000 - rmse: 23722.6348 - val_loss: 1305826176.0000 - val_rmse: 36136.2188\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399566784.0000 - rmse: 19989.1641 - val_loss: 832690688.0000 - val_rmse: 28856.3789\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477608640.0000 - rmse: 21854.2578 - val_loss: 1952263424.0000 - val_rmse: 44184.4258\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626320768.0000 - rmse: 25026.4023 - val_loss: 858344384.0000 - val_rmse: 29297.5156\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471504512.0000 - rmse: 21714.1543 - val_loss: 959781632.0000 - val_rmse: 30980.3418\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433789056.0000 - rmse: 20827.6035 - val_loss: 1075446656.0000 - val_rmse: 32794.0039\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450458112.0000 - rmse: 21223.9980 - val_loss: 1124735616.0000 - val_rmse: 33537.0781\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380410752.0000 - rmse: 19504.1191 - val_loss: 1679137536.0000 - val_rmse: 40977.2812\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423526144.0000 - rmse: 20579.7500 - val_loss: 1834532864.0000 - val_rmse: 42831.4453\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415532256.0000 - rmse: 20384.6055 - val_loss: 1542374912.0000 - val_rmse: 39273.0781\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433833120.0000 - rmse: 20828.6602 - val_loss: 1393300352.0000 - val_rmse: 37326.9375\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397641952.0000 - rmse: 19940.9609 - val_loss: 855283264.0000 - val_rmse: 29245.2266\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448448640.0000 - rmse: 21176.6055 - val_loss: 831273728.0000 - val_rmse: 28831.8184\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367039840.0000 - rmse: 19158.2832 - val_loss: 981611968.0000 - val_rmse: 31330.6855\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402628896.0000 - rmse: 20065.6133 - val_loss: 1043204096.0000 - val_rmse: 32298.6699\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358359552.0000 - rmse: 18930.3848 - val_loss: 669636224.0000 - val_rmse: 25877.3281\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399047680.0000 - rmse: 19976.1758 - val_loss: 1548818944.0000 - val_rmse: 39355.0352\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406542016.0000 - rmse: 20162.8848 - val_loss: 993298752.0000 - val_rmse: 31516.6426\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404672384.0000 - rmse: 20116.4688 - val_loss: 811213696.0000 - val_rmse: 28481.8125\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445408128.0000 - rmse: 21104.6953 - val_loss: 1471909248.0000 - val_rmse: 38365.4688\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578363904.0000 - rmse: 24049.1953 - val_loss: 1214897152.0000 - val_rmse: 34855.3750\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453460384.0000 - rmse: 21294.6074 - val_loss: 1003439424.0000 - val_rmse: 31677.1113\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409443680.0000 - rmse: 20234.7129 - val_loss: 1192975488.0000 - val_rmse: 34539.4766\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371441184.0000 - rmse: 19272.8066 - val_loss: 864614912.0000 - val_rmse: 29404.3320\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390159264.0000 - rmse: 19752.4492 - val_loss: 1796878336.0000 - val_rmse: 42389.5977\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384115264.0000 - rmse: 19598.8574 - val_loss: 1671276928.0000 - val_rmse: 40881.2539\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460160320.0000 - rmse: 21451.3477 - val_loss: 1499628544.0000 - val_rmse: 38725.0352\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351678560.0000 - rmse: 18753.0938 - val_loss: 903248768.0000 - val_rmse: 30054.0977\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374560352.0000 - rmse: 19353.5586 - val_loss: 834569152.0000 - val_rmse: 28888.9082\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364140128.0000 - rmse: 19082.4531 - val_loss: 1193768960.0000 - val_rmse: 34550.9609\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366861280.0000 - rmse: 19153.6230 - val_loss: 2399090176.0000 - val_rmse: 48980.5078\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355811360.0000 - rmse: 18862.9609 - val_loss: 1186124800.0000 - val_rmse: 34440.1641\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394956704.0000 - rmse: 19873.5156 - val_loss: 1001940224.0000 - val_rmse: 31653.4395\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358253536.0000 - rmse: 18927.5840 - val_loss: 822825856.0000 - val_rmse: 28684.9395\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360114592.0000 - rmse: 18976.6836 - val_loss: 2253477888.0000 - val_rmse: 47470.8125\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399113632.0000 - rmse: 19977.8281 - val_loss: 1428293760.0000 - val_rmse: 37792.7734\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371231072.0000 - rmse: 19267.3555 - val_loss: 1128536576.0000 - val_rmse: 33593.6992\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375506112.0000 - rmse: 19377.9785 - val_loss: 1337804416.0000 - val_rmse: 36576.0078\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360352928.0000 - rmse: 18982.9629 - val_loss: 969815872.0000 - val_rmse: 31141.8672\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445992224.0000 - rmse: 21118.5273 - val_loss: 1941989248.0000 - val_rmse: 44068.0039\n",
      "Epoch 111/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318634368.0000 - rmse: 17850.3301 - val_loss: 1418374272.0000 - val_rmse: 37661.3086\n",
      "Epoch 112/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405610880.0000 - rmse: 20139.7832 - val_loss: 1134597376.0000 - val_rmse: 33683.7812\n",
      "Epoch 113/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450450464.0000 - rmse: 21223.8184 - val_loss: 640610944.0000 - val_rmse: 25310.2910\n",
      "Epoch 114/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327368800.0000 - rmse: 18093.3340 - val_loss: 1178020608.0000 - val_rmse: 34322.3047\n",
      "Epoch 115/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385495008.0000 - rmse: 19634.0234 - val_loss: 767032576.0000 - val_rmse: 27695.3516\n",
      "Epoch 116/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379500416.0000 - rmse: 19480.7695 - val_loss: 752834048.0000 - val_rmse: 27437.8203\n",
      "Epoch 117/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383372832.0000 - rmse: 19579.9082 - val_loss: 771362624.0000 - val_rmse: 27773.4141\n",
      "Epoch 118/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388756096.0000 - rmse: 19716.8984 - val_loss: 1577802496.0000 - val_rmse: 39721.5625\n",
      "Epoch 119/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361957280.0000 - rmse: 19025.1719 - val_loss: 1187709824.0000 - val_rmse: 34463.1641\n",
      "Epoch 120/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309051776.0000 - rmse: 17579.8672 - val_loss: 1305696768.0000 - val_rmse: 36134.4258\n",
      "Epoch 121/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317203904.0000 - rmse: 17810.2168 - val_loss: 1549726464.0000 - val_rmse: 39366.5664\n",
      "Epoch 122/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294450912.0000 - rmse: 17159.5723 - val_loss: 1817690240.0000 - val_rmse: 42634.3789\n",
      "Epoch 123/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355030304.0000 - rmse: 18842.2461 - val_loss: 686275584.0000 - val_rmse: 26196.8613\n",
      "Epoch 124/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318644288.0000 - rmse: 17850.6074 - val_loss: 840385792.0000 - val_rmse: 28989.4082\n",
      "Epoch 125/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374118048.0000 - rmse: 19342.1309 - val_loss: 732697024.0000 - val_rmse: 27068.3770\n",
      "Epoch 126/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376592224.0000 - rmse: 19405.9824 - val_loss: 814525312.0000 - val_rmse: 28539.8906\n",
      "Epoch 127/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346594112.0000 - rmse: 18617.0352 - val_loss: 621018432.0000 - val_rmse: 24920.2402\n",
      "Epoch 128/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312147808.0000 - rmse: 17667.7031 - val_loss: 939808960.0000 - val_rmse: 30656.3008\n",
      "Epoch 129/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347845792.0000 - rmse: 18650.6230 - val_loss: 853689472.0000 - val_rmse: 29217.9648\n",
      "Epoch 130/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316052192.0000 - rmse: 17777.8555 - val_loss: 911448512.0000 - val_rmse: 30190.2031\n",
      "Epoch 131/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327352992.0000 - rmse: 18092.8984 - val_loss: 928869184.0000 - val_rmse: 30477.3535\n",
      "Epoch 132/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324296960.0000 - rmse: 18008.2441 - val_loss: 735239680.0000 - val_rmse: 27115.3008\n",
      "Epoch 133/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306635424.0000 - rmse: 17511.0059 - val_loss: 763316864.0000 - val_rmse: 27628.1895\n",
      "Epoch 134/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330362720.0000 - rmse: 18175.8828 - val_loss: 615328064.0000 - val_rmse: 24805.8027\n",
      "Epoch 135/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332089248.0000 - rmse: 18223.3145 - val_loss: 1050340992.0000 - val_rmse: 32408.9629\n",
      "Epoch 136/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400892416.0000 - rmse: 20022.2949 - val_loss: 870577344.0000 - val_rmse: 29505.5449\n",
      "Epoch 137/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300743392.0000 - rmse: 17341.9531 - val_loss: 1238217600.0000 - val_rmse: 35188.3125\n",
      "Epoch 138/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350524448.0000 - rmse: 18722.2969 - val_loss: 1307511680.0000 - val_rmse: 36159.5312\n",
      "Epoch 139/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303241536.0000 - rmse: 17413.8301 - val_loss: 638188992.0000 - val_rmse: 25262.3984\n",
      "Epoch 140/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328162720.0000 - rmse: 18115.2617 - val_loss: 587656704.0000 - val_rmse: 24241.6309\n",
      "Epoch 141/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314777888.0000 - rmse: 17741.9805 - val_loss: 1003881664.0000 - val_rmse: 31684.0918\n",
      "Epoch 142/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362028032.0000 - rmse: 19027.0332 - val_loss: 605531328.0000 - val_rmse: 24607.5430\n",
      "104/104 [==============================] - 0s 703us/step - loss: 511335808.0000 - rmse: 22612.7344\n",
      "[511335808.0, 22612.734375]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,577\n",
      "Trainable params: 20,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 7031419392.0000 - rmse: 83853.5625 - val_loss: 1259277824.0000 - val_rmse: 35486.3047\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1827004416.0000 - rmse: 42743.4727 - val_loss: 1023198016.0000 - val_rmse: 31987.4668\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1675368832.0000 - rmse: 40931.2695 - val_loss: 1047294336.0000 - val_rmse: 32361.9277\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1491759360.0000 - rmse: 38623.3008 - val_loss: 921324224.0000 - val_rmse: 30353.3223\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1429152000.0000 - rmse: 37804.1250 - val_loss: 962883840.0000 - val_rmse: 31030.3691\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1329709568.0000 - rmse: 36465.1836 - val_loss: 856153728.0000 - val_rmse: 29260.1055\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1296322560.0000 - rmse: 36004.4805 - val_loss: 869666432.0000 - val_rmse: 29490.1074\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1220225280.0000 - rmse: 34931.7227 - val_loss: 843088064.0000 - val_rmse: 29035.9785\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1176476160.0000 - rmse: 34299.7969 - val_loss: 876353984.0000 - val_rmse: 29603.2773\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1164324864.0000 - rmse: 34122.2031 - val_loss: 1078495104.0000 - val_rmse: 32840.4492\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1077916032.0000 - rmse: 32831.6328 - val_loss: 829041792.0000 - val_rmse: 28793.0859\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1068554432.0000 - rmse: 32688.7500 - val_loss: 979860672.0000 - val_rmse: 31302.7246\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1041262528.0000 - rmse: 32268.5996 - val_loss: 858067136.0000 - val_rmse: 29292.7832\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1008620096.0000 - rmse: 31758.7793 - val_loss: 815622912.0000 - val_rmse: 28559.1094\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 917561856.0000 - rmse: 30291.2832 - val_loss: 868081536.0000 - val_rmse: 29463.2207\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 907982464.0000 - rmse: 30132.7480 - val_loss: 754911744.0000 - val_rmse: 27475.6582\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 953933952.0000 - rmse: 30885.8203 - val_loss: 760984640.0000 - val_rmse: 27585.9492\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 918130880.0000 - rmse: 30300.6738 - val_loss: 840164608.0000 - val_rmse: 28985.5938\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 910126464.0000 - rmse: 30168.3027 - val_loss: 632370880.0000 - val_rmse: 25146.9863\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 904663040.0000 - rmse: 30077.6172 - val_loss: 795739968.0000 - val_rmse: 28208.8633\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 840487168.0000 - rmse: 28991.1543 - val_loss: 616946944.0000 - val_rmse: 24838.4160\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 841590016.0000 - rmse: 29010.1719 - val_loss: 788439808.0000 - val_rmse: 28079.1699\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 814592320.0000 - rmse: 28541.0645 - val_loss: 791077696.0000 - val_rmse: 28126.1035\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 752786752.0000 - rmse: 27436.9590 - val_loss: 675267968.0000 - val_rmse: 25985.9180\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 705099392.0000 - rmse: 26553.7070 - val_loss: 1102246144.0000 - val_rmse: 33200.0938\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 848089088.0000 - rmse: 29121.9688 - val_loss: 690002560.0000 - val_rmse: 26267.9004\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 753791552.0000 - rmse: 27455.2637 - val_loss: 789946688.0000 - val_rmse: 28105.9902\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 748586944.0000 - rmse: 27360.3164 - val_loss: 624058432.0000 - val_rmse: 24981.1621\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 769523200.0000 - rmse: 27740.2812 - val_loss: 663362816.0000 - val_rmse: 25755.8301\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 667621760.0000 - rmse: 25838.3750 - val_loss: 698506112.0000 - val_rmse: 26429.2656\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 688982656.0000 - rmse: 26248.4785 - val_loss: 737577024.0000 - val_rmse: 27158.3672\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628139200.0000 - rmse: 25062.7031 - val_loss: 773472448.0000 - val_rmse: 27811.3711\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 669955456.0000 - rmse: 25883.4980 - val_loss: 581573696.0000 - val_rmse: 24115.8398\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 645393856.0000 - rmse: 25404.6035 - val_loss: 1212998144.0000 - val_rmse: 34828.1211\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 703902720.0000 - rmse: 26531.1621 - val_loss: 1291336192.0000 - val_rmse: 35935.1680\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621572160.0000 - rmse: 24931.3496 - val_loss: 671771712.0000 - val_rmse: 25918.5566\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623559808.0000 - rmse: 24971.1797 - val_loss: 623438208.0000 - val_rmse: 24968.7441\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704834816.0000 - rmse: 26548.7246 - val_loss: 563040832.0000 - val_rmse: 23728.4805\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575183872.0000 - rmse: 23982.9922 - val_loss: 738221248.0000 - val_rmse: 27170.2266\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642540032.0000 - rmse: 25348.3730 - val_loss: 750416448.0000 - val_rmse: 27393.7305\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641205312.0000 - rmse: 25322.0312 - val_loss: 773268288.0000 - val_rmse: 27807.7012\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559498880.0000 - rmse: 23653.7285 - val_loss: 784022848.0000 - val_rmse: 28000.4082\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599599296.0000 - rmse: 24486.7168 - val_loss: 825958784.0000 - val_rmse: 28739.4980\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525839936.0000 - rmse: 22931.1992 - val_loss: 791435520.0000 - val_rmse: 28132.4629\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585913472.0000 - rmse: 24205.6504 - val_loss: 1111150464.0000 - val_rmse: 33333.9219\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647074048.0000 - rmse: 25437.6504 - val_loss: 697926144.0000 - val_rmse: 26418.2910\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473473856.0000 - rmse: 21759.4551 - val_loss: 740978432.0000 - val_rmse: 27220.9199\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487713088.0000 - rmse: 22084.2266 - val_loss: 695065856.0000 - val_rmse: 26364.1016\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587917888.0000 - rmse: 24247.0176 - val_loss: 1120030592.0000 - val_rmse: 33466.8594\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565475904.0000 - rmse: 23779.7363 - val_loss: 718212480.0000 - val_rmse: 26799.4844\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530965184.0000 - rmse: 23042.6797 - val_loss: 805852800.0000 - val_rmse: 28387.5449\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512407648.0000 - rmse: 22636.4238 - val_loss: 808171008.0000 - val_rmse: 28428.3457\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483635008.0000 - rmse: 21991.7031 - val_loss: 690926208.0000 - val_rmse: 26285.4746\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537574208.0000 - rmse: 23185.6465 - val_loss: 636500032.0000 - val_rmse: 25228.9531\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422309856.0000 - rmse: 20550.1758 - val_loss: 905365312.0000 - val_rmse: 30089.2871\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593424384.0000 - rmse: 24360.3027 - val_loss: 861422592.0000 - val_rmse: 29350.0020\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441497568.0000 - rmse: 21011.8418 - val_loss: 856855104.0000 - val_rmse: 29272.0859\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476586976.0000 - rmse: 21830.8711 - val_loss: 671047360.0000 - val_rmse: 25904.5801\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607459328.0000 - rmse: 24646.6895 - val_loss: 839727488.0000 - val_rmse: 28978.0508\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509427200.0000 - rmse: 22570.4922 - val_loss: 739997440.0000 - val_rmse: 27202.8945\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496734752.0000 - rmse: 22287.5469 - val_loss: 809227776.0000 - val_rmse: 28446.9297\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529863680.0000 - rmse: 23018.7656 - val_loss: 612829504.0000 - val_rmse: 24755.3906\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459581376.0000 - rmse: 21437.8496 - val_loss: 800393088.0000 - val_rmse: 28291.2188\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475959232.0000 - rmse: 21816.4883 - val_loss: 886009664.0000 - val_rmse: 29765.9141\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485991968.0000 - rmse: 22045.2246 - val_loss: 666987840.0000 - val_rmse: 25826.1074\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437801376.0000 - rmse: 20923.7031 - val_loss: 1125416576.0000 - val_rmse: 33547.2266\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477082080.0000 - rmse: 21842.2070 - val_loss: 637533888.0000 - val_rmse: 25249.4336\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472429760.0000 - rmse: 21735.4473 - val_loss: 662360064.0000 - val_rmse: 25736.3555\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520298496.0000 - rmse: 22810.0508 - val_loss: 927259520.0000 - val_rmse: 30450.9355\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412190752.0000 - rmse: 20302.4805 - val_loss: 748518272.0000 - val_rmse: 27359.0625\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395909408.0000 - rmse: 19897.4707 - val_loss: 819968192.0000 - val_rmse: 28635.0859\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471754176.0000 - rmse: 21719.9023 - val_loss: 1241754368.0000 - val_rmse: 35238.5352\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449489376.0000 - rmse: 21201.1641 - val_loss: 769612992.0000 - val_rmse: 27741.8965\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388562848.0000 - rmse: 19711.9961 - val_loss: 678245248.0000 - val_rmse: 26043.1387\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488235744.0000 - rmse: 22096.0566 - val_loss: 647415168.0000 - val_rmse: 25444.3535\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379630656.0000 - rmse: 19484.1113 - val_loss: 1477557888.0000 - val_rmse: 38439.0156\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396487136.0000 - rmse: 19911.9844 - val_loss: 776639936.0000 - val_rmse: 27868.2598\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358217632.0000 - rmse: 18926.6367 - val_loss: 837702144.0000 - val_rmse: 28943.0820\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439682240.0000 - rmse: 20968.5996 - val_loss: 718070528.0000 - val_rmse: 26796.8379\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437991040.0000 - rmse: 20928.2363 - val_loss: 685472960.0000 - val_rmse: 26181.5391\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404007168.0000 - rmse: 20099.9277 - val_loss: 731862592.0000 - val_rmse: 27052.9590\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422547296.0000 - rmse: 20555.9531 - val_loss: 1352246144.0000 - val_rmse: 36772.8945\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375707776.0000 - rmse: 19383.1816 - val_loss: 859912320.0000 - val_rmse: 29324.2617\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402190336.0000 - rmse: 20054.6816 - val_loss: 840849152.0000 - val_rmse: 28997.3984\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435323680.0000 - rmse: 20864.4102 - val_loss: 791836928.0000 - val_rmse: 28139.5957\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361727712.0000 - rmse: 19019.1387 - val_loss: 969456704.0000 - val_rmse: 31136.0996\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385391040.0000 - rmse: 19631.3770 - val_loss: 768699968.0000 - val_rmse: 27725.4375\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394481792.0000 - rmse: 19861.5645 - val_loss: 597882752.0000 - val_rmse: 24451.6387\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390138400.0000 - rmse: 19751.9199 - val_loss: 950901824.0000 - val_rmse: 30836.6953\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467021504.0000 - rmse: 21610.6777 - val_loss: 727590400.0000 - val_rmse: 26973.8828\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413589216.0000 - rmse: 20336.8926 - val_loss: 646128320.0000 - val_rmse: 25419.0547\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427952640.0000 - rmse: 20687.0156 - val_loss: 929620800.0000 - val_rmse: 30489.6836\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385158464.0000 - rmse: 19625.4512 - val_loss: 651340672.0000 - val_rmse: 25521.3750\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413098880.0000 - rmse: 20324.8320 - val_loss: 770900800.0000 - val_rmse: 27765.0977\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382799616.0000 - rmse: 19565.2656 - val_loss: 894031104.0000 - val_rmse: 29900.3516\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414557344.0000 - rmse: 20360.6797 - val_loss: 643811328.0000 - val_rmse: 25373.4355\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362132768.0000 - rmse: 19029.7871 - val_loss: 761090112.0000 - val_rmse: 27587.8574\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485887232.0000 - rmse: 22042.8477 - val_loss: 949819776.0000 - val_rmse: 30819.1465\n",
      "104/104 [==============================] - 0s 665us/step - loss: 917943808.0000 - rmse: 30297.5859\n",
      "[917943808.0, 30297.5859375]\n",
      "[20680.2265625, 31069.564453125, 31242.61328125, 22612.734375, 30297.5859375]\n",
      "27180.544921875\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!python train.py kfold baseline\n",
    "#d 0.25 p 20 epoch 300"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 18:42:38.379710: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 18:42:38.379748: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 18:42:38.380136: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 18:42:38.569288: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 6984013312.0000 - rmse: 83570.4062 - val_loss: 1229929088.0000 - val_rmse: 35070.3438\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1883112704.0000 - rmse: 43394.8477 - val_loss: 1100008320.0000 - val_rmse: 33166.3750\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1625083392.0000 - rmse: 40312.3242 - val_loss: 799821568.0000 - val_rmse: 28281.1172\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1508007296.0000 - rmse: 38833.0703 - val_loss: 737019264.0000 - val_rmse: 27148.0996\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1418189312.0000 - rmse: 37658.8555 - val_loss: 702068672.0000 - val_rmse: 26496.5781\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1373565184.0000 - rmse: 37061.6406 - val_loss: 867748288.0000 - val_rmse: 29457.5684\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1313596416.0000 - rmse: 36243.5703 - val_loss: 676329472.0000 - val_rmse: 26006.3359\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1261190144.0000 - rmse: 35513.2383 - val_loss: 885765504.0000 - val_rmse: 29761.8125\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1283743616.0000 - rmse: 35829.3672 - val_loss: 680683072.0000 - val_rmse: 26089.9043\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1310019840.0000 - rmse: 36194.1953 - val_loss: 710060992.0000 - val_rmse: 26646.9688\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1136582016.0000 - rmse: 33713.2305 - val_loss: 673186624.0000 - val_rmse: 25945.8398\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1161112576.0000 - rmse: 34075.1016 - val_loss: 701896960.0000 - val_rmse: 26493.3379\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1128797568.0000 - rmse: 33597.5820 - val_loss: 690951360.0000 - val_rmse: 26285.9531\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1155105920.0000 - rmse: 33986.8477 - val_loss: 697439936.0000 - val_rmse: 26409.0879\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1132654848.0000 - rmse: 33654.9375 - val_loss: 730046912.0000 - val_rmse: 27019.3809\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 919660992.0000 - rmse: 30325.9121 - val_loss: 824885888.0000 - val_rmse: 28720.8262\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1086195584.0000 - rmse: 32957.4805 - val_loss: 619988032.0000 - val_rmse: 24899.5586\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 947196224.0000 - rmse: 30776.5527 - val_loss: 581487936.0000 - val_rmse: 24114.0605\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1087920000.0000 - rmse: 32983.6328 - val_loss: 926429120.0000 - val_rmse: 30437.2988\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1018775168.0000 - rmse: 31918.2578 - val_loss: 615229248.0000 - val_rmse: 24803.8145\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 986528256.0000 - rmse: 31409.0469 - val_loss: 778459648.0000 - val_rmse: 27900.8887\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 882764800.0000 - rmse: 29711.3555 - val_loss: 942651776.0000 - val_rmse: 30702.6328\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 885027200.0000 - rmse: 29749.4043 - val_loss: 667104384.0000 - val_rmse: 25828.3633\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 965566848.0000 - rmse: 31073.5723 - val_loss: 561828224.0000 - val_rmse: 23702.9160\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 966181376.0000 - rmse: 31083.4590 - val_loss: 646505792.0000 - val_rmse: 25426.4766\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 929048512.0000 - rmse: 30480.2969 - val_loss: 499367104.0000 - val_rmse: 22346.5215\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 722363264.0000 - rmse: 26876.8164 - val_loss: 672769984.0000 - val_rmse: 25937.8105\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 914619200.0000 - rmse: 30242.6719 - val_loss: 677876544.0000 - val_rmse: 26036.0625\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733058432.0000 - rmse: 27075.0527 - val_loss: 1087100928.0000 - val_rmse: 32971.2148\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 787073920.0000 - rmse: 28054.8359 - val_loss: 542710976.0000 - val_rmse: 23296.1582\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 881098304.0000 - rmse: 29683.3008 - val_loss: 831474240.0000 - val_rmse: 28835.2949\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 825286656.0000 - rmse: 28727.8008 - val_loss: 540905792.0000 - val_rmse: 23257.3809\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 766178176.0000 - rmse: 27679.9219 - val_loss: 530709600.0000 - val_rmse: 23037.1348\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 722947456.0000 - rmse: 26887.6816 - val_loss: 1721768704.0000 - val_rmse: 41494.1992\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 854562688.0000 - rmse: 29232.9043 - val_loss: 651271424.0000 - val_rmse: 25520.0195\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 699486016.0000 - rmse: 26447.7988 - val_loss: 718590144.0000 - val_rmse: 26806.5312\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 720047232.0000 - rmse: 26833.6953 - val_loss: 858180480.0000 - val_rmse: 29294.7168\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 762654656.0000 - rmse: 27616.2031 - val_loss: 580598336.0000 - val_rmse: 24095.6074\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 663653952.0000 - rmse: 25761.4824 - val_loss: 578529536.0000 - val_rmse: 24052.6387\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 627171328.0000 - rmse: 25043.3887 - val_loss: 630302272.0000 - val_rmse: 25105.8223\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661460864.0000 - rmse: 25718.8809 - val_loss: 569571456.0000 - val_rmse: 23865.6953\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 638283712.0000 - rmse: 25264.2773 - val_loss: 531188448.0000 - val_rmse: 23047.5254\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 690022976.0000 - rmse: 26268.2891 - val_loss: 685355584.0000 - val_rmse: 26179.2969\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649965248.0000 - rmse: 25494.4141 - val_loss: 773709696.0000 - val_rmse: 27815.6367\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584117888.0000 - rmse: 24168.5312 - val_loss: 601550720.0000 - val_rmse: 24526.5293\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 686232128.0000 - rmse: 26196.0332 - val_loss: 735328768.0000 - val_rmse: 27116.9453\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605012672.0000 - rmse: 24597.0059 - val_loss: 519483296.0000 - val_rmse: 22792.1758\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536035744.0000 - rmse: 23152.4414 - val_loss: 856941440.0000 - val_rmse: 29273.5625\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616329280.0000 - rmse: 24825.9805 - val_loss: 762953984.0000 - val_rmse: 27621.6211\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553400768.0000 - rmse: 23524.4727 - val_loss: 531630464.0000 - val_rmse: 23057.1113\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496550624.0000 - rmse: 22283.4160 - val_loss: 370594368.0000 - val_rmse: 19250.8262\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518765152.0000 - rmse: 22776.4160 - val_loss: 420047104.0000 - val_rmse: 20495.0488\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624323904.0000 - rmse: 24986.4746 - val_loss: 617331072.0000 - val_rmse: 24846.1484\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652343808.0000 - rmse: 25541.0215 - val_loss: 894657856.0000 - val_rmse: 29910.8320\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609591744.0000 - rmse: 24689.9102 - val_loss: 679963776.0000 - val_rmse: 26076.1133\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 581115200.0000 - rmse: 24106.3281 - val_loss: 555064256.0000 - val_rmse: 23559.8008\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488981536.0000 - rmse: 22112.9277 - val_loss: 677732416.0000 - val_rmse: 26033.2949\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535188352.0000 - rmse: 23134.1367 - val_loss: 626401536.0000 - val_rmse: 25028.0156\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513168448.0000 - rmse: 22653.2207 - val_loss: 1246131712.0000 - val_rmse: 35300.5898\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491536160.0000 - rmse: 22170.6152 - val_loss: 417620032.0000 - val_rmse: 20435.7520\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538756672.0000 - rmse: 23211.1328 - val_loss: 640571008.0000 - val_rmse: 25309.5039\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474395296.0000 - rmse: 21780.6172 - val_loss: 757739328.0000 - val_rmse: 27527.0645\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585880256.0000 - rmse: 24204.9629 - val_loss: 545799936.0000 - val_rmse: 23362.3594\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448885984.0000 - rmse: 21186.9277 - val_loss: 720137024.0000 - val_rmse: 26835.3672\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500316992.0000 - rmse: 22367.7656 - val_loss: 550496832.0000 - val_rmse: 23462.6680\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425858752.0000 - rmse: 20636.3457 - val_loss: 873739072.0000 - val_rmse: 29559.0781\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489433472.0000 - rmse: 22123.1426 - val_loss: 545647168.0000 - val_rmse: 23359.0898\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434474976.0000 - rmse: 20844.0605 - val_loss: 920883328.0000 - val_rmse: 30346.0586\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437615392.0000 - rmse: 20919.2598 - val_loss: 948811584.0000 - val_rmse: 30802.7852\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460930528.0000 - rmse: 21469.2910 - val_loss: 634578816.0000 - val_rmse: 25190.8457\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427221472.0000 - rmse: 20669.3359 - val_loss: 395319360.0000 - val_rmse: 19882.6387\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454902112.0000 - rmse: 21328.4316 - val_loss: 395232800.0000 - val_rmse: 19880.4629\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432811168.0000 - rmse: 20804.1133 - val_loss: 1015185472.0000 - val_rmse: 31861.9727\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450315040.0000 - rmse: 21220.6270 - val_loss: 762141120.0000 - val_rmse: 27606.9023\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450690720.0000 - rmse: 21229.4766 - val_loss: 586166848.0000 - val_rmse: 24210.8828\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373670720.0000 - rmse: 19330.5625 - val_loss: 836939072.0000 - val_rmse: 28929.8984\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413642944.0000 - rmse: 20338.2129 - val_loss: 499488640.0000 - val_rmse: 22349.2402\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427390624.0000 - rmse: 20673.4277 - val_loss: 410970688.0000 - val_rmse: 20272.4121\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391495200.0000 - rmse: 19786.2383 - val_loss: 335735296.0000 - val_rmse: 18323.0801\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497351360.0000 - rmse: 22301.3750 - val_loss: 317749120.0000 - val_rmse: 17825.5156\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339446336.0000 - rmse: 18424.0684 - val_loss: 1047088064.0000 - val_rmse: 32358.7402\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408433248.0000 - rmse: 20209.7285 - val_loss: 469864448.0000 - val_rmse: 21676.3555\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344238432.0000 - rmse: 18553.6621 - val_loss: 434270432.0000 - val_rmse: 20839.1562\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345117920.0000 - rmse: 18577.3477 - val_loss: 546092032.0000 - val_rmse: 23368.6113\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406157792.0000 - rmse: 20153.3574 - val_loss: 994427392.0000 - val_rmse: 31534.5391\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437342912.0000 - rmse: 20912.7461 - val_loss: 525554432.0000 - val_rmse: 22924.9727\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466159104.0000 - rmse: 21590.7168 - val_loss: 692796800.0000 - val_rmse: 26321.0312\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433577088.0000 - rmse: 20822.5117 - val_loss: 1551158912.0000 - val_rmse: 39384.7539\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437258272.0000 - rmse: 20910.7207 - val_loss: 550820480.0000 - val_rmse: 23469.5625\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382816640.0000 - rmse: 19565.6992 - val_loss: 858734272.0000 - val_rmse: 29304.1680\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406022112.0000 - rmse: 20149.9902 - val_loss: 395414272.0000 - val_rmse: 19885.0234\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375164256.0000 - rmse: 19369.1562 - val_loss: 645466048.0000 - val_rmse: 25406.0234\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435943488.0000 - rmse: 20879.2598 - val_loss: 837727872.0000 - val_rmse: 28943.5293\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410500768.0000 - rmse: 20260.8184 - val_loss: 392816064.0000 - val_rmse: 19819.5879\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385560608.0000 - rmse: 19635.6973 - val_loss: 1062686848.0000 - val_rmse: 32598.8789\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440792320.0000 - rmse: 20995.0527 - val_loss: 671336960.0000 - val_rmse: 25910.1680\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396409376.0000 - rmse: 19910.0293 - val_loss: 610577472.0000 - val_rmse: 24709.8652\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384430176.0000 - rmse: 19606.8906 - val_loss: 893293440.0000 - val_rmse: 29888.0156\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436583744.0000 - rmse: 20894.5840 - val_loss: 669915584.0000 - val_rmse: 25882.7246\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373668576.0000 - rmse: 19330.5078 - val_loss: 431956544.0000 - val_rmse: 20783.5625\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336848288.0000 - rmse: 18353.4238 - val_loss: 1387700992.0000 - val_rmse: 37251.8594\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328719552.0000 - rmse: 18130.6211 - val_loss: 404898720.0000 - val_rmse: 20122.0938\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375752864.0000 - rmse: 19384.3438 - val_loss: 936280640.0000 - val_rmse: 30598.7031\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387485472.0000 - rmse: 19684.6504 - val_loss: 535805760.0000 - val_rmse: 23147.4785\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440160416.0000 - rmse: 20980.0000 - val_loss: 556641088.0000 - val_rmse: 23593.2422\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367355744.0000 - rmse: 19166.5254 - val_loss: 580697664.0000 - val_rmse: 24097.6660\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336928320.0000 - rmse: 18355.6074 - val_loss: 886396544.0000 - val_rmse: 29772.4102\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328604512.0000 - rmse: 18127.4512 - val_loss: 605920256.0000 - val_rmse: 24615.4453\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376879840.0000 - rmse: 19413.3926 - val_loss: 691647104.0000 - val_rmse: 26299.1816\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421687776.0000 - rmse: 20535.0371 - val_loss: 525122304.0000 - val_rmse: 22915.5469\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374448064.0000 - rmse: 19350.6582 - val_loss: 465287232.0000 - val_rmse: 21570.5176\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334286688.0000 - rmse: 18283.5078 - val_loss: 380940736.0000 - val_rmse: 19517.7031\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370744544.0000 - rmse: 19254.7246 - val_loss: 535139040.0000 - val_rmse: 23133.0703\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359475968.0000 - rmse: 18959.8516 - val_loss: 1172478720.0000 - val_rmse: 34241.4766\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305313120.0000 - rmse: 17473.2090 - val_loss: 1068522560.0000 - val_rmse: 32688.2637\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352969920.0000 - rmse: 18787.4902 - val_loss: 1451221120.0000 - val_rmse: 38094.8945\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330088320.0000 - rmse: 18168.3301 - val_loss: 951510912.0000 - val_rmse: 30846.5703\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359675104.0000 - rmse: 18965.0996 - val_loss: 665053888.0000 - val_rmse: 25788.6367\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321410080.0000 - rmse: 17927.9121 - val_loss: 485517152.0000 - val_rmse: 22034.4531\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314017376.0000 - rmse: 17720.5332 - val_loss: 563280448.0000 - val_rmse: 23733.5254\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388531904.0000 - rmse: 19711.2109 - val_loss: 672372992.0000 - val_rmse: 25930.1562\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379311040.0000 - rmse: 19475.9062 - val_loss: 785231488.0000 - val_rmse: 28021.9805\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352857952.0000 - rmse: 18784.5117 - val_loss: 949428096.0000 - val_rmse: 30812.7910\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374858048.0000 - rmse: 19361.2500 - val_loss: 532689184.0000 - val_rmse: 23080.0605\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317033120.0000 - rmse: 17805.4219 - val_loss: 1277572864.0000 - val_rmse: 35743.1484\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314376512.0000 - rmse: 17730.6621 - val_loss: 752505920.0000 - val_rmse: 27431.8398\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366821216.0000 - rmse: 19152.5781 - val_loss: 392886048.0000 - val_rmse: 19821.3516\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353623648.0000 - rmse: 18804.8828 - val_loss: 484977792.0000 - val_rmse: 22022.2109\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338853952.0000 - rmse: 18407.9844 - val_loss: 648037760.0000 - val_rmse: 25456.5859\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299250400.0000 - rmse: 17298.8535 - val_loss: 541925888.0000 - val_rmse: 23279.3008\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335810080.0000 - rmse: 18325.1211 - val_loss: 988710976.0000 - val_rmse: 31443.7754\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310084000.0000 - rmse: 17609.2012 - val_loss: 747116096.0000 - val_rmse: 27333.4238\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301522016.0000 - rmse: 17364.3867 - val_loss: 453914080.0000 - val_rmse: 21305.2578\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293432288.0000 - rmse: 17129.8633 - val_loss: 631607488.0000 - val_rmse: 25131.7988\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274725312.0000 - rmse: 16574.8379 - val_loss: 497873312.0000 - val_rmse: 22313.0742\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317010336.0000 - rmse: 17804.7812 - val_loss: 802569920.0000 - val_rmse: 28329.6641\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315847872.0000 - rmse: 17772.1094 - val_loss: 713050048.0000 - val_rmse: 26702.9961\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330794464.0000 - rmse: 18187.7559 - val_loss: 669485120.0000 - val_rmse: 25874.4102\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285815840.0000 - rmse: 16906.0879 - val_loss: 451354464.0000 - val_rmse: 21245.1016\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311046464.0000 - rmse: 17636.5078 - val_loss: 564096256.0000 - val_rmse: 23750.7090\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327942624.0000 - rmse: 18109.1836 - val_loss: 696684736.0000 - val_rmse: 26394.7852\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326900576.0000 - rmse: 18080.3887 - val_loss: 1433315328.0000 - val_rmse: 37859.1523\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371460128.0000 - rmse: 19273.2988 - val_loss: 574103488.0000 - val_rmse: 23960.4551\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317421696.0000 - rmse: 17816.3262 - val_loss: 564792192.0000 - val_rmse: 23765.3535\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361183936.0000 - rmse: 19004.8379 - val_loss: 1565677056.0000 - val_rmse: 39568.6367\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335068992.0000 - rmse: 18304.8887 - val_loss: 1012547072.0000 - val_rmse: 31820.5449\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340066720.0000 - rmse: 18440.8965 - val_loss: 427123520.0000 - val_rmse: 20666.9648\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379777408.0000 - rmse: 19487.8770 - val_loss: 460046528.0000 - val_rmse: 21448.6934\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315899936.0000 - rmse: 17773.5742 - val_loss: 750312832.0000 - val_rmse: 27391.8379\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314212384.0000 - rmse: 17726.0352 - val_loss: 1273443712.0000 - val_rmse: 35685.3398\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317564512.0000 - rmse: 17820.3359 - val_loss: 804320896.0000 - val_rmse: 28360.5508\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327390848.0000 - rmse: 18093.9434 - val_loss: 775497088.0000 - val_rmse: 27847.7461\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280449504.0000 - rmse: 16746.6250 - val_loss: 732454656.0000 - val_rmse: 27063.8984\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354597088.0000 - rmse: 18830.7461 - val_loss: 597896256.0000 - val_rmse: 24451.9141\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339409920.0000 - rmse: 18423.0801 - val_loss: 1051986048.0000 - val_rmse: 32434.3320\n",
      "104/104 [==============================] - 0s 751us/step - loss: 669259584.0000 - rmse: 25870.0527\n",
      "[669259584.0, 25870.052734375]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 6380062208.0000 - rmse: 79875.2891 - val_loss: 1293010688.0000 - val_rmse: 35958.4570\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1624902528.0000 - rmse: 40310.0781 - val_loss: 1222849024.0000 - val_rmse: 34969.2578\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1383382016.0000 - rmse: 37193.8438 - val_loss: 1018999360.0000 - val_rmse: 31921.7695\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1252410880.0000 - rmse: 35389.4180 - val_loss: 912115392.0000 - val_rmse: 30201.2480\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1200752640.0000 - rmse: 34651.8789 - val_loss: 1162824832.0000 - val_rmse: 34100.2188\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1151137536.0000 - rmse: 33928.4180 - val_loss: 847216064.0000 - val_rmse: 29106.9766\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1170881792.0000 - rmse: 34218.1484 - val_loss: 847529216.0000 - val_rmse: 29112.3555\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1084049152.0000 - rmse: 32924.9023 - val_loss: 901325248.0000 - val_rmse: 30022.0801\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1071771072.0000 - rmse: 32737.9141 - val_loss: 813165440.0000 - val_rmse: 28516.0566\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1124119552.0000 - rmse: 33527.8906 - val_loss: 806826816.0000 - val_rmse: 28404.6973\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 997685696.0000 - rmse: 31586.1621 - val_loss: 1392418944.0000 - val_rmse: 37315.1289\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1027924928.0000 - rmse: 32061.2676 - val_loss: 794480256.0000 - val_rmse: 28186.5254\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1028438400.0000 - rmse: 32069.2754 - val_loss: 792379136.0000 - val_rmse: 28149.2285\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 973249216.0000 - rmse: 31196.9434 - val_loss: 814956032.0000 - val_rmse: 28547.4355\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 975520384.0000 - rmse: 31233.3223 - val_loss: 776306304.0000 - val_rmse: 27862.2734\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 931943040.0000 - rmse: 30527.7422 - val_loss: 735406976.0000 - val_rmse: 27118.3887\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 906780416.0000 - rmse: 30112.7949 - val_loss: 750855872.0000 - val_rmse: 27401.7500\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 947404288.0000 - rmse: 30779.9336 - val_loss: 764024448.0000 - val_rmse: 27640.9922\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 907039744.0000 - rmse: 30117.0996 - val_loss: 745068416.0000 - val_rmse: 27295.9414\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 841301952.0000 - rmse: 29005.2051 - val_loss: 751287296.0000 - val_rmse: 27409.6191\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 743169152.0000 - rmse: 27261.1289 - val_loss: 685282176.0000 - val_rmse: 26177.8945\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 806927552.0000 - rmse: 28406.4707 - val_loss: 644210304.0000 - val_rmse: 25381.2969\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 759987840.0000 - rmse: 27567.8770 - val_loss: 645159936.0000 - val_rmse: 25399.9980\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 795431808.0000 - rmse: 28203.4004 - val_loss: 617799488.0000 - val_rmse: 24855.5703\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 765396800.0000 - rmse: 27665.8047 - val_loss: 896329472.0000 - val_rmse: 29938.7617\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 752141184.0000 - rmse: 27425.1934 - val_loss: 665416192.0000 - val_rmse: 25795.6621\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 689109632.0000 - rmse: 26250.8984 - val_loss: 626091520.0000 - val_rmse: 25021.8203\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 657716544.0000 - rmse: 25645.9844 - val_loss: 587484224.0000 - val_rmse: 24238.0742\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 669086080.0000 - rmse: 25866.6953 - val_loss: 567719552.0000 - val_rmse: 23826.8652\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626033984.0000 - rmse: 25020.6719 - val_loss: 824210944.0000 - val_rmse: 28709.0723\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 595054976.0000 - rmse: 24393.7480 - val_loss: 707255680.0000 - val_rmse: 26594.2793\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 650278848.0000 - rmse: 25500.5664 - val_loss: 578320704.0000 - val_rmse: 24048.2988\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598452928.0000 - rmse: 24463.2969 - val_loss: 545090560.0000 - val_rmse: 23347.1738\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529655616.0000 - rmse: 23014.2480 - val_loss: 621785600.0000 - val_rmse: 24935.6289\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504372672.0000 - rmse: 22458.2422 - val_loss: 573485952.0000 - val_rmse: 23947.5645\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523682976.0000 - rmse: 22884.1191 - val_loss: 556724864.0000 - val_rmse: 23595.0156\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527512512.0000 - rmse: 22967.6387 - val_loss: 558424704.0000 - val_rmse: 23631.0117\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549031296.0000 - rmse: 23431.4160 - val_loss: 559067968.0000 - val_rmse: 23644.6172\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508964224.0000 - rmse: 22560.2363 - val_loss: 523137376.0000 - val_rmse: 22872.1953\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507366784.0000 - rmse: 22524.8047 - val_loss: 616680000.0000 - val_rmse: 24833.0410\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538298944.0000 - rmse: 23201.2695 - val_loss: 551602624.0000 - val_rmse: 23486.2188\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481259776.0000 - rmse: 21937.6328 - val_loss: 602172096.0000 - val_rmse: 24539.1953\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495057024.0000 - rmse: 22249.8770 - val_loss: 517945952.0000 - val_rmse: 22758.4258\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522636352.0000 - rmse: 22861.2422 - val_loss: 537758656.0000 - val_rmse: 23189.6230\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455349376.0000 - rmse: 21338.9160 - val_loss: 532374336.0000 - val_rmse: 23073.2383\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430520640.0000 - rmse: 20748.9922 - val_loss: 560806144.0000 - val_rmse: 23681.3438\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457352736.0000 - rmse: 21385.8066 - val_loss: 541680640.0000 - val_rmse: 23274.0332\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444492352.0000 - rmse: 21082.9863 - val_loss: 544813248.0000 - val_rmse: 23341.2344\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471532320.0000 - rmse: 21714.7930 - val_loss: 586065984.0000 - val_rmse: 24208.7988\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459102112.0000 - rmse: 21426.6680 - val_loss: 621291520.0000 - val_rmse: 24925.7207\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357250848.0000 - rmse: 18901.0781 - val_loss: 624861760.0000 - val_rmse: 24997.2344\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423010432.0000 - rmse: 20567.2168 - val_loss: 797891520.0000 - val_rmse: 28246.9746\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435633440.0000 - rmse: 20871.8340 - val_loss: 522984672.0000 - val_rmse: 22868.8555\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386218048.0000 - rmse: 19652.4297 - val_loss: 905169088.0000 - val_rmse: 30086.0273\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419660160.0000 - rmse: 20485.6074 - val_loss: 771862144.0000 - val_rmse: 27782.4043\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401582656.0000 - rmse: 20039.5254 - val_loss: 578941056.0000 - val_rmse: 24061.1934\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411671968.0000 - rmse: 20289.7012 - val_loss: 527988480.0000 - val_rmse: 22977.9980\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379426912.0000 - rmse: 19478.8828 - val_loss: 508036544.0000 - val_rmse: 22539.6660\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412053248.0000 - rmse: 20299.0938 - val_loss: 565459008.0000 - val_rmse: 23779.3809\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408619968.0000 - rmse: 20214.3496 - val_loss: 545613952.0000 - val_rmse: 23358.3789\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413591520.0000 - rmse: 20336.9492 - val_loss: 619210752.0000 - val_rmse: 24883.9453\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385507552.0000 - rmse: 19634.3457 - val_loss: 643554304.0000 - val_rmse: 25368.3691\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374883808.0000 - rmse: 19361.9141 - val_loss: 566983040.0000 - val_rmse: 23811.4062\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383959776.0000 - rmse: 19594.8906 - val_loss: 639581056.0000 - val_rmse: 25289.9395\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392268064.0000 - rmse: 19805.7578 - val_loss: 861397184.0000 - val_rmse: 29349.5664\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370346720.0000 - rmse: 19244.3945 - val_loss: 598726848.0000 - val_rmse: 24468.8945\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370180000.0000 - rmse: 19240.0605 - val_loss: 563011712.0000 - val_rmse: 23727.8652\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356766688.0000 - rmse: 18888.2656 - val_loss: 911759616.0000 - val_rmse: 30195.3574\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393847072.0000 - rmse: 19845.5801 - val_loss: 610962752.0000 - val_rmse: 24717.6602\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332935584.0000 - rmse: 18246.5215 - val_loss: 616390784.0000 - val_rmse: 24827.2168\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380418592.0000 - rmse: 19504.3223 - val_loss: 579238528.0000 - val_rmse: 24067.3730\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331019744.0000 - rmse: 18193.9453 - val_loss: 637673152.0000 - val_rmse: 25252.1914\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342836512.0000 - rmse: 18515.8418 - val_loss: 638021568.0000 - val_rmse: 25259.0879\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400613568.0000 - rmse: 20015.3301 - val_loss: 537402816.0000 - val_rmse: 23181.9512\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356653792.0000 - rmse: 18885.2793 - val_loss: 606318016.0000 - val_rmse: 24623.5254\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363742880.0000 - rmse: 19072.0430 - val_loss: 494619904.0000 - val_rmse: 22240.0488\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327507808.0000 - rmse: 18097.1758 - val_loss: 569182720.0000 - val_rmse: 23857.5488\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367564768.0000 - rmse: 19171.9766 - val_loss: 537908224.0000 - val_rmse: 23192.8477\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331772800.0000 - rmse: 18214.6289 - val_loss: 804021504.0000 - val_rmse: 28355.2715\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317823744.0000 - rmse: 17827.6094 - val_loss: 649964992.0000 - val_rmse: 25494.4082\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401060672.0000 - rmse: 20026.4980 - val_loss: 603967808.0000 - val_rmse: 24575.7559\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329714240.0000 - rmse: 18158.0352 - val_loss: 559402624.0000 - val_rmse: 23651.6934\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375174304.0000 - rmse: 19369.4160 - val_loss: 541952960.0000 - val_rmse: 23279.8809\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335966432.0000 - rmse: 18329.3867 - val_loss: 520149920.0000 - val_rmse: 22806.7949\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315548704.0000 - rmse: 17763.6895 - val_loss: 1095886720.0000 - val_rmse: 33104.1797\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339349120.0000 - rmse: 18421.4297 - val_loss: 637843776.0000 - val_rmse: 25255.5664\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350470240.0000 - rmse: 18720.8477 - val_loss: 727407360.0000 - val_rmse: 26970.4902\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303607872.0000 - rmse: 17424.3457 - val_loss: 583203840.0000 - val_rmse: 24149.6133\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359640832.0000 - rmse: 18964.1973 - val_loss: 597063040.0000 - val_rmse: 24434.8730\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334727552.0000 - rmse: 18295.5586 - val_loss: 555830016.0000 - val_rmse: 23576.0469\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340735232.0000 - rmse: 18459.0137 - val_loss: 650998912.0000 - val_rmse: 25514.6797\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300665984.0000 - rmse: 17339.7207 - val_loss: 528813696.0000 - val_rmse: 22995.9492\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308759008.0000 - rmse: 17571.5391 - val_loss: 619583488.0000 - val_rmse: 24891.4336\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328536576.0000 - rmse: 18125.5762 - val_loss: 526863616.0000 - val_rmse: 22953.5098\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299466720.0000 - rmse: 17305.1055 - val_loss: 514265856.0000 - val_rmse: 22677.4297\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346906656.0000 - rmse: 18625.4297 - val_loss: 606033216.0000 - val_rmse: 24617.7402\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343206592.0000 - rmse: 18525.8340 - val_loss: 687632320.0000 - val_rmse: 26222.7441\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304487648.0000 - rmse: 17449.5723 - val_loss: 695648832.0000 - val_rmse: 26375.1523\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313267904.0000 - rmse: 17699.3750 - val_loss: 579649024.0000 - val_rmse: 24075.9004\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278235456.0000 - rmse: 16680.3906 - val_loss: 548992768.0000 - val_rmse: 23430.5938\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414094016.0000 - rmse: 20349.2988 - val_loss: 521478432.0000 - val_rmse: 22835.9004\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297703488.0000 - rmse: 17254.0840 - val_loss: 534784832.0000 - val_rmse: 23125.4160\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325782624.0000 - rmse: 18049.4492 - val_loss: 524846112.0000 - val_rmse: 22909.5195\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315151456.0000 - rmse: 17752.5039 - val_loss: 494063040.0000 - val_rmse: 22227.5273\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333830048.0000 - rmse: 18271.0156 - val_loss: 542867264.0000 - val_rmse: 23299.5117\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317267712.0000 - rmse: 17812.0078 - val_loss: 483689792.0000 - val_rmse: 21992.9473\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301205664.0000 - rmse: 17355.2773 - val_loss: 485288416.0000 - val_rmse: 22029.2598\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270157824.0000 - rmse: 16436.4766 - val_loss: 866944384.0000 - val_rmse: 29443.9199\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307175424.0000 - rmse: 17526.4180 - val_loss: 471850080.0000 - val_rmse: 21722.1094\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285596448.0000 - rmse: 16899.5957 - val_loss: 465930400.0000 - val_rmse: 21585.4199\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289827040.0000 - rmse: 17024.3047 - val_loss: 508231744.0000 - val_rmse: 22543.9941\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308978272.0000 - rmse: 17577.7773 - val_loss: 422184000.0000 - val_rmse: 20547.1133\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298492000.0000 - rmse: 17276.9199 - val_loss: 448684096.0000 - val_rmse: 21182.1621\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297003264.0000 - rmse: 17233.7832 - val_loss: 485652608.0000 - val_rmse: 22037.5273\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281504640.0000 - rmse: 16778.0977 - val_loss: 440046080.0000 - val_rmse: 20977.2754\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379405696.0000 - rmse: 19478.3379 - val_loss: 484487136.0000 - val_rmse: 22011.0684\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288538656.0000 - rmse: 16986.4238 - val_loss: 507480512.0000 - val_rmse: 22527.3281\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270266720.0000 - rmse: 16439.7891 - val_loss: 489519808.0000 - val_rmse: 22125.0938\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280301728.0000 - rmse: 16742.2109 - val_loss: 460628736.0000 - val_rmse: 21462.2617\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294462656.0000 - rmse: 17159.9121 - val_loss: 630576512.0000 - val_rmse: 25111.2832\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315088832.0000 - rmse: 17750.7402 - val_loss: 434206080.0000 - val_rmse: 20837.6094\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289099616.0000 - rmse: 17002.9277 - val_loss: 470904672.0000 - val_rmse: 21700.3359\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332428512.0000 - rmse: 18232.6191 - val_loss: 470866944.0000 - val_rmse: 21699.4668\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280157152.0000 - rmse: 16737.8926 - val_loss: 603447616.0000 - val_rmse: 24565.1680\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273963136.0000 - rmse: 16551.8301 - val_loss: 471516736.0000 - val_rmse: 21714.4355\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354579136.0000 - rmse: 18830.2695 - val_loss: 459409824.0000 - val_rmse: 21433.8438\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272764256.0000 - rmse: 16515.5742 - val_loss: 495321792.0000 - val_rmse: 22255.8242\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296841376.0000 - rmse: 17229.0840 - val_loss: 475857856.0000 - val_rmse: 21814.1641\n",
      "104/104 [==============================] - 0s 696us/step - loss: 771458816.0000 - rmse: 27775.1465\n",
      "[771458816.0, 27775.146484375]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 5704500224.0000 - rmse: 75528.1406 - val_loss: 1583425280.0000 - val_rmse: 39792.2773\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1574708352.0000 - rmse: 39682.5938 - val_loss: 1339591296.0000 - val_rmse: 36600.4258\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1294772736.0000 - rmse: 35982.9492 - val_loss: 1091066880.0000 - val_rmse: 33031.3008\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1260369536.0000 - rmse: 35501.6836 - val_loss: 1084820864.0000 - val_rmse: 32936.6172\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1169463552.0000 - rmse: 34197.4219 - val_loss: 966711040.0000 - val_rmse: 31091.9766\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1086886528.0000 - rmse: 32967.9609 - val_loss: 1094938624.0000 - val_rmse: 33089.8555\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1096631168.0000 - rmse: 33115.4219 - val_loss: 916622080.0000 - val_rmse: 30275.7676\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1128642176.0000 - rmse: 33595.2695 - val_loss: 1361811328.0000 - val_rmse: 36902.7266\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1041019968.0000 - rmse: 32264.8418 - val_loss: 905246848.0000 - val_rmse: 30087.3203\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1014348352.0000 - rmse: 31848.8359 - val_loss: 1016603712.0000 - val_rmse: 31884.2227\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 978993472.0000 - rmse: 31288.8711 - val_loss: 1170491008.0000 - val_rmse: 34212.4375\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 996443136.0000 - rmse: 31566.4883 - val_loss: 924602112.0000 - val_rmse: 30407.2695\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 938471552.0000 - rmse: 30634.4805 - val_loss: 2214104064.0000 - val_rmse: 47054.2656\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 971843072.0000 - rmse: 31174.3984 - val_loss: 846680512.0000 - val_rmse: 29097.7754\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 961691072.0000 - rmse: 31011.1445 - val_loss: 823022336.0000 - val_rmse: 28688.3652\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 925131072.0000 - rmse: 30415.9668 - val_loss: 902967616.0000 - val_rmse: 30049.4199\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 917487296.0000 - rmse: 30290.0527 - val_loss: 873113216.0000 - val_rmse: 29548.4883\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 892970432.0000 - rmse: 29882.6113 - val_loss: 1173606784.0000 - val_rmse: 34257.9453\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 901815168.0000 - rmse: 30030.2383 - val_loss: 1083607168.0000 - val_rmse: 32918.1875\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 819781312.0000 - rmse: 28631.8242 - val_loss: 830036736.0000 - val_rmse: 28810.3574\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 882077056.0000 - rmse: 29699.7812 - val_loss: 895669696.0000 - val_rmse: 29927.7422\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 857666048.0000 - rmse: 29285.9355 - val_loss: 943924480.0000 - val_rmse: 30723.3535\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 915573056.0000 - rmse: 30258.4375 - val_loss: 779250944.0000 - val_rmse: 27915.0664\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 876191936.0000 - rmse: 29600.5391 - val_loss: 874919488.0000 - val_rmse: 29579.0352\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 761840128.0000 - rmse: 27601.4512 - val_loss: 815010560.0000 - val_rmse: 28548.3906\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 774734784.0000 - rmse: 27834.0586 - val_loss: 940568704.0000 - val_rmse: 30668.6934\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 761058880.0000 - rmse: 27587.2949 - val_loss: 831318656.0000 - val_rmse: 28832.5977\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 757710336.0000 - rmse: 27526.5391 - val_loss: 776328704.0000 - val_rmse: 27862.6758\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730489408.0000 - rmse: 27027.5645 - val_loss: 1196934272.0000 - val_rmse: 34596.7383\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 770138048.0000 - rmse: 27751.3613 - val_loss: 819287808.0000 - val_rmse: 28623.2031\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 788875520.0000 - rmse: 28086.9277 - val_loss: 784888768.0000 - val_rmse: 28015.8672\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 712999552.0000 - rmse: 26702.0508 - val_loss: 804994304.0000 - val_rmse: 28372.4219\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 709995776.0000 - rmse: 26645.7461 - val_loss: 1767077504.0000 - val_rmse: 42036.6172\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691843968.0000 - rmse: 26302.9258 - val_loss: 921922112.0000 - val_rmse: 30363.1699\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 835110080.0000 - rmse: 28898.2715 - val_loss: 763444416.0000 - val_rmse: 27630.4980\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 675438592.0000 - rmse: 25989.2012 - val_loss: 818025984.0000 - val_rmse: 28601.1543\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 708081536.0000 - rmse: 26609.7988 - val_loss: 699406528.0000 - val_rmse: 26446.2949\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 681764800.0000 - rmse: 26110.6270 - val_loss: 1316429568.0000 - val_rmse: 36282.6328\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698924736.0000 - rmse: 26437.1836 - val_loss: 838430336.0000 - val_rmse: 28955.6602\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642617984.0000 - rmse: 25349.9102 - val_loss: 1164480768.0000 - val_rmse: 34124.4883\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729239936.0000 - rmse: 27004.4434 - val_loss: 583855168.0000 - val_rmse: 24163.0957\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648199168.0000 - rmse: 25459.7559 - val_loss: 806929088.0000 - val_rmse: 28406.4961\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621642176.0000 - rmse: 24932.7539 - val_loss: 1031784256.0000 - val_rmse: 32121.3984\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618801856.0000 - rmse: 24875.7285 - val_loss: 622811904.0000 - val_rmse: 24956.1992\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614014144.0000 - rmse: 24779.3086 - val_loss: 617396928.0000 - val_rmse: 24847.4727\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651970048.0000 - rmse: 25533.7051 - val_loss: 776797312.0000 - val_rmse: 27871.0820\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623148224.0000 - rmse: 24962.9355 - val_loss: 596394176.0000 - val_rmse: 24421.1816\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 617682112.0000 - rmse: 24853.2109 - val_loss: 486345952.0000 - val_rmse: 22053.2520\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576851840.0000 - rmse: 24017.7383 - val_loss: 710484672.0000 - val_rmse: 26654.9180\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552714048.0000 - rmse: 23509.8711 - val_loss: 855674048.0000 - val_rmse: 29251.9062\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574532096.0000 - rmse: 23969.3965 - val_loss: 571583744.0000 - val_rmse: 23907.8164\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544974272.0000 - rmse: 23344.6816 - val_loss: 492380320.0000 - val_rmse: 22189.6445\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562487808.0000 - rmse: 23716.8262 - val_loss: 479110272.0000 - val_rmse: 21888.5879\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538642560.0000 - rmse: 23208.6738 - val_loss: 864686784.0000 - val_rmse: 29405.5566\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522235232.0000 - rmse: 22852.4668 - val_loss: 456459552.0000 - val_rmse: 21364.9141\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537099200.0000 - rmse: 23175.4004 - val_loss: 743669376.0000 - val_rmse: 27270.3027\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543842688.0000 - rmse: 23320.4355 - val_loss: 659662400.0000 - val_rmse: 25683.8926\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523856672.0000 - rmse: 22887.9160 - val_loss: 653199680.0000 - val_rmse: 25557.7715\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 705121408.0000 - rmse: 26554.1211 - val_loss: 837262784.0000 - val_rmse: 28935.4941\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526243200.0000 - rmse: 22939.9922 - val_loss: 663531648.0000 - val_rmse: 25759.1055\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549733312.0000 - rmse: 23446.3926 - val_loss: 698570688.0000 - val_rmse: 26430.4883\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510542592.0000 - rmse: 22595.1895 - val_loss: 440744832.0000 - val_rmse: 20993.9219\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500548256.0000 - rmse: 22372.9355 - val_loss: 778858048.0000 - val_rmse: 27908.0254\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565428352.0000 - rmse: 23778.7363 - val_loss: 580698304.0000 - val_rmse: 24097.6797\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537951744.0000 - rmse: 23193.7871 - val_loss: 548596672.0000 - val_rmse: 23422.1406\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433146944.0000 - rmse: 20812.1816 - val_loss: 1483944960.0000 - val_rmse: 38522.0078\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524096768.0000 - rmse: 22893.1602 - val_loss: 760172224.0000 - val_rmse: 27571.2207\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491870400.0000 - rmse: 22178.1504 - val_loss: 810920128.0000 - val_rmse: 28476.6602\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539800256.0000 - rmse: 23233.5996 - val_loss: 524320544.0000 - val_rmse: 22898.0469\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450530368.0000 - rmse: 21225.7012 - val_loss: 706430784.0000 - val_rmse: 26578.7637\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501839904.0000 - rmse: 22401.7812 - val_loss: 501034656.0000 - val_rmse: 22383.8027\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449364640.0000 - rmse: 21198.2227 - val_loss: 657547776.0000 - val_rmse: 25642.6953\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500666688.0000 - rmse: 22375.5820 - val_loss: 502863776.0000 - val_rmse: 22424.6230\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426012608.0000 - rmse: 20640.0723 - val_loss: 420844384.0000 - val_rmse: 20514.4922\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462654720.0000 - rmse: 21509.4102 - val_loss: 514681440.0000 - val_rmse: 22686.5918\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465666304.0000 - rmse: 21579.3008 - val_loss: 982075136.0000 - val_rmse: 31338.0762\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479928960.0000 - rmse: 21907.2793 - val_loss: 401208288.0000 - val_rmse: 20030.1816\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456742912.0000 - rmse: 21371.5430 - val_loss: 565255936.0000 - val_rmse: 23775.1113\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529733024.0000 - rmse: 23015.9297 - val_loss: 516917696.0000 - val_rmse: 22735.8242\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508827680.0000 - rmse: 22557.2070 - val_loss: 523518112.0000 - val_rmse: 22880.5156\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470877344.0000 - rmse: 21699.7070 - val_loss: 998232512.0000 - val_rmse: 31594.8184\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487475200.0000 - rmse: 22078.8379 - val_loss: 655224192.0000 - val_rmse: 25597.3457\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467445536.0000 - rmse: 21620.4883 - val_loss: 733470784.0000 - val_rmse: 27082.6641\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471229824.0000 - rmse: 21707.8281 - val_loss: 702976000.0000 - val_rmse: 26513.6934\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450035904.0000 - rmse: 21214.0469 - val_loss: 548321792.0000 - val_rmse: 23416.2715\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416573792.0000 - rmse: 20410.1367 - val_loss: 1186737536.0000 - val_rmse: 34449.0586\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445510912.0000 - rmse: 21107.1270 - val_loss: 655330176.0000 - val_rmse: 25599.4160\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408765568.0000 - rmse: 20217.9492 - val_loss: 680940928.0000 - val_rmse: 26094.8438\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486856416.0000 - rmse: 22064.8203 - val_loss: 432253184.0000 - val_rmse: 20790.6992\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488697248.0000 - rmse: 22106.4980 - val_loss: 460005984.0000 - val_rmse: 21447.7500\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439298528.0000 - rmse: 20959.4473 - val_loss: 830325440.0000 - val_rmse: 28815.3672\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461730208.0000 - rmse: 21487.9062 - val_loss: 491349088.0000 - val_rmse: 22166.3945\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470375968.0000 - rmse: 21688.1504 - val_loss: 504474944.0000 - val_rmse: 22460.5195\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407693568.0000 - rmse: 20191.4219 - val_loss: 619176192.0000 - val_rmse: 24883.2480\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409207008.0000 - rmse: 20228.8633 - val_loss: 488768320.0000 - val_rmse: 22108.1055\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424067008.0000 - rmse: 20592.8828 - val_loss: 563319936.0000 - val_rmse: 23734.3613\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415061024.0000 - rmse: 20373.0449 - val_loss: 571736640.0000 - val_rmse: 23911.0137\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424069376.0000 - rmse: 20592.9434 - val_loss: 716047296.0000 - val_rmse: 26759.0586\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346700928.0000 - rmse: 18619.9062 - val_loss: 576751360.0000 - val_rmse: 24015.6465\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449605152.0000 - rmse: 21203.8945 - val_loss: 551289536.0000 - val_rmse: 23479.5547\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374781280.0000 - rmse: 19359.2676 - val_loss: 435338784.0000 - val_rmse: 20864.7715\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415331456.0000 - rmse: 20379.6797 - val_loss: 443763936.0000 - val_rmse: 21065.7031\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388184544.0000 - rmse: 19702.3984 - val_loss: 560232576.0000 - val_rmse: 23669.2305\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444400800.0000 - rmse: 21080.8145 - val_loss: 537469504.0000 - val_rmse: 23183.3867\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367369248.0000 - rmse: 19166.8789 - val_loss: 467140992.0000 - val_rmse: 21613.4453\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458300416.0000 - rmse: 21407.9492 - val_loss: 407095232.0000 - val_rmse: 20176.5996\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418769888.0000 - rmse: 20463.8672 - val_loss: 555008832.0000 - val_rmse: 23558.6250\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373366304.0000 - rmse: 19322.6875 - val_loss: 1531881600.0000 - val_rmse: 39139.2578\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420472512.0000 - rmse: 20505.4219 - val_loss: 552375040.0000 - val_rmse: 23502.6582\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417763648.0000 - rmse: 20439.2637 - val_loss: 537137024.0000 - val_rmse: 23176.2148\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398070496.0000 - rmse: 19951.7031 - val_loss: 658921664.0000 - val_rmse: 25669.4688\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380823616.0000 - rmse: 19514.7012 - val_loss: 477907712.0000 - val_rmse: 21861.0996\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356162784.0000 - rmse: 18872.2754 - val_loss: 405887136.0000 - val_rmse: 20146.6387\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421326592.0000 - rmse: 20526.2402 - val_loss: 445051648.0000 - val_rmse: 21096.2461\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394715936.0000 - rmse: 19867.4590 - val_loss: 423563232.0000 - val_rmse: 20580.6504\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351079936.0000 - rmse: 18737.1270 - val_loss: 441997056.0000 - val_rmse: 21023.7266\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338608224.0000 - rmse: 18401.3086 - val_loss: 532374464.0000 - val_rmse: 23073.2402\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354414528.0000 - rmse: 18825.8984 - val_loss: 911864576.0000 - val_rmse: 30197.0957\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409780672.0000 - rmse: 20243.0371 - val_loss: 367304160.0000 - val_rmse: 19165.1777\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402532224.0000 - rmse: 20063.2031 - val_loss: 486533184.0000 - val_rmse: 22057.4941\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357202560.0000 - rmse: 18899.8008 - val_loss: 390910720.0000 - val_rmse: 19771.4609\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370410752.0000 - rmse: 19246.0566 - val_loss: 364278656.0000 - val_rmse: 19086.0820\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354575776.0000 - rmse: 18830.1816 - val_loss: 581180480.0000 - val_rmse: 24107.6855\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399299488.0000 - rmse: 19982.4785 - val_loss: 421600864.0000 - val_rmse: 20532.9219\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340402944.0000 - rmse: 18450.0098 - val_loss: 407816160.0000 - val_rmse: 20194.4570\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394335264.0000 - rmse: 19857.8750 - val_loss: 462223168.0000 - val_rmse: 21499.3750\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359450048.0000 - rmse: 18959.1660 - val_loss: 552246976.0000 - val_rmse: 23499.9355\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379723040.0000 - rmse: 19486.4805 - val_loss: 458015232.0000 - val_rmse: 21401.2891\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358415456.0000 - rmse: 18931.8613 - val_loss: 371096512.0000 - val_rmse: 19263.8633\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370292128.0000 - rmse: 19242.9746 - val_loss: 898640512.0000 - val_rmse: 29977.3340\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382569280.0000 - rmse: 19559.3770 - val_loss: 400466976.0000 - val_rmse: 20011.6699\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381000416.0000 - rmse: 19519.2305 - val_loss: 826066176.0000 - val_rmse: 28741.3652\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368299488.0000 - rmse: 19191.1289 - val_loss: 458876096.0000 - val_rmse: 21421.3906\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334874688.0000 - rmse: 18299.5801 - val_loss: 414833440.0000 - val_rmse: 20367.4570\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341721888.0000 - rmse: 18485.7188 - val_loss: 435474048.0000 - val_rmse: 20868.0117\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386686016.0000 - rmse: 19664.3320 - val_loss: 1495498624.0000 - val_rmse: 38671.6758\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314117760.0000 - rmse: 17723.3652 - val_loss: 603450624.0000 - val_rmse: 24565.2305\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368110240.0000 - rmse: 19186.1953 - val_loss: 407672928.0000 - val_rmse: 20190.9102\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355571712.0000 - rmse: 18856.6074 - val_loss: 476225664.0000 - val_rmse: 21822.5938\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387788544.0000 - rmse: 19692.3457 - val_loss: 397414528.0000 - val_rmse: 19935.2559\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332157280.0000 - rmse: 18225.1777 - val_loss: 556932224.0000 - val_rmse: 23599.4102\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295505024.0000 - rmse: 17190.2578 - val_loss: 491093120.0000 - val_rmse: 22160.6191\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316301120.0000 - rmse: 17784.8555 - val_loss: 349269504.0000 - val_rmse: 18688.7500\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345363776.0000 - rmse: 18583.9629 - val_loss: 476528288.0000 - val_rmse: 21829.5273\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303572832.0000 - rmse: 17423.3379 - val_loss: 395467840.0000 - val_rmse: 19886.3691\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312482400.0000 - rmse: 17677.1699 - val_loss: 425799968.0000 - val_rmse: 20634.9199\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353996320.0000 - rmse: 18814.7891 - val_loss: 440197888.0000 - val_rmse: 20980.8926\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307953888.0000 - rmse: 17548.6133 - val_loss: 381815968.0000 - val_rmse: 19540.1113\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334211904.0000 - rmse: 18281.4629 - val_loss: 431771904.0000 - val_rmse: 20779.1191\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323134080.0000 - rmse: 17975.9297 - val_loss: 481737888.0000 - val_rmse: 21948.5254\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310869088.0000 - rmse: 17631.4785 - val_loss: 457379584.0000 - val_rmse: 21386.4336\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343699840.0000 - rmse: 18539.1426 - val_loss: 496799968.0000 - val_rmse: 22289.0078\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359305696.0000 - rmse: 18955.3594 - val_loss: 469347744.0000 - val_rmse: 21664.4336\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263211424.0000 - rmse: 16223.7881 - val_loss: 361454304.0000 - val_rmse: 19011.9512\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338445152.0000 - rmse: 18396.8770 - val_loss: 570667264.0000 - val_rmse: 23888.6406\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300562368.0000 - rmse: 17336.7305 - val_loss: 391518880.0000 - val_rmse: 19786.8340\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342578912.0000 - rmse: 18508.8848 - val_loss: 364103744.0000 - val_rmse: 19081.5020\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294881152.0000 - rmse: 17172.1016 - val_loss: 407162048.0000 - val_rmse: 20178.2539\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303625312.0000 - rmse: 17424.8457 - val_loss: 465380832.0000 - val_rmse: 21572.6855\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270828864.0000 - rmse: 16456.8770 - val_loss: 367801312.0000 - val_rmse: 19178.1445\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352337248.0000 - rmse: 18770.6465 - val_loss: 442612608.0000 - val_rmse: 21038.3594\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270934272.0000 - rmse: 16460.0781 - val_loss: 585359616.0000 - val_rmse: 24194.2031\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276494848.0000 - rmse: 16628.1328 - val_loss: 376422656.0000 - val_rmse: 19401.6113\n",
      "Epoch 164/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281129824.0000 - rmse: 16766.9238 - val_loss: 408475584.0000 - val_rmse: 20210.7773\n",
      "Epoch 165/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304575968.0000 - rmse: 17452.1016 - val_loss: 414724704.0000 - val_rmse: 20364.7891\n",
      "Epoch 166/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413380512.0000 - rmse: 20331.7598 - val_loss: 460162688.0000 - val_rmse: 21451.4023\n",
      "Epoch 167/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253331232.0000 - rmse: 15916.3799 - val_loss: 542456704.0000 - val_rmse: 23290.6973\n",
      "Epoch 168/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291357952.0000 - rmse: 17069.2070 - val_loss: 754706752.0000 - val_rmse: 27471.9238\n",
      "Epoch 169/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280957312.0000 - rmse: 16761.7793 - val_loss: 437646016.0000 - val_rmse: 20919.9863\n",
      "Epoch 170/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282828896.0000 - rmse: 16817.5137 - val_loss: 451824544.0000 - val_rmse: 21256.1602\n",
      "Epoch 171/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285748256.0000 - rmse: 16904.0879 - val_loss: 503185120.0000 - val_rmse: 22431.7871\n",
      "Epoch 172/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296536768.0000 - rmse: 17220.2402 - val_loss: 491446208.0000 - val_rmse: 22168.5859\n",
      "Epoch 173/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278874976.0000 - rmse: 16699.5488 - val_loss: 595906048.0000 - val_rmse: 24411.1836\n",
      "Epoch 174/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356934688.0000 - rmse: 18892.7129 - val_loss: 877527104.0000 - val_rmse: 29623.0820\n",
      "Epoch 175/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258421024.0000 - rmse: 16075.4756 - val_loss: 934110976.0000 - val_rmse: 30563.2285\n",
      "Epoch 176/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350746336.0000 - rmse: 18728.2207 - val_loss: 423368672.0000 - val_rmse: 20575.9238\n",
      "Epoch 177/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294565120.0000 - rmse: 17162.8945 - val_loss: 437458944.0000 - val_rmse: 20915.5176\n",
      "Epoch 178/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306138176.0000 - rmse: 17496.8027 - val_loss: 821530432.0000 - val_rmse: 28662.3516\n",
      "Epoch 179/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336029248.0000 - rmse: 18331.0996 - val_loss: 491714816.0000 - val_rmse: 22174.6406\n",
      "Epoch 180/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297054848.0000 - rmse: 17235.2754 - val_loss: 613703424.0000 - val_rmse: 24773.0391\n",
      "Epoch 181/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317907424.0000 - rmse: 17829.9570 - val_loss: 1164899072.0000 - val_rmse: 34130.6172\n",
      "Epoch 182/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265461520.0000 - rmse: 16292.9873 - val_loss: 731270720.0000 - val_rmse: 27042.0176\n",
      "Epoch 183/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308659776.0000 - rmse: 17568.7129 - val_loss: 560819200.0000 - val_rmse: 23681.6211\n",
      "Epoch 184/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311266272.0000 - rmse: 17642.7363 - val_loss: 460709440.0000 - val_rmse: 21464.1387\n",
      "Epoch 185/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304895424.0000 - rmse: 17461.2500 - val_loss: 479920896.0000 - val_rmse: 21907.0957\n",
      "Epoch 186/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304417472.0000 - rmse: 17447.5625 - val_loss: 426660288.0000 - val_rmse: 20655.7520\n",
      "Epoch 187/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305205312.0000 - rmse: 17470.1230 - val_loss: 479895520.0000 - val_rmse: 21906.5156\n",
      "104/104 [==============================] - 0s 694us/step - loss: 808111040.0000 - rmse: 28427.2930\n",
      "[808111040.0, 28427.29296875]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 2s 3ms/step - loss: 6213274112.0000 - rmse: 78824.3281 - val_loss: 1271825152.0000 - val_rmse: 35662.6562\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1729909248.0000 - rmse: 41592.1758 - val_loss: 1014748288.0000 - val_rmse: 31855.1133\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1523029120.0000 - rmse: 39026.0039 - val_loss: 1000311360.0000 - val_rmse: 31627.6992\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1428230528.0000 - rmse: 37791.9375 - val_loss: 863604608.0000 - val_rmse: 29387.1504\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1397140736.0000 - rmse: 37378.3477 - val_loss: 898043264.0000 - val_rmse: 29967.3672\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1303484160.0000 - rmse: 36103.7969 - val_loss: 904191232.0000 - val_rmse: 30069.7734\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1248892544.0000 - rmse: 35339.6719 - val_loss: 839878400.0000 - val_rmse: 28980.6562\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1174969088.0000 - rmse: 34277.8203 - val_loss: 990594496.0000 - val_rmse: 31473.7109\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1234951040.0000 - rmse: 35141.8672 - val_loss: 858665920.0000 - val_rmse: 29303.0020\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1158881152.0000 - rmse: 34042.3438 - val_loss: 917920640.0000 - val_rmse: 30297.2051\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1184787328.0000 - rmse: 34420.7383 - val_loss: 855854272.0000 - val_rmse: 29254.9863\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1017039232.0000 - rmse: 31891.0527 - val_loss: 905127424.0000 - val_rmse: 30085.3359\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1074512000.0000 - rmse: 32779.7500 - val_loss: 825990528.0000 - val_rmse: 28740.0488\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1041464128.0000 - rmse: 32271.7227 - val_loss: 866246784.0000 - val_rmse: 29432.0703\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1003660480.0000 - rmse: 31680.6016 - val_loss: 1092545280.0000 - val_rmse: 33053.6719\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 971606400.0000 - rmse: 31170.6016 - val_loss: 921739200.0000 - val_rmse: 30360.1562\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1069464128.0000 - rmse: 32702.6621 - val_loss: 977082752.0000 - val_rmse: 31258.3223\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 933668160.0000 - rmse: 30555.9844 - val_loss: 927278464.0000 - val_rmse: 30451.2480\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 925199808.0000 - rmse: 30417.0957 - val_loss: 1302313344.0000 - val_rmse: 36087.5781\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 935384896.0000 - rmse: 30584.0625 - val_loss: 852358912.0000 - val_rmse: 29195.1855\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 889481344.0000 - rmse: 29824.1738 - val_loss: 1239012992.0000 - val_rmse: 35199.6172\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 836460032.0000 - rmse: 28921.6172 - val_loss: 833472576.0000 - val_rmse: 28869.9238\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 826727872.0000 - rmse: 28752.8750 - val_loss: 1366925312.0000 - val_rmse: 36971.9531\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 937888320.0000 - rmse: 30624.9629 - val_loss: 1046614144.0000 - val_rmse: 32351.4160\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 860988928.0000 - rmse: 29342.6113 - val_loss: 884627584.0000 - val_rmse: 29742.6895\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 831431168.0000 - rmse: 28834.5488 - val_loss: 915096832.0000 - val_rmse: 30250.5684\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 861355008.0000 - rmse: 29348.8496 - val_loss: 876110528.0000 - val_rmse: 29599.1641\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 712506432.0000 - rmse: 26692.8164 - val_loss: 1286258304.0000 - val_rmse: 35864.4453\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 807532480.0000 - rmse: 28417.1152 - val_loss: 1088233984.0000 - val_rmse: 32988.3906\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 799952704.0000 - rmse: 28283.4336 - val_loss: 1046148800.0000 - val_rmse: 32344.2227\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 886395712.0000 - rmse: 29772.3984 - val_loss: 942693504.0000 - val_rmse: 30703.3145\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 907811264.0000 - rmse: 30129.9062 - val_loss: 1220455424.0000 - val_rmse: 34935.0156\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 869918272.0000 - rmse: 29494.3770 - val_loss: 1211199232.0000 - val_rmse: 34802.2891\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 776699968.0000 - rmse: 27869.3359 - val_loss: 1050210944.0000 - val_rmse: 32406.9590\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 836566784.0000 - rmse: 28923.4629 - val_loss: 1233043840.0000 - val_rmse: 35114.7227\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698358720.0000 - rmse: 26426.4766 - val_loss: 1184945536.0000 - val_rmse: 34423.0391\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 695673152.0000 - rmse: 26375.6172 - val_loss: 1539823744.0000 - val_rmse: 39240.5859\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 633072192.0000 - rmse: 25160.9258 - val_loss: 1112470144.0000 - val_rmse: 33353.7109\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 663622272.0000 - rmse: 25760.8672 - val_loss: 1162959616.0000 - val_rmse: 34102.1914\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 608920576.0000 - rmse: 24676.3164 - val_loss: 1893456384.0000 - val_rmse: 43513.8633\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 756559616.0000 - rmse: 27505.6270 - val_loss: 1191231744.0000 - val_rmse: 34514.2266\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612526144.0000 - rmse: 24749.2656 - val_loss: 1738253696.0000 - val_rmse: 41692.3711\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 679782016.0000 - rmse: 26072.6289 - val_loss: 1442356224.0000 - val_rmse: 37978.3633\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724146624.0000 - rmse: 26909.9727 - val_loss: 950537472.0000 - val_rmse: 30830.7871\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 752455808.0000 - rmse: 27430.9277 - val_loss: 949645696.0000 - val_rmse: 30816.3223\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729796672.0000 - rmse: 27014.7500 - val_loss: 1047304960.0000 - val_rmse: 32362.0918\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 715341120.0000 - rmse: 26745.8613 - val_loss: 1201022720.0000 - val_rmse: 34655.7734\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579971456.0000 - rmse: 24082.5957 - val_loss: 748806464.0000 - val_rmse: 27364.3281\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529627872.0000 - rmse: 23013.6445 - val_loss: 1051393920.0000 - val_rmse: 32425.2051\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563767168.0000 - rmse: 23743.7812 - val_loss: 1147988736.0000 - val_rmse: 33881.9844\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 709813824.0000 - rmse: 26642.3320 - val_loss: 1465874176.0000 - val_rmse: 38286.7344\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569913472.0000 - rmse: 23872.8594 - val_loss: 1959197696.0000 - val_rmse: 44262.8242\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642565568.0000 - rmse: 25348.8750 - val_loss: 1731597696.0000 - val_rmse: 41612.4688\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 678463680.0000 - rmse: 26047.3340 - val_loss: 1091256192.0000 - val_rmse: 33034.1641\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 636173760.0000 - rmse: 25222.4844 - val_loss: 762350464.0000 - val_rmse: 27610.6953\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594385024.0000 - rmse: 24380.0098 - val_loss: 751749696.0000 - val_rmse: 27418.0547\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652982400.0000 - rmse: 25553.5195 - val_loss: 1369488384.0000 - val_rmse: 37006.5977\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569982272.0000 - rmse: 23874.3008 - val_loss: 1146516096.0000 - val_rmse: 33860.2422\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 581657600.0000 - rmse: 24117.5781 - val_loss: 1034419904.0000 - val_rmse: 32162.3984\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579029120.0000 - rmse: 24063.0234 - val_loss: 1055944640.0000 - val_rmse: 32495.3008\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524088512.0000 - rmse: 22892.9785 - val_loss: 991240448.0000 - val_rmse: 31483.9688\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525595904.0000 - rmse: 22925.8789 - val_loss: 929140352.0000 - val_rmse: 30481.8008\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593623488.0000 - rmse: 24364.3906 - val_loss: 1019983552.0000 - val_rmse: 31937.1797\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 608653760.0000 - rmse: 24670.9082 - val_loss: 903690944.0000 - val_rmse: 30061.4512\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527462464.0000 - rmse: 22966.5469 - val_loss: 846689088.0000 - val_rmse: 29097.9199\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543594752.0000 - rmse: 23315.1172 - val_loss: 2192077056.0000 - val_rmse: 46819.6211\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634206464.0000 - rmse: 25183.4570 - val_loss: 1232621696.0000 - val_rmse: 35108.7109\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533289056.0000 - rmse: 23093.0508 - val_loss: 2082574080.0000 - val_rmse: 45635.2266\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521551872.0000 - rmse: 22837.5098 - val_loss: 1070256640.0000 - val_rmse: 32714.7773\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598270912.0000 - rmse: 24459.5742 - val_loss: 1585142400.0000 - val_rmse: 39813.8438\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497123104.0000 - rmse: 22296.2578 - val_loss: 835820096.0000 - val_rmse: 28910.5527\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563419776.0000 - rmse: 23736.4629 - val_loss: 1575321728.0000 - val_rmse: 39690.3242\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648663168.0000 - rmse: 25468.8652 - val_loss: 994332608.0000 - val_rmse: 31533.0391\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542709504.0000 - rmse: 23296.1250 - val_loss: 1260959104.0000 - val_rmse: 35509.9805\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 581039680.0000 - rmse: 24104.7656 - val_loss: 1026280704.0000 - val_rmse: 32035.6152\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440540608.0000 - rmse: 20989.0586 - val_loss: 2213410560.0000 - val_rmse: 47046.8984\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583126976.0000 - rmse: 24148.0215 - val_loss: 1468997760.0000 - val_rmse: 38327.5078\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562505984.0000 - rmse: 23717.2070 - val_loss: 1535348864.0000 - val_rmse: 39183.5273\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487428032.0000 - rmse: 22077.7715 - val_loss: 2175950336.0000 - val_rmse: 46647.0820\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 643828416.0000 - rmse: 25373.7734 - val_loss: 1270029056.0000 - val_rmse: 35637.4688\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548468096.0000 - rmse: 23419.3926 - val_loss: 779143744.0000 - val_rmse: 27913.1465\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463272704.0000 - rmse: 21523.7715 - val_loss: 807396544.0000 - val_rmse: 28414.7227\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485693408.0000 - rmse: 22038.4531 - val_loss: 956995200.0000 - val_rmse: 30935.3398\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509312512.0000 - rmse: 22567.9512 - val_loss: 944130304.0000 - val_rmse: 30726.7031\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477460480.0000 - rmse: 21850.8672 - val_loss: 780846400.0000 - val_rmse: 27943.6250\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498852192.0000 - rmse: 22334.9980 - val_loss: 1510004352.0000 - val_rmse: 38858.7734\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441358784.0000 - rmse: 21008.5391 - val_loss: 2056564352.0000 - val_rmse: 45349.3594\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537236800.0000 - rmse: 23178.3652 - val_loss: 1005028992.0000 - val_rmse: 31702.1895\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460670752.0000 - rmse: 21463.2402 - val_loss: 1401536768.0000 - val_rmse: 37437.1016\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464379296.0000 - rmse: 21549.4590 - val_loss: 866800640.0000 - val_rmse: 29441.4785\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434099168.0000 - rmse: 20835.0449 - val_loss: 1235864064.0000 - val_rmse: 35154.8594\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474398912.0000 - rmse: 21780.6992 - val_loss: 1417336832.0000 - val_rmse: 37647.5352\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462129888.0000 - rmse: 21497.2051 - val_loss: 936502848.0000 - val_rmse: 30602.3320\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487272224.0000 - rmse: 22074.2402 - val_loss: 2018617984.0000 - val_rmse: 44929.0312\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472049824.0000 - rmse: 21726.7070 - val_loss: 1908752512.0000 - val_rmse: 43689.2734\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439191712.0000 - rmse: 20956.9004 - val_loss: 1037764736.0000 - val_rmse: 32214.3555\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422474144.0000 - rmse: 20554.1738 - val_loss: 1606886400.0000 - val_rmse: 40085.9883\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403878656.0000 - rmse: 20096.7305 - val_loss: 1709024640.0000 - val_rmse: 41340.3516\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397321088.0000 - rmse: 19932.9121 - val_loss: 1372591744.0000 - val_rmse: 37048.5039\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420399552.0000 - rmse: 20503.6445 - val_loss: 1090444928.0000 - val_rmse: 33021.8867\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513396032.0000 - rmse: 22658.2402 - val_loss: 1098076416.0000 - val_rmse: 33137.2344\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432859744.0000 - rmse: 20805.2793 - val_loss: 1095569408.0000 - val_rmse: 33099.3828\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405248992.0000 - rmse: 20130.7969 - val_loss: 1221360896.0000 - val_rmse: 34947.9688\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452701248.0000 - rmse: 21276.7773 - val_loss: 1600397312.0000 - val_rmse: 40004.9648\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478174272.0000 - rmse: 21867.1953 - val_loss: 918207616.0000 - val_rmse: 30301.9414\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483309728.0000 - rmse: 21984.3047 - val_loss: 1497991168.0000 - val_rmse: 38703.8867\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394963328.0000 - rmse: 19873.6816 - val_loss: 938448960.0000 - val_rmse: 30634.1113\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405190016.0000 - rmse: 20129.3301 - val_loss: 1296659456.0000 - val_rmse: 36009.1562\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566970432.0000 - rmse: 23811.1387 - val_loss: 990425152.0000 - val_rmse: 31471.0215\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393809568.0000 - rmse: 19844.6328 - val_loss: 1159238144.0000 - val_rmse: 34047.5859\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416946208.0000 - rmse: 20419.2598 - val_loss: 1415520000.0000 - val_rmse: 37623.3984\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387450176.0000 - rmse: 19683.7520 - val_loss: 814678400.0000 - val_rmse: 28542.5703\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399791872.0000 - rmse: 19994.7969 - val_loss: 1023524288.0000 - val_rmse: 31992.5645\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418183776.0000 - rmse: 20449.5391 - val_loss: 1218637568.0000 - val_rmse: 34908.9883\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397875488.0000 - rmse: 19946.8164 - val_loss: 937434944.0000 - val_rmse: 30617.5566\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410473856.0000 - rmse: 20260.1523 - val_loss: 1016432000.0000 - val_rmse: 31881.5293\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405527456.0000 - rmse: 20137.7090 - val_loss: 1677196928.0000 - val_rmse: 40953.5938\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387946240.0000 - rmse: 19696.3496 - val_loss: 2409978880.0000 - val_rmse: 49091.5352\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414911616.0000 - rmse: 20369.3770 - val_loss: 935709632.0000 - val_rmse: 30589.3711\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376389408.0000 - rmse: 19400.7559 - val_loss: 1641762688.0000 - val_rmse: 40518.6719\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491915136.0000 - rmse: 22179.1602 - val_loss: 1081275392.0000 - val_rmse: 32882.7539\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368261792.0000 - rmse: 19190.1484 - val_loss: 857107968.0000 - val_rmse: 29276.4023\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403320768.0000 - rmse: 20082.8477 - val_loss: 1182252416.0000 - val_rmse: 34383.8984\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376243104.0000 - rmse: 19396.9844 - val_loss: 1449477120.0000 - val_rmse: 38072.0000\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431795232.0000 - rmse: 20779.6816 - val_loss: 1900572288.0000 - val_rmse: 43595.5508\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364749280.0000 - rmse: 19098.4082 - val_loss: 1231158272.0000 - val_rmse: 35087.8633\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343351072.0000 - rmse: 18529.7344 - val_loss: 1765722752.0000 - val_rmse: 42020.5039\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353716512.0000 - rmse: 18807.3496 - val_loss: 895074432.0000 - val_rmse: 29917.7949\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371804608.0000 - rmse: 19282.2344 - val_loss: 2355642112.0000 - val_rmse: 48534.9531\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364474720.0000 - rmse: 19091.2188 - val_loss: 1278074880.0000 - val_rmse: 35750.1719\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349258336.0000 - rmse: 18688.4512 - val_loss: 2277722880.0000 - val_rmse: 47725.4922\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378188224.0000 - rmse: 19447.0586 - val_loss: 752636928.0000 - val_rmse: 27434.2285\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334586080.0000 - rmse: 18291.6934 - val_loss: 2049819904.0000 - val_rmse: 45274.9375\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342910912.0000 - rmse: 18517.8516 - val_loss: 924678848.0000 - val_rmse: 30408.5332\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388961408.0000 - rmse: 19722.1035 - val_loss: 980270144.0000 - val_rmse: 31309.2656\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386901120.0000 - rmse: 19669.7988 - val_loss: 1192089344.0000 - val_rmse: 34526.6445\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332966304.0000 - rmse: 18247.3652 - val_loss: 1054738432.0000 - val_rmse: 32476.7344\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324030528.0000 - rmse: 18000.8457 - val_loss: 2448668160.0000 - val_rmse: 49484.0195\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399159136.0000 - rmse: 19978.9648 - val_loss: 1013673344.0000 - val_rmse: 31838.2344\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332463328.0000 - rmse: 18233.5762 - val_loss: 1063660992.0000 - val_rmse: 32613.8125\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343263232.0000 - rmse: 18527.3613 - val_loss: 872540160.0000 - val_rmse: 29538.7891\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383649664.0000 - rmse: 19586.9746 - val_loss: 1718724480.0000 - val_rmse: 41457.5039\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345166528.0000 - rmse: 18578.6562 - val_loss: 851411648.0000 - val_rmse: 29178.9590\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359794944.0000 - rmse: 18968.2598 - val_loss: 1026243776.0000 - val_rmse: 32035.0391\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338029376.0000 - rmse: 18385.5742 - val_loss: 1296683648.0000 - val_rmse: 36009.4922\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370720960.0000 - rmse: 19254.1133 - val_loss: 1054762688.0000 - val_rmse: 32477.1094\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335505664.0000 - rmse: 18316.8145 - val_loss: 1523646336.0000 - val_rmse: 39033.9102\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296212416.0000 - rmse: 17210.8203 - val_loss: 1104038400.0000 - val_rmse: 33227.0742\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319632768.0000 - rmse: 17878.2754 - val_loss: 1666680320.0000 - val_rmse: 40824.9961\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361509504.0000 - rmse: 19013.4023 - val_loss: 814674112.0000 - val_rmse: 28542.4961\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335149984.0000 - rmse: 18307.0996 - val_loss: 1687043200.0000 - val_rmse: 41073.6328\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337060288.0000 - rmse: 18359.1992 - val_loss: 1091393152.0000 - val_rmse: 33036.2383\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305583136.0000 - rmse: 17480.9336 - val_loss: 2548001280.0000 - val_rmse: 50477.7305\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325248992.0000 - rmse: 18034.6582 - val_loss: 2094181632.0000 - val_rmse: 45762.2305\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335558944.0000 - rmse: 18318.2656 - val_loss: 1689688064.0000 - val_rmse: 41105.8164\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325874592.0000 - rmse: 18051.9941 - val_loss: 2510898432.0000 - val_rmse: 50108.8672\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335159872.0000 - rmse: 18307.3691 - val_loss: 2630678016.0000 - val_rmse: 51290.1367\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348799072.0000 - rmse: 18676.1621 - val_loss: 1515620992.0000 - val_rmse: 38930.9766\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316349216.0000 - rmse: 17786.2051 - val_loss: 2129336576.0000 - val_rmse: 46144.7344\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325619232.0000 - rmse: 18044.9199 - val_loss: 874697344.0000 - val_rmse: 29575.2812\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334182080.0000 - rmse: 18280.6465 - val_loss: 880139968.0000 - val_rmse: 29667.1523\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386330400.0000 - rmse: 19655.2871 - val_loss: 1007023936.0000 - val_rmse: 31733.6406\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331992576.0000 - rmse: 18220.6621 - val_loss: 874743104.0000 - val_rmse: 29576.0566\n",
      "Epoch 164/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295241888.0000 - rmse: 17182.6016 - val_loss: 705390912.0000 - val_rmse: 26559.1934\n",
      "Epoch 165/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338427072.0000 - rmse: 18396.3848 - val_loss: 1566229888.0000 - val_rmse: 39575.6211\n",
      "Epoch 166/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338641664.0000 - rmse: 18402.2148 - val_loss: 813563328.0000 - val_rmse: 28523.0312\n",
      "Epoch 167/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334137760.0000 - rmse: 18279.4316 - val_loss: 840829824.0000 - val_rmse: 28997.0664\n",
      "Epoch 168/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326394752.0000 - rmse: 18066.3984 - val_loss: 760758720.0000 - val_rmse: 27581.8535\n",
      "Epoch 169/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334016256.0000 - rmse: 18276.1113 - val_loss: 1183668480.0000 - val_rmse: 34404.4844\n",
      "Epoch 170/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437676288.0000 - rmse: 20920.7109 - val_loss: 1101927552.0000 - val_rmse: 33195.2930\n",
      "Epoch 171/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300400320.0000 - rmse: 17332.0586 - val_loss: 1252786304.0000 - val_rmse: 35394.7188\n",
      "Epoch 172/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312392768.0000 - rmse: 17674.6348 - val_loss: 1326276224.0000 - val_rmse: 36418.0742\n",
      "Epoch 173/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290934176.0000 - rmse: 17056.7910 - val_loss: 1189869184.0000 - val_rmse: 34494.4766\n",
      "Epoch 174/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285444000.0000 - rmse: 16895.0859 - val_loss: 678419712.0000 - val_rmse: 26046.4902\n",
      "Epoch 175/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313042432.0000 - rmse: 17693.0039 - val_loss: 1313174656.0000 - val_rmse: 36237.7500\n",
      "Epoch 176/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356357920.0000 - rmse: 18877.4434 - val_loss: 1216680960.0000 - val_rmse: 34880.9531\n",
      "Epoch 177/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311168672.0000 - rmse: 17639.9727 - val_loss: 1077850112.0000 - val_rmse: 32830.6289\n",
      "Epoch 178/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319062400.0000 - rmse: 17862.3164 - val_loss: 1691702784.0000 - val_rmse: 41130.3164\n",
      "Epoch 179/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276942784.0000 - rmse: 16641.5938 - val_loss: 833590656.0000 - val_rmse: 28871.9668\n",
      "Epoch 180/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284770528.0000 - rmse: 16875.1387 - val_loss: 1833042944.0000 - val_rmse: 42814.0508\n",
      "Epoch 181/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279172608.0000 - rmse: 16708.4570 - val_loss: 3479904256.0000 - val_rmse: 58990.7148\n",
      "Epoch 182/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337840512.0000 - rmse: 18380.4355 - val_loss: 740212224.0000 - val_rmse: 27206.8398\n",
      "Epoch 183/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320239520.0000 - rmse: 17895.2344 - val_loss: 1413606528.0000 - val_rmse: 37597.9609\n",
      "Epoch 184/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298933664.0000 - rmse: 17289.6973 - val_loss: 1244379264.0000 - val_rmse: 35275.7617\n",
      "Epoch 185/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290501600.0000 - rmse: 17044.1055 - val_loss: 3331506432.0000 - val_rmse: 57719.1992\n",
      "Epoch 186/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281162112.0000 - rmse: 16767.8867 - val_loss: 1214527488.0000 - val_rmse: 34850.0664\n",
      "Epoch 187/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280979744.0000 - rmse: 16762.4492 - val_loss: 886541504.0000 - val_rmse: 29774.8477\n",
      "Epoch 188/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306032256.0000 - rmse: 17493.7754 - val_loss: 1204036352.0000 - val_rmse: 34699.2266\n",
      "Epoch 189/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416615072.0000 - rmse: 20411.1484 - val_loss: 678079616.0000 - val_rmse: 26039.9609\n",
      "Epoch 190/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278071552.0000 - rmse: 16675.4727 - val_loss: 951424256.0000 - val_rmse: 30845.1660\n",
      "Epoch 191/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255529312.0000 - rmse: 15985.2812 - val_loss: 1101236352.0000 - val_rmse: 33184.8789\n",
      "Epoch 192/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291939040.0000 - rmse: 17086.2207 - val_loss: 1117280000.0000 - val_rmse: 33425.7383\n",
      "Epoch 193/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284179648.0000 - rmse: 16857.6250 - val_loss: 722995200.0000 - val_rmse: 26888.5684\n",
      "Epoch 194/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310292896.0000 - rmse: 17615.1309 - val_loss: 1769889664.0000 - val_rmse: 42070.0547\n",
      "Epoch 195/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247191872.0000 - rmse: 15722.3350 - val_loss: 1039769152.0000 - val_rmse: 32245.4492\n",
      "Epoch 196/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351682464.0000 - rmse: 18753.1953 - val_loss: 959626560.0000 - val_rmse: 30977.8359\n",
      "Epoch 197/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275451104.0000 - rmse: 16596.7168 - val_loss: 640764288.0000 - val_rmse: 25313.3203\n",
      "Epoch 198/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254562832.0000 - rmse: 15955.0234 - val_loss: 1846034688.0000 - val_rmse: 42965.5039\n",
      "Epoch 199/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286459872.0000 - rmse: 16925.1230 - val_loss: 916238080.0000 - val_rmse: 30269.4238\n",
      "Epoch 200/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303061792.0000 - rmse: 17408.6660 - val_loss: 1278242176.0000 - val_rmse: 35752.5117\n",
      "Epoch 201/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296316864.0000 - rmse: 17213.8535 - val_loss: 871371008.0000 - val_rmse: 29518.9902\n",
      "Epoch 202/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283365600.0000 - rmse: 16833.4648 - val_loss: 574276800.0000 - val_rmse: 23964.0703\n",
      "Epoch 203/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336756352.0000 - rmse: 18350.9199 - val_loss: 769392768.0000 - val_rmse: 27737.9277\n",
      "Epoch 204/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308257888.0000 - rmse: 17557.2734 - val_loss: 1136093312.0000 - val_rmse: 33705.9805\n",
      "Epoch 205/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266878672.0000 - rmse: 16336.4180 - val_loss: 809905664.0000 - val_rmse: 28458.8398\n",
      "Epoch 206/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262911744.0000 - rmse: 16214.5508 - val_loss: 1692937600.0000 - val_rmse: 41145.3242\n",
      "Epoch 207/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265852592.0000 - rmse: 16304.9844 - val_loss: 1421817472.0000 - val_rmse: 37706.9922\n",
      "Epoch 208/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299182272.0000 - rmse: 17296.8809 - val_loss: 1074474112.0000 - val_rmse: 32779.1719\n",
      "Epoch 209/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267551776.0000 - rmse: 16357.0088 - val_loss: 673726080.0000 - val_rmse: 25956.2324\n",
      "Epoch 210/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288582816.0000 - rmse: 16987.7227 - val_loss: 794882240.0000 - val_rmse: 28193.6543\n",
      "Epoch 211/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315424800.0000 - rmse: 17760.2012 - val_loss: 750159744.0000 - val_rmse: 27389.0430\n",
      "Epoch 212/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270927008.0000 - rmse: 16459.8574 - val_loss: 761598016.0000 - val_rmse: 27597.0625\n",
      "Epoch 213/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376281376.0000 - rmse: 19397.9727 - val_loss: 966856512.0000 - val_rmse: 31094.3145\n",
      "Epoch 214/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255568912.0000 - rmse: 15986.5195 - val_loss: 1231475968.0000 - val_rmse: 35092.3906\n",
      "Epoch 215/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274481856.0000 - rmse: 16567.4922 - val_loss: 2160547072.0000 - val_rmse: 46481.6836\n",
      "104/104 [==============================] - 0s 718us/step - loss: 940323328.0000 - rmse: 30664.6875\n",
      "[940323328.0, 30664.6875]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 6012804608.0000 - rmse: 77542.2734 - val_loss: 1453937280.0000 - val_rmse: 38130.5312\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1817133696.0000 - rmse: 42627.8516 - val_loss: 1066996800.0000 - val_rmse: 32664.9160\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1615850752.0000 - rmse: 40197.6445 - val_loss: 1017983040.0000 - val_rmse: 31905.8457\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1457021696.0000 - rmse: 38170.9531 - val_loss: 1078862464.0000 - val_rmse: 32846.0430\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1315108736.0000 - rmse: 36264.4258 - val_loss: 1108238336.0000 - val_rmse: 33290.2148\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1294800000.0000 - rmse: 35983.3281 - val_loss: 977446336.0000 - val_rmse: 31264.1387\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1275672192.0000 - rmse: 35716.5547 - val_loss: 1045565952.0000 - val_rmse: 32335.2129\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1259161600.0000 - rmse: 35484.6680 - val_loss: 1074833664.0000 - val_rmse: 32784.6562\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1234159616.0000 - rmse: 35130.6094 - val_loss: 1089928064.0000 - val_rmse: 33014.0547\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1161461120.0000 - rmse: 34080.2148 - val_loss: 895686528.0000 - val_rmse: 29928.0215\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1144840192.0000 - rmse: 33835.4883 - val_loss: 871677248.0000 - val_rmse: 29524.1816\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1191685120.0000 - rmse: 34520.7930 - val_loss: 883138752.0000 - val_rmse: 29717.6504\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1126479744.0000 - rmse: 33563.0703 - val_loss: 886176960.0000 - val_rmse: 29768.7246\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1084850176.0000 - rmse: 32937.0625 - val_loss: 898350400.0000 - val_rmse: 29972.4941\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1053784896.0000 - rmse: 32462.0527 - val_loss: 1169820800.0000 - val_rmse: 34202.6445\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1127922176.0000 - rmse: 33584.5508 - val_loss: 1071214848.0000 - val_rmse: 32729.4180\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 962914240.0000 - rmse: 31030.8594 - val_loss: 888999744.0000 - val_rmse: 29816.0996\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1007304576.0000 - rmse: 31738.0625 - val_loss: 839832960.0000 - val_rmse: 28979.8691\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 949257600.0000 - rmse: 30810.0234 - val_loss: 827401472.0000 - val_rmse: 28764.5859\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 965243968.0000 - rmse: 31068.3750 - val_loss: 797587712.0000 - val_rmse: 28241.5957\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 915172480.0000 - rmse: 30251.8184 - val_loss: 1061295872.0000 - val_rmse: 32577.5352\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 994102976.0000 - rmse: 31529.3984 - val_loss: 1906059392.0000 - val_rmse: 43658.4414\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 795923776.0000 - rmse: 28212.1211 - val_loss: 843772736.0000 - val_rmse: 29047.7656\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 932358592.0000 - rmse: 30534.5469 - val_loss: 692470976.0000 - val_rmse: 26314.8438\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 905443776.0000 - rmse: 30090.5938 - val_loss: 687565824.0000 - val_rmse: 26221.4766\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 821512640.0000 - rmse: 28662.0410 - val_loss: 766569984.0000 - val_rmse: 27687.0000\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 753120256.0000 - rmse: 27443.0371 - val_loss: 969308416.0000 - val_rmse: 31133.7188\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 748499456.0000 - rmse: 27358.7168 - val_loss: 690618688.0000 - val_rmse: 26279.6230\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 827787968.0000 - rmse: 28771.3047 - val_loss: 877386880.0000 - val_rmse: 29620.7168\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724395328.0000 - rmse: 26914.5918 - val_loss: 850938368.0000 - val_rmse: 29170.8438\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 699692544.0000 - rmse: 26451.7012 - val_loss: 1032965760.0000 - val_rmse: 32139.7852\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668730560.0000 - rmse: 25859.8262 - val_loss: 914812160.0000 - val_rmse: 30245.8613\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733611328.0000 - rmse: 27085.2598 - val_loss: 796562368.0000 - val_rmse: 28223.4336\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 672239552.0000 - rmse: 25927.5820 - val_loss: 630486080.0000 - val_rmse: 25109.4805\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 739859136.0000 - rmse: 27200.3516 - val_loss: 663748608.0000 - val_rmse: 25763.3184\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692740224.0000 - rmse: 26319.9590 - val_loss: 688018176.0000 - val_rmse: 26230.0977\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 674049088.0000 - rmse: 25962.4551 - val_loss: 755796480.0000 - val_rmse: 27491.7520\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618236736.0000 - rmse: 24864.3633 - val_loss: 540633984.0000 - val_rmse: 23251.5371\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557778688.0000 - rmse: 23617.3379 - val_loss: 956215808.0000 - val_rmse: 30922.7402\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 658469120.0000 - rmse: 25660.6523 - val_loss: 892002048.0000 - val_rmse: 29866.4023\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648915648.0000 - rmse: 25473.8223 - val_loss: 774451328.0000 - val_rmse: 27828.9629\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563501888.0000 - rmse: 23738.1934 - val_loss: 717340992.0000 - val_rmse: 26783.2227\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629473024.0000 - rmse: 25089.3008 - val_loss: 973307520.0000 - val_rmse: 31197.8750\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637848000.0000 - rmse: 25255.6523 - val_loss: 659983872.0000 - val_rmse: 25690.1504\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513384416.0000 - rmse: 22657.9883 - val_loss: 908155776.0000 - val_rmse: 30135.6230\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623220544.0000 - rmse: 24964.3848 - val_loss: 878316864.0000 - val_rmse: 29636.4102\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623278464.0000 - rmse: 24965.5449 - val_loss: 642453888.0000 - val_rmse: 25346.6719\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558984960.0000 - rmse: 23642.8633 - val_loss: 730973120.0000 - val_rmse: 27036.5137\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514819200.0000 - rmse: 22689.6250 - val_loss: 684524224.0000 - val_rmse: 26163.4121\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626188736.0000 - rmse: 25023.7617 - val_loss: 680555712.0000 - val_rmse: 26087.4609\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628040256.0000 - rmse: 25060.7305 - val_loss: 841387648.0000 - val_rmse: 29006.6816\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539744320.0000 - rmse: 23232.3945 - val_loss: 678558336.0000 - val_rmse: 26049.1504\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498641632.0000 - rmse: 22330.2852 - val_loss: 675555776.0000 - val_rmse: 25991.4551\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498931232.0000 - rmse: 22336.7656 - val_loss: 638519616.0000 - val_rmse: 25268.9453\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531898432.0000 - rmse: 23062.9219 - val_loss: 898849600.0000 - val_rmse: 29980.8203\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500571136.0000 - rmse: 22373.4473 - val_loss: 824319104.0000 - val_rmse: 28710.9570\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544222656.0000 - rmse: 23328.5801 - val_loss: 1169165184.0000 - val_rmse: 34193.0586\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478549024.0000 - rmse: 21875.7637 - val_loss: 976885568.0000 - val_rmse: 31255.1680\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481229280.0000 - rmse: 21936.9375 - val_loss: 760942080.0000 - val_rmse: 27585.1758\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530960576.0000 - rmse: 23042.5820 - val_loss: 1140571776.0000 - val_rmse: 33772.3516\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615304192.0000 - rmse: 24805.3242 - val_loss: 617202816.0000 - val_rmse: 24843.5664\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493173024.0000 - rmse: 22207.5000 - val_loss: 724853632.0000 - val_rmse: 26923.1055\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466086144.0000 - rmse: 21589.0293 - val_loss: 826352896.0000 - val_rmse: 28746.3535\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476124704.0000 - rmse: 21820.2793 - val_loss: 842290304.0000 - val_rmse: 29022.2383\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502976448.0000 - rmse: 22427.1348 - val_loss: 785019200.0000 - val_rmse: 28018.1934\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443111104.0000 - rmse: 21050.2051 - val_loss: 817809280.0000 - val_rmse: 28597.3652\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460881056.0000 - rmse: 21468.1387 - val_loss: 718513984.0000 - val_rmse: 26805.1113\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433829344.0000 - rmse: 20828.5703 - val_loss: 815793920.0000 - val_rmse: 28562.1055\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551577984.0000 - rmse: 23485.6973 - val_loss: 921220352.0000 - val_rmse: 30351.6074\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501037952.0000 - rmse: 22383.8770 - val_loss: 772364224.0000 - val_rmse: 27791.4395\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373985664.0000 - rmse: 19338.7070 - val_loss: 1139028480.0000 - val_rmse: 33749.4961\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509760576.0000 - rmse: 22577.8750 - val_loss: 813842560.0000 - val_rmse: 28527.9258\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451368896.0000 - rmse: 21245.4414 - val_loss: 1073859200.0000 - val_rmse: 32769.7891\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400436320.0000 - rmse: 20010.9043 - val_loss: 648932736.0000 - val_rmse: 25474.1582\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468631584.0000 - rmse: 21647.9004 - val_loss: 790547392.0000 - val_rmse: 28116.6719\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436496064.0000 - rmse: 20892.4863 - val_loss: 997959104.0000 - val_rmse: 31590.4902\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454268896.0000 - rmse: 21313.5840 - val_loss: 646863040.0000 - val_rmse: 25433.5020\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464633632.0000 - rmse: 21555.3613 - val_loss: 673606336.0000 - val_rmse: 25953.9258\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442796896.0000 - rmse: 21042.7383 - val_loss: 654341632.0000 - val_rmse: 25580.1016\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383233920.0000 - rmse: 19576.3613 - val_loss: 786731200.0000 - val_rmse: 28048.7285\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452982080.0000 - rmse: 21283.3730 - val_loss: 862714816.0000 - val_rmse: 29372.0078\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415062176.0000 - rmse: 20373.0723 - val_loss: 770715520.0000 - val_rmse: 27761.7617\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392499744.0000 - rmse: 19811.6055 - val_loss: 662269632.0000 - val_rmse: 25734.5996\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378879328.0000 - rmse: 19464.8223 - val_loss: 816897152.0000 - val_rmse: 28581.4121\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338780032.0000 - rmse: 18405.9746 - val_loss: 918012096.0000 - val_rmse: 30298.7148\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353324448.0000 - rmse: 18796.9258 - val_loss: 725224640.0000 - val_rmse: 26929.9922\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385957792.0000 - rmse: 19645.8086 - val_loss: 845997952.0000 - val_rmse: 29086.0430\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375573696.0000 - rmse: 19379.7227 - val_loss: 743928768.0000 - val_rmse: 27275.0566\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461627904.0000 - rmse: 21485.5254 - val_loss: 1374588928.0000 - val_rmse: 37075.4492\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430362720.0000 - rmse: 20745.1836 - val_loss: 733770112.0000 - val_rmse: 27088.1914\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335875584.0000 - rmse: 18326.9082 - val_loss: 1354886912.0000 - val_rmse: 36808.7891\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348250464.0000 - rmse: 18661.4688 - val_loss: 1658748288.0000 - val_rmse: 40727.7344\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331261984.0000 - rmse: 18200.6016 - val_loss: 1092365056.0000 - val_rmse: 33050.9453\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376973792.0000 - rmse: 19415.8105 - val_loss: 1283715456.0000 - val_rmse: 35828.9727\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481556416.0000 - rmse: 21944.3926 - val_loss: 1198478080.0000 - val_rmse: 34619.0430\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355386592.0000 - rmse: 18851.6992 - val_loss: 739248832.0000 - val_rmse: 27189.1289\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363702848.0000 - rmse: 19070.9941 - val_loss: 951707392.0000 - val_rmse: 30849.7539\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444277568.0000 - rmse: 21077.8906 - val_loss: 884975232.0000 - val_rmse: 29748.5332\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394256320.0000 - rmse: 19855.8887 - val_loss: 882568768.0000 - val_rmse: 29708.0566\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388495360.0000 - rmse: 19710.2832 - val_loss: 1158492672.0000 - val_rmse: 34036.6367\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357300320.0000 - rmse: 18902.3867 - val_loss: 1197887488.0000 - val_rmse: 34610.5117\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381194752.0000 - rmse: 19524.2070 - val_loss: 861505472.0000 - val_rmse: 29351.4141\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335752768.0000 - rmse: 18323.5566 - val_loss: 817120896.0000 - val_rmse: 28585.3262\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366418784.0000 - rmse: 19142.0645 - val_loss: 793637888.0000 - val_rmse: 28171.5781\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382036096.0000 - rmse: 19545.7422 - val_loss: 793643200.0000 - val_rmse: 28171.6738\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380205280.0000 - rmse: 19498.8516 - val_loss: 1085929216.0000 - val_rmse: 32953.4375\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351128256.0000 - rmse: 18738.4141 - val_loss: 821167360.0000 - val_rmse: 28656.0176\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379931520.0000 - rmse: 19491.8301 - val_loss: 989388608.0000 - val_rmse: 31454.5488\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322841088.0000 - rmse: 17967.7754 - val_loss: 1002853952.0000 - val_rmse: 31667.8691\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397852064.0000 - rmse: 19946.2285 - val_loss: 1106822272.0000 - val_rmse: 33268.9375\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388671296.0000 - rmse: 19714.7461 - val_loss: 877916480.0000 - val_rmse: 29629.6543\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376711872.0000 - rmse: 19409.0645 - val_loss: 954024768.0000 - val_rmse: 30887.2910\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319097568.0000 - rmse: 17863.3008 - val_loss: 807779200.0000 - val_rmse: 28421.4570\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344742688.0000 - rmse: 18567.2441 - val_loss: 1436382208.0000 - val_rmse: 37899.6328\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305935392.0000 - rmse: 17491.0078 - val_loss: 840764352.0000 - val_rmse: 28995.9355\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389796544.0000 - rmse: 19743.2656 - val_loss: 1202940544.0000 - val_rmse: 34683.4336\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334777984.0000 - rmse: 18296.9375 - val_loss: 934875776.0000 - val_rmse: 30575.7383\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394842784.0000 - rmse: 19870.6484 - val_loss: 833009600.0000 - val_rmse: 28861.9043\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292563168.0000 - rmse: 17104.4766 - val_loss: 987566080.0000 - val_rmse: 31425.5645\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411297856.0000 - rmse: 20280.4785 - val_loss: 958381504.0000 - val_rmse: 30957.7344\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351099840.0000 - rmse: 18737.6582 - val_loss: 827950912.0000 - val_rmse: 28774.1348\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334646304.0000 - rmse: 18293.3398 - val_loss: 1403593984.0000 - val_rmse: 37464.5703\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301249792.0000 - rmse: 17356.5469 - val_loss: 1054522560.0000 - val_rmse: 32473.4141\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343994592.0000 - rmse: 18547.0879 - val_loss: 1041138624.0000 - val_rmse: 32266.6777\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336147520.0000 - rmse: 18334.3262 - val_loss: 840082688.0000 - val_rmse: 28984.1797\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294250336.0000 - rmse: 17153.7246 - val_loss: 1105304704.0000 - val_rmse: 33246.1250\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303854976.0000 - rmse: 17431.4336 - val_loss: 763658816.0000 - val_rmse: 27634.3770\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342981280.0000 - rmse: 18519.7520 - val_loss: 1204072832.0000 - val_rmse: 34699.7539\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342009056.0000 - rmse: 18493.4863 - val_loss: 1105288704.0000 - val_rmse: 33245.8828\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345525536.0000 - rmse: 18588.3145 - val_loss: 1094255232.0000 - val_rmse: 33079.5273\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321222272.0000 - rmse: 17922.6738 - val_loss: 925401088.0000 - val_rmse: 30420.4062\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328409248.0000 - rmse: 18122.0645 - val_loss: 746086528.0000 - val_rmse: 27314.5840\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360181792.0000 - rmse: 18978.4531 - val_loss: 982275264.0000 - val_rmse: 31341.2695\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320147232.0000 - rmse: 17892.6562 - val_loss: 1004808896.0000 - val_rmse: 31698.7207\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343649088.0000 - rmse: 18537.7715 - val_loss: 949575936.0000 - val_rmse: 30815.1895\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346639040.0000 - rmse: 18618.2441 - val_loss: 832695424.0000 - val_rmse: 28856.4629\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318543616.0000 - rmse: 17847.7871 - val_loss: 1064973952.0000 - val_rmse: 32633.9375\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310070400.0000 - rmse: 17608.8145 - val_loss: 1061432320.0000 - val_rmse: 32579.6289\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365221088.0000 - rmse: 19110.7559 - val_loss: 1420330624.0000 - val_rmse: 37687.2734\n",
      "104/104 [==============================] - 0s 653us/step - loss: 433869024.0000 - rmse: 20829.5215\n",
      "[433869024.0, 20829.521484375]\n",
      "[25870.052734375, 27775.146484375, 28427.29296875, 30664.6875, 20829.521484375]\n",
      "26713.340234375\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "!python train.py kfold baseline\n",
    "#d 0.25 p 20 layer -1 epoch 150"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 18:48:23.607948: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 18:48:23.607989: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 18:48:23.608394: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 18:48:23.824192: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 7936335872.0000 - rmse: 89086.1172 - val_loss: 1267372544.0000 - val_rmse: 35600.1758\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1829099392.0000 - rmse: 42767.9727 - val_loss: 928581184.0000 - val_rmse: 30472.6309\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1662046080.0000 - rmse: 40768.1992 - val_loss: 825887808.0000 - val_rmse: 28738.2637\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1527956736.0000 - rmse: 39089.0859 - val_loss: 764084096.0000 - val_rmse: 27642.0703\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1475887360.0000 - rmse: 38417.2812 - val_loss: 732576384.0000 - val_rmse: 27066.1484\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1401062400.0000 - rmse: 37430.7695 - val_loss: 812584832.0000 - val_rmse: 28505.8730\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1298861056.0000 - rmse: 36039.7148 - val_loss: 762595648.0000 - val_rmse: 27615.1348\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1266309504.0000 - rmse: 35585.2422 - val_loss: 884825664.0000 - val_rmse: 29746.0176\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1252930944.0000 - rmse: 35396.7656 - val_loss: 712506944.0000 - val_rmse: 26692.8262\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1236792192.0000 - rmse: 35168.0547 - val_loss: 675398400.0000 - val_rmse: 25988.4277\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1256492288.0000 - rmse: 35447.0352 - val_loss: 696222592.0000 - val_rmse: 26386.0293\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1161198208.0000 - rmse: 34076.3594 - val_loss: 682743360.0000 - val_rmse: 26129.3574\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1150583168.0000 - rmse: 33920.2461 - val_loss: 693694464.0000 - val_rmse: 26338.0801\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1088901376.0000 - rmse: 32998.5039 - val_loss: 760493184.0000 - val_rmse: 27577.0410\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 984075712.0000 - rmse: 31369.9805 - val_loss: 698816000.0000 - val_rmse: 26435.1270\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 996970560.0000 - rmse: 31574.8398 - val_loss: 597085312.0000 - val_rmse: 24435.3301\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1033618752.0000 - rmse: 32149.9414 - val_loss: 638385344.0000 - val_rmse: 25266.2891\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 960124608.0000 - rmse: 30985.8770 - val_loss: 678108480.0000 - val_rmse: 26040.5156\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 974377088.0000 - rmse: 31215.0137 - val_loss: 643039808.0000 - val_rmse: 25358.2285\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 959478848.0000 - rmse: 30975.4551 - val_loss: 789604480.0000 - val_rmse: 28099.9023\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 960383616.0000 - rmse: 30990.0566 - val_loss: 695848896.0000 - val_rmse: 26378.9473\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 912553408.0000 - rmse: 30208.4961 - val_loss: 542062976.0000 - val_rmse: 23282.2461\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 813494336.0000 - rmse: 28521.8223 - val_loss: 1310558720.0000 - val_rmse: 36201.6406\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 965081536.0000 - rmse: 31065.7617 - val_loss: 585266880.0000 - val_rmse: 24192.2891\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 813854336.0000 - rmse: 28528.1328 - val_loss: 2000633600.0000 - val_rmse: 44728.4414\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 872819136.0000 - rmse: 29543.5098 - val_loss: 582201216.0000 - val_rmse: 24128.8457\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 806927232.0000 - rmse: 28406.4629 - val_loss: 561277632.0000 - val_rmse: 23691.2988\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 858627264.0000 - rmse: 29302.3418 - val_loss: 678464064.0000 - val_rmse: 26047.3418\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 769898112.0000 - rmse: 27747.0371 - val_loss: 531005152.0000 - val_rmse: 23043.5488\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742168832.0000 - rmse: 27242.7754 - val_loss: 740176000.0000 - val_rmse: 27206.1738\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691796800.0000 - rmse: 26302.0312 - val_loss: 454784736.0000 - val_rmse: 21325.6816\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 694370752.0000 - rmse: 26350.9141 - val_loss: 1028319744.0000 - val_rmse: 32067.4238\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 759097216.0000 - rmse: 27551.7188 - val_loss: 595700352.0000 - val_rmse: 24406.9727\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 750211008.0000 - rmse: 27389.9785 - val_loss: 481152736.0000 - val_rmse: 21935.1934\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 764553088.0000 - rmse: 27650.5527 - val_loss: 532517600.0000 - val_rmse: 23076.3418\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 734639232.0000 - rmse: 27104.2285 - val_loss: 771845824.0000 - val_rmse: 27782.1133\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 700361728.0000 - rmse: 26464.3477 - val_loss: 1285248128.0000 - val_rmse: 35850.3594\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729723904.0000 - rmse: 27013.4023 - val_loss: 940733120.0000 - val_rmse: 30671.3711\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 722458560.0000 - rmse: 26878.5898 - val_loss: 792841472.0000 - val_rmse: 28157.4414\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628765376.0000 - rmse: 25075.1914 - val_loss: 659800384.0000 - val_rmse: 25686.5801\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 650058816.0000 - rmse: 25496.2520 - val_loss: 433074624.0000 - val_rmse: 20810.4453\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571081664.0000 - rmse: 23897.3145 - val_loss: 942031104.0000 - val_rmse: 30692.5254\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 631852608.0000 - rmse: 25136.6777 - val_loss: 846544320.0000 - val_rmse: 29095.4355\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640298816.0000 - rmse: 25304.1270 - val_loss: 739691776.0000 - val_rmse: 27197.2734\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634169664.0000 - rmse: 25182.7246 - val_loss: 632832576.0000 - val_rmse: 25156.1641\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554020736.0000 - rmse: 23537.6445 - val_loss: 405535648.0000 - val_rmse: 20137.9160\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 739330752.0000 - rmse: 27190.6367 - val_loss: 652963392.0000 - val_rmse: 25553.1484\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616312064.0000 - rmse: 24825.6328 - val_loss: 608271488.0000 - val_rmse: 24663.1602\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593088064.0000 - rmse: 24353.3965 - val_loss: 593288128.0000 - val_rmse: 24357.5059\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 671993152.0000 - rmse: 25922.8301 - val_loss: 707534144.0000 - val_rmse: 26599.5137\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548939392.0000 - rmse: 23429.4551 - val_loss: 400407744.0000 - val_rmse: 20010.1895\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 673993920.0000 - rmse: 25961.3926 - val_loss: 651087616.0000 - val_rmse: 25516.4180\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604178816.0000 - rmse: 24580.0469 - val_loss: 483024512.0000 - val_rmse: 21977.8164\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459404480.0000 - rmse: 21433.7227 - val_loss: 819444416.0000 - val_rmse: 28625.9375\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537348928.0000 - rmse: 23180.7871 - val_loss: 1012567296.0000 - val_rmse: 31820.8633\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567193600.0000 - rmse: 23815.8242 - val_loss: 919133056.0000 - val_rmse: 30317.2070\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552623168.0000 - rmse: 23507.9375 - val_loss: 731149504.0000 - val_rmse: 27039.7734\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480102464.0000 - rmse: 21911.2383 - val_loss: 786227136.0000 - val_rmse: 28039.7402\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666234752.0000 - rmse: 25811.5234 - val_loss: 847562304.0000 - val_rmse: 29112.9238\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542205376.0000 - rmse: 23285.3047 - val_loss: 573253120.0000 - val_rmse: 23942.7051\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469316640.0000 - rmse: 21663.7148 - val_loss: 492361024.0000 - val_rmse: 22189.2090\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479052896.0000 - rmse: 21887.2754 - val_loss: 1505542400.0000 - val_rmse: 38801.3164\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454570240.0000 - rmse: 21320.6523 - val_loss: 681726272.0000 - val_rmse: 26109.8887\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454564416.0000 - rmse: 21320.5156 - val_loss: 499429984.0000 - val_rmse: 22347.9258\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517900384.0000 - rmse: 22757.4238 - val_loss: 478423264.0000 - val_rmse: 21872.8887\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445009600.0000 - rmse: 21095.2500 - val_loss: 736819328.0000 - val_rmse: 27144.4160\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468426336.0000 - rmse: 21643.1582 - val_loss: 1096131712.0000 - val_rmse: 33107.8789\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461797568.0000 - rmse: 21489.4746 - val_loss: 529620576.0000 - val_rmse: 23013.4863\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445194496.0000 - rmse: 21099.6309 - val_loss: 610121344.0000 - val_rmse: 24700.6348\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430197952.0000 - rmse: 20741.2129 - val_loss: 818184448.0000 - val_rmse: 28603.9238\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414400928.0000 - rmse: 20356.8379 - val_loss: 1642401664.0000 - val_rmse: 40526.5547\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412790752.0000 - rmse: 20317.2500 - val_loss: 1474915968.0000 - val_rmse: 38404.6328\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420230912.0000 - rmse: 20499.5332 - val_loss: 805791936.0000 - val_rmse: 28386.4727\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390188672.0000 - rmse: 19753.1934 - val_loss: 503861664.0000 - val_rmse: 22446.8594\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533082624.0000 - rmse: 23088.5801 - val_loss: 959222016.0000 - val_rmse: 30971.3086\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400927872.0000 - rmse: 20023.1816 - val_loss: 1097912832.0000 - val_rmse: 33134.7695\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419985792.0000 - rmse: 20493.5547 - val_loss: 1597272960.0000 - val_rmse: 39965.8945\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429781088.0000 - rmse: 20731.1621 - val_loss: 1079745920.0000 - val_rmse: 32859.4883\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447703936.0000 - rmse: 21159.0137 - val_loss: 1338927616.0000 - val_rmse: 36591.3594\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491514752.0000 - rmse: 22170.1309 - val_loss: 775290432.0000 - val_rmse: 27844.0371\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421622240.0000 - rmse: 20533.4414 - val_loss: 531726336.0000 - val_rmse: 23059.1914\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412781056.0000 - rmse: 20317.0137 - val_loss: 1361075840.0000 - val_rmse: 36892.7617\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477278336.0000 - rmse: 21846.6973 - val_loss: 623928320.0000 - val_rmse: 24978.5566\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439295072.0000 - rmse: 20959.3652 - val_loss: 874673792.0000 - val_rmse: 29574.8848\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373145824.0000 - rmse: 19316.9805 - val_loss: 381925952.0000 - val_rmse: 19542.9238\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428311456.0000 - rmse: 20695.6875 - val_loss: 1518383872.0000 - val_rmse: 38966.4453\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399466304.0000 - rmse: 19986.6523 - val_loss: 828065280.0000 - val_rmse: 28776.1230\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353172896.0000 - rmse: 18792.8926 - val_loss: 772971136.0000 - val_rmse: 27802.3574\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419281344.0000 - rmse: 20476.3594 - val_loss: 612926656.0000 - val_rmse: 24757.3535\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397351744.0000 - rmse: 19933.6836 - val_loss: 470801696.0000 - val_rmse: 21697.9629\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365973120.0000 - rmse: 19130.4238 - val_loss: 588120704.0000 - val_rmse: 24251.1992\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358519840.0000 - rmse: 18934.6211 - val_loss: 560167680.0000 - val_rmse: 23667.8613\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423076960.0000 - rmse: 20568.8340 - val_loss: 1002154304.0000 - val_rmse: 31656.8184\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347849024.0000 - rmse: 18650.7109 - val_loss: 691940544.0000 - val_rmse: 26304.7598\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365953568.0000 - rmse: 19129.9121 - val_loss: 1179060608.0000 - val_rmse: 34337.4531\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397428928.0000 - rmse: 19935.6172 - val_loss: 556864512.0000 - val_rmse: 23597.9766\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424291168.0000 - rmse: 20598.3262 - val_loss: 548008704.0000 - val_rmse: 23409.5840\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319843520.0000 - rmse: 17884.1699 - val_loss: 2134246144.0000 - val_rmse: 46197.8984\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315966848.0000 - rmse: 17775.4551 - val_loss: 624829632.0000 - val_rmse: 24996.5918\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361979392.0000 - rmse: 19025.7559 - val_loss: 415526048.0000 - val_rmse: 20384.4551\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346981920.0000 - rmse: 18627.4492 - val_loss: 598676416.0000 - val_rmse: 24467.8633\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322544256.0000 - rmse: 17959.5176 - val_loss: 1066536256.0000 - val_rmse: 32657.8672\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344498208.0000 - rmse: 18560.6602 - val_loss: 433055456.0000 - val_rmse: 20809.9824\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333064256.0000 - rmse: 18250.0449 - val_loss: 467718720.0000 - val_rmse: 21626.8047\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302544000.0000 - rmse: 17393.7910 - val_loss: 1827348864.0000 - val_rmse: 42747.5000\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338255264.0000 - rmse: 18391.7148 - val_loss: 628064128.0000 - val_rmse: 25061.2070\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329948096.0000 - rmse: 18164.4727 - val_loss: 712881728.0000 - val_rmse: 26699.8438\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331546304.0000 - rmse: 18208.4121 - val_loss: 468022080.0000 - val_rmse: 21633.8164\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384564896.0000 - rmse: 19610.3262 - val_loss: 749149376.0000 - val_rmse: 27370.5898\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338742560.0000 - rmse: 18404.9590 - val_loss: 702068672.0000 - val_rmse: 26496.5781\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296037280.0000 - rmse: 17205.7305 - val_loss: 525754272.0000 - val_rmse: 22929.3301\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330482496.0000 - rmse: 18179.1758 - val_loss: 534338432.0000 - val_rmse: 23115.7598\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357113504.0000 - rmse: 18897.4453 - val_loss: 541876416.0000 - val_rmse: 23278.2383\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374173536.0000 - rmse: 19343.5664 - val_loss: 597814336.0000 - val_rmse: 24450.2402\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296119008.0000 - rmse: 17208.1074 - val_loss: 484214688.0000 - val_rmse: 22004.8750\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357956992.0000 - rmse: 18919.7480 - val_loss: 608490112.0000 - val_rmse: 24667.5918\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289133824.0000 - rmse: 17003.9336 - val_loss: 625362688.0000 - val_rmse: 25007.2500\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370061792.0000 - rmse: 19236.9883 - val_loss: 899175936.0000 - val_rmse: 29986.2598\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295959872.0000 - rmse: 17203.4824 - val_loss: 1067826176.0000 - val_rmse: 32677.6074\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343585056.0000 - rmse: 18536.0449 - val_loss: 556772928.0000 - val_rmse: 23596.0352\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344513728.0000 - rmse: 18561.0801 - val_loss: 751263232.0000 - val_rmse: 27409.1816\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332209344.0000 - rmse: 18226.6094 - val_loss: 528394976.0000 - val_rmse: 22986.8418\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316076384.0000 - rmse: 17778.5352 - val_loss: 722590784.0000 - val_rmse: 26881.0488\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305182016.0000 - rmse: 17469.4570 - val_loss: 664550144.0000 - val_rmse: 25778.8691\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339275104.0000 - rmse: 18419.4199 - val_loss: 745821248.0000 - val_rmse: 27309.7266\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285086304.0000 - rmse: 16884.4980 - val_loss: 695467072.0000 - val_rmse: 26371.7090\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335421312.0000 - rmse: 18314.5117 - val_loss: 1052015744.0000 - val_rmse: 32434.7910\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269302464.0000 - rmse: 16410.4336 - val_loss: 654518592.0000 - val_rmse: 25583.5605\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361831040.0000 - rmse: 19021.8535 - val_loss: 693726144.0000 - val_rmse: 26338.6797\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279774304.0000 - rmse: 16726.4492 - val_loss: 734651392.0000 - val_rmse: 27104.4531\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416188512.0000 - rmse: 20400.6992 - val_loss: 867411264.0000 - val_rmse: 29451.8457\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331917728.0000 - rmse: 18218.6055 - val_loss: 565870528.0000 - val_rmse: 23788.0332\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350226720.0000 - rmse: 18714.3438 - val_loss: 635111040.0000 - val_rmse: 25201.4082\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314110752.0000 - rmse: 17723.1680 - val_loss: 929147712.0000 - val_rmse: 30481.9238\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328662656.0000 - rmse: 18129.0527 - val_loss: 549744448.0000 - val_rmse: 23446.6289\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306015072.0000 - rmse: 17493.2852 - val_loss: 1695780608.0000 - val_rmse: 41179.8555\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324346592.0000 - rmse: 18009.6250 - val_loss: 604019584.0000 - val_rmse: 24576.8105\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284771872.0000 - rmse: 16875.1836 - val_loss: 669721088.0000 - val_rmse: 25878.9688\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338185824.0000 - rmse: 18389.8242 - val_loss: 1018528960.0000 - val_rmse: 31914.4004\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308947776.0000 - rmse: 17576.9082 - val_loss: 1061035712.0000 - val_rmse: 32573.5430\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299860352.0000 - rmse: 17316.4746 - val_loss: 993620416.0000 - val_rmse: 31521.7461\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267901888.0000 - rmse: 16367.7070 - val_loss: 878503104.0000 - val_rmse: 29639.5508\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316165760.0000 - rmse: 17781.0488 - val_loss: 716048640.0000 - val_rmse: 26759.0859\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265519328.0000 - rmse: 16294.7598 - val_loss: 1058213248.0000 - val_rmse: 32530.1875\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341242432.0000 - rmse: 18472.7461 - val_loss: 1124800128.0000 - val_rmse: 33538.0391\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299034656.0000 - rmse: 17292.6172 - val_loss: 772820928.0000 - val_rmse: 27799.6562\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313378592.0000 - rmse: 17702.5000 - val_loss: 849535744.0000 - val_rmse: 29146.7969\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305423744.0000 - rmse: 17476.3730 - val_loss: 1014526208.0000 - val_rmse: 31851.6270\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307453248.0000 - rmse: 17534.3418 - val_loss: 906209984.0000 - val_rmse: 30103.3203\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278595968.0000 - rmse: 16691.1934 - val_loss: 914541632.0000 - val_rmse: 30241.3867\n",
      "104/104 [==============================] - 0s 650us/step - loss: 491455808.0000 - rmse: 22168.8027\n",
      "[491455808.0, 22168.802734375]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 7256769536.0000 - rmse: 85186.6719 - val_loss: 1365241856.0000 - val_rmse: 36949.1797\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1645825152.0000 - rmse: 40568.7695 - val_loss: 1056762496.0000 - val_rmse: 32507.8828\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1380606208.0000 - rmse: 37156.5078 - val_loss: 939511168.0000 - val_rmse: 30651.4473\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1239023488.0000 - rmse: 35199.7656 - val_loss: 939459200.0000 - val_rmse: 30650.5996\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1208523008.0000 - rmse: 34763.8164 - val_loss: 900057728.0000 - val_rmse: 30000.9629\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1166818560.0000 - rmse: 34158.7266 - val_loss: 856180736.0000 - val_rmse: 29260.5664\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1113392768.0000 - rmse: 33367.5391 - val_loss: 843662336.0000 - val_rmse: 29045.8652\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1133303168.0000 - rmse: 33664.5703 - val_loss: 836968832.0000 - val_rmse: 28930.4141\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1079286784.0000 - rmse: 32852.5000 - val_loss: 1113785216.0000 - val_rmse: 33373.4219\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1014062016.0000 - rmse: 31844.3379 - val_loss: 817748864.0000 - val_rmse: 28596.3086\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1041892224.0000 - rmse: 32278.3555 - val_loss: 880639808.0000 - val_rmse: 29675.5762\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 989492160.0000 - rmse: 31456.1934 - val_loss: 813464832.0000 - val_rmse: 28521.3047\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 866673536.0000 - rmse: 29439.3203 - val_loss: 829210944.0000 - val_rmse: 28796.0234\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 938330816.0000 - rmse: 30632.1855 - val_loss: 807391744.0000 - val_rmse: 28414.6387\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 955753536.0000 - rmse: 30915.2637 - val_loss: 796295488.0000 - val_rmse: 28218.7090\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 876388096.0000 - rmse: 29603.8535 - val_loss: 757470784.0000 - val_rmse: 27522.1855\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 854107456.0000 - rmse: 29225.1172 - val_loss: 1055885824.0000 - val_rmse: 32494.3965\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 918271296.0000 - rmse: 30302.9922 - val_loss: 910073216.0000 - val_rmse: 30167.4199\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 842681024.0000 - rmse: 29028.9688 - val_loss: 805366528.0000 - val_rmse: 28378.9805\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 856522560.0000 - rmse: 29266.4062 - val_loss: 863932032.0000 - val_rmse: 29392.7207\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 767980928.0000 - rmse: 27712.4668 - val_loss: 797158848.0000 - val_rmse: 28234.0000\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 846532544.0000 - rmse: 29095.2324 - val_loss: 743064960.0000 - val_rmse: 27259.2188\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 812840704.0000 - rmse: 28510.3613 - val_loss: 884273216.0000 - val_rmse: 29736.7324\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 787428800.0000 - rmse: 28061.1602 - val_loss: 719898368.0000 - val_rmse: 26830.9219\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 786944000.0000 - rmse: 28052.5215 - val_loss: 753694336.0000 - val_rmse: 27453.4941\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 796743744.0000 - rmse: 28226.6504 - val_loss: 681671040.0000 - val_rmse: 26108.8301\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 689963008.0000 - rmse: 26267.1465 - val_loss: 652585024.0000 - val_rmse: 25545.7422\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 659099840.0000 - rmse: 25672.9395 - val_loss: 637151104.0000 - val_rmse: 25241.8516\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 683257920.0000 - rmse: 26139.2012 - val_loss: 712883520.0000 - val_rmse: 26699.8770\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618265344.0000 - rmse: 24864.9414 - val_loss: 647857472.0000 - val_rmse: 25453.0449\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680756928.0000 - rmse: 26091.3184 - val_loss: 629924032.0000 - val_rmse: 25098.2852\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646044928.0000 - rmse: 25417.4141 - val_loss: 625614848.0000 - val_rmse: 25012.2930\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610753216.0000 - rmse: 24713.4199 - val_loss: 700805120.0000 - val_rmse: 26472.7246\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561383040.0000 - rmse: 23693.5215 - val_loss: 582503424.0000 - val_rmse: 24135.1074\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573218176.0000 - rmse: 23941.9746 - val_loss: 611803200.0000 - val_rmse: 24734.6543\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578637376.0000 - rmse: 24054.8809 - val_loss: 593830720.0000 - val_rmse: 24368.6406\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578521280.0000 - rmse: 24052.4688 - val_loss: 565664768.0000 - val_rmse: 23783.7090\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548322304.0000 - rmse: 23416.2812 - val_loss: 604172288.0000 - val_rmse: 24579.9160\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 581138112.0000 - rmse: 24106.8066 - val_loss: 581239168.0000 - val_rmse: 24108.9004\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549191744.0000 - rmse: 23434.8398 - val_loss: 593426880.0000 - val_rmse: 24360.3535\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621113216.0000 - rmse: 24922.1426 - val_loss: 602287040.0000 - val_rmse: 24541.5371\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497596928.0000 - rmse: 22306.8789 - val_loss: 729552192.0000 - val_rmse: 27010.2246\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497638368.0000 - rmse: 22307.8105 - val_loss: 562356608.0000 - val_rmse: 23714.0586\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479756064.0000 - rmse: 21903.3340 - val_loss: 593946240.0000 - val_rmse: 24371.0117\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510647136.0000 - rmse: 22597.5020 - val_loss: 571836736.0000 - val_rmse: 23913.1074\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482257056.0000 - rmse: 21960.3496 - val_loss: 607914560.0000 - val_rmse: 24655.9219\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457036864.0000 - rmse: 21378.4199 - val_loss: 686867072.0000 - val_rmse: 26208.1484\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482516736.0000 - rmse: 21966.2637 - val_loss: 692892032.0000 - val_rmse: 26322.8418\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492994880.0000 - rmse: 22203.4863 - val_loss: 633153600.0000 - val_rmse: 25162.5430\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573635072.0000 - rmse: 23950.6777 - val_loss: 536190272.0000 - val_rmse: 23155.7812\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434917056.0000 - rmse: 20854.6641 - val_loss: 844266368.0000 - val_rmse: 29056.2617\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462564544.0000 - rmse: 21507.3145 - val_loss: 866452544.0000 - val_rmse: 29435.5664\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426477664.0000 - rmse: 20651.3340 - val_loss: 886184192.0000 - val_rmse: 29768.8457\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441822688.0000 - rmse: 21019.5781 - val_loss: 582294656.0000 - val_rmse: 24130.7832\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433369472.0000 - rmse: 20817.5273 - val_loss: 703000256.0000 - val_rmse: 26514.1504\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408849120.0000 - rmse: 20220.0176 - val_loss: 663970816.0000 - val_rmse: 25767.6309\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383500768.0000 - rmse: 19583.1758 - val_loss: 733703552.0000 - val_rmse: 27086.9609\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437265824.0000 - rmse: 20910.9004 - val_loss: 539023872.0000 - val_rmse: 23216.8867\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438482912.0000 - rmse: 20939.9844 - val_loss: 649418240.0000 - val_rmse: 25483.6855\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365390560.0000 - rmse: 19115.1895 - val_loss: 542457920.0000 - val_rmse: 23290.7246\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410931520.0000 - rmse: 20271.4453 - val_loss: 519112352.0000 - val_rmse: 22784.0371\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340946784.0000 - rmse: 18464.7422 - val_loss: 615848896.0000 - val_rmse: 24816.3008\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413839552.0000 - rmse: 20343.0449 - val_loss: 578451776.0000 - val_rmse: 24051.0234\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406564704.0000 - rmse: 20163.4473 - val_loss: 769210880.0000 - val_rmse: 27734.6504\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421244032.0000 - rmse: 20524.2305 - val_loss: 639014848.0000 - val_rmse: 25278.7422\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328539424.0000 - rmse: 18125.6543 - val_loss: 522973920.0000 - val_rmse: 22868.6230\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423175584.0000 - rmse: 20571.2305 - val_loss: 549459264.0000 - val_rmse: 23440.5469\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388784928.0000 - rmse: 19717.6289 - val_loss: 536968768.0000 - val_rmse: 23172.5859\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350735456.0000 - rmse: 18727.9316 - val_loss: 556977664.0000 - val_rmse: 23600.3730\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369246624.0000 - rmse: 19215.7891 - val_loss: 607844672.0000 - val_rmse: 24654.5039\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350899200.0000 - rmse: 18732.3027 - val_loss: 539303744.0000 - val_rmse: 23222.9141\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329893376.0000 - rmse: 18162.9648 - val_loss: 630470336.0000 - val_rmse: 25109.1680\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368581664.0000 - rmse: 19198.4805 - val_loss: 577853632.0000 - val_rmse: 24038.5859\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332622240.0000 - rmse: 18237.9336 - val_loss: 584631232.0000 - val_rmse: 24179.1465\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356295712.0000 - rmse: 18875.7949 - val_loss: 553850048.0000 - val_rmse: 23534.0176\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327071264.0000 - rmse: 18085.1094 - val_loss: 508459328.0000 - val_rmse: 22549.0430\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304759072.0000 - rmse: 17457.3496 - val_loss: 586583488.0000 - val_rmse: 24219.4844\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295710592.0000 - rmse: 17196.2363 - val_loss: 693856832.0000 - val_rmse: 26341.1602\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341033536.0000 - rmse: 18467.0938 - val_loss: 487528288.0000 - val_rmse: 22080.0430\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319760960.0000 - rmse: 17881.8594 - val_loss: 602739712.0000 - val_rmse: 24550.7578\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308251904.0000 - rmse: 17557.1035 - val_loss: 575051776.0000 - val_rmse: 23980.2363\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352884992.0000 - rmse: 18785.2324 - val_loss: 575174784.0000 - val_rmse: 23982.8008\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396291264.0000 - rmse: 19907.0625 - val_loss: 520164832.0000 - val_rmse: 22807.1211\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333907104.0000 - rmse: 18273.1250 - val_loss: 578483136.0000 - val_rmse: 24051.6758\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313800288.0000 - rmse: 17714.4082 - val_loss: 614328960.0000 - val_rmse: 24785.6582\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364906560.0000 - rmse: 19102.5273 - val_loss: 615159616.0000 - val_rmse: 24802.4121\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379259456.0000 - rmse: 19474.5840 - val_loss: 491748192.0000 - val_rmse: 22175.3945\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292309760.0000 - rmse: 17097.0684 - val_loss: 614928320.0000 - val_rmse: 24797.7461\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342303360.0000 - rmse: 18501.4414 - val_loss: 500740864.0000 - val_rmse: 22377.2402\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300823584.0000 - rmse: 17344.2637 - val_loss: 558150976.0000 - val_rmse: 23625.2168\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316890944.0000 - rmse: 17801.4297 - val_loss: 564093504.0000 - val_rmse: 23750.6504\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302520608.0000 - rmse: 17393.1172 - val_loss: 515067936.0000 - val_rmse: 22695.1055\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288426432.0000 - rmse: 16983.1211 - val_loss: 548418688.0000 - val_rmse: 23418.3418\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294596416.0000 - rmse: 17163.8086 - val_loss: 586140672.0000 - val_rmse: 24210.3418\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292563264.0000 - rmse: 17104.4805 - val_loss: 611645760.0000 - val_rmse: 24731.4727\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293515168.0000 - rmse: 17132.2832 - val_loss: 418183968.0000 - val_rmse: 20449.5449\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406078944.0000 - rmse: 20151.3984 - val_loss: 585648064.0000 - val_rmse: 24200.1641\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348686944.0000 - rmse: 18673.1582 - val_loss: 448424896.0000 - val_rmse: 21176.0430\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332243744.0000 - rmse: 18227.5527 - val_loss: 639457088.0000 - val_rmse: 25287.4883\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353594304.0000 - rmse: 18804.1016 - val_loss: 535271840.0000 - val_rmse: 23135.9414\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332180288.0000 - rmse: 18225.8145 - val_loss: 533558624.0000 - val_rmse: 23098.8867\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343447872.0000 - rmse: 18532.3438 - val_loss: 541437376.0000 - val_rmse: 23268.8047\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307471520.0000 - rmse: 17534.8633 - val_loss: 561407808.0000 - val_rmse: 23694.0449\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311061184.0000 - rmse: 17636.9238 - val_loss: 537342848.0000 - val_rmse: 23180.6562\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366467776.0000 - rmse: 19143.3457 - val_loss: 560325760.0000 - val_rmse: 23671.1992\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306292064.0000 - rmse: 17501.1992 - val_loss: 541704384.0000 - val_rmse: 23274.5430\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322883840.0000 - rmse: 17968.9668 - val_loss: 494181120.0000 - val_rmse: 22230.1836\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315153280.0000 - rmse: 17752.5566 - val_loss: 607066176.0000 - val_rmse: 24638.7109\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431576736.0000 - rmse: 20774.4258 - val_loss: 558480640.0000 - val_rmse: 23632.1953\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303139904.0000 - rmse: 17410.9121 - val_loss: 468068032.0000 - val_rmse: 21634.8770\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334397664.0000 - rmse: 18286.5410 - val_loss: 498665856.0000 - val_rmse: 22330.8262\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320846400.0000 - rmse: 17912.1855 - val_loss: 526903552.0000 - val_rmse: 22954.3770\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275156192.0000 - rmse: 16587.8301 - val_loss: 510622592.0000 - val_rmse: 22596.9590\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380311712.0000 - rmse: 19501.5801 - val_loss: 500520032.0000 - val_rmse: 22372.3047\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301760288.0000 - rmse: 17371.2461 - val_loss: 478588160.0000 - val_rmse: 21876.6562\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325528448.0000 - rmse: 18042.4043 - val_loss: 527195296.0000 - val_rmse: 22960.7344\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301536352.0000 - rmse: 17364.7988 - val_loss: 641435520.0000 - val_rmse: 25326.5762\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277833120.0000 - rmse: 16668.3242 - val_loss: 557148800.0000 - val_rmse: 23604.0000\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370699648.0000 - rmse: 19253.5605 - val_loss: 480971680.0000 - val_rmse: 21931.0645\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278390688.0000 - rmse: 16685.0410 - val_loss: 636051840.0000 - val_rmse: 25220.0664\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292894944.0000 - rmse: 17114.1719 - val_loss: 770724096.0000 - val_rmse: 27761.9160\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287239040.0000 - rmse: 16948.1230 - val_loss: 741027072.0000 - val_rmse: 27221.8105\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339135968.0000 - rmse: 18415.6445 - val_loss: 622367808.0000 - val_rmse: 24947.2988\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289505184.0000 - rmse: 17014.8496 - val_loss: 649059904.0000 - val_rmse: 25476.6523\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349224160.0000 - rmse: 18687.5391 - val_loss: 698680128.0000 - val_rmse: 26432.5566\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293333504.0000 - rmse: 17126.9805 - val_loss: 741111872.0000 - val_rmse: 27223.3691\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299010784.0000 - rmse: 17291.9258 - val_loss: 836172096.0000 - val_rmse: 28916.6406\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303542144.0000 - rmse: 17422.4590 - val_loss: 549603328.0000 - val_rmse: 23443.6172\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284842656.0000 - rmse: 16877.2812 - val_loss: 563369792.0000 - val_rmse: 23735.4102\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444403424.0000 - rmse: 21080.8750 - val_loss: 513640544.0000 - val_rmse: 22663.6387\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359962336.0000 - rmse: 18972.6719 - val_loss: 477560448.0000 - val_rmse: 21853.1543\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272777536.0000 - rmse: 16515.9746 - val_loss: 467904096.0000 - val_rmse: 21631.0898\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271347200.0000 - rmse: 16472.6191 - val_loss: 559622656.0000 - val_rmse: 23656.3438\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252934928.0000 - rmse: 15903.9268 - val_loss: 609775616.0000 - val_rmse: 24693.6348\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292195584.0000 - rmse: 17093.7266 - val_loss: 520979168.0000 - val_rmse: 22824.9668\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302261856.0000 - rmse: 17385.6758 - val_loss: 581796096.0000 - val_rmse: 24120.4492\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312581280.0000 - rmse: 17679.9668 - val_loss: 473670944.0000 - val_rmse: 21763.9824\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258211104.0000 - rmse: 16068.9473 - val_loss: 527845312.0000 - val_rmse: 22974.8828\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263702128.0000 - rmse: 16238.9043 - val_loss: 581715456.0000 - val_rmse: 24118.7773\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278991936.0000 - rmse: 16703.0508 - val_loss: 636607104.0000 - val_rmse: 25231.0723\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274195744.0000 - rmse: 16558.8555 - val_loss: 476661024.0000 - val_rmse: 21832.5684\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307047456.0000 - rmse: 17522.7676 - val_loss: 531186400.0000 - val_rmse: 23047.4785\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252972048.0000 - rmse: 15905.0918 - val_loss: 626776384.0000 - val_rmse: 25035.5000\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293035680.0000 - rmse: 17118.2832 - val_loss: 576261376.0000 - val_rmse: 24005.4434\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301005696.0000 - rmse: 17349.5137 - val_loss: 543680384.0000 - val_rmse: 23316.9512\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273908992.0000 - rmse: 16550.1914 - val_loss: 586527488.0000 - val_rmse: 24218.3281\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313615968.0000 - rmse: 17709.2012 - val_loss: 704360064.0000 - val_rmse: 26539.7812\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274608160.0000 - rmse: 16571.3008 - val_loss: 567972672.0000 - val_rmse: 23832.1738\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293182720.0000 - rmse: 17122.5781 - val_loss: 490932864.0000 - val_rmse: 22157.0039\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296492160.0000 - rmse: 17218.9453 - val_loss: 461568544.0000 - val_rmse: 21484.1445\n",
      "104/104 [==============================] - 0s 665us/step - loss: 752509248.0000 - rmse: 27431.9023\n",
      "[752509248.0, 27431.90234375]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 6712804864.0000 - rmse: 81931.7109 - val_loss: 1544660736.0000 - val_rmse: 39302.1719\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1600659840.0000 - rmse: 40008.2461 - val_loss: 1216830720.0000 - val_rmse: 34883.1016\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1376365184.0000 - rmse: 37099.3945 - val_loss: 1132827648.0000 - val_rmse: 33657.5039\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1276534656.0000 - rmse: 35728.6250 - val_loss: 984049408.0000 - val_rmse: 31369.5625\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1153006464.0000 - rmse: 33955.9492 - val_loss: 980571008.0000 - val_rmse: 31314.0703\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1173106560.0000 - rmse: 34250.6445 - val_loss: 954232704.0000 - val_rmse: 30890.6582\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1138491136.0000 - rmse: 33741.5352 - val_loss: 910595136.0000 - val_rmse: 30176.0684\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1074977536.0000 - rmse: 32786.8516 - val_loss: 905166144.0000 - val_rmse: 30085.9785\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1002838976.0000 - rmse: 31667.6328 - val_loss: 971285696.0000 - val_rmse: 31165.4570\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1029905472.0000 - rmse: 32092.1387 - val_loss: 962289088.0000 - val_rmse: 31020.7852\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1019310400.0000 - rmse: 31926.6406 - val_loss: 969579200.0000 - val_rmse: 31138.0664\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 945095616.0000 - rmse: 30742.4082 - val_loss: 897964864.0000 - val_rmse: 29966.0625\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 962774336.0000 - rmse: 31028.6055 - val_loss: 1310957952.0000 - val_rmse: 36207.1523\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 984232896.0000 - rmse: 31372.4863 - val_loss: 868628736.0000 - val_rmse: 29472.5078\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 969779712.0000 - rmse: 31141.2871 - val_loss: 892744448.0000 - val_rmse: 29878.8301\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 951509312.0000 - rmse: 30846.5449 - val_loss: 870164416.0000 - val_rmse: 29498.5469\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 923315904.0000 - rmse: 30386.1133 - val_loss: 855504640.0000 - val_rmse: 29249.0117\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 929711552.0000 - rmse: 30491.1719 - val_loss: 930918464.0000 - val_rmse: 30510.9551\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 864581120.0000 - rmse: 29403.7598 - val_loss: 894153344.0000 - val_rmse: 29902.3965\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 826676032.0000 - rmse: 28751.9746 - val_loss: 922825152.0000 - val_rmse: 30378.0371\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 857647680.0000 - rmse: 29285.6230 - val_loss: 1675365760.0000 - val_rmse: 40931.2305\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 854474560.0000 - rmse: 29231.3965 - val_loss: 935733888.0000 - val_rmse: 30589.7676\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 765850112.0000 - rmse: 27673.9961 - val_loss: 2504960000.0000 - val_rmse: 50049.5742\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 850955456.0000 - rmse: 29171.1406 - val_loss: 1142204928.0000 - val_rmse: 33796.5234\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 831674752.0000 - rmse: 28838.7715 - val_loss: 2237773056.0000 - val_rmse: 47305.1055\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 845247744.0000 - rmse: 29073.1445 - val_loss: 908425408.0000 - val_rmse: 30140.0957\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 757070400.0000 - rmse: 27514.9121 - val_loss: 1022047424.0000 - val_rmse: 31969.4766\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 764213824.0000 - rmse: 27644.4180 - val_loss: 938471552.0000 - val_rmse: 30634.4824\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 811901184.0000 - rmse: 28493.8789 - val_loss: 997185408.0000 - val_rmse: 31578.2402\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 780266688.0000 - rmse: 27933.2539 - val_loss: 1115777408.0000 - val_rmse: 33403.2539\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782183744.0000 - rmse: 27967.5488 - val_loss: 1193047680.0000 - val_rmse: 34540.5234\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 735096576.0000 - rmse: 27112.6641 - val_loss: 977443136.0000 - val_rmse: 31264.0859\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 717126656.0000 - rmse: 26779.2188 - val_loss: 1221501952.0000 - val_rmse: 34949.9922\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680776960.0000 - rmse: 26091.7012 - val_loss: 948551360.0000 - val_rmse: 30798.5586\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634440960.0000 - rmse: 25188.1113 - val_loss: 1451864704.0000 - val_rmse: 38103.3398\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704000576.0000 - rmse: 26533.0098 - val_loss: 974444160.0000 - val_rmse: 31216.0879\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599219328.0000 - rmse: 24478.9570 - val_loss: 1465324544.0000 - val_rmse: 38279.5586\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677680960.0000 - rmse: 26032.3066 - val_loss: 661488448.0000 - val_rmse: 25719.4160\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 688572352.0000 - rmse: 26240.6621 - val_loss: 728086272.0000 - val_rmse: 26983.0742\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 645493440.0000 - rmse: 25406.5625 - val_loss: 753957376.0000 - val_rmse: 27458.2852\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677658816.0000 - rmse: 26031.8809 - val_loss: 844941056.0000 - val_rmse: 29067.8691\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648630464.0000 - rmse: 25468.2246 - val_loss: 839096512.0000 - val_rmse: 28967.1621\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612530944.0000 - rmse: 24749.3633 - val_loss: 899570688.0000 - val_rmse: 29992.8438\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570543680.0000 - rmse: 23886.0547 - val_loss: 730807296.0000 - val_rmse: 27033.4453\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579623296.0000 - rmse: 24075.3672 - val_loss: 918355392.0000 - val_rmse: 30304.3789\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589724608.0000 - rmse: 24284.2461 - val_loss: 1837830016.0000 - val_rmse: 42869.9180\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582183872.0000 - rmse: 24128.4863 - val_loss: 1049873984.0000 - val_rmse: 32401.7598\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532256416.0000 - rmse: 23070.6816 - val_loss: 1233666688.0000 - val_rmse: 35123.5898\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549282432.0000 - rmse: 23436.7734 - val_loss: 1236080768.0000 - val_rmse: 35157.9375\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550206592.0000 - rmse: 23456.4824 - val_loss: 675960704.0000 - val_rmse: 25999.2422\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477667264.0000 - rmse: 21855.5996 - val_loss: 770418496.0000 - val_rmse: 27756.4141\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545209344.0000 - rmse: 23349.7188 - val_loss: 625708480.0000 - val_rmse: 25014.1660\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578491328.0000 - rmse: 24051.8457 - val_loss: 817930432.0000 - val_rmse: 28599.4824\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472155456.0000 - rmse: 21729.1367 - val_loss: 643434304.0000 - val_rmse: 25366.0078\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468753376.0000 - rmse: 21650.7109 - val_loss: 872711616.0000 - val_rmse: 29541.6914\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555639296.0000 - rmse: 23572.0000 - val_loss: 771984768.0000 - val_rmse: 27784.6133\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536925504.0000 - rmse: 23171.6523 - val_loss: 659597568.0000 - val_rmse: 25682.6309\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546489920.0000 - rmse: 23377.1230 - val_loss: 1022792960.0000 - val_rmse: 31981.1328\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524930176.0000 - rmse: 22911.3535 - val_loss: 763518464.0000 - val_rmse: 27631.8359\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477142976.0000 - rmse: 21843.5996 - val_loss: 490418208.0000 - val_rmse: 22145.3867\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465690528.0000 - rmse: 21579.8613 - val_loss: 839914816.0000 - val_rmse: 28981.2832\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462165952.0000 - rmse: 21498.0410 - val_loss: 487391616.0000 - val_rmse: 22076.9453\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420856544.0000 - rmse: 20514.7871 - val_loss: 576485824.0000 - val_rmse: 24010.1191\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504068640.0000 - rmse: 22451.4707 - val_loss: 672521984.0000 - val_rmse: 25933.0273\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500056064.0000 - rmse: 22361.9336 - val_loss: 475047200.0000 - val_rmse: 21795.5781\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438303680.0000 - rmse: 20935.7031 - val_loss: 516452800.0000 - val_rmse: 22725.5977\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526705376.0000 - rmse: 22950.0605 - val_loss: 511951584.0000 - val_rmse: 22626.3477\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428813120.0000 - rmse: 20707.8027 - val_loss: 517419680.0000 - val_rmse: 22746.8594\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387203040.0000 - rmse: 19677.4746 - val_loss: 707174144.0000 - val_rmse: 26592.7461\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400271008.0000 - rmse: 20006.7734 - val_loss: 890318464.0000 - val_rmse: 29838.2031\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500002144.0000 - rmse: 22360.7246 - val_loss: 957609536.0000 - val_rmse: 30945.2656\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417387904.0000 - rmse: 20430.0703 - val_loss: 1173147392.0000 - val_rmse: 34251.2383\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440616096.0000 - rmse: 20990.8574 - val_loss: 517962368.0000 - val_rmse: 22758.7871\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366769696.0000 - rmse: 19151.2324 - val_loss: 1692382464.0000 - val_rmse: 41138.5781\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403051872.0000 - rmse: 20076.1504 - val_loss: 783737280.0000 - val_rmse: 27995.3086\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414570560.0000 - rmse: 20361.0039 - val_loss: 648708032.0000 - val_rmse: 25469.7441\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393684064.0000 - rmse: 19841.4727 - val_loss: 1318935808.0000 - val_rmse: 36317.1562\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413688384.0000 - rmse: 20339.3301 - val_loss: 1788828800.0000 - val_rmse: 42294.5469\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371584032.0000 - rmse: 19276.5137 - val_loss: 527435168.0000 - val_rmse: 22965.9551\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386249536.0000 - rmse: 19653.2305 - val_loss: 511360640.0000 - val_rmse: 22613.2832\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409405792.0000 - rmse: 20233.7773 - val_loss: 749105536.0000 - val_rmse: 27369.7930\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397757536.0000 - rmse: 19943.8594 - val_loss: 559526080.0000 - val_rmse: 23654.3027\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396256224.0000 - rmse: 19906.1836 - val_loss: 828062912.0000 - val_rmse: 28776.0801\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382447072.0000 - rmse: 19556.2539 - val_loss: 723590464.0000 - val_rmse: 26899.6367\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431921952.0000 - rmse: 20782.7324 - val_loss: 597666368.0000 - val_rmse: 24447.2148\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435239744.0000 - rmse: 20862.3984 - val_loss: 564792576.0000 - val_rmse: 23765.3633\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398165472.0000 - rmse: 19954.0840 - val_loss: 1188938624.0000 - val_rmse: 34480.9883\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423793760.0000 - rmse: 20586.2500 - val_loss: 545786304.0000 - val_rmse: 23362.0664\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408955104.0000 - rmse: 20222.6367 - val_loss: 488728288.0000 - val_rmse: 22107.1992\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443930752.0000 - rmse: 21069.6621 - val_loss: 746155200.0000 - val_rmse: 27315.8398\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329484128.0000 - rmse: 18151.6953 - val_loss: 945301120.0000 - val_rmse: 30745.7461\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370665536.0000 - rmse: 19252.6758 - val_loss: 1025732032.0000 - val_rmse: 32027.0508\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416629536.0000 - rmse: 20411.5039 - val_loss: 1331335680.0000 - val_rmse: 36487.4727\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390420608.0000 - rmse: 19759.0605 - val_loss: 835515200.0000 - val_rmse: 28905.2793\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386046880.0000 - rmse: 19648.0742 - val_loss: 943274496.0000 - val_rmse: 30712.7734\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330572928.0000 - rmse: 18181.6621 - val_loss: 842341312.0000 - val_rmse: 29023.1172\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345399808.0000 - rmse: 18584.9336 - val_loss: 438263136.0000 - val_rmse: 20934.7324\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350217664.0000 - rmse: 18714.1035 - val_loss: 902044288.0000 - val_rmse: 30034.0527\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372234368.0000 - rmse: 19293.3730 - val_loss: 1306798464.0000 - val_rmse: 36149.6680\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419276160.0000 - rmse: 20476.2324 - val_loss: 996798976.0000 - val_rmse: 31572.1211\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356349920.0000 - rmse: 18877.2285 - val_loss: 2503057664.0000 - val_rmse: 50030.5664\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422586048.0000 - rmse: 20556.8965 - val_loss: 541649920.0000 - val_rmse: 23273.3691\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371762112.0000 - rmse: 19281.1309 - val_loss: 1152888704.0000 - val_rmse: 33954.2148\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376430304.0000 - rmse: 19401.8086 - val_loss: 650564672.0000 - val_rmse: 25506.1699\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388089856.0000 - rmse: 19699.9941 - val_loss: 411929440.0000 - val_rmse: 20296.0430\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405596480.0000 - rmse: 20139.4238 - val_loss: 420966944.0000 - val_rmse: 20517.4766\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327299872.0000 - rmse: 18091.4277 - val_loss: 1022753280.0000 - val_rmse: 31980.5117\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372540864.0000 - rmse: 19301.3164 - val_loss: 575980800.0000 - val_rmse: 23999.5977\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310769056.0000 - rmse: 17628.6426 - val_loss: 736780096.0000 - val_rmse: 27143.6914\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323157664.0000 - rmse: 17976.5840 - val_loss: 520257888.0000 - val_rmse: 22809.1602\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330172928.0000 - rmse: 18170.6602 - val_loss: 451116896.0000 - val_rmse: 21239.5098\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328329664.0000 - rmse: 18119.8672 - val_loss: 644830016.0000 - val_rmse: 25393.5000\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351078784.0000 - rmse: 18737.0957 - val_loss: 956467776.0000 - val_rmse: 30926.8125\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368412288.0000 - rmse: 19194.0664 - val_loss: 448717760.0000 - val_rmse: 21182.9570\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328803456.0000 - rmse: 18132.9355 - val_loss: 617230720.0000 - val_rmse: 24844.1289\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347537024.0000 - rmse: 18642.3438 - val_loss: 872361792.0000 - val_rmse: 29535.7715\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376645984.0000 - rmse: 19407.3672 - val_loss: 700703552.0000 - val_rmse: 26470.8047\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363011424.0000 - rmse: 19052.8594 - val_loss: 987207872.0000 - val_rmse: 31419.8652\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344140576.0000 - rmse: 18551.0254 - val_loss: 837039936.0000 - val_rmse: 28931.6406\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378917760.0000 - rmse: 19465.8066 - val_loss: 1486266624.0000 - val_rmse: 38552.1289\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342515168.0000 - rmse: 18507.1621 - val_loss: 776262720.0000 - val_rmse: 27861.4883\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371988032.0000 - rmse: 19286.9883 - val_loss: 764278784.0000 - val_rmse: 27645.5918\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349110176.0000 - rmse: 18684.4883 - val_loss: 480050688.0000 - val_rmse: 21910.0566\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305456704.0000 - rmse: 17477.3184 - val_loss: 640729536.0000 - val_rmse: 25312.6348\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327670432.0000 - rmse: 18101.6660 - val_loss: 511139520.0000 - val_rmse: 22608.3926\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358614368.0000 - rmse: 18937.1152 - val_loss: 699006848.0000 - val_rmse: 26438.7383\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302435648.0000 - rmse: 17390.6758 - val_loss: 498743136.0000 - val_rmse: 22332.5547\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338859488.0000 - rmse: 18408.1348 - val_loss: 544481088.0000 - val_rmse: 23334.1191\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321899904.0000 - rmse: 17941.5664 - val_loss: 523478624.0000 - val_rmse: 22879.6523\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283656320.0000 - rmse: 16842.0977 - val_loss: 1399400704.0000 - val_rmse: 37408.5664\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284719040.0000 - rmse: 16873.6191 - val_loss: 836909568.0000 - val_rmse: 28929.3887\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256656656.0000 - rmse: 16020.5029 - val_loss: 820378304.0000 - val_rmse: 28642.2461\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273228864.0000 - rmse: 16529.6328 - val_loss: 514027392.0000 - val_rmse: 22672.1719\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355035136.0000 - rmse: 18842.3750 - val_loss: 541553984.0000 - val_rmse: 23271.3105\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325429088.0000 - rmse: 18039.6504 - val_loss: 680367424.0000 - val_rmse: 26083.8516\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306829888.0000 - rmse: 17516.5586 - val_loss: 981109632.0000 - val_rmse: 31322.6699\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290865344.0000 - rmse: 17054.7715 - val_loss: 602680192.0000 - val_rmse: 24549.5410\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295350080.0000 - rmse: 17185.7520 - val_loss: 586846848.0000 - val_rmse: 24224.9199\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299006016.0000 - rmse: 17291.7871 - val_loss: 445016928.0000 - val_rmse: 21095.4238\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261894992.0000 - rmse: 16183.1689 - val_loss: 863416640.0000 - val_rmse: 29383.9492\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347676768.0000 - rmse: 18646.0898 - val_loss: 673825280.0000 - val_rmse: 25958.1426\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315148864.0000 - rmse: 17752.4297 - val_loss: 472641056.0000 - val_rmse: 21740.3066\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327747456.0000 - rmse: 18103.7949 - val_loss: 771672832.0000 - val_rmse: 27778.9980\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303979200.0000 - rmse: 17434.9961 - val_loss: 535833632.0000 - val_rmse: 23148.0801\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334907936.0000 - rmse: 18300.4883 - val_loss: 697782912.0000 - val_rmse: 26415.5781\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268212320.0000 - rmse: 16377.1865 - val_loss: 511742272.0000 - val_rmse: 22621.7207\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304767072.0000 - rmse: 17457.5781 - val_loss: 695256064.0000 - val_rmse: 26367.7070\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308619040.0000 - rmse: 17567.5527 - val_loss: 450624192.0000 - val_rmse: 21227.9102\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313060992.0000 - rmse: 17693.5273 - val_loss: 1062947328.0000 - val_rmse: 32602.8691\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298373504.0000 - rmse: 17273.4883 - val_loss: 440711072.0000 - val_rmse: 20993.1172\n",
      "104/104 [==============================] - 0s 713us/step - loss: 693414720.0000 - rmse: 26332.7656\n",
      "[693414720.0, 26332.765625]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 7757052928.0000 - rmse: 88074.1328 - val_loss: 1455796224.0000 - val_rmse: 38154.8984\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1784855680.0000 - rmse: 42247.5508 - val_loss: 1350659456.0000 - val_rmse: 36751.3203\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1558466304.0000 - rmse: 39477.4141 - val_loss: 1023537664.0000 - val_rmse: 31992.7754\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1506879616.0000 - rmse: 38818.5469 - val_loss: 892234432.0000 - val_rmse: 29870.2930\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1392869120.0000 - rmse: 37321.1602 - val_loss: 898592704.0000 - val_rmse: 29976.5352\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1300892928.0000 - rmse: 36067.8945 - val_loss: 931055040.0000 - val_rmse: 30513.1953\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1279168256.0000 - rmse: 35765.4609 - val_loss: 850860288.0000 - val_rmse: 29169.5098\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1233107840.0000 - rmse: 35115.6328 - val_loss: 856135616.0000 - val_rmse: 29259.7949\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1194492800.0000 - rmse: 34561.4336 - val_loss: 831363776.0000 - val_rmse: 28833.3770\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1146275968.0000 - rmse: 33856.6992 - val_loss: 868897152.0000 - val_rmse: 29477.0605\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1152675712.0000 - rmse: 33951.0781 - val_loss: 835574656.0000 - val_rmse: 28906.3086\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1070120192.0000 - rmse: 32712.6914 - val_loss: 848114112.0000 - val_rmse: 29122.3984\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1011329152.0000 - rmse: 31801.4023 - val_loss: 837060736.0000 - val_rmse: 28932.0000\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1020863040.0000 - rmse: 31950.9473 - val_loss: 816976192.0000 - val_rmse: 28582.7949\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1022108736.0000 - rmse: 31970.4355 - val_loss: 874643392.0000 - val_rmse: 29574.3711\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1014251840.0000 - rmse: 31847.3203 - val_loss: 825457600.0000 - val_rmse: 28730.7773\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 990266816.0000 - rmse: 31468.5059 - val_loss: 813912512.0000 - val_rmse: 28529.1523\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1011736768.0000 - rmse: 31807.8086 - val_loss: 823107200.0000 - val_rmse: 28689.8457\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 893423616.0000 - rmse: 29890.1934 - val_loss: 1001747328.0000 - val_rmse: 31650.3926\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 887010560.0000 - rmse: 29782.7227 - val_loss: 985440448.0000 - val_rmse: 31391.7246\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 914864576.0000 - rmse: 30246.7285 - val_loss: 919849600.0000 - val_rmse: 30329.0215\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 889080128.0000 - rmse: 29817.4453 - val_loss: 871227712.0000 - val_rmse: 29516.5664\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 915930752.0000 - rmse: 30264.3477 - val_loss: 919975360.0000 - val_rmse: 30331.0957\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 828673344.0000 - rmse: 28786.6875 - val_loss: 946249600.0000 - val_rmse: 30761.1699\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 770684224.0000 - rmse: 27761.1992 - val_loss: 690037760.0000 - val_rmse: 26268.5664\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 760658752.0000 - rmse: 27580.0430 - val_loss: 905410112.0000 - val_rmse: 30090.0332\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 845813824.0000 - rmse: 29082.8789 - val_loss: 957909056.0000 - val_rmse: 30950.1055\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 699378880.0000 - rmse: 26445.7715 - val_loss: 861308608.0000 - val_rmse: 29348.0605\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 705180416.0000 - rmse: 26555.2324 - val_loss: 772524864.0000 - val_rmse: 27794.3320\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 776732096.0000 - rmse: 27869.9121 - val_loss: 712894208.0000 - val_rmse: 26700.0781\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724436544.0000 - rmse: 26915.3594 - val_loss: 826300736.0000 - val_rmse: 28745.4473\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 688260224.0000 - rmse: 26234.7109 - val_loss: 948008960.0000 - val_rmse: 30789.7539\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 757817408.0000 - rmse: 27528.4844 - val_loss: 1027607424.0000 - val_rmse: 32056.3164\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 697156288.0000 - rmse: 26403.7168 - val_loss: 1123911424.0000 - val_rmse: 33524.7852\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 709749184.0000 - rmse: 26641.1152 - val_loss: 1269681024.0000 - val_rmse: 35632.5820\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 662705600.0000 - rmse: 25743.0684 - val_loss: 952616192.0000 - val_rmse: 30864.4805\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 779350784.0000 - rmse: 27916.8555 - val_loss: 932174208.0000 - val_rmse: 30531.5273\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549605504.0000 - rmse: 23443.6641 - val_loss: 920760832.0000 - val_rmse: 30344.0391\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624247680.0000 - rmse: 24984.9473 - val_loss: 832409024.0000 - val_rmse: 28851.4980\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 686868864.0000 - rmse: 26208.1816 - val_loss: 973173696.0000 - val_rmse: 31195.7324\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 664353920.0000 - rmse: 25775.0645 - val_loss: 1205109888.0000 - val_rmse: 34714.6914\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561535296.0000 - rmse: 23696.7363 - val_loss: 709801664.0000 - val_rmse: 26642.1035\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 674178112.0000 - rmse: 25964.9395 - val_loss: 1016643904.0000 - val_rmse: 31884.8516\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576591808.0000 - rmse: 24012.3262 - val_loss: 1035975104.0000 - val_rmse: 32186.5664\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582041472.0000 - rmse: 24125.5352 - val_loss: 1288532992.0000 - val_rmse: 35896.1406\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 638806848.0000 - rmse: 25274.6270 - val_loss: 1159374976.0000 - val_rmse: 34049.5977\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580891072.0000 - rmse: 24101.6816 - val_loss: 742067776.0000 - val_rmse: 27240.9180\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556811712.0000 - rmse: 23596.8574 - val_loss: 1058265856.0000 - val_rmse: 32530.9961\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632363648.0000 - rmse: 25146.8418 - val_loss: 1109570304.0000 - val_rmse: 33310.2148\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596489024.0000 - rmse: 24423.1250 - val_loss: 826805504.0000 - val_rmse: 28754.2266\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616657728.0000 - rmse: 24832.5938 - val_loss: 595492160.0000 - val_rmse: 24402.7090\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525550208.0000 - rmse: 22924.8809 - val_loss: 1544708480.0000 - val_rmse: 39302.7812\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515813696.0000 - rmse: 22711.5312 - val_loss: 868245632.0000 - val_rmse: 29466.0059\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742784512.0000 - rmse: 27254.0742 - val_loss: 772174464.0000 - val_rmse: 27788.0273\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565343168.0000 - rmse: 23776.9434 - val_loss: 2157048576.0000 - val_rmse: 46444.0352\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568105984.0000 - rmse: 23834.9727 - val_loss: 961104640.0000 - val_rmse: 31001.6875\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637437952.0000 - rmse: 25247.5332 - val_loss: 805295872.0000 - val_rmse: 28377.7363\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494959776.0000 - rmse: 22247.6914 - val_loss: 1044028032.0000 - val_rmse: 32311.4219\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548889152.0000 - rmse: 23428.3828 - val_loss: 1226486400.0000 - val_rmse: 35021.2266\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596562048.0000 - rmse: 24424.6191 - val_loss: 883697984.0000 - val_rmse: 29727.0566\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506483136.0000 - rmse: 22505.1797 - val_loss: 3068155904.0000 - val_rmse: 55390.9336\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427103904.0000 - rmse: 20666.4902 - val_loss: 996491584.0000 - val_rmse: 31567.2559\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565642048.0000 - rmse: 23783.2285 - val_loss: 1674687488.0000 - val_rmse: 40922.9453\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478039936.0000 - rmse: 21864.1230 - val_loss: 761593472.0000 - val_rmse: 27596.9805\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486120704.0000 - rmse: 22048.1426 - val_loss: 1354190592.0000 - val_rmse: 36799.3281\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511131264.0000 - rmse: 22608.2109 - val_loss: 1206864512.0000 - val_rmse: 34739.9531\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495166400.0000 - rmse: 22252.3340 - val_loss: 1691138048.0000 - val_rmse: 41123.4492\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535392832.0000 - rmse: 23138.5566 - val_loss: 1365857536.0000 - val_rmse: 36957.5117\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537263744.0000 - rmse: 23178.9512 - val_loss: 1623366912.0000 - val_rmse: 40291.0273\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525224224.0000 - rmse: 22917.7695 - val_loss: 989784768.0000 - val_rmse: 31460.8438\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442931552.0000 - rmse: 21045.9395 - val_loss: 1451425920.0000 - val_rmse: 38097.5820\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450410720.0000 - rmse: 21222.8789 - val_loss: 681775616.0000 - val_rmse: 26110.8320\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550048704.0000 - rmse: 23453.1172 - val_loss: 1034169344.0000 - val_rmse: 32158.5039\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533728384.0000 - rmse: 23102.5625 - val_loss: 1179339136.0000 - val_rmse: 34341.5039\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471541504.0000 - rmse: 21715.0059 - val_loss: 908521216.0000 - val_rmse: 30141.6836\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441033824.0000 - rmse: 21000.8027 - val_loss: 1012871168.0000 - val_rmse: 31825.6367\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453153632.0000 - rmse: 21287.4043 - val_loss: 593431808.0000 - val_rmse: 24360.4551\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440848736.0000 - rmse: 20996.3945 - val_loss: 2230355968.0000 - val_rmse: 47226.6445\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378215648.0000 - rmse: 19447.7656 - val_loss: 816679616.0000 - val_rmse: 28577.6055\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516707744.0000 - rmse: 22731.2051 - val_loss: 711993984.0000 - val_rmse: 26683.2148\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360619968.0000 - rmse: 18989.9941 - val_loss: 1674669696.0000 - val_rmse: 40922.7266\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447606528.0000 - rmse: 21156.7129 - val_loss: 804978368.0000 - val_rmse: 28372.1387\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469616992.0000 - rmse: 21670.6465 - val_loss: 1142022912.0000 - val_rmse: 33793.8281\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339747040.0000 - rmse: 18432.2266 - val_loss: 1407424640.0000 - val_rmse: 37515.6562\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466141280.0000 - rmse: 21590.3047 - val_loss: 988776448.0000 - val_rmse: 31444.8164\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392634336.0000 - rmse: 19815.0020 - val_loss: 665011264.0000 - val_rmse: 25787.8125\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429557472.0000 - rmse: 20725.7676 - val_loss: 856189184.0000 - val_rmse: 29260.7090\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410489440.0000 - rmse: 20260.5371 - val_loss: 793628992.0000 - val_rmse: 28171.4199\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464198016.0000 - rmse: 21545.2539 - val_loss: 1132196224.0000 - val_rmse: 33648.1250\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365005024.0000 - rmse: 19105.1035 - val_loss: 1214181248.0000 - val_rmse: 34845.1016\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425436352.0000 - rmse: 20626.1074 - val_loss: 1121553408.0000 - val_rmse: 33489.6016\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487002272.0000 - rmse: 22068.1270 - val_loss: 713554240.0000 - val_rmse: 26712.4355\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385624864.0000 - rmse: 19637.3320 - val_loss: 941690240.0000 - val_rmse: 30686.9707\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382756032.0000 - rmse: 19564.1484 - val_loss: 758823744.0000 - val_rmse: 27546.7559\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415718944.0000 - rmse: 20389.1855 - val_loss: 694443328.0000 - val_rmse: 26352.2910\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433115328.0000 - rmse: 20811.4219 - val_loss: 1005773760.0000 - val_rmse: 31713.9355\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439663552.0000 - rmse: 20968.1543 - val_loss: 678786624.0000 - val_rmse: 26053.5332\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452181248.0000 - rmse: 21264.5527 - val_loss: 781687808.0000 - val_rmse: 27958.6797\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398823712.0000 - rmse: 19970.5684 - val_loss: 772242368.0000 - val_rmse: 27789.2480\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379638112.0000 - rmse: 19484.3027 - val_loss: 797459392.0000 - val_rmse: 28239.3184\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356800864.0000 - rmse: 18889.1738 - val_loss: 808143744.0000 - val_rmse: 28427.8652\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351268992.0000 - rmse: 18742.1680 - val_loss: 899527936.0000 - val_rmse: 29992.1289\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406209344.0000 - rmse: 20154.6328 - val_loss: 696452096.0000 - val_rmse: 26390.3770\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348359904.0000 - rmse: 18664.4004 - val_loss: 641038720.0000 - val_rmse: 25318.7402\n",
      "104/104 [==============================] - 0s 692us/step - loss: 359016000.0000 - rmse: 18947.7168\n",
      "[359016000.0, 18947.716796875]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 5470393856.0000 - rmse: 73962.1094 - val_loss: 1361628928.0000 - val_rmse: 36900.2578\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1784109696.0000 - rmse: 42238.7227 - val_loss: 1006787968.0000 - val_rmse: 31729.9219\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1606414720.0000 - rmse: 40080.1055 - val_loss: 999224832.0000 - val_rmse: 31610.5176\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1458119936.0000 - rmse: 38185.3359 - val_loss: 1077483648.0000 - val_rmse: 32825.0469\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1337527424.0000 - rmse: 36572.2227 - val_loss: 876147136.0000 - val_rmse: 29599.7832\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1304769920.0000 - rmse: 36121.5977 - val_loss: 885904512.0000 - val_rmse: 29764.1484\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1220994176.0000 - rmse: 34942.7266 - val_loss: 1714206336.0000 - val_rmse: 41402.9766\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1174790784.0000 - rmse: 34275.2227 - val_loss: 849430912.0000 - val_rmse: 29144.9980\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1135267712.0000 - rmse: 33693.7344 - val_loss: 911763200.0000 - val_rmse: 30195.4160\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1123531392.0000 - rmse: 33519.1211 - val_loss: 1255386624.0000 - val_rmse: 35431.4336\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1103779200.0000 - rmse: 33223.1719 - val_loss: 839982464.0000 - val_rmse: 28982.4512\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1004427456.0000 - rmse: 31692.7031 - val_loss: 836185664.0000 - val_rmse: 28916.8750\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1087223936.0000 - rmse: 32973.0781 - val_loss: 879632128.0000 - val_rmse: 29658.5938\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 969707712.0000 - rmse: 31140.1309 - val_loss: 724537536.0000 - val_rmse: 26917.2344\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 964716672.0000 - rmse: 31059.8887 - val_loss: 903298368.0000 - val_rmse: 30054.9199\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 931196352.0000 - rmse: 30515.5098 - val_loss: 1657920640.0000 - val_rmse: 40717.5703\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 952544448.0000 - rmse: 30863.3184 - val_loss: 739109312.0000 - val_rmse: 27186.5645\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 983859904.0000 - rmse: 31366.5410 - val_loss: 749922816.0000 - val_rmse: 27384.7188\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 951259584.0000 - rmse: 30842.4961 - val_loss: 888287808.0000 - val_rmse: 29804.1582\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 865644800.0000 - rmse: 29421.8418 - val_loss: 807215936.0000 - val_rmse: 28411.5449\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 856427904.0000 - rmse: 29264.7891 - val_loss: 798956672.0000 - val_rmse: 28265.8223\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 834499584.0000 - rmse: 28887.7051 - val_loss: 719767424.0000 - val_rmse: 26828.4805\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 854709248.0000 - rmse: 29235.4102 - val_loss: 930859200.0000 - val_rmse: 30509.9844\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 817802432.0000 - rmse: 28597.2461 - val_loss: 730330368.0000 - val_rmse: 27024.6250\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 748533248.0000 - rmse: 27359.3340 - val_loss: 965455808.0000 - val_rmse: 31071.7832\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 814673408.0000 - rmse: 28542.4824 - val_loss: 647230720.0000 - val_rmse: 25440.7305\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 753020160.0000 - rmse: 27441.2129 - val_loss: 587697216.0000 - val_rmse: 24242.4668\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782065472.0000 - rmse: 27965.4336 - val_loss: 713916352.0000 - val_rmse: 26719.2109\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 719272448.0000 - rmse: 26819.2559 - val_loss: 905797952.0000 - val_rmse: 30096.4766\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 644951744.0000 - rmse: 25395.9004 - val_loss: 669257472.0000 - val_rmse: 25870.0117\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 689894656.0000 - rmse: 26265.8457 - val_loss: 826873280.0000 - val_rmse: 28755.4043\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 671231104.0000 - rmse: 25908.1289 - val_loss: 651280832.0000 - val_rmse: 25520.2031\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665610368.0000 - rmse: 25799.4258 - val_loss: 615933504.0000 - val_rmse: 24818.0078\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742629760.0000 - rmse: 27251.2344 - val_loss: 678072896.0000 - val_rmse: 26039.8320\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642013120.0000 - rmse: 25337.9785 - val_loss: 839762432.0000 - val_rmse: 28978.6543\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 639915776.0000 - rmse: 25296.5547 - val_loss: 705866496.0000 - val_rmse: 26568.1484\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552124480.0000 - rmse: 23497.3301 - val_loss: 672686016.0000 - val_rmse: 25936.1914\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623813184.0000 - rmse: 24976.2520 - val_loss: 1123147648.0000 - val_rmse: 33513.3906\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652160448.0000 - rmse: 25537.4297 - val_loss: 671913024.0000 - val_rmse: 25921.2852\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551205568.0000 - rmse: 23477.7676 - val_loss: 879477120.0000 - val_rmse: 29655.9805\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606920768.0000 - rmse: 24635.7598 - val_loss: 1166900736.0000 - val_rmse: 34159.9297\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 690662272.0000 - rmse: 26280.4551 - val_loss: 672806528.0000 - val_rmse: 25938.5137\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550403968.0000 - rmse: 23460.6895 - val_loss: 605793344.0000 - val_rmse: 24612.8691\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599430912.0000 - rmse: 24483.2773 - val_loss: 960149248.0000 - val_rmse: 30986.2754\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542667136.0000 - rmse: 23295.2168 - val_loss: 865114688.0000 - val_rmse: 29412.8301\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648725312.0000 - rmse: 25470.0859 - val_loss: 692718016.0000 - val_rmse: 26319.5352\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562105152.0000 - rmse: 23708.7578 - val_loss: 683509248.0000 - val_rmse: 26144.0098\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558739584.0000 - rmse: 23637.6699 - val_loss: 709401280.0000 - val_rmse: 26634.5859\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503225568.0000 - rmse: 22432.6875 - val_loss: 665309824.0000 - val_rmse: 25793.5977\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531776192.0000 - rmse: 23060.2715 - val_loss: 730038592.0000 - val_rmse: 27019.2246\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560892864.0000 - rmse: 23683.1777 - val_loss: 794073216.0000 - val_rmse: 28179.3047\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540595584.0000 - rmse: 23250.7109 - val_loss: 758718144.0000 - val_rmse: 27544.8359\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500791232.0000 - rmse: 22378.3652 - val_loss: 610158976.0000 - val_rmse: 24701.3945\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519256064.0000 - rmse: 22787.1895 - val_loss: 691327872.0000 - val_rmse: 26293.1152\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466071680.0000 - rmse: 21588.6934 - val_loss: 609886912.0000 - val_rmse: 24695.8887\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453169280.0000 - rmse: 21287.7734 - val_loss: 928819136.0000 - val_rmse: 30476.5352\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507858688.0000 - rmse: 22535.7188 - val_loss: 722628096.0000 - val_rmse: 26881.7422\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516867808.0000 - rmse: 22734.7266 - val_loss: 798736448.0000 - val_rmse: 28261.9238\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524156160.0000 - rmse: 22894.4551 - val_loss: 622558400.0000 - val_rmse: 24951.1191\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514318592.0000 - rmse: 22678.5898 - val_loss: 581201408.0000 - val_rmse: 24108.1172\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536301600.0000 - rmse: 23158.1855 - val_loss: 636331776.0000 - val_rmse: 25225.6172\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470952896.0000 - rmse: 21701.4492 - val_loss: 893699072.0000 - val_rmse: 29894.8008\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470574240.0000 - rmse: 21692.7227 - val_loss: 644551232.0000 - val_rmse: 25388.0137\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390585952.0000 - rmse: 19763.2461 - val_loss: 682882368.0000 - val_rmse: 26132.0176\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405577056.0000 - rmse: 20138.9434 - val_loss: 881970816.0000 - val_rmse: 29697.9941\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397986304.0000 - rmse: 19949.5918 - val_loss: 599748416.0000 - val_rmse: 24489.7617\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436754368.0000 - rmse: 20898.6680 - val_loss: 594192448.0000 - val_rmse: 24376.0625\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475718144.0000 - rmse: 21810.9629 - val_loss: 743673984.0000 - val_rmse: 27270.3848\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423902976.0000 - rmse: 20588.9023 - val_loss: 766106048.0000 - val_rmse: 27678.6211\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407669632.0000 - rmse: 20190.8301 - val_loss: 644234368.0000 - val_rmse: 25381.7715\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394300192.0000 - rmse: 19856.9922 - val_loss: 628760896.0000 - val_rmse: 25075.1055\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446488480.0000 - rmse: 21130.2715 - val_loss: 687111232.0000 - val_rmse: 26212.8066\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436965472.0000 - rmse: 20903.7188 - val_loss: 595918208.0000 - val_rmse: 24411.4355\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412757184.0000 - rmse: 20316.4258 - val_loss: 646112832.0000 - val_rmse: 25418.7480\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380763616.0000 - rmse: 19513.1660 - val_loss: 1013520000.0000 - val_rmse: 31835.8281\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450199648.0000 - rmse: 21217.9082 - val_loss: 670266048.0000 - val_rmse: 25889.4961\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411885568.0000 - rmse: 20294.9629 - val_loss: 693724032.0000 - val_rmse: 26338.6406\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392023808.0000 - rmse: 19799.5898 - val_loss: 622641984.0000 - val_rmse: 24952.7949\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395090656.0000 - rmse: 19876.8867 - val_loss: 667189568.0000 - val_rmse: 25830.0137\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422263552.0000 - rmse: 20549.0508 - val_loss: 773441152.0000 - val_rmse: 27810.8086\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356302880.0000 - rmse: 18875.9844 - val_loss: 926943040.0000 - val_rmse: 30445.7383\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401138304.0000 - rmse: 20028.4375 - val_loss: 955929344.0000 - val_rmse: 30918.1074\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388473760.0000 - rmse: 19709.7363 - val_loss: 592971136.0000 - val_rmse: 24350.9980\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359137760.0000 - rmse: 18950.9277 - val_loss: 1180260608.0000 - val_rmse: 34354.9219\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379206624.0000 - rmse: 19473.2266 - val_loss: 812575360.0000 - val_rmse: 28505.7070\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462198016.0000 - rmse: 21498.7910 - val_loss: 701924096.0000 - val_rmse: 26493.8496\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378743040.0000 - rmse: 19461.3223 - val_loss: 1072270464.0000 - val_rmse: 32745.5410\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417823008.0000 - rmse: 20440.7188 - val_loss: 664858432.0000 - val_rmse: 25784.8496\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421371008.0000 - rmse: 20527.3242 - val_loss: 782499968.0000 - val_rmse: 27973.2012\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392692704.0000 - rmse: 19816.4746 - val_loss: 742099968.0000 - val_rmse: 27241.5117\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397896768.0000 - rmse: 19947.3477 - val_loss: 736169472.0000 - val_rmse: 27132.4414\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379643328.0000 - rmse: 19484.4355 - val_loss: 896403520.0000 - val_rmse: 29939.9980\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382645792.0000 - rmse: 19561.3340 - val_loss: 1055369600.0000 - val_rmse: 32486.4531\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403386784.0000 - rmse: 20084.4902 - val_loss: 842170304.0000 - val_rmse: 29020.1699\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374487840.0000 - rmse: 19351.6855 - val_loss: 756466496.0000 - val_rmse: 27503.9355\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328544832.0000 - rmse: 18125.8027 - val_loss: 756157440.0000 - val_rmse: 27498.3164\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404524128.0000 - rmse: 20112.7852 - val_loss: 673825728.0000 - val_rmse: 25958.1543\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307372480.0000 - rmse: 17532.0410 - val_loss: 844056064.0000 - val_rmse: 29052.6426\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314264896.0000 - rmse: 17727.5176 - val_loss: 690679168.0000 - val_rmse: 26280.7754\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357928160.0000 - rmse: 18918.9883 - val_loss: 760079168.0000 - val_rmse: 27569.5332\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346136064.0000 - rmse: 18604.7324 - val_loss: 772641536.0000 - val_rmse: 27796.4297\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369479616.0000 - rmse: 19221.8535 - val_loss: 941981248.0000 - val_rmse: 30691.7129\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395976288.0000 - rmse: 19899.1523 - val_loss: 691617536.0000 - val_rmse: 26298.6211\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375124768.0000 - rmse: 19368.1367 - val_loss: 744043968.0000 - val_rmse: 27277.1680\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375153504.0000 - rmse: 19368.8789 - val_loss: 688776192.0000 - val_rmse: 26244.5449\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404619680.0000 - rmse: 20115.1602 - val_loss: 687318784.0000 - val_rmse: 26216.7637\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332539360.0000 - rmse: 18235.6602 - val_loss: 798825600.0000 - val_rmse: 28263.5039\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363630624.0000 - rmse: 19069.0996 - val_loss: 809480256.0000 - val_rmse: 28451.3672\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393676896.0000 - rmse: 19841.2930 - val_loss: 778072576.0000 - val_rmse: 27893.9512\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305421280.0000 - rmse: 17476.3047 - val_loss: 863206016.0000 - val_rmse: 29380.3672\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332604384.0000 - rmse: 18237.4434 - val_loss: 626444096.0000 - val_rmse: 25028.8652\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378220864.0000 - rmse: 19447.9004 - val_loss: 653883264.0000 - val_rmse: 25571.1387\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329699488.0000 - rmse: 18157.6250 - val_loss: 995837248.0000 - val_rmse: 31556.8887\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342683520.0000 - rmse: 18511.7109 - val_loss: 695657920.0000 - val_rmse: 26375.3281\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336438592.0000 - rmse: 18342.2598 - val_loss: 792286656.0000 - val_rmse: 28147.5879\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358776480.0000 - rmse: 18941.3945 - val_loss: 724238784.0000 - val_rmse: 26911.6855\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334837120.0000 - rmse: 18298.5527 - val_loss: 710671552.0000 - val_rmse: 26658.4219\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351077376.0000 - rmse: 18737.0586 - val_loss: 861173760.0000 - val_rmse: 29345.7617\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281753056.0000 - rmse: 16785.5000 - val_loss: 727117056.0000 - val_rmse: 26965.1074\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291673280.0000 - rmse: 17078.4434 - val_loss: 815398400.0000 - val_rmse: 28555.1797\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358117152.0000 - rmse: 18923.9805 - val_loss: 848869184.0000 - val_rmse: 29135.3574\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321017664.0000 - rmse: 17916.9648 - val_loss: 1541697536.0000 - val_rmse: 39264.4531\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329349408.0000 - rmse: 18147.9844 - val_loss: 745055232.0000 - val_rmse: 27295.6992\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342748416.0000 - rmse: 18513.4648 - val_loss: 968472896.0000 - val_rmse: 31120.2969\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333776416.0000 - rmse: 18269.5488 - val_loss: 722790336.0000 - val_rmse: 26884.7598\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330011232.0000 - rmse: 18166.2109 - val_loss: 629888384.0000 - val_rmse: 25097.5762\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327617920.0000 - rmse: 18100.2188 - val_loss: 836136576.0000 - val_rmse: 28916.0234\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288129440.0000 - rmse: 16974.3711 - val_loss: 698564608.0000 - val_rmse: 26430.3730\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333982624.0000 - rmse: 18275.1895 - val_loss: 913954560.0000 - val_rmse: 30231.6816\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360202624.0000 - rmse: 18979.0039 - val_loss: 809656832.0000 - val_rmse: 28454.4688\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329760736.0000 - rmse: 18159.3145 - val_loss: 872969344.0000 - val_rmse: 29546.0547\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334564448.0000 - rmse: 18291.1016 - val_loss: 738012096.0000 - val_rmse: 27166.3789\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338025792.0000 - rmse: 18385.4766 - val_loss: 915053056.0000 - val_rmse: 30249.8438\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323290336.0000 - rmse: 17980.2734 - val_loss: 855810432.0000 - val_rmse: 29254.2363\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285713440.0000 - rmse: 16903.0566 - val_loss: 677640384.0000 - val_rmse: 26031.5234\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319064928.0000 - rmse: 17862.3848 - val_loss: 916937280.0000 - val_rmse: 30280.9727\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322360992.0000 - rmse: 17954.4141 - val_loss: 779371520.0000 - val_rmse: 27917.2246\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306880096.0000 - rmse: 17517.9941 - val_loss: 672872384.0000 - val_rmse: 25939.7832\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304383072.0000 - rmse: 17446.5742 - val_loss: 809362944.0000 - val_rmse: 28449.3047\n",
      "104/104 [==============================] - 0s 665us/step - loss: 360853024.0000 - rmse: 18996.1309\n",
      "[360853024.0, 18996.130859375]\n",
      "[22168.802734375, 27431.90234375, 26332.765625, 18947.716796875, 18996.130859375]\n",
      "22775.463671875\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "!python train.py kfold baseline\n",
    "#d 0.25 p 20 epoch 150 layer -1 (16 16)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 18:59:50.036149: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 18:59:50.036187: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 18:59:50.036580: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 18:59:50.257382: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 6584821248.0000 - rmse: 81146.9141 - val_loss: 1199382528.0000 - val_rmse: 34632.1016\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1820463104.0000 - rmse: 42666.8867 - val_loss: 969170944.0000 - val_rmse: 31131.5098\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1634960000.0000 - rmse: 40434.6367 - val_loss: 791359936.0000 - val_rmse: 28131.1211\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1493892224.0000 - rmse: 38650.9023 - val_loss: 723149824.0000 - val_rmse: 26891.4453\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1375818880.0000 - rmse: 37092.0312 - val_loss: 720097536.0000 - val_rmse: 26834.6309\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1369239040.0000 - rmse: 37003.2305 - val_loss: 673279168.0000 - val_rmse: 25947.6230\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1340955008.0000 - rmse: 36619.0508 - val_loss: 673954368.0000 - val_rmse: 25960.6309\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1339851008.0000 - rmse: 36603.9766 - val_loss: 756675264.0000 - val_rmse: 27507.7305\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1180461952.0000 - rmse: 34357.8516 - val_loss: 650754496.0000 - val_rmse: 25509.8906\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1145585920.0000 - rmse: 33846.5039 - val_loss: 605586880.0000 - val_rmse: 24608.6758\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1140065152.0000 - rmse: 33764.8516 - val_loss: 767709184.0000 - val_rmse: 27707.5645\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1149984384.0000 - rmse: 33911.4180 - val_loss: 615395008.0000 - val_rmse: 24807.1562\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1029423872.0000 - rmse: 32084.6367 - val_loss: 587594752.0000 - val_rmse: 24240.3535\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 954899200.0000 - rmse: 30901.4434 - val_loss: 1037474624.0000 - val_rmse: 32209.8535\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 957202496.0000 - rmse: 30938.6895 - val_loss: 659348928.0000 - val_rmse: 25677.7910\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 982545856.0000 - rmse: 31345.5879 - val_loss: 729588096.0000 - val_rmse: 27010.8887\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 996156608.0000 - rmse: 31561.9492 - val_loss: 506500128.0000 - val_rmse: 22505.5586\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 955942784.0000 - rmse: 30918.3242 - val_loss: 560689472.0000 - val_rmse: 23678.8828\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 852739840.0000 - rmse: 29201.7070 - val_loss: 2748216320.0000 - val_rmse: 52423.4336\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 897661376.0000 - rmse: 29960.9980 - val_loss: 578662592.0000 - val_rmse: 24055.4062\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 828208320.0000 - rmse: 28778.6094 - val_loss: 574058176.0000 - val_rmse: 23959.5117\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 812213952.0000 - rmse: 28499.3672 - val_loss: 1038929600.0000 - val_rmse: 32232.4316\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 874342016.0000 - rmse: 29569.2754 - val_loss: 615199296.0000 - val_rmse: 24803.2109\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 796263168.0000 - rmse: 28218.1348 - val_loss: 969315520.0000 - val_rmse: 31133.8320\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 796882048.0000 - rmse: 28229.0996 - val_loss: 494324928.0000 - val_rmse: 22233.4199\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 779325312.0000 - rmse: 27916.3984 - val_loss: 962788544.0000 - val_rmse: 31028.8340\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 769730880.0000 - rmse: 27744.0234 - val_loss: 779593600.0000 - val_rmse: 27921.2031\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 737606720.0000 - rmse: 27158.9160 - val_loss: 593128128.0000 - val_rmse: 24354.2207\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 708689024.0000 - rmse: 26621.2129 - val_loss: 946660544.0000 - val_rmse: 30767.8496\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 690499456.0000 - rmse: 26277.3555 - val_loss: 1537527040.0000 - val_rmse: 39211.3125\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 822313600.0000 - rmse: 28676.0098 - val_loss: 927342656.0000 - val_rmse: 30452.3008\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687399808.0000 - rmse: 26218.3105 - val_loss: 653455616.0000 - val_rmse: 25562.7773\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 817320512.0000 - rmse: 28588.8164 - val_loss: 436134464.0000 - val_rmse: 20883.8320\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 767947392.0000 - rmse: 27711.8613 - val_loss: 637617792.0000 - val_rmse: 25251.0938\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656304000.0000 - rmse: 25618.4297 - val_loss: 741395840.0000 - val_rmse: 27228.5840\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691462848.0000 - rmse: 26295.6816 - val_loss: 742478720.0000 - val_rmse: 27248.4609\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 625993088.0000 - rmse: 25019.8535 - val_loss: 488392352.0000 - val_rmse: 22099.5996\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654832896.0000 - rmse: 25589.7012 - val_loss: 771533696.0000 - val_rmse: 27776.4922\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579480000.0000 - rmse: 24072.3906 - val_loss: 618926592.0000 - val_rmse: 24878.2344\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666708416.0000 - rmse: 25820.6973 - val_loss: 529131040.0000 - val_rmse: 23002.8457\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654859328.0000 - rmse: 25590.2188 - val_loss: 876236480.0000 - val_rmse: 29601.2910\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584225984.0000 - rmse: 24170.7676 - val_loss: 558933312.0000 - val_rmse: 23641.7695\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628266112.0000 - rmse: 25065.2363 - val_loss: 652929536.0000 - val_rmse: 25552.4863\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624434496.0000 - rmse: 24988.6875 - val_loss: 1363539456.0000 - val_rmse: 36926.1367\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618205760.0000 - rmse: 24863.7422 - val_loss: 930455616.0000 - val_rmse: 30503.3691\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558769984.0000 - rmse: 23638.3164 - val_loss: 558017856.0000 - val_rmse: 23622.3984\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575349440.0000 - rmse: 23986.4414 - val_loss: 553230016.0000 - val_rmse: 23520.8418\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582374336.0000 - rmse: 24132.4336 - val_loss: 929451712.0000 - val_rmse: 30486.9102\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527695520.0000 - rmse: 22971.6250 - val_loss: 878573376.0000 - val_rmse: 29640.7383\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583728640.0000 - rmse: 24160.4766 - val_loss: 514211840.0000 - val_rmse: 22676.2402\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614939520.0000 - rmse: 24797.9727 - val_loss: 868864320.0000 - val_rmse: 29476.5039\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548309824.0000 - rmse: 23416.0156 - val_loss: 515173792.0000 - val_rmse: 22697.4395\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470115360.0000 - rmse: 21682.1445 - val_loss: 1398521984.0000 - val_rmse: 37396.8164\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524123296.0000 - rmse: 22893.7383 - val_loss: 774527616.0000 - val_rmse: 27830.3359\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543842752.0000 - rmse: 23320.4355 - val_loss: 669092544.0000 - val_rmse: 25866.8223\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460144192.0000 - rmse: 21450.9688 - val_loss: 671261824.0000 - val_rmse: 25908.7207\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520163136.0000 - rmse: 22807.0859 - val_loss: 987860288.0000 - val_rmse: 31430.2422\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505573632.0000 - rmse: 22484.9629 - val_loss: 1323057664.0000 - val_rmse: 36373.8594\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560127744.0000 - rmse: 23667.0176 - val_loss: 793235136.0000 - val_rmse: 28164.4297\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536474400.0000 - rmse: 23161.9180 - val_loss: 1108278528.0000 - val_rmse: 33290.8164\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548005184.0000 - rmse: 23409.5098 - val_loss: 1316280064.0000 - val_rmse: 36280.5742\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547606464.0000 - rmse: 23400.9922 - val_loss: 1259039872.0000 - val_rmse: 35482.9531\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460433216.0000 - rmse: 21457.7051 - val_loss: 1009082496.0000 - val_rmse: 31766.0586\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516414400.0000 - rmse: 22724.7520 - val_loss: 911362752.0000 - val_rmse: 30188.7852\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485592416.0000 - rmse: 22036.1562 - val_loss: 1068487296.0000 - val_rmse: 32687.7246\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518613792.0000 - rmse: 22773.0918 - val_loss: 955844352.0000 - val_rmse: 30916.7285\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534437152.0000 - rmse: 23117.8965 - val_loss: 774449024.0000 - val_rmse: 27828.9238\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532057536.0000 - rmse: 23066.3711 - val_loss: 560824064.0000 - val_rmse: 23681.7246\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415154464.0000 - rmse: 20375.3398 - val_loss: 754608576.0000 - val_rmse: 27470.1406\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459356992.0000 - rmse: 21432.6133 - val_loss: 999273600.0000 - val_rmse: 31611.2891\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464459424.0000 - rmse: 21551.3203 - val_loss: 810345728.0000 - val_rmse: 28466.5723\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471287264.0000 - rmse: 21709.1484 - val_loss: 1498188928.0000 - val_rmse: 38706.4453\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468435392.0000 - rmse: 21643.3652 - val_loss: 1478622464.0000 - val_rmse: 38452.8594\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 638732608.0000 - rmse: 25273.1582 - val_loss: 1075814528.0000 - val_rmse: 32799.6133\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447300992.0000 - rmse: 21149.4902 - val_loss: 612238336.0000 - val_rmse: 24743.4512\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529687872.0000 - rmse: 23014.9473 - val_loss: 1160251264.0000 - val_rmse: 34062.4609\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434088672.0000 - rmse: 20834.7949 - val_loss: 1033549184.0000 - val_rmse: 32148.8594\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405853056.0000 - rmse: 20145.7930 - val_loss: 1304410880.0000 - val_rmse: 36116.6289\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407186752.0000 - rmse: 20178.8672 - val_loss: 706537792.0000 - val_rmse: 26580.7773\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431703328.0000 - rmse: 20777.4707 - val_loss: 876730688.0000 - val_rmse: 29609.6387\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404871808.0000 - rmse: 20121.4258 - val_loss: 975583552.0000 - val_rmse: 31234.3320\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423791328.0000 - rmse: 20586.1895 - val_loss: 1286040320.0000 - val_rmse: 35861.4062\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379792256.0000 - rmse: 19488.2578 - val_loss: 870083456.0000 - val_rmse: 29497.1777\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480646304.0000 - rmse: 21923.6445 - val_loss: 2482078720.0000 - val_rmse: 49820.4648\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478141664.0000 - rmse: 21866.4512 - val_loss: 878726528.0000 - val_rmse: 29643.3223\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418221728.0000 - rmse: 20450.4668 - val_loss: 1474124032.0000 - val_rmse: 38394.3242\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426593792.0000 - rmse: 20654.1465 - val_loss: 1009132160.0000 - val_rmse: 31766.8398\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549480896.0000 - rmse: 23441.0078 - val_loss: 1379725184.0000 - val_rmse: 37144.6523\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355889824.0000 - rmse: 18865.0410 - val_loss: 1679200640.0000 - val_rmse: 40978.0508\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410348224.0000 - rmse: 20257.0527 - val_loss: 1068120576.0000 - val_rmse: 32682.1133\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362036256.0000 - rmse: 19027.2500 - val_loss: 1394018048.0000 - val_rmse: 37336.5508\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371983808.0000 - rmse: 19286.8809 - val_loss: 998255232.0000 - val_rmse: 31595.1758\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446611200.0000 - rmse: 21133.1758 - val_loss: 861935680.0000 - val_rmse: 29358.7402\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399593472.0000 - rmse: 19989.8320 - val_loss: 1150038912.0000 - val_rmse: 33912.2227\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430713696.0000 - rmse: 20753.6426 - val_loss: 739225728.0000 - val_rmse: 27188.7051\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412029536.0000 - rmse: 20298.5078 - val_loss: 1215496704.0000 - val_rmse: 34863.9727\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399538208.0000 - rmse: 19988.4492 - val_loss: 1326541440.0000 - val_rmse: 36421.7148\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444538240.0000 - rmse: 21084.0742 - val_loss: 1343899776.0000 - val_rmse: 36659.2383\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410217664.0000 - rmse: 20253.8301 - val_loss: 678112384.0000 - val_rmse: 26040.5898\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391763488.0000 - rmse: 19793.0137 - val_loss: 789246144.0000 - val_rmse: 28093.5254\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402232832.0000 - rmse: 20055.7422 - val_loss: 1106827904.0000 - val_rmse: 33269.0234\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357555488.0000 - rmse: 18909.1367 - val_loss: 898942848.0000 - val_rmse: 29982.3730\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368720288.0000 - rmse: 19202.0898 - val_loss: 836600960.0000 - val_rmse: 28924.0527\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393200064.0000 - rmse: 19829.2715 - val_loss: 1322638080.0000 - val_rmse: 36368.0898\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396535040.0000 - rmse: 19913.1855 - val_loss: 627569600.0000 - val_rmse: 25051.3398\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464767520.0000 - rmse: 21558.4668 - val_loss: 992312384.0000 - val_rmse: 31500.9902\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369966336.0000 - rmse: 19234.5078 - val_loss: 1979579008.0000 - val_rmse: 44492.4609\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479996960.0000 - rmse: 21908.8320 - val_loss: 1502809344.0000 - val_rmse: 38766.0820\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318251968.0000 - rmse: 17839.6152 - val_loss: 1162744832.0000 - val_rmse: 34099.0430\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357462464.0000 - rmse: 18906.6758 - val_loss: 1001984640.0000 - val_rmse: 31654.1406\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384001440.0000 - rmse: 19595.9531 - val_loss: 1652472192.0000 - val_rmse: 40650.6094\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363764320.0000 - rmse: 19072.6055 - val_loss: 816422400.0000 - val_rmse: 28573.1035\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371106368.0000 - rmse: 19264.1211 - val_loss: 1666757760.0000 - val_rmse: 40825.9453\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374436672.0000 - rmse: 19350.3652 - val_loss: 829802752.0000 - val_rmse: 28806.2969\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374578912.0000 - rmse: 19354.0371 - val_loss: 1997052288.0000 - val_rmse: 44688.3867\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368758784.0000 - rmse: 19203.0898 - val_loss: 1084076544.0000 - val_rmse: 32925.3164\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354533344.0000 - rmse: 18829.0527 - val_loss: 1177091200.0000 - val_rmse: 34308.7617\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404208256.0000 - rmse: 20104.9297 - val_loss: 1214914304.0000 - val_rmse: 34855.6172\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363006112.0000 - rmse: 19052.7168 - val_loss: 2015210368.0000 - val_rmse: 44891.0938\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379554624.0000 - rmse: 19482.1582 - val_loss: 879878272.0000 - val_rmse: 29662.7422\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331136608.0000 - rmse: 18197.1582 - val_loss: 2326822656.0000 - val_rmse: 48237.1484\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328140608.0000 - rmse: 18114.6484 - val_loss: 2337046784.0000 - val_rmse: 48343.0117\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348031904.0000 - rmse: 18655.6113 - val_loss: 801325248.0000 - val_rmse: 28307.6895\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343185376.0000 - rmse: 18525.2598 - val_loss: 972421952.0000 - val_rmse: 31183.6797\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357046688.0000 - rmse: 18895.6758 - val_loss: 1622853120.0000 - val_rmse: 40284.6484\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324187328.0000 - rmse: 18005.2012 - val_loss: 873437376.0000 - val_rmse: 29553.9688\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342570784.0000 - rmse: 18508.6660 - val_loss: 2701158656.0000 - val_rmse: 51972.6719\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340728672.0000 - rmse: 18458.8340 - val_loss: 809661120.0000 - val_rmse: 28454.5430\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331753792.0000 - rmse: 18214.1055 - val_loss: 1318179456.0000 - val_rmse: 36306.7422\n",
      "104/104 [==============================] - 0s 706us/step - loss: 514379296.0000 - rmse: 22679.9297\n",
      "[514379296.0, 22679.9296875]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 6369573888.0000 - rmse: 79809.6094 - val_loss: 1191225728.0000 - val_rmse: 34514.1367\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1611146496.0000 - rmse: 40139.0898 - val_loss: 1002315456.0000 - val_rmse: 31659.3652\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1355755392.0000 - rmse: 36820.5820 - val_loss: 960289472.0000 - val_rmse: 30988.5371\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1267843200.0000 - rmse: 35606.7852 - val_loss: 885363392.0000 - val_rmse: 29755.0566\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1182184448.0000 - rmse: 34382.9102 - val_loss: 878507328.0000 - val_rmse: 29639.6250\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1124721280.0000 - rmse: 33536.8633 - val_loss: 893923968.0000 - val_rmse: 29898.5605\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1101715456.0000 - rmse: 33192.0977 - val_loss: 833530624.0000 - val_rmse: 28870.9297\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1042620160.0000 - rmse: 32289.6289 - val_loss: 863742272.0000 - val_rmse: 29389.4922\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1055121216.0000 - rmse: 32482.6289 - val_loss: 879548416.0000 - val_rmse: 29657.1816\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1052031488.0000 - rmse: 32435.0352 - val_loss: 904400256.0000 - val_rmse: 30073.2480\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 943239616.0000 - rmse: 30712.2070 - val_loss: 830424512.0000 - val_rmse: 28817.0879\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1040354304.0000 - rmse: 32254.5234 - val_loss: 873779456.0000 - val_rmse: 29559.7617\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 994475264.0000 - rmse: 31535.3027 - val_loss: 856219456.0000 - val_rmse: 29261.2285\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 928425024.0000 - rmse: 30470.0684 - val_loss: 755038400.0000 - val_rmse: 27477.9629\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 935858816.0000 - rmse: 30591.8105 - val_loss: 760971776.0000 - val_rmse: 27585.7168\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 886002560.0000 - rmse: 29765.7949 - val_loss: 729134208.0000 - val_rmse: 27002.4844\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 823070464.0000 - rmse: 28689.2031 - val_loss: 701077248.0000 - val_rmse: 26477.8633\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 788434304.0000 - rmse: 28079.0703 - val_loss: 1126823552.0000 - val_rmse: 33568.1914\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 827871616.0000 - rmse: 28772.7578 - val_loss: 657440000.0000 - val_rmse: 25640.5918\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 810542848.0000 - rmse: 28470.0332 - val_loss: 1138476800.0000 - val_rmse: 33741.3203\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 813568960.0000 - rmse: 28523.1309 - val_loss: 637521600.0000 - val_rmse: 25249.1895\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 791341696.0000 - rmse: 28130.7969 - val_loss: 914857792.0000 - val_rmse: 30246.6133\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 770717568.0000 - rmse: 27761.7988 - val_loss: 702280704.0000 - val_rmse: 26500.5801\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 799975744.0000 - rmse: 28283.8418 - val_loss: 1179291008.0000 - val_rmse: 34340.8086\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729336000.0000 - rmse: 27006.2207 - val_loss: 902154688.0000 - val_rmse: 30035.8906\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677603648.0000 - rmse: 26030.8203 - val_loss: 643807424.0000 - val_rmse: 25373.3613\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 715374720.0000 - rmse: 26746.4902 - val_loss: 619199360.0000 - val_rmse: 24883.7168\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676449088.0000 - rmse: 26008.6348 - val_loss: 679780544.0000 - val_rmse: 26072.6016\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 657002048.0000 - rmse: 25632.0508 - val_loss: 621802304.0000 - val_rmse: 24935.9648\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661714112.0000 - rmse: 25723.8047 - val_loss: 615050816.0000 - val_rmse: 24800.2168\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 633250240.0000 - rmse: 25164.4629 - val_loss: 595829952.0000 - val_rmse: 24409.6270\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615421888.0000 - rmse: 24807.6953 - val_loss: 601690240.0000 - val_rmse: 24529.3730\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607257536.0000 - rmse: 24642.5938 - val_loss: 790316352.0000 - val_rmse: 28112.5645\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555683968.0000 - rmse: 23572.9492 - val_loss: 634340032.0000 - val_rmse: 25186.1055\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642565888.0000 - rmse: 25348.8828 - val_loss: 553427776.0000 - val_rmse: 23525.0449\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555681152.0000 - rmse: 23572.8867 - val_loss: 602592064.0000 - val_rmse: 24547.7500\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512560576.0000 - rmse: 22639.8008 - val_loss: 624235072.0000 - val_rmse: 24984.6953\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551362048.0000 - rmse: 23481.0996 - val_loss: 547977280.0000 - val_rmse: 23408.9141\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499261152.0000 - rmse: 22344.1504 - val_loss: 563388480.0000 - val_rmse: 23735.8066\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524141216.0000 - rmse: 22894.1309 - val_loss: 669448320.0000 - val_rmse: 25873.6973\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579943232.0000 - rmse: 24082.0098 - val_loss: 578324352.0000 - val_rmse: 24048.3750\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539877888.0000 - rmse: 23235.2715 - val_loss: 646349376.0000 - val_rmse: 25423.4004\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539238656.0000 - rmse: 23221.5117 - val_loss: 587430464.0000 - val_rmse: 24236.9629\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548780416.0000 - rmse: 23426.0605 - val_loss: 616067392.0000 - val_rmse: 24820.7051\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492045216.0000 - rmse: 22182.0918 - val_loss: 581858176.0000 - val_rmse: 24121.7363\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522327648.0000 - rmse: 22854.4883 - val_loss: 553199296.0000 - val_rmse: 23520.1855\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519945600.0000 - rmse: 22802.3125 - val_loss: 600833472.0000 - val_rmse: 24511.9043\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530977312.0000 - rmse: 23042.9453 - val_loss: 551497280.0000 - val_rmse: 23483.9785\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550739072.0000 - rmse: 23467.8301 - val_loss: 557610496.0000 - val_rmse: 23613.7773\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523967392.0000 - rmse: 22890.3320 - val_loss: 599477248.0000 - val_rmse: 24484.2227\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474388768.0000 - rmse: 21780.4668 - val_loss: 544157056.0000 - val_rmse: 23327.1738\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533867456.0000 - rmse: 23105.5703 - val_loss: 581094080.0000 - val_rmse: 24105.8926\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494793408.0000 - rmse: 22243.9512 - val_loss: 526672128.0000 - val_rmse: 22949.3359\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447175040.0000 - rmse: 21146.5137 - val_loss: 587980608.0000 - val_rmse: 24248.3105\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433211840.0000 - rmse: 20813.7402 - val_loss: 566575232.0000 - val_rmse: 23802.8398\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469127776.0000 - rmse: 21659.3555 - val_loss: 596165696.0000 - val_rmse: 24416.5039\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468013408.0000 - rmse: 21633.6172 - val_loss: 516462688.0000 - val_rmse: 22725.8164\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417604416.0000 - rmse: 20435.3711 - val_loss: 1064517312.0000 - val_rmse: 32626.9414\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400332800.0000 - rmse: 20008.3184 - val_loss: 528136384.0000 - val_rmse: 22981.2188\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422076032.0000 - rmse: 20544.4883 - val_loss: 926811840.0000 - val_rmse: 30443.5820\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404119808.0000 - rmse: 20102.7285 - val_loss: 601202496.0000 - val_rmse: 24519.4297\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428712416.0000 - rmse: 20705.3711 - val_loss: 532279136.0000 - val_rmse: 23071.1738\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440597440.0000 - rmse: 20990.4121 - val_loss: 575855168.0000 - val_rmse: 23996.9805\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460362848.0000 - rmse: 21456.0664 - val_loss: 897502464.0000 - val_rmse: 29958.3457\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426648288.0000 - rmse: 20655.4648 - val_loss: 583781632.0000 - val_rmse: 24161.5742\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435149120.0000 - rmse: 20860.2266 - val_loss: 535447488.0000 - val_rmse: 23139.7363\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428821472.0000 - rmse: 20708.0039 - val_loss: 561169344.0000 - val_rmse: 23689.0137\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383022880.0000 - rmse: 19570.9688 - val_loss: 478958368.0000 - val_rmse: 21885.1152\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440662048.0000 - rmse: 20991.9512 - val_loss: 529242624.0000 - val_rmse: 23005.2715\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360573728.0000 - rmse: 18988.7793 - val_loss: 532641696.0000 - val_rmse: 23079.0312\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383171328.0000 - rmse: 19574.7617 - val_loss: 551069056.0000 - val_rmse: 23474.8594\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386071680.0000 - rmse: 19648.7051 - val_loss: 474624064.0000 - val_rmse: 21785.8672\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376994624.0000 - rmse: 19416.3477 - val_loss: 540687936.0000 - val_rmse: 23252.6953\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409729152.0000 - rmse: 20241.7656 - val_loss: 549784576.0000 - val_rmse: 23447.4844\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413719072.0000 - rmse: 20340.0820 - val_loss: 516626336.0000 - val_rmse: 22729.4141\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495369824.0000 - rmse: 22256.9043 - val_loss: 551042368.0000 - val_rmse: 23474.2910\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378850464.0000 - rmse: 19464.0801 - val_loss: 604943360.0000 - val_rmse: 24595.5957\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396924352.0000 - rmse: 19922.9570 - val_loss: 561080384.0000 - val_rmse: 23687.1348\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378421024.0000 - rmse: 19453.0449 - val_loss: 478053216.0000 - val_rmse: 21864.4258\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385492576.0000 - rmse: 19633.9648 - val_loss: 643767296.0000 - val_rmse: 25372.5703\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331188800.0000 - rmse: 18198.5918 - val_loss: 588852160.0000 - val_rmse: 24266.2734\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386877952.0000 - rmse: 19669.2109 - val_loss: 609358976.0000 - val_rmse: 24685.1953\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391778112.0000 - rmse: 19793.3848 - val_loss: 613076160.0000 - val_rmse: 24760.3730\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408400896.0000 - rmse: 20208.9297 - val_loss: 529512256.0000 - val_rmse: 23011.1309\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356215360.0000 - rmse: 18873.6660 - val_loss: 531652064.0000 - val_rmse: 23057.5801\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356066016.0000 - rmse: 18869.7109 - val_loss: 610434368.0000 - val_rmse: 24706.9707\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344238432.0000 - rmse: 18553.6621 - val_loss: 565008512.0000 - val_rmse: 23769.9082\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398245664.0000 - rmse: 19956.0938 - val_loss: 543387072.0000 - val_rmse: 23310.6621\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338706016.0000 - rmse: 18403.9648 - val_loss: 535803072.0000 - val_rmse: 23147.4199\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384557376.0000 - rmse: 19610.1328 - val_loss: 476779328.0000 - val_rmse: 21835.2754\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379771360.0000 - rmse: 19487.7227 - val_loss: 669981440.0000 - val_rmse: 25884.0000\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366289184.0000 - rmse: 19138.6816 - val_loss: 494241408.0000 - val_rmse: 22231.5410\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378750112.0000 - rmse: 19461.5020 - val_loss: 553800256.0000 - val_rmse: 23532.9609\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333541664.0000 - rmse: 18263.1230 - val_loss: 536369984.0000 - val_rmse: 23159.6621\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366780448.0000 - rmse: 19151.5117 - val_loss: 489532352.0000 - val_rmse: 22125.3789\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368502496.0000 - rmse: 19196.4180 - val_loss: 529464032.0000 - val_rmse: 23010.0859\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338713248.0000 - rmse: 18404.1621 - val_loss: 491506368.0000 - val_rmse: 22169.9434\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359232096.0000 - rmse: 18953.4180 - val_loss: 483969568.0000 - val_rmse: 21999.3066\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337845056.0000 - rmse: 18380.5586 - val_loss: 511272576.0000 - val_rmse: 22611.3359\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356158944.0000 - rmse: 18872.1738 - val_loss: 704597504.0000 - val_rmse: 26544.2539\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310281344.0000 - rmse: 17614.8047 - val_loss: 572349184.0000 - val_rmse: 23923.8184\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364736128.0000 - rmse: 19098.0625 - val_loss: 553453824.0000 - val_rmse: 23525.5977\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357948832.0000 - rmse: 18919.5332 - val_loss: 649377664.0000 - val_rmse: 25482.8887\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372760768.0000 - rmse: 19307.0117 - val_loss: 622667136.0000 - val_rmse: 24953.2969\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293902688.0000 - rmse: 17143.5879 - val_loss: 480938656.0000 - val_rmse: 21930.3105\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302203776.0000 - rmse: 17384.0078 - val_loss: 527873408.0000 - val_rmse: 22975.4941\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297249760.0000 - rmse: 17240.9316 - val_loss: 489155328.0000 - val_rmse: 22116.8555\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361359744.0000 - rmse: 19009.4629 - val_loss: 493972000.0000 - val_rmse: 22225.4785\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333420384.0000 - rmse: 18259.8008 - val_loss: 763682816.0000 - val_rmse: 27634.8125\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304779872.0000 - rmse: 17457.9453 - val_loss: 468767872.0000 - val_rmse: 21651.0469\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394812288.0000 - rmse: 19869.8828 - val_loss: 495525280.0000 - val_rmse: 22260.3965\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329999680.0000 - rmse: 18165.8887 - val_loss: 490741376.0000 - val_rmse: 22152.6816\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321944512.0000 - rmse: 17942.8105 - val_loss: 547659520.0000 - val_rmse: 23402.1270\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346633760.0000 - rmse: 18618.1016 - val_loss: 498837280.0000 - val_rmse: 22334.6641\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369571552.0000 - rmse: 19224.2422 - val_loss: 808308160.0000 - val_rmse: 28430.7617\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327404736.0000 - rmse: 18094.3281 - val_loss: 461768192.0000 - val_rmse: 21488.7930\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334996288.0000 - rmse: 18302.9043 - val_loss: 432069888.0000 - val_rmse: 20786.2891\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317511616.0000 - rmse: 17818.8535 - val_loss: 504992672.0000 - val_rmse: 22472.0391\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329226048.0000 - rmse: 18144.5859 - val_loss: 475920000.0000 - val_rmse: 21815.5898\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320457888.0000 - rmse: 17901.3359 - val_loss: 487466112.0000 - val_rmse: 22078.6328\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286655584.0000 - rmse: 16930.9043 - val_loss: 532803520.0000 - val_rmse: 23082.5352\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292359520.0000 - rmse: 17098.5234 - val_loss: 535447968.0000 - val_rmse: 23139.7480\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276541376.0000 - rmse: 16629.5312 - val_loss: 437672960.0000 - val_rmse: 20920.6328\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465523744.0000 - rmse: 21575.9961 - val_loss: 546420480.0000 - val_rmse: 23375.6387\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283175040.0000 - rmse: 16827.8027 - val_loss: 508033440.0000 - val_rmse: 22539.5957\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293011744.0000 - rmse: 17117.5840 - val_loss: 515585856.0000 - val_rmse: 22706.5137\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316141248.0000 - rmse: 17780.3594 - val_loss: 431139744.0000 - val_rmse: 20763.9043\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308777728.0000 - rmse: 17572.0703 - val_loss: 474458240.0000 - val_rmse: 21782.0605\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297579712.0000 - rmse: 17250.4980 - val_loss: 634826688.0000 - val_rmse: 25195.7676\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302129984.0000 - rmse: 17381.8848 - val_loss: 484754208.0000 - val_rmse: 22017.1328\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301903008.0000 - rmse: 17375.3555 - val_loss: 543887872.0000 - val_rmse: 23321.4023\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295509344.0000 - rmse: 17190.3809 - val_loss: 588832512.0000 - val_rmse: 24265.8691\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342450688.0000 - rmse: 18505.4199 - val_loss: 508769856.0000 - val_rmse: 22555.9258\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291770528.0000 - rmse: 17081.2891 - val_loss: 458805376.0000 - val_rmse: 21419.7402\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302481024.0000 - rmse: 17391.9785 - val_loss: 468891232.0000 - val_rmse: 21653.8965\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298772192.0000 - rmse: 17285.0254 - val_loss: 457775392.0000 - val_rmse: 21395.6836\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287086880.0000 - rmse: 16943.6348 - val_loss: 536789856.0000 - val_rmse: 23168.7246\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305299456.0000 - rmse: 17472.8184 - val_loss: 494267776.0000 - val_rmse: 22232.1328\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293453888.0000 - rmse: 17130.4941 - val_loss: 457156096.0000 - val_rmse: 21381.2070\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323668832.0000 - rmse: 17990.7949 - val_loss: 491279584.0000 - val_rmse: 22164.8281\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317819136.0000 - rmse: 17827.4824 - val_loss: 512604928.0000 - val_rmse: 22640.7793\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369250336.0000 - rmse: 19215.8867 - val_loss: 442347360.0000 - val_rmse: 21032.0547\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297197888.0000 - rmse: 17239.4258 - val_loss: 547270976.0000 - val_rmse: 23393.8242\n",
      "104/104 [==============================] - 0s 667us/step - loss: 916824128.0000 - rmse: 30279.1035\n",
      "[916824128.0, 30279.103515625]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 6077975040.0000 - rmse: 77961.3672 - val_loss: 1367674624.0000 - val_rmse: 36982.0859\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1435809664.0000 - rmse: 37892.0781 - val_loss: 1146323200.0000 - val_rmse: 33857.3945\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1311149440.0000 - rmse: 36209.7969 - val_loss: 988729088.0000 - val_rmse: 31444.0625\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1212061824.0000 - rmse: 34814.6797 - val_loss: 1045313088.0000 - val_rmse: 32331.3027\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1097708416.0000 - rmse: 33131.6836 - val_loss: 926843712.0000 - val_rmse: 30444.1074\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1102912384.0000 - rmse: 33210.1250 - val_loss: 933003776.0000 - val_rmse: 30545.1113\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1082014336.0000 - rmse: 32893.9844 - val_loss: 1026471488.0000 - val_rmse: 32038.5938\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1063297536.0000 - rmse: 32608.2441 - val_loss: 1070077440.0000 - val_rmse: 32712.0391\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1030943488.0000 - rmse: 32108.3086 - val_loss: 1080701312.0000 - val_rmse: 32874.0234\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1025260864.0000 - rmse: 32019.6953 - val_loss: 990806400.0000 - val_rmse: 31477.0781\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 994830464.0000 - rmse: 31540.9336 - val_loss: 893304704.0000 - val_rmse: 29888.2031\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 966524736.0000 - rmse: 31088.9805 - val_loss: 1118372352.0000 - val_rmse: 33442.0742\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1027238848.0000 - rmse: 32050.5664 - val_loss: 1279744384.0000 - val_rmse: 35773.5156\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 921326720.0000 - rmse: 30353.3633 - val_loss: 1102037120.0000 - val_rmse: 33196.9453\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 958884544.0000 - rmse: 30965.8613 - val_loss: 874997248.0000 - val_rmse: 29580.3516\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 906041600.0000 - rmse: 30100.5234 - val_loss: 941147008.0000 - val_rmse: 30678.1191\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 938223232.0000 - rmse: 30630.4297 - val_loss: 818828032.0000 - val_rmse: 28615.1719\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 936612736.0000 - rmse: 30604.1289 - val_loss: 1174228352.0000 - val_rmse: 34267.0156\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 854223104.0000 - rmse: 29227.0957 - val_loss: 948979520.0000 - val_rmse: 30805.5117\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 824397184.0000 - rmse: 28712.3184 - val_loss: 1320277120.0000 - val_rmse: 36335.6172\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 848724096.0000 - rmse: 29132.8691 - val_loss: 865625088.0000 - val_rmse: 29421.5078\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 809707968.0000 - rmse: 28455.3672 - val_loss: 760904832.0000 - val_rmse: 27584.5039\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 833392640.0000 - rmse: 28868.5410 - val_loss: 775797376.0000 - val_rmse: 27853.1387\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 803058432.0000 - rmse: 28338.2852 - val_loss: 788334336.0000 - val_rmse: 28077.2930\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 740808960.0000 - rmse: 27217.8047 - val_loss: 1122589056.0000 - val_rmse: 33505.0586\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 765918912.0000 - rmse: 27675.2402 - val_loss: 1046504512.0000 - val_rmse: 32349.7227\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 817832512.0000 - rmse: 28597.7715 - val_loss: 988959808.0000 - val_rmse: 31447.7305\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706262272.0000 - rmse: 26575.5938 - val_loss: 806785216.0000 - val_rmse: 28403.9648\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730036672.0000 - rmse: 27019.1875 - val_loss: 709427200.0000 - val_rmse: 26635.0742\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 663873536.0000 - rmse: 25765.7402 - val_loss: 634898880.0000 - val_rmse: 25197.1992\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 719738496.0000 - rmse: 26827.9434 - val_loss: 664596032.0000 - val_rmse: 25779.7598\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 625119040.0000 - rmse: 25002.3789 - val_loss: 559687424.0000 - val_rmse: 23657.7129\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629365056.0000 - rmse: 25087.1484 - val_loss: 860777216.0000 - val_rmse: 29339.0059\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665387840.0000 - rmse: 25795.1133 - val_loss: 575561024.0000 - val_rmse: 23990.8496\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624926528.0000 - rmse: 24998.5312 - val_loss: 548036096.0000 - val_rmse: 23410.1680\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562863872.0000 - rmse: 23724.7520 - val_loss: 946304576.0000 - val_rmse: 30762.0645\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 581523456.0000 - rmse: 24114.7969 - val_loss: 689326848.0000 - val_rmse: 26255.0352\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599324864.0000 - rmse: 24481.1113 - val_loss: 875243008.0000 - val_rmse: 29584.5059\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589830656.0000 - rmse: 24286.4297 - val_loss: 757140928.0000 - val_rmse: 27516.1934\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626574336.0000 - rmse: 25031.4668 - val_loss: 492340064.0000 - val_rmse: 22188.7363\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 635734784.0000 - rmse: 25213.7812 - val_loss: 832922624.0000 - val_rmse: 28860.3965\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591597632.0000 - rmse: 24322.7793 - val_loss: 545025856.0000 - val_rmse: 23345.7871\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536519264.0000 - rmse: 23162.8848 - val_loss: 1274140288.0000 - val_rmse: 35695.1016\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 603741440.0000 - rmse: 24571.1484 - val_loss: 601739840.0000 - val_rmse: 24530.3867\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469306592.0000 - rmse: 21663.4844 - val_loss: 1027356480.0000 - val_rmse: 32052.4023\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567810048.0000 - rmse: 23828.7656 - val_loss: 1254582016.0000 - val_rmse: 35420.0781\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614760128.0000 - rmse: 24794.3574 - val_loss: 639108928.0000 - val_rmse: 25280.6035\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543027968.0000 - rmse: 23302.9609 - val_loss: 567353984.0000 - val_rmse: 23819.1934\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508215488.0000 - rmse: 22543.6328 - val_loss: 885598528.0000 - val_rmse: 29759.0078\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514004128.0000 - rmse: 22671.6582 - val_loss: 507336864.0000 - val_rmse: 22524.1387\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527187040.0000 - rmse: 22960.5547 - val_loss: 534863360.0000 - val_rmse: 23127.1133\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443834240.0000 - rmse: 21067.3730 - val_loss: 533919488.0000 - val_rmse: 23106.6973\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550324416.0000 - rmse: 23458.9941 - val_loss: 548245696.0000 - val_rmse: 23414.6465\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502952896.0000 - rmse: 22426.6113 - val_loss: 495922336.0000 - val_rmse: 22269.3125\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534647296.0000 - rmse: 23122.4395 - val_loss: 815666752.0000 - val_rmse: 28559.8809\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463282528.0000 - rmse: 21523.9961 - val_loss: 403716384.0000 - val_rmse: 20092.6934\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532089056.0000 - rmse: 23067.0547 - val_loss: 505298912.0000 - val_rmse: 22478.8555\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493208832.0000 - rmse: 22208.3047 - val_loss: 744711936.0000 - val_rmse: 27289.4062\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466763328.0000 - rmse: 21604.7051 - val_loss: 547329536.0000 - val_rmse: 23395.0742\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519055104.0000 - rmse: 22782.7793 - val_loss: 519701504.0000 - val_rmse: 22796.9609\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484174080.0000 - rmse: 22003.9551 - val_loss: 556610880.0000 - val_rmse: 23592.5996\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461964064.0000 - rmse: 21493.3496 - val_loss: 1100646016.0000 - val_rmse: 33175.9844\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571553600.0000 - rmse: 23907.1855 - val_loss: 964066432.0000 - val_rmse: 31049.4199\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537089472.0000 - rmse: 23175.1914 - val_loss: 811982912.0000 - val_rmse: 28495.3125\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463319712.0000 - rmse: 21524.8633 - val_loss: 374785472.0000 - val_rmse: 19359.3770\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474813600.0000 - rmse: 21790.2188 - val_loss: 353824832.0000 - val_rmse: 18810.2305\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373249824.0000 - rmse: 19319.6719 - val_loss: 708901824.0000 - val_rmse: 26625.2109\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425873536.0000 - rmse: 20636.7012 - val_loss: 772117952.0000 - val_rmse: 27787.0078\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470752000.0000 - rmse: 21696.8203 - val_loss: 582391680.0000 - val_rmse: 24132.7930\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405190496.0000 - rmse: 20129.3418 - val_loss: 408657408.0000 - val_rmse: 20215.2734\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489865504.0000 - rmse: 22132.9062 - val_loss: 401657536.0000 - val_rmse: 20041.3945\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490030880.0000 - rmse: 22136.6387 - val_loss: 533040736.0000 - val_rmse: 23087.6758\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415462720.0000 - rmse: 20382.9023 - val_loss: 528662112.0000 - val_rmse: 22992.6523\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428267680.0000 - rmse: 20694.6270 - val_loss: 1026677120.0000 - val_rmse: 32041.7988\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473173440.0000 - rmse: 21752.5469 - val_loss: 547284864.0000 - val_rmse: 23394.1211\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402586272.0000 - rmse: 20064.5527 - val_loss: 601133120.0000 - val_rmse: 24518.0156\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405597888.0000 - rmse: 20139.4609 - val_loss: 990416256.0000 - val_rmse: 31470.8789\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445711616.0000 - rmse: 21111.8828 - val_loss: 887992384.0000 - val_rmse: 29799.2012\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385657376.0000 - rmse: 19638.1602 - val_loss: 744694976.0000 - val_rmse: 27289.0996\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374503680.0000 - rmse: 19352.0957 - val_loss: 699481792.0000 - val_rmse: 26447.7188\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373761120.0000 - rmse: 19332.9004 - val_loss: 674733888.0000 - val_rmse: 25975.6387\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438069888.0000 - rmse: 20930.1172 - val_loss: 390960864.0000 - val_rmse: 19772.7305\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452479648.0000 - rmse: 21271.5684 - val_loss: 1045949440.0000 - val_rmse: 32341.1406\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416790688.0000 - rmse: 20415.4492 - val_loss: 414177856.0000 - val_rmse: 20351.3594\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383957856.0000 - rmse: 19594.8418 - val_loss: 408665472.0000 - val_rmse: 20215.4746\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407170080.0000 - rmse: 20178.4551 - val_loss: 442152704.0000 - val_rmse: 21027.4258\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420701472.0000 - rmse: 20511.0078 - val_loss: 866663040.0000 - val_rmse: 29439.1406\n",
      "104/104 [==============================] - 0s 684us/step - loss: 1663777664.0000 - rmse: 40789.4258\n",
      "[1663777664.0, 40789.42578125]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 7664432128.0000 - rmse: 87546.7422 - val_loss: 1419610240.0000 - val_rmse: 37677.7148\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1786327680.0000 - rmse: 42264.9688 - val_loss: 1275318272.0000 - val_rmse: 35711.5977\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1591658112.0000 - rmse: 39895.5898 - val_loss: 1018252480.0000 - val_rmse: 31910.0684\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1453081216.0000 - rmse: 38119.3008 - val_loss: 903526336.0000 - val_rmse: 30058.7148\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1330884480.0000 - rmse: 36481.2891 - val_loss: 849001408.0000 - val_rmse: 29137.6289\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1317604992.0000 - rmse: 36298.8281 - val_loss: 838914112.0000 - val_rmse: 28964.0137\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1279263744.0000 - rmse: 35766.7969 - val_loss: 853080448.0000 - val_rmse: 29207.5410\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1231950208.0000 - rmse: 35099.1484 - val_loss: 860006464.0000 - val_rmse: 29325.8672\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1211773056.0000 - rmse: 34810.5312 - val_loss: 917509312.0000 - val_rmse: 30290.4160\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1211956352.0000 - rmse: 34813.1641 - val_loss: 846457728.0000 - val_rmse: 29093.9473\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1058422400.0000 - rmse: 32533.4043 - val_loss: 850642880.0000 - val_rmse: 29165.7832\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1054485824.0000 - rmse: 32472.8477 - val_loss: 1079402752.0000 - val_rmse: 32854.2656\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1089137536.0000 - rmse: 33002.0820 - val_loss: 875584640.0000 - val_rmse: 29590.2793\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1024931776.0000 - rmse: 32014.5566 - val_loss: 1190672512.0000 - val_rmse: 34506.1211\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1011290112.0000 - rmse: 31800.7871 - val_loss: 1156276480.0000 - val_rmse: 34004.0664\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 968463616.0000 - rmse: 31120.1484 - val_loss: 887419264.0000 - val_rmse: 29789.5840\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1004438144.0000 - rmse: 31692.8730 - val_loss: 1371255808.0000 - val_rmse: 37030.4727\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 916020608.0000 - rmse: 30265.8320 - val_loss: 787214080.0000 - val_rmse: 28057.3359\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 900709824.0000 - rmse: 30011.8281 - val_loss: 847475136.0000 - val_rmse: 29111.4258\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 926605184.0000 - rmse: 30440.1895 - val_loss: 922837696.0000 - val_rmse: 30378.2441\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 821850368.0000 - rmse: 28667.9336 - val_loss: 1204852352.0000 - val_rmse: 34710.9844\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 876000000.0000 - rmse: 29597.2969 - val_loss: 951266368.0000 - val_rmse: 30842.6055\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 866793472.0000 - rmse: 29441.3574 - val_loss: 1204706944.0000 - val_rmse: 34708.8867\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 830811904.0000 - rmse: 28823.8086 - val_loss: 1347779584.0000 - val_rmse: 36712.1172\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 790496704.0000 - rmse: 28115.7734 - val_loss: 979273344.0000 - val_rmse: 31293.3438\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 834262528.0000 - rmse: 28883.6035 - val_loss: 832282816.0000 - val_rmse: 28849.3125\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 820871680.0000 - rmse: 28650.8574 - val_loss: 1319053952.0000 - val_rmse: 36318.7812\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 821227520.0000 - rmse: 28657.0684 - val_loss: 952516160.0000 - val_rmse: 30862.8613\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 741127488.0000 - rmse: 27223.6562 - val_loss: 787415424.0000 - val_rmse: 28060.9238\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 785713792.0000 - rmse: 28030.5859 - val_loss: 1217028992.0000 - val_rmse: 34885.9414\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 688519936.0000 - rmse: 26239.6621 - val_loss: 852075456.0000 - val_rmse: 29190.3320\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 787330112.0000 - rmse: 28059.4023 - val_loss: 987920192.0000 - val_rmse: 31431.1973\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 702802944.0000 - rmse: 26510.4316 - val_loss: 800695424.0000 - val_rmse: 28296.5625\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 734331712.0000 - rmse: 27098.5547 - val_loss: 1790149632.0000 - val_rmse: 42310.1602\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 722013952.0000 - rmse: 26870.3164 - val_loss: 955077696.0000 - val_rmse: 30904.3320\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 747347648.0000 - rmse: 27337.6602 - val_loss: 826638144.0000 - val_rmse: 28751.3164\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 709681472.0000 - rmse: 26639.8477 - val_loss: 650608000.0000 - val_rmse: 25507.0195\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654664512.0000 - rmse: 25586.4121 - val_loss: 817053824.0000 - val_rmse: 28584.1523\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 645378496.0000 - rmse: 25404.2988 - val_loss: 1193903360.0000 - val_rmse: 34552.9062\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634253504.0000 - rmse: 25184.3887 - val_loss: 714720832.0000 - val_rmse: 26734.2637\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536101504.0000 - rmse: 23153.8652 - val_loss: 1135018240.0000 - val_rmse: 33690.0312\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 633154112.0000 - rmse: 25162.5527 - val_loss: 788515456.0000 - val_rmse: 28080.5176\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654868096.0000 - rmse: 25590.3906 - val_loss: 1088319616.0000 - val_rmse: 32989.6914\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656085888.0000 - rmse: 25614.1738 - val_loss: 764550848.0000 - val_rmse: 27650.5117\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568143168.0000 - rmse: 23835.7520 - val_loss: 1270684800.0000 - val_rmse: 35646.6680\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621485312.0000 - rmse: 24929.6074 - val_loss: 1390532992.0000 - val_rmse: 37289.8516\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579967296.0000 - rmse: 24082.5098 - val_loss: 1035915264.0000 - val_rmse: 32185.6367\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597278656.0000 - rmse: 24439.2852 - val_loss: 847796480.0000 - val_rmse: 29116.9434\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547954176.0000 - rmse: 23408.4219 - val_loss: 686658240.0000 - val_rmse: 26204.1641\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554226048.0000 - rmse: 23542.0039 - val_loss: 1040010624.0000 - val_rmse: 32249.1953\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593110016.0000 - rmse: 24353.8496 - val_loss: 882654336.0000 - val_rmse: 29709.5000\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530699264.0000 - rmse: 23036.9102 - val_loss: 841181760.0000 - val_rmse: 29003.1328\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508605088.0000 - rmse: 22552.2734 - val_loss: 1801852800.0000 - val_rmse: 42448.2383\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452210400.0000 - rmse: 21265.2363 - val_loss: 953095936.0000 - val_rmse: 30872.2500\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537754880.0000 - rmse: 23189.5410 - val_loss: 1529081984.0000 - val_rmse: 39103.4766\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552381504.0000 - rmse: 23502.7969 - val_loss: 1033194688.0000 - val_rmse: 32143.3457\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556125760.0000 - rmse: 23582.3184 - val_loss: 1361428864.0000 - val_rmse: 36897.5469\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571687488.0000 - rmse: 23909.9863 - val_loss: 1108435712.0000 - val_rmse: 33293.1758\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546492928.0000 - rmse: 23377.1875 - val_loss: 1718839040.0000 - val_rmse: 41458.8828\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484824576.0000 - rmse: 22018.7285 - val_loss: 926624960.0000 - val_rmse: 30440.5137\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499877312.0000 - rmse: 22357.9355 - val_loss: 1104888192.0000 - val_rmse: 33239.8594\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579209408.0000 - rmse: 24066.7695 - val_loss: 1507541504.0000 - val_rmse: 38827.0703\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517228736.0000 - rmse: 22742.6641 - val_loss: 642920128.0000 - val_rmse: 25355.8691\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473878432.0000 - rmse: 21768.7480 - val_loss: 978982144.0000 - val_rmse: 31288.6895\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537443392.0000 - rmse: 23182.8242 - val_loss: 926597312.0000 - val_rmse: 30440.0586\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522411456.0000 - rmse: 22856.3223 - val_loss: 876324416.0000 - val_rmse: 29602.7773\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513067232.0000 - rmse: 22650.9844 - val_loss: 829751040.0000 - val_rmse: 28805.4004\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447475712.0000 - rmse: 21153.6211 - val_loss: 1135123584.0000 - val_rmse: 33691.5938\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406708096.0000 - rmse: 20167.0039 - val_loss: 884715968.0000 - val_rmse: 29744.1758\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476781280.0000 - rmse: 21835.3203 - val_loss: 844701632.0000 - val_rmse: 29063.7520\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420914720.0000 - rmse: 20516.2051 - val_loss: 926082752.0000 - val_rmse: 30431.6074\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430258720.0000 - rmse: 20742.6758 - val_loss: 1663860864.0000 - val_rmse: 40790.4492\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447604704.0000 - rmse: 21156.6699 - val_loss: 1157534080.0000 - val_rmse: 34022.5508\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414373920.0000 - rmse: 20356.1758 - val_loss: 1033600320.0000 - val_rmse: 32149.6543\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556626496.0000 - rmse: 23592.9316 - val_loss: 809969344.0000 - val_rmse: 28459.9609\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433207072.0000 - rmse: 20813.6250 - val_loss: 934440000.0000 - val_rmse: 30568.6113\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434990336.0000 - rmse: 20856.4199 - val_loss: 1184416000.0000 - val_rmse: 34415.3438\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420980736.0000 - rmse: 20517.8145 - val_loss: 998100672.0000 - val_rmse: 31592.7305\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473508480.0000 - rmse: 21760.2500 - val_loss: 956156864.0000 - val_rmse: 30921.7871\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467528800.0000 - rmse: 21622.4141 - val_loss: 948784064.0000 - val_rmse: 30802.3379\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388680736.0000 - rmse: 19714.9863 - val_loss: 954076992.0000 - val_rmse: 30888.1367\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382107840.0000 - rmse: 19547.5781 - val_loss: 1199837952.0000 - val_rmse: 34638.6758\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410700032.0000 - rmse: 20265.7344 - val_loss: 763657728.0000 - val_rmse: 27634.3574\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407943648.0000 - rmse: 20197.6133 - val_loss: 1064917760.0000 - val_rmse: 32633.0781\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392040736.0000 - rmse: 19800.0195 - val_loss: 800033216.0000 - val_rmse: 28284.8594\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426883424.0000 - rmse: 20661.1582 - val_loss: 1086302080.0000 - val_rmse: 32959.0977\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477146368.0000 - rmse: 21843.6797 - val_loss: 1057544192.0000 - val_rmse: 32519.9043\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386011136.0000 - rmse: 19647.1641 - val_loss: 2097639936.0000 - val_rmse: 45800.0000\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386091552.0000 - rmse: 19649.2129 - val_loss: 919505408.0000 - val_rmse: 30323.3477\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388801120.0000 - rmse: 19718.0391 - val_loss: 1783916416.0000 - val_rmse: 42236.4336\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435958080.0000 - rmse: 20879.6074 - val_loss: 736044992.0000 - val_rmse: 27130.1484\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394378016.0000 - rmse: 19858.9531 - val_loss: 1791568512.0000 - val_rmse: 42326.9258\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434327456.0000 - rmse: 20840.5234 - val_loss: 1082586752.0000 - val_rmse: 32902.6875\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395280000.0000 - rmse: 19881.6484 - val_loss: 1517514624.0000 - val_rmse: 38955.2891\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394178720.0000 - rmse: 19853.9336 - val_loss: 968387712.0000 - val_rmse: 31118.9258\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363927200.0000 - rmse: 19076.8730 - val_loss: 2792286720.0000 - val_rmse: 52842.0938\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381101728.0000 - rmse: 19521.8262 - val_loss: 974525824.0000 - val_rmse: 31217.3965\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428852608.0000 - rmse: 20708.7559 - val_loss: 1116359296.0000 - val_rmse: 33411.9648\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393508480.0000 - rmse: 19837.0469 - val_loss: 779022528.0000 - val_rmse: 27910.9746\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406151808.0000 - rmse: 20153.2070 - val_loss: 934572160.0000 - val_rmse: 30570.7734\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366399936.0000 - rmse: 19141.5742 - val_loss: 1217947520.0000 - val_rmse: 34899.1016\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371510272.0000 - rmse: 19274.6016 - val_loss: 748208768.0000 - val_rmse: 27353.4043\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409105600.0000 - rmse: 20226.3574 - val_loss: 811214784.0000 - val_rmse: 28481.8301\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289117184.0000 - rmse: 17003.4453 - val_loss: 1327611264.0000 - val_rmse: 36436.3984\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368278784.0000 - rmse: 19190.5898 - val_loss: 767668992.0000 - val_rmse: 27706.8398\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426831232.0000 - rmse: 20659.8926 - val_loss: 1126085376.0000 - val_rmse: 33557.1953\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334530176.0000 - rmse: 18290.1660 - val_loss: 801589824.0000 - val_rmse: 28312.3594\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326138592.0000 - rmse: 18059.3066 - val_loss: 737870336.0000 - val_rmse: 27163.7676\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358632832.0000 - rmse: 18937.6035 - val_loss: 789355200.0000 - val_rmse: 28095.4668\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327931808.0000 - rmse: 18108.8867 - val_loss: 694840576.0000 - val_rmse: 26359.8262\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339402336.0000 - rmse: 18422.8750 - val_loss: 1129733760.0000 - val_rmse: 33611.5117\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333240448.0000 - rmse: 18254.8730 - val_loss: 1100163072.0000 - val_rmse: 33168.7031\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360635168.0000 - rmse: 18990.3965 - val_loss: 710595776.0000 - val_rmse: 26657.0000\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327996736.0000 - rmse: 18110.6797 - val_loss: 707621952.0000 - val_rmse: 26601.1641\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310906176.0000 - rmse: 17632.5312 - val_loss: 765013568.0000 - val_rmse: 27658.8770\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381581632.0000 - rmse: 19534.1133 - val_loss: 891390336.0000 - val_rmse: 29856.1582\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366484384.0000 - rmse: 19143.7793 - val_loss: 731798656.0000 - val_rmse: 27051.7754\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356301856.0000 - rmse: 18875.9570 - val_loss: 1314257664.0000 - val_rmse: 36252.6914\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463278720.0000 - rmse: 21523.9102 - val_loss: 743725056.0000 - val_rmse: 27271.3184\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320284320.0000 - rmse: 17896.4883 - val_loss: 1172129920.0000 - val_rmse: 34236.3828\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348327712.0000 - rmse: 18663.5391 - val_loss: 1422940416.0000 - val_rmse: 37721.8828\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343178464.0000 - rmse: 18525.0742 - val_loss: 987427456.0000 - val_rmse: 31423.3555\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298714752.0000 - rmse: 17283.3652 - val_loss: 771418880.0000 - val_rmse: 27774.4258\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293907680.0000 - rmse: 17143.7344 - val_loss: 686983232.0000 - val_rmse: 26210.3633\n",
      "104/104 [==============================] - 0s 678us/step - loss: 418626624.0000 - rmse: 20460.3652\n",
      "[418626624.0, 20460.365234375]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 6783840256.0000 - rmse: 82364.0703 - val_loss: 1292072320.0000 - val_rmse: 35945.4062\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1849100032.0000 - rmse: 43001.1641 - val_loss: 1076931712.0000 - val_rmse: 32816.6367\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1663846016.0000 - rmse: 40790.2695 - val_loss: 982758720.0000 - val_rmse: 31348.9824\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1498613760.0000 - rmse: 38711.9336 - val_loss: 934097792.0000 - val_rmse: 30563.0137\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1319379584.0000 - rmse: 36323.2656 - val_loss: 1141518336.0000 - val_rmse: 33786.3633\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1270364800.0000 - rmse: 35642.1758 - val_loss: 912052608.0000 - val_rmse: 30200.2090\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1261759360.0000 - rmse: 35521.2539 - val_loss: 853840320.0000 - val_rmse: 29220.5469\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1190118528.0000 - rmse: 34498.0938 - val_loss: 839606080.0000 - val_rmse: 28975.9570\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1057071680.0000 - rmse: 32512.6387 - val_loss: 953558912.0000 - val_rmse: 30879.7500\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1132625792.0000 - rmse: 33654.5078 - val_loss: 1121306752.0000 - val_rmse: 33485.9180\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1103535232.0000 - rmse: 33219.5000 - val_loss: 848296192.0000 - val_rmse: 29125.5254\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1048933056.0000 - rmse: 32387.2363 - val_loss: 777622208.0000 - val_rmse: 27885.8789\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1058949440.0000 - rmse: 32541.5039 - val_loss: 758479104.0000 - val_rmse: 27540.5000\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 988094272.0000 - rmse: 31433.9668 - val_loss: 754146752.0000 - val_rmse: 27461.7324\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 950620992.0000 - rmse: 30832.1426 - val_loss: 1293265920.0000 - val_rmse: 35962.0078\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 957331648.0000 - rmse: 30940.7773 - val_loss: 748133824.0000 - val_rmse: 27352.0352\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 951748672.0000 - rmse: 30850.4238 - val_loss: 737731456.0000 - val_rmse: 27161.2109\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 873395456.0000 - rmse: 29553.2637 - val_loss: 695266304.0000 - val_rmse: 26367.9023\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 884696192.0000 - rmse: 29743.8438 - val_loss: 679012160.0000 - val_rmse: 26057.8613\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 868394176.0000 - rmse: 29468.5293 - val_loss: 682406208.0000 - val_rmse: 26122.9062\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 830780352.0000 - rmse: 28823.2598 - val_loss: 673437568.0000 - val_rmse: 25950.6758\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 873065984.0000 - rmse: 29547.6895 - val_loss: 765775488.0000 - val_rmse: 27672.6484\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 754248448.0000 - rmse: 27463.5840 - val_loss: 635733568.0000 - val_rmse: 25213.7578\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 831781504.0000 - rmse: 28840.6211 - val_loss: 629929216.0000 - val_rmse: 25098.3906\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 766761792.0000 - rmse: 27690.4648 - val_loss: 880283776.0000 - val_rmse: 29669.5762\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 773984896.0000 - rmse: 27820.5820 - val_loss: 959063168.0000 - val_rmse: 30968.7441\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 671864256.0000 - rmse: 25920.3438 - val_loss: 708136768.0000 - val_rmse: 26610.8379\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 735487744.0000 - rmse: 27119.8770 - val_loss: 1067975424.0000 - val_rmse: 32679.8926\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 697457280.0000 - rmse: 26409.4160 - val_loss: 716502080.0000 - val_rmse: 26767.5566\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 694835264.0000 - rmse: 26359.7285 - val_loss: 1101670528.0000 - val_rmse: 33191.4219\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 650060800.0000 - rmse: 25496.2891 - val_loss: 727253696.0000 - val_rmse: 26967.6406\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 686978688.0000 - rmse: 26210.2773 - val_loss: 697149056.0000 - val_rmse: 26403.5801\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637142336.0000 - rmse: 25241.6777 - val_loss: 648742144.0000 - val_rmse: 25470.4180\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 703814208.0000 - rmse: 26529.4961 - val_loss: 1166670080.0000 - val_rmse: 34156.5469\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 633066176.0000 - rmse: 25160.8066 - val_loss: 604259968.0000 - val_rmse: 24581.6992\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499782368.0000 - rmse: 22355.8105 - val_loss: 1043456512.0000 - val_rmse: 32302.5781\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585859840.0000 - rmse: 24204.5410 - val_loss: 947704000.0000 - val_rmse: 30784.8008\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654329856.0000 - rmse: 25579.8691 - val_loss: 832926784.0000 - val_rmse: 28860.4707\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 627273408.0000 - rmse: 25045.4277 - val_loss: 638381056.0000 - val_rmse: 25266.2031\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 600862208.0000 - rmse: 24512.4902 - val_loss: 585084992.0000 - val_rmse: 24188.5293\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555665600.0000 - rmse: 23572.5605 - val_loss: 1491687680.0000 - val_rmse: 38622.3711\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 528553408.0000 - rmse: 22990.2891 - val_loss: 962511104.0000 - val_rmse: 31024.3633\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561513280.0000 - rmse: 23696.2695 - val_loss: 842010688.0000 - val_rmse: 29017.4199\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 625793600.0000 - rmse: 25015.8672 - val_loss: 914231872.0000 - val_rmse: 30236.2676\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522018368.0000 - rmse: 22847.7188 - val_loss: 638678656.0000 - val_rmse: 25272.0918\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575536576.0000 - rmse: 23990.3418 - val_loss: 671907840.0000 - val_rmse: 25921.1855\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496429536.0000 - rmse: 22280.6992 - val_loss: 682912192.0000 - val_rmse: 26132.5879\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506567552.0000 - rmse: 22507.0547 - val_loss: 1142401024.0000 - val_rmse: 33799.4219\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558749120.0000 - rmse: 23637.8730 - val_loss: 621401792.0000 - val_rmse: 24927.9297\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478153184.0000 - rmse: 21866.7129 - val_loss: 671563136.0000 - val_rmse: 25914.5352\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500160672.0000 - rmse: 22364.2715 - val_loss: 538945088.0000 - val_rmse: 23215.1914\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506095808.0000 - rmse: 22496.5742 - val_loss: 641570304.0000 - val_rmse: 25329.2383\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533508800.0000 - rmse: 23097.8086 - val_loss: 676908736.0000 - val_rmse: 26017.4707\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539162560.0000 - rmse: 23219.8730 - val_loss: 759498240.0000 - val_rmse: 27558.9961\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486342560.0000 - rmse: 22053.1758 - val_loss: 1026930432.0000 - val_rmse: 32045.7539\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498176800.0000 - rmse: 22319.8750 - val_loss: 939778816.0000 - val_rmse: 30655.8125\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552805888.0000 - rmse: 23511.8242 - val_loss: 1197261568.0000 - val_rmse: 34601.4688\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488884224.0000 - rmse: 22110.7246 - val_loss: 648213120.0000 - val_rmse: 25460.0293\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439999296.0000 - rmse: 20976.1602 - val_loss: 782504960.0000 - val_rmse: 27973.2910\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477570560.0000 - rmse: 21853.3867 - val_loss: 863971712.0000 - val_rmse: 29393.3965\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406197792.0000 - rmse: 20154.3457 - val_loss: 865763712.0000 - val_rmse: 29423.8613\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467127072.0000 - rmse: 21613.1211 - val_loss: 687433664.0000 - val_rmse: 26218.9551\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483332512.0000 - rmse: 21984.8242 - val_loss: 818334656.0000 - val_rmse: 28606.5488\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480314912.0000 - rmse: 21916.0879 - val_loss: 921724672.0000 - val_rmse: 30359.9160\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590293568.0000 - rmse: 24295.9570 - val_loss: 740886592.0000 - val_rmse: 27219.2305\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434441952.0000 - rmse: 20843.2695 - val_loss: 1092301184.0000 - val_rmse: 33049.9805\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439170720.0000 - rmse: 20956.4004 - val_loss: 827835840.0000 - val_rmse: 28772.1367\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453853216.0000 - rmse: 21303.8281 - val_loss: 655315264.0000 - val_rmse: 25599.1250\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474158528.0000 - rmse: 21775.1797 - val_loss: 1058553536.0000 - val_rmse: 32535.4199\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472821024.0000 - rmse: 21744.4473 - val_loss: 692307840.0000 - val_rmse: 26311.7422\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425142656.0000 - rmse: 20618.9863 - val_loss: 723309120.0000 - val_rmse: 26894.4062\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396750848.0000 - rmse: 19918.6055 - val_loss: 656766400.0000 - val_rmse: 25627.4531\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422713120.0000 - rmse: 20559.9863 - val_loss: 656761792.0000 - val_rmse: 25627.3633\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583564928.0000 - rmse: 24157.0879 - val_loss: 767284992.0000 - val_rmse: 27699.9082\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428934848.0000 - rmse: 20710.7402 - val_loss: 736267136.0000 - val_rmse: 27134.2422\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422819136.0000 - rmse: 20562.5645 - val_loss: 1607185152.0000 - val_rmse: 40089.7148\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366225024.0000 - rmse: 19137.0059 - val_loss: 750549888.0000 - val_rmse: 27396.1660\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434504672.0000 - rmse: 20844.7754 - val_loss: 641370816.0000 - val_rmse: 25325.2988\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412511360.0000 - rmse: 20310.3750 - val_loss: 728311552.0000 - val_rmse: 26987.2480\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420150400.0000 - rmse: 20497.5684 - val_loss: 833370496.0000 - val_rmse: 28868.1562\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418672000.0000 - rmse: 20461.4766 - val_loss: 620514624.0000 - val_rmse: 24910.1309\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371919680.0000 - rmse: 19285.2168 - val_loss: 760051520.0000 - val_rmse: 27569.0312\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379103520.0000 - rmse: 19470.5801 - val_loss: 782035392.0000 - val_rmse: 27964.8945\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387360448.0000 - rmse: 19681.4746 - val_loss: 637678528.0000 - val_rmse: 25252.2969\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386585376.0000 - rmse: 19661.7734 - val_loss: 737154368.0000 - val_rmse: 27150.5840\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442789600.0000 - rmse: 21042.5664 - val_loss: 862315136.0000 - val_rmse: 29365.1992\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367391360.0000 - rmse: 19167.4531 - val_loss: 748916544.0000 - val_rmse: 27366.3398\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396552448.0000 - rmse: 19913.6230 - val_loss: 792348480.0000 - val_rmse: 28148.6836\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348014272.0000 - rmse: 18655.1367 - val_loss: 724733248.0000 - val_rmse: 26920.8691\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413404480.0000 - rmse: 20332.3477 - val_loss: 676553920.0000 - val_rmse: 26010.6484\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348128320.0000 - rmse: 18658.1953 - val_loss: 664768960.0000 - val_rmse: 25783.1133\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363946528.0000 - rmse: 19077.3809 - val_loss: 582362496.0000 - val_rmse: 24132.1875\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401428384.0000 - rmse: 20035.6758 - val_loss: 886013376.0000 - val_rmse: 29765.9746\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378351168.0000 - rmse: 19451.2500 - val_loss: 794132672.0000 - val_rmse: 28180.3574\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401728608.0000 - rmse: 20043.1680 - val_loss: 801624512.0000 - val_rmse: 28312.9727\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445488224.0000 - rmse: 21106.5898 - val_loss: 749958976.0000 - val_rmse: 27385.3789\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369307904.0000 - rmse: 19217.3848 - val_loss: 1483036928.0000 - val_rmse: 38510.2148\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340933088.0000 - rmse: 18464.3730 - val_loss: 1004552768.0000 - val_rmse: 31694.6797\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424369664.0000 - rmse: 20600.2344 - val_loss: 986281472.0000 - val_rmse: 31405.1191\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370175744.0000 - rmse: 19239.9512 - val_loss: 1104494336.0000 - val_rmse: 33233.9336\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365081568.0000 - rmse: 19107.1074 - val_loss: 665320000.0000 - val_rmse: 25793.7949\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400201888.0000 - rmse: 20005.0449 - val_loss: 1012741952.0000 - val_rmse: 31823.6055\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323196544.0000 - rmse: 17977.6660 - val_loss: 767298432.0000 - val_rmse: 27700.1484\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331644384.0000 - rmse: 18211.1035 - val_loss: 885519488.0000 - val_rmse: 29757.6777\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355412256.0000 - rmse: 18852.3789 - val_loss: 738774528.0000 - val_rmse: 27180.4062\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348621152.0000 - rmse: 18671.3965 - val_loss: 1102971904.0000 - val_rmse: 33211.0195\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375872384.0000 - rmse: 19387.4277 - val_loss: 867981312.0000 - val_rmse: 29461.5234\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354976448.0000 - rmse: 18840.8164 - val_loss: 839460928.0000 - val_rmse: 28973.4492\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352490400.0000 - rmse: 18774.7266 - val_loss: 645640512.0000 - val_rmse: 25409.4551\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313139776.0000 - rmse: 17695.7520 - val_loss: 794497664.0000 - val_rmse: 28186.8340\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345503808.0000 - rmse: 18587.7305 - val_loss: 721778560.0000 - val_rmse: 26865.9375\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358814112.0000 - rmse: 18942.3867 - val_loss: 1200682240.0000 - val_rmse: 34650.8633\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319004256.0000 - rmse: 17860.6895 - val_loss: 887020032.0000 - val_rmse: 29782.8809\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296037376.0000 - rmse: 17205.7344 - val_loss: 753626752.0000 - val_rmse: 27452.2617\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328780608.0000 - rmse: 18132.3066 - val_loss: 801178688.0000 - val_rmse: 28305.0996\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284198080.0000 - rmse: 16858.1719 - val_loss: 1087863552.0000 - val_rmse: 32982.7734\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356021376.0000 - rmse: 18868.5273 - val_loss: 771869440.0000 - val_rmse: 27782.5352\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348479264.0000 - rmse: 18667.5977 - val_loss: 760457856.0000 - val_rmse: 27576.3965\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313306944.0000 - rmse: 17700.4766 - val_loss: 826081472.0000 - val_rmse: 28741.6328\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329998816.0000 - rmse: 18165.8691 - val_loss: 759873664.0000 - val_rmse: 27565.8047\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340120032.0000 - rmse: 18442.3398 - val_loss: 920436480.0000 - val_rmse: 30338.6953\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277757248.0000 - rmse: 16666.0488 - val_loss: 951864128.0000 - val_rmse: 30852.2930\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340408480.0000 - rmse: 18450.1582 - val_loss: 785207936.0000 - val_rmse: 28021.5605\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295269216.0000 - rmse: 17183.3965 - val_loss: 896319616.0000 - val_rmse: 29938.5977\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321766304.0000 - rmse: 17937.8438 - val_loss: 870889344.0000 - val_rmse: 29510.8340\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278158464.0000 - rmse: 16678.0820 - val_loss: 1082964480.0000 - val_rmse: 32908.4219\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291224448.0000 - rmse: 17065.2988 - val_loss: 895261376.0000 - val_rmse: 29920.9180\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302925280.0000 - rmse: 17404.7461 - val_loss: 734918464.0000 - val_rmse: 27109.3770\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320740096.0000 - rmse: 17909.2188 - val_loss: 760891264.0000 - val_rmse: 27584.2559\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317764224.0000 - rmse: 17825.9414 - val_loss: 771943232.0000 - val_rmse: 27783.8652\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330882176.0000 - rmse: 18190.1660 - val_loss: 811133696.0000 - val_rmse: 28480.4082\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295515136.0000 - rmse: 17190.5527 - val_loss: 789073600.0000 - val_rmse: 28090.4531\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307206976.0000 - rmse: 17527.3184 - val_loss: 783737024.0000 - val_rmse: 27995.3008\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333308704.0000 - rmse: 18256.7422 - val_loss: 1186195712.0000 - val_rmse: 34441.1914\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318243744.0000 - rmse: 17839.3828 - val_loss: 781016320.0000 - val_rmse: 27946.6699\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335059968.0000 - rmse: 18304.6406 - val_loss: 797262208.0000 - val_rmse: 28235.8301\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295069856.0000 - rmse: 17177.5938 - val_loss: 881222848.0000 - val_rmse: 29685.3945\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300696064.0000 - rmse: 17340.5879 - val_loss: 932000704.0000 - val_rmse: 30528.6875\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315008736.0000 - rmse: 17748.4844 - val_loss: 937472064.0000 - val_rmse: 30618.1621\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314674720.0000 - rmse: 17739.0723 - val_loss: 835605568.0000 - val_rmse: 28906.8438\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312330432.0000 - rmse: 17672.8711 - val_loss: 800459328.0000 - val_rmse: 28292.3906\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343635072.0000 - rmse: 18537.3945 - val_loss: 873705920.0000 - val_rmse: 29558.5137\n",
      "104/104 [==============================] - 0s 697us/step - loss: 338991360.0000 - rmse: 18411.7148\n",
      "[338991360.0, 18411.71484375]\n",
      "[22679.9296875, 30279.103515625, 40789.42578125, 20460.365234375, 18411.71484375]\n",
      "26524.1078125\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "!python train.py kfold baseline\n",
    "#d 0.25 p 20 epoch 150 layer -1 (8,8)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 19:08:38.831949: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 19:08:38.831993: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 19:08:38.832325: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,217\n",
      "Trainable params: 20,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,217\n",
      "Trainable params: 20,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 19:08:39.033373: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 8588829696.0000 - rmse: 92675.9375 - val_loss: 1366115072.0000 - val_rmse: 36960.9922\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1941556608.0000 - rmse: 44063.0977 - val_loss: 1021690176.0000 - val_rmse: 31963.8887\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1713747328.0000 - rmse: 41397.4297 - val_loss: 963855040.0000 - val_rmse: 31046.0156\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1593015936.0000 - rmse: 39912.6055 - val_loss: 789011904.0000 - val_rmse: 28089.3555\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1475601792.0000 - rmse: 38413.5625 - val_loss: 740838080.0000 - val_rmse: 27218.3398\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1440420864.0000 - rmse: 37952.8750 - val_loss: 733843968.0000 - val_rmse: 27089.5547\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1341415552.0000 - rmse: 36625.3398 - val_loss: 759323968.0000 - val_rmse: 27555.8320\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1362823296.0000 - rmse: 36916.4375 - val_loss: 698367168.0000 - val_rmse: 26426.6367\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1345565312.0000 - rmse: 36681.9492 - val_loss: 706023936.0000 - val_rmse: 26571.1094\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1295350656.0000 - rmse: 35990.9805 - val_loss: 705872896.0000 - val_rmse: 26568.2695\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1294969088.0000 - rmse: 35985.6797 - val_loss: 701853312.0000 - val_rmse: 26492.5137\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1226424960.0000 - rmse: 35020.3516 - val_loss: 766588160.0000 - val_rmse: 27687.3281\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1262712448.0000 - rmse: 35534.6641 - val_loss: 704542848.0000 - val_rmse: 26543.2266\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1160514304.0000 - rmse: 34066.3203 - val_loss: 722342592.0000 - val_rmse: 26876.4316\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1187852928.0000 - rmse: 34465.2422 - val_loss: 700416896.0000 - val_rmse: 26465.3906\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1141508992.0000 - rmse: 33786.2266 - val_loss: 683937792.0000 - val_rmse: 26152.2031\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1140060544.0000 - rmse: 33764.7812 - val_loss: 711413184.0000 - val_rmse: 26672.3301\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1193096064.0000 - rmse: 34541.2227 - val_loss: 853613248.0000 - val_rmse: 29216.6602\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1190837888.0000 - rmse: 34508.5156 - val_loss: 748214976.0000 - val_rmse: 27353.5176\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1104488448.0000 - rmse: 33233.8438 - val_loss: 690071168.0000 - val_rmse: 26269.2051\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1086743680.0000 - rmse: 32965.7969 - val_loss: 677097920.0000 - val_rmse: 26021.1055\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1001285696.0000 - rmse: 31643.0977 - val_loss: 867052480.0000 - val_rmse: 29445.7539\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 969449920.0000 - rmse: 31135.9902 - val_loss: 744780352.0000 - val_rmse: 27290.6641\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1010503680.0000 - rmse: 31788.4199 - val_loss: 829305024.0000 - val_rmse: 28797.6562\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1023264576.0000 - rmse: 31988.5059 - val_loss: 747655232.0000 - val_rmse: 27343.2852\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1046905728.0000 - rmse: 32355.9219 - val_loss: 778992000.0000 - val_rmse: 27910.4277\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 946080064.0000 - rmse: 30758.4121 - val_loss: 1189261056.0000 - val_rmse: 34485.6641\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 958573888.0000 - rmse: 30960.8438 - val_loss: 686587072.0000 - val_rmse: 26202.8066\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 875259904.0000 - rmse: 29584.7910 - val_loss: 657548672.0000 - val_rmse: 25642.7109\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 809209408.0000 - rmse: 28446.6055 - val_loss: 797460800.0000 - val_rmse: 28239.3477\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 775639424.0000 - rmse: 27850.3047 - val_loss: 1293894272.0000 - val_rmse: 35970.7422\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 832384256.0000 - rmse: 28851.0684 - val_loss: 668630144.0000 - val_rmse: 25857.8828\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 851415168.0000 - rmse: 29179.0176 - val_loss: 668476864.0000 - val_rmse: 25854.9180\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 807970752.0000 - rmse: 28424.8262 - val_loss: 683879808.0000 - val_rmse: 26151.0938\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 763947136.0000 - rmse: 27639.5918 - val_loss: 602305408.0000 - val_rmse: 24541.9102\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 744845952.0000 - rmse: 27291.8633 - val_loss: 590161408.0000 - val_rmse: 24293.2383\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 778403456.0000 - rmse: 27899.8828 - val_loss: 874501440.0000 - val_rmse: 29571.9707\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 790874688.0000 - rmse: 28122.4941 - val_loss: 910259712.0000 - val_rmse: 30170.5098\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 737487104.0000 - rmse: 27156.7129 - val_loss: 700999872.0000 - val_rmse: 26476.4004\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 756963264.0000 - rmse: 27512.9648 - val_loss: 622552512.0000 - val_rmse: 24951.0000\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637722048.0000 - rmse: 25253.1582 - val_loss: 622810624.0000 - val_rmse: 24956.1738\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 633759808.0000 - rmse: 25174.5859 - val_loss: 1299476096.0000 - val_rmse: 36048.2461\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 743681280.0000 - rmse: 27270.5195 - val_loss: 742653440.0000 - val_rmse: 27251.6680\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665502080.0000 - rmse: 25797.3262 - val_loss: 875202752.0000 - val_rmse: 29583.8262\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704710144.0000 - rmse: 26546.3770 - val_loss: 811930560.0000 - val_rmse: 28494.3945\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540602496.0000 - rmse: 23250.8594 - val_loss: 650241984.0000 - val_rmse: 25499.8438\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 636216960.0000 - rmse: 25223.3398 - val_loss: 879237184.0000 - val_rmse: 29651.9336\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599063936.0000 - rmse: 24475.7793 - val_loss: 890514816.0000 - val_rmse: 29841.4941\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501877056.0000 - rmse: 22402.6113 - val_loss: 990737728.0000 - val_rmse: 31475.9844\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556229888.0000 - rmse: 23584.5273 - val_loss: 592793344.0000 - val_rmse: 24347.3457\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586294912.0000 - rmse: 24213.5273 - val_loss: 690913216.0000 - val_rmse: 26285.2246\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604718720.0000 - rmse: 24591.0293 - val_loss: 877611136.0000 - val_rmse: 29624.5000\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624254720.0000 - rmse: 24985.0898 - val_loss: 669624768.0000 - val_rmse: 25877.1094\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485447168.0000 - rmse: 22032.8633 - val_loss: 932882688.0000 - val_rmse: 30543.1270\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512188288.0000 - rmse: 22631.5762 - val_loss: 1327380480.0000 - val_rmse: 36433.2305\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516941536.0000 - rmse: 22736.3477 - val_loss: 1806258816.0000 - val_rmse: 42500.1055\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539372480.0000 - rmse: 23224.3906 - val_loss: 1478096640.0000 - val_rmse: 38446.0195\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545552704.0000 - rmse: 23357.0664 - val_loss: 595537664.0000 - val_rmse: 24403.6387\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585079040.0000 - rmse: 24188.4062 - val_loss: 569268672.0000 - val_rmse: 23859.3516\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515205472.0000 - rmse: 22698.1367 - val_loss: 1515572096.0000 - val_rmse: 38930.3477\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471896000.0000 - rmse: 21723.1660 - val_loss: 979366848.0000 - val_rmse: 31294.8379\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475126272.0000 - rmse: 21797.3906 - val_loss: 728627968.0000 - val_rmse: 26993.1074\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457752768.0000 - rmse: 21395.1582 - val_loss: 660176640.0000 - val_rmse: 25693.9023\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401704768.0000 - rmse: 20042.5723 - val_loss: 638686272.0000 - val_rmse: 25272.2402\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482618304.0000 - rmse: 21968.5742 - val_loss: 1040276672.0000 - val_rmse: 32253.3184\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422391264.0000 - rmse: 20552.1562 - val_loss: 726123968.0000 - val_rmse: 26946.6875\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483392416.0000 - rmse: 21986.1836 - val_loss: 605254272.0000 - val_rmse: 24601.9141\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489292576.0000 - rmse: 22119.9590 - val_loss: 990601792.0000 - val_rmse: 31473.8262\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410159936.0000 - rmse: 20252.4043 - val_loss: 1271461888.0000 - val_rmse: 35657.5625\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452380384.0000 - rmse: 21269.2344 - val_loss: 877153280.0000 - val_rmse: 29616.7695\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494719552.0000 - rmse: 22242.2910 - val_loss: 895531328.0000 - val_rmse: 29925.4277\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453147968.0000 - rmse: 21287.2695 - val_loss: 500175392.0000 - val_rmse: 22364.5996\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481422720.0000 - rmse: 21941.3457 - val_loss: 1314791040.0000 - val_rmse: 36260.0469\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439483136.0000 - rmse: 20963.8516 - val_loss: 808134144.0000 - val_rmse: 28427.7012\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478141184.0000 - rmse: 21866.4375 - val_loss: 683126336.0000 - val_rmse: 26136.6836\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511107904.0000 - rmse: 22607.6934 - val_loss: 887218432.0000 - val_rmse: 29786.2109\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512353824.0000 - rmse: 22635.2344 - val_loss: 1028828288.0000 - val_rmse: 32075.3535\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407731744.0000 - rmse: 20192.3652 - val_loss: 965174400.0000 - val_rmse: 31067.2559\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404825888.0000 - rmse: 20120.2832 - val_loss: 763082048.0000 - val_rmse: 27623.9395\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410604608.0000 - rmse: 20263.3789 - val_loss: 1242693248.0000 - val_rmse: 35251.8555\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423695776.0000 - rmse: 20583.8672 - val_loss: 868354112.0000 - val_rmse: 29467.8457\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442589696.0000 - rmse: 21037.8145 - val_loss: 874842432.0000 - val_rmse: 29577.7344\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395254944.0000 - rmse: 19881.0176 - val_loss: 828244736.0000 - val_rmse: 28779.2402\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410893632.0000 - rmse: 20270.5098 - val_loss: 1215874688.0000 - val_rmse: 34869.3945\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435053408.0000 - rmse: 20857.9316 - val_loss: 1078802688.0000 - val_rmse: 32845.1289\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399053376.0000 - rmse: 19976.3184 - val_loss: 1314000256.0000 - val_rmse: 36249.1406\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415764288.0000 - rmse: 20390.2969 - val_loss: 795853248.0000 - val_rmse: 28210.8691\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373531040.0000 - rmse: 19326.9492 - val_loss: 1638560000.0000 - val_rmse: 40479.1289\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425367552.0000 - rmse: 20624.4375 - val_loss: 2481605376.0000 - val_rmse: 49815.7109\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375260576.0000 - rmse: 19371.6406 - val_loss: 1119732096.0000 - val_rmse: 33462.3945\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369033088.0000 - rmse: 19210.2324 - val_loss: 1144240256.0000 - val_rmse: 33826.6211\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410997216.0000 - rmse: 20273.0625 - val_loss: 876769664.0000 - val_rmse: 29610.2949\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386929696.0000 - rmse: 19670.5273 - val_loss: 1469489280.0000 - val_rmse: 38333.9180\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379339712.0000 - rmse: 19476.6445 - val_loss: 1006967488.0000 - val_rmse: 31732.7480\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358822208.0000 - rmse: 18942.5996 - val_loss: 1464602624.0000 - val_rmse: 38270.1289\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359854048.0000 - rmse: 18969.8184 - val_loss: 1129936512.0000 - val_rmse: 33614.5273\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329219360.0000 - rmse: 18144.4004 - val_loss: 1235966976.0000 - val_rmse: 35156.3164\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436048704.0000 - rmse: 20881.7773 - val_loss: 1087075968.0000 - val_rmse: 32970.8359\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427453088.0000 - rmse: 20674.9355 - val_loss: 1467522048.0000 - val_rmse: 38308.2500\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345942432.0000 - rmse: 18599.5273 - val_loss: 1303208320.0000 - val_rmse: 36099.9766\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417425408.0000 - rmse: 20430.9883 - val_loss: 1481353600.0000 - val_rmse: 38488.3555\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352942752.0000 - rmse: 18786.7676 - val_loss: 1553063808.0000 - val_rmse: 39408.9258\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417984672.0000 - rmse: 20444.6719 - val_loss: 1573895296.0000 - val_rmse: 39672.3477\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340269664.0000 - rmse: 18446.3965 - val_loss: 2127065216.0000 - val_rmse: 46120.1172\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363080640.0000 - rmse: 19054.6719 - val_loss: 1243923456.0000 - val_rmse: 35269.2969\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425167104.0000 - rmse: 20619.5781 - val_loss: 1586560896.0000 - val_rmse: 39831.6562\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401877728.0000 - rmse: 20046.8867 - val_loss: 1604711424.0000 - val_rmse: 40058.8477\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331485056.0000 - rmse: 18206.7285 - val_loss: 1035883904.0000 - val_rmse: 32185.1504\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374533248.0000 - rmse: 19352.8594 - val_loss: 1529591936.0000 - val_rmse: 39109.9961\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388090624.0000 - rmse: 19700.0137 - val_loss: 1518470400.0000 - val_rmse: 38967.5547\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357846688.0000 - rmse: 18916.8340 - val_loss: 1056920576.0000 - val_rmse: 32510.3125\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333348672.0000 - rmse: 18257.8340 - val_loss: 1578352640.0000 - val_rmse: 39728.4844\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315425120.0000 - rmse: 17760.2090 - val_loss: 1194625536.0000 - val_rmse: 34563.3555\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348066304.0000 - rmse: 18656.5332 - val_loss: 1335640320.0000 - val_rmse: 36546.4141\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317180704.0000 - rmse: 17809.5664 - val_loss: 1541874816.0000 - val_rmse: 39266.7148\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315991776.0000 - rmse: 17776.1562 - val_loss: 1424626816.0000 - val_rmse: 37744.2266\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363194080.0000 - rmse: 19057.6484 - val_loss: 1777496832.0000 - val_rmse: 42160.3672\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390173024.0000 - rmse: 19752.7969 - val_loss: 1026210496.0000 - val_rmse: 32034.5195\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317072608.0000 - rmse: 17806.5293 - val_loss: 2320635392.0000 - val_rmse: 48172.9727\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310037120.0000 - rmse: 17607.8691 - val_loss: 757712064.0000 - val_rmse: 27526.5684\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293885856.0000 - rmse: 17143.0957 - val_loss: 1697484416.0000 - val_rmse: 41200.5391\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311662112.0000 - rmse: 17653.9531 - val_loss: 1571014272.0000 - val_rmse: 39636.0195\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383630400.0000 - rmse: 19586.4824 - val_loss: 1344241792.0000 - val_rmse: 36663.9023\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348720160.0000 - rmse: 18674.0488 - val_loss: 913995520.0000 - val_rmse: 30232.3574\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314595488.0000 - rmse: 17736.8359 - val_loss: 1168063744.0000 - val_rmse: 34176.9492\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338133696.0000 - rmse: 18388.4102 - val_loss: 1484653952.0000 - val_rmse: 38531.2070\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401830016.0000 - rmse: 20045.6953 - val_loss: 915045696.0000 - val_rmse: 30249.7227\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302496384.0000 - rmse: 17392.4199 - val_loss: 1913332480.0000 - val_rmse: 43741.6562\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355933568.0000 - rmse: 18866.1973 - val_loss: 1741576960.0000 - val_rmse: 41732.2031\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345595104.0000 - rmse: 18590.1875 - val_loss: 1088342528.0000 - val_rmse: 32990.0352\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382825344.0000 - rmse: 19565.9199 - val_loss: 1266684288.0000 - val_rmse: 35590.5078\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312119872.0000 - rmse: 17666.9121 - val_loss: 847238336.0000 - val_rmse: 29107.3574\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330319168.0000 - rmse: 18174.6836 - val_loss: 943244736.0000 - val_rmse: 30712.2891\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321991712.0000 - rmse: 17944.1250 - val_loss: 1873973376.0000 - val_rmse: 43289.4141\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326752576.0000 - rmse: 18076.2949 - val_loss: 1539961088.0000 - val_rmse: 39242.3398\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366615008.0000 - rmse: 19147.1914 - val_loss: 1316111744.0000 - val_rmse: 36278.2500\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335996672.0000 - rmse: 18330.2109 - val_loss: 995040256.0000 - val_rmse: 31544.2539\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319985760.0000 - rmse: 17888.1426 - val_loss: 1719673472.0000 - val_rmse: 41468.9453\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324841696.0000 - rmse: 18023.3633 - val_loss: 1558002432.0000 - val_rmse: 39471.5391\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300149152.0000 - rmse: 17324.8125 - val_loss: 1098199552.0000 - val_rmse: 33139.0938\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342142848.0000 - rmse: 18497.1016 - val_loss: 1534706304.0000 - val_rmse: 39175.3242\n",
      "104/104 [==============================] - 0s 681us/step - loss: 513964288.0000 - rmse: 22670.7793\n",
      "[513964288.0, 22670.779296875]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,217\n",
      "Trainable params: 20,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,217\n",
      "Trainable params: 20,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 7179012096.0000 - rmse: 84729.0547 - val_loss: 1300091136.0000 - val_rmse: 36056.7773\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1613247488.0000 - rmse: 40165.2539 - val_loss: 1081425408.0000 - val_rmse: 32885.0312\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1389577088.0000 - rmse: 37277.0312 - val_loss: 990093504.0000 - val_rmse: 31465.7520\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1327123968.0000 - rmse: 36429.7109 - val_loss: 986320192.0000 - val_rmse: 31405.7344\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1204735616.0000 - rmse: 34709.3008 - val_loss: 893373120.0000 - val_rmse: 29889.3477\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1120649856.0000 - rmse: 33476.1094 - val_loss: 869842752.0000 - val_rmse: 29493.0977\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1139066112.0000 - rmse: 33750.0547 - val_loss: 888760000.0000 - val_rmse: 29812.0781\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1086927232.0000 - rmse: 32968.5781 - val_loss: 1314040960.0000 - val_rmse: 36249.7031\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1099420160.0000 - rmse: 33157.5039 - val_loss: 813238400.0000 - val_rmse: 28517.3359\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1036309632.0000 - rmse: 32191.7637 - val_loss: 840895744.0000 - val_rmse: 28998.2031\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1010182272.0000 - rmse: 31783.3652 - val_loss: 1007942976.0000 - val_rmse: 31748.1172\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 972290624.0000 - rmse: 31181.5742 - val_loss: 900801152.0000 - val_rmse: 30013.3496\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 953162880.0000 - rmse: 30873.3359 - val_loss: 761997248.0000 - val_rmse: 27604.2969\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 939678464.0000 - rmse: 30654.1758 - val_loss: 735928704.0000 - val_rmse: 27128.0059\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 938008320.0000 - rmse: 30626.9199 - val_loss: 805755136.0000 - val_rmse: 28385.8262\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 928953792.0000 - rmse: 30478.7441 - val_loss: 704602560.0000 - val_rmse: 26544.3496\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 865815232.0000 - rmse: 29424.7383 - val_loss: 692058112.0000 - val_rmse: 26306.9941\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 860793856.0000 - rmse: 29339.2891 - val_loss: 670691008.0000 - val_rmse: 25897.7012\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 849576896.0000 - rmse: 29147.5000 - val_loss: 669855488.0000 - val_rmse: 25881.5664\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 774678016.0000 - rmse: 27833.0391 - val_loss: 663709120.0000 - val_rmse: 25762.5508\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 784375616.0000 - rmse: 28006.7070 - val_loss: 954885568.0000 - val_rmse: 30901.2227\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 793930752.0000 - rmse: 28176.7773 - val_loss: 686583168.0000 - val_rmse: 26202.7324\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 762581120.0000 - rmse: 27614.8711 - val_loss: 672481280.0000 - val_rmse: 25932.2441\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 779830016.0000 - rmse: 27925.4375 - val_loss: 619724288.0000 - val_rmse: 24894.2617\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 793914496.0000 - rmse: 28176.4863 - val_loss: 677116224.0000 - val_rmse: 26021.4570\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 739142336.0000 - rmse: 27187.1719 - val_loss: 673536512.0000 - val_rmse: 25952.5820\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733487168.0000 - rmse: 27082.9688 - val_loss: 615281792.0000 - val_rmse: 24804.8750\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706768128.0000 - rmse: 26585.1094 - val_loss: 582668992.0000 - val_rmse: 24138.5371\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 633984064.0000 - rmse: 25179.0391 - val_loss: 647029568.0000 - val_rmse: 25436.7754\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 653133760.0000 - rmse: 25556.4805 - val_loss: 602770624.0000 - val_rmse: 24551.3867\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666437952.0000 - rmse: 25815.4590 - val_loss: 712769280.0000 - val_rmse: 26697.7383\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656359104.0000 - rmse: 25619.5059 - val_loss: 577358656.0000 - val_rmse: 24028.2871\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 675690560.0000 - rmse: 25994.0469 - val_loss: 600421440.0000 - val_rmse: 24503.4980\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550207680.0000 - rmse: 23456.5059 - val_loss: 521208320.0000 - val_rmse: 22829.9863\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544759424.0000 - rmse: 23340.0820 - val_loss: 568826048.0000 - val_rmse: 23850.0742\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551794176.0000 - rmse: 23490.2988 - val_loss: 577624320.0000 - val_rmse: 24033.8145\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553258688.0000 - rmse: 23521.4512 - val_loss: 580263872.0000 - val_rmse: 24088.6660\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586154688.0000 - rmse: 24210.6309 - val_loss: 496777792.0000 - val_rmse: 22288.5098\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552925632.0000 - rmse: 23514.3711 - val_loss: 518129056.0000 - val_rmse: 22762.4492\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517443424.0000 - rmse: 22747.3809 - val_loss: 522192640.0000 - val_rmse: 22851.5352\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534231840.0000 - rmse: 23113.4551 - val_loss: 468339264.0000 - val_rmse: 21641.1465\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498248384.0000 - rmse: 22321.4766 - val_loss: 507386080.0000 - val_rmse: 22525.2324\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533567872.0000 - rmse: 23099.0859 - val_loss: 511075296.0000 - val_rmse: 22606.9746\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463298976.0000 - rmse: 21524.3789 - val_loss: 663374400.0000 - val_rmse: 25756.0547\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470352416.0000 - rmse: 21687.6074 - val_loss: 503238880.0000 - val_rmse: 22432.9863\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504963488.0000 - rmse: 22471.3906 - val_loss: 524508960.0000 - val_rmse: 22902.1602\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477928576.0000 - rmse: 21861.5762 - val_loss: 460076256.0000 - val_rmse: 21449.3887\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439295744.0000 - rmse: 20959.3809 - val_loss: 517853760.0000 - val_rmse: 22756.4004\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480109664.0000 - rmse: 21911.4043 - val_loss: 545169984.0000 - val_rmse: 23348.8750\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476968928.0000 - rmse: 21839.6191 - val_loss: 468918848.0000 - val_rmse: 21654.5332\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464867136.0000 - rmse: 21560.7773 - val_loss: 482649312.0000 - val_rmse: 21969.2812\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452133216.0000 - rmse: 21263.4219 - val_loss: 504264256.0000 - val_rmse: 22455.8281\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422924064.0000 - rmse: 20565.1152 - val_loss: 462376704.0000 - val_rmse: 21502.9434\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368654080.0000 - rmse: 19200.3672 - val_loss: 671389312.0000 - val_rmse: 25911.1797\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450854208.0000 - rmse: 21233.3262 - val_loss: 471823616.0000 - val_rmse: 21721.5000\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409797472.0000 - rmse: 20243.4531 - val_loss: 499028864.0000 - val_rmse: 22338.9531\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404628256.0000 - rmse: 20115.3711 - val_loss: 581750272.0000 - val_rmse: 24119.4980\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421178528.0000 - rmse: 20522.6348 - val_loss: 478772352.0000 - val_rmse: 21880.8652\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448724992.0000 - rmse: 21183.1270 - val_loss: 544592192.0000 - val_rmse: 23336.5000\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411278944.0000 - rmse: 20280.0117 - val_loss: 598494784.0000 - val_rmse: 24464.1523\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394032224.0000 - rmse: 19850.2441 - val_loss: 442578560.0000 - val_rmse: 21037.5488\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397734336.0000 - rmse: 19943.2773 - val_loss: 468032992.0000 - val_rmse: 21634.0703\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416070368.0000 - rmse: 20397.8027 - val_loss: 502352608.0000 - val_rmse: 22413.2227\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410106016.0000 - rmse: 20251.0723 - val_loss: 519251072.0000 - val_rmse: 22787.0801\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402577408.0000 - rmse: 20064.3301 - val_loss: 572326016.0000 - val_rmse: 23923.3359\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390300544.0000 - rmse: 19756.0234 - val_loss: 482310144.0000 - val_rmse: 21961.5605\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356300032.0000 - rmse: 18875.9121 - val_loss: 505315744.0000 - val_rmse: 22479.2266\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373695424.0000 - rmse: 19331.2031 - val_loss: 539120192.0000 - val_rmse: 23218.9609\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400987808.0000 - rmse: 20024.6777 - val_loss: 671521792.0000 - val_rmse: 25913.7363\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411563072.0000 - rmse: 20287.0156 - val_loss: 518524000.0000 - val_rmse: 22771.1191\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363233216.0000 - rmse: 19058.6758 - val_loss: 572899648.0000 - val_rmse: 23935.3203\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414612288.0000 - rmse: 20362.0293 - val_loss: 523382592.0000 - val_rmse: 22877.5566\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341448288.0000 - rmse: 18478.3184 - val_loss: 607140800.0000 - val_rmse: 24640.2266\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341074048.0000 - rmse: 18468.1895 - val_loss: 475356384.0000 - val_rmse: 21802.6680\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342668448.0000 - rmse: 18511.3027 - val_loss: 571966912.0000 - val_rmse: 23915.8281\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332642176.0000 - rmse: 18238.4805 - val_loss: 462652064.0000 - val_rmse: 21509.3477\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378201120.0000 - rmse: 19447.3926 - val_loss: 493576832.0000 - val_rmse: 22216.5859\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338945568.0000 - rmse: 18410.4727 - val_loss: 539672448.0000 - val_rmse: 23230.8496\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385405216.0000 - rmse: 19631.7383 - val_loss: 512789888.0000 - val_rmse: 22644.8633\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317233024.0000 - rmse: 17811.0352 - val_loss: 561580160.0000 - val_rmse: 23697.6797\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341761696.0000 - rmse: 18486.7969 - val_loss: 517640512.0000 - val_rmse: 22751.7148\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259751200.0000 - rmse: 16116.7969 - val_loss: 497187840.0000 - val_rmse: 22297.7070\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351569344.0000 - rmse: 18750.1797 - val_loss: 596214016.0000 - val_rmse: 24417.4922\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318231744.0000 - rmse: 17839.0469 - val_loss: 1140867456.0000 - val_rmse: 33776.7305\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343141024.0000 - rmse: 18524.0645 - val_loss: 538278528.0000 - val_rmse: 23200.8281\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337984224.0000 - rmse: 18384.3438 - val_loss: 671096768.0000 - val_rmse: 25905.5352\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295199808.0000 - rmse: 17181.3750 - val_loss: 505756800.0000 - val_rmse: 22489.0352\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334576640.0000 - rmse: 18291.4355 - val_loss: 483433920.0000 - val_rmse: 21987.1289\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365380768.0000 - rmse: 19114.9336 - val_loss: 564277568.0000 - val_rmse: 23754.5254\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347863648.0000 - rmse: 18651.1016 - val_loss: 515128672.0000 - val_rmse: 22696.4453\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369242400.0000 - rmse: 19215.6797 - val_loss: 469191552.0000 - val_rmse: 21660.8281\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330121088.0000 - rmse: 18169.2344 - val_loss: 663418432.0000 - val_rmse: 25756.9102\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335033184.0000 - rmse: 18303.9082 - val_loss: 596809152.0000 - val_rmse: 24429.6758\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297234528.0000 - rmse: 17240.4883 - val_loss: 444681344.0000 - val_rmse: 21087.4668\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291806368.0000 - rmse: 17082.3398 - val_loss: 457635680.0000 - val_rmse: 21392.4180\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291271648.0000 - rmse: 17066.6816 - val_loss: 585693376.0000 - val_rmse: 24201.1016\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359854432.0000 - rmse: 18969.8262 - val_loss: 514209440.0000 - val_rmse: 22676.1855\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302875840.0000 - rmse: 17403.3262 - val_loss: 695659648.0000 - val_rmse: 26375.3613\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301401504.0000 - rmse: 17360.9160 - val_loss: 503944480.0000 - val_rmse: 22448.7031\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350656256.0000 - rmse: 18725.8145 - val_loss: 486365152.0000 - val_rmse: 22053.6855\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283604128.0000 - rmse: 16840.5488 - val_loss: 433477600.0000 - val_rmse: 20820.1230\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330431552.0000 - rmse: 18177.7734 - val_loss: 502217248.0000 - val_rmse: 22410.2012\n",
      "104/104 [==============================] - 0s 654us/step - loss: 916921344.0000 - rmse: 30280.7051\n",
      "[916921344.0, 30280.705078125]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,217\n",
      "Trainable params: 20,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,217\n",
      "Trainable params: 20,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 9732338688.0000 - rmse: 98652.6172 - val_loss: 1681264896.0000 - val_rmse: 41003.2305\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1734429440.0000 - rmse: 41646.4805 - val_loss: 1300943232.0000 - val_rmse: 36068.5898\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1430238464.0000 - rmse: 37818.4922 - val_loss: 1161962112.0000 - val_rmse: 34087.5625\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1330967424.0000 - rmse: 36482.4258 - val_loss: 1048896896.0000 - val_rmse: 32386.6777\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1210680192.0000 - rmse: 34794.8281 - val_loss: 1078935424.0000 - val_rmse: 32847.1523\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1181758720.0000 - rmse: 34376.7188 - val_loss: 957902528.0000 - val_rmse: 30950.0000\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1116816768.0000 - rmse: 33418.8086 - val_loss: 972069824.0000 - val_rmse: 31178.0352\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1100081024.0000 - rmse: 33167.4688 - val_loss: 943047488.0000 - val_rmse: 30709.0781\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1081187200.0000 - rmse: 32881.4102 - val_loss: 919605312.0000 - val_rmse: 30324.9941\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1005666688.0000 - rmse: 31712.2480 - val_loss: 943091008.0000 - val_rmse: 30709.7871\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1018891200.0000 - rmse: 31920.0742 - val_loss: 959053696.0000 - val_rmse: 30968.5918\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1011318336.0000 - rmse: 31801.2324 - val_loss: 955930944.0000 - val_rmse: 30918.1328\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1004960128.0000 - rmse: 31701.1055 - val_loss: 956550144.0000 - val_rmse: 30928.1445\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 947490944.0000 - rmse: 30781.3418 - val_loss: 955471936.0000 - val_rmse: 30910.7070\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 963149056.0000 - rmse: 31034.6426 - val_loss: 1070544960.0000 - val_rmse: 32719.1836\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 960002944.0000 - rmse: 30983.9141 - val_loss: 899365824.0000 - val_rmse: 29989.4277\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 925681856.0000 - rmse: 30425.0195 - val_loss: 1134270080.0000 - val_rmse: 33678.9258\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 942849472.0000 - rmse: 30705.8535 - val_loss: 891701824.0000 - val_rmse: 29861.3770\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 930849536.0000 - rmse: 30509.8262 - val_loss: 901705344.0000 - val_rmse: 30028.4082\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 834975872.0000 - rmse: 28895.9492 - val_loss: 1152255872.0000 - val_rmse: 33944.8945\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 871109440.0000 - rmse: 29514.5625 - val_loss: 933474432.0000 - val_rmse: 30552.8145\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 932641664.0000 - rmse: 30539.1816 - val_loss: 893154816.0000 - val_rmse: 29885.6953\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 845052480.0000 - rmse: 29069.7871 - val_loss: 1122721792.0000 - val_rmse: 33507.0391\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 842752960.0000 - rmse: 29030.2051 - val_loss: 883280320.0000 - val_rmse: 29720.0312\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 846974208.0000 - rmse: 29102.8203 - val_loss: 1131564544.0000 - val_rmse: 33638.7344\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 829115648.0000 - rmse: 28794.3652 - val_loss: 1405510528.0000 - val_rmse: 37490.1406\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 784957120.0000 - rmse: 28017.0859 - val_loss: 969277376.0000 - val_rmse: 31133.2188\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 781414976.0000 - rmse: 27953.8008 - val_loss: 864942912.0000 - val_rmse: 29409.9121\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 799251520.0000 - rmse: 28271.0352 - val_loss: 912944000.0000 - val_rmse: 30214.9629\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 814530816.0000 - rmse: 28539.9863 - val_loss: 909005120.0000 - val_rmse: 30149.7090\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 726710464.0000 - rmse: 26957.5684 - val_loss: 972896576.0000 - val_rmse: 31191.2910\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 723741504.0000 - rmse: 26902.4434 - val_loss: 925968832.0000 - val_rmse: 30429.7363\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 745259968.0000 - rmse: 27299.4492 - val_loss: 956867072.0000 - val_rmse: 30933.2676\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698958656.0000 - rmse: 26437.8242 - val_loss: 883799296.0000 - val_rmse: 29728.7617\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 736443712.0000 - rmse: 27137.4961 - val_loss: 1062708480.0000 - val_rmse: 32599.2090\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 707670848.0000 - rmse: 26602.0840 - val_loss: 990091008.0000 - val_rmse: 31465.7109\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651299712.0000 - rmse: 25520.5742 - val_loss: 1336252416.0000 - val_rmse: 36554.7852\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 700903808.0000 - rmse: 26474.5859 - val_loss: 1168191488.0000 - val_rmse: 34178.8164\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 703752704.0000 - rmse: 26528.3379 - val_loss: 1128763392.0000 - val_rmse: 33597.0742\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 747830272.0000 - rmse: 27346.4844 - val_loss: 957534912.0000 - val_rmse: 30944.0605\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733508224.0000 - rmse: 27083.3555 - val_loss: 897727552.0000 - val_rmse: 29962.1016\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 671505984.0000 - rmse: 25913.4316 - val_loss: 906760448.0000 - val_rmse: 30112.4629\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577734528.0000 - rmse: 24036.1094 - val_loss: 769832064.0000 - val_rmse: 27745.8457\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648554048.0000 - rmse: 25466.7227 - val_loss: 759273408.0000 - val_rmse: 27554.9160\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580461248.0000 - rmse: 24092.7637 - val_loss: 811770048.0000 - val_rmse: 28491.5762\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610858176.0000 - rmse: 24715.5449 - val_loss: 1208306688.0000 - val_rmse: 34760.7070\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 715529536.0000 - rmse: 26749.3828 - val_loss: 1423439744.0000 - val_rmse: 37728.5000\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628260992.0000 - rmse: 25065.1328 - val_loss: 1232741888.0000 - val_rmse: 35110.4219\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637108544.0000 - rmse: 25241.0059 - val_loss: 791272512.0000 - val_rmse: 28129.5664\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560144832.0000 - rmse: 23667.3770 - val_loss: 885207488.0000 - val_rmse: 29752.4375\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514721408.0000 - rmse: 22687.4727 - val_loss: 770025856.0000 - val_rmse: 27749.3379\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563220672.0000 - rmse: 23732.2695 - val_loss: 1272196352.0000 - val_rmse: 35667.8633\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589962048.0000 - rmse: 24289.1328 - val_loss: 1140106752.0000 - val_rmse: 33765.4688\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571983488.0000 - rmse: 23916.1758 - val_loss: 825210752.0000 - val_rmse: 28726.4805\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538200320.0000 - rmse: 23199.1426 - val_loss: 934701824.0000 - val_rmse: 30572.8945\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503526016.0000 - rmse: 22439.3828 - val_loss: 790780736.0000 - val_rmse: 28120.8223\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567082496.0000 - rmse: 23813.4941 - val_loss: 753739712.0000 - val_rmse: 27454.3203\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511928064.0000 - rmse: 22625.8281 - val_loss: 666808064.0000 - val_rmse: 25822.6250\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502628320.0000 - rmse: 22419.3730 - val_loss: 1055810752.0000 - val_rmse: 32493.2402\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531141216.0000 - rmse: 23046.5000 - val_loss: 843643008.0000 - val_rmse: 29045.5312\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444168640.0000 - rmse: 21075.3066 - val_loss: 1848084736.0000 - val_rmse: 42989.3555\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477268512.0000 - rmse: 21846.4746 - val_loss: 804131584.0000 - val_rmse: 28357.2129\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474071264.0000 - rmse: 21773.1777 - val_loss: 1297505792.0000 - val_rmse: 36020.9023\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472509120.0000 - rmse: 21737.2734 - val_loss: 700522240.0000 - val_rmse: 26467.3789\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479363872.0000 - rmse: 21894.3770 - val_loss: 1006153408.0000 - val_rmse: 31719.9199\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472619200.0000 - rmse: 21739.8066 - val_loss: 906170816.0000 - val_rmse: 30102.6699\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492827328.0000 - rmse: 22199.7148 - val_loss: 600879936.0000 - val_rmse: 24512.8516\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485364800.0000 - rmse: 22030.9941 - val_loss: 1058785408.0000 - val_rmse: 32538.9824\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524717120.0000 - rmse: 22906.7031 - val_loss: 490273472.0000 - val_rmse: 22142.1191\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457298400.0000 - rmse: 21384.5352 - val_loss: 709139200.0000 - val_rmse: 26629.6660\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484895776.0000 - rmse: 22020.3477 - val_loss: 762249920.0000 - val_rmse: 27608.8711\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474896928.0000 - rmse: 21792.1270 - val_loss: 1531575552.0000 - val_rmse: 39135.3477\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465371072.0000 - rmse: 21572.4609 - val_loss: 719471232.0000 - val_rmse: 26822.9590\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421547648.0000 - rmse: 20531.6250 - val_loss: 644642880.0000 - val_rmse: 25389.8184\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438307776.0000 - rmse: 20935.7988 - val_loss: 768116800.0000 - val_rmse: 27714.9199\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416593056.0000 - rmse: 20410.6094 - val_loss: 795324288.0000 - val_rmse: 28201.4922\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419795264.0000 - rmse: 20488.9043 - val_loss: 695644672.0000 - val_rmse: 26375.0742\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415083104.0000 - rmse: 20373.5859 - val_loss: 921518592.0000 - val_rmse: 30356.5254\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395885632.0000 - rmse: 19896.8730 - val_loss: 1126205184.0000 - val_rmse: 33558.9805\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405882848.0000 - rmse: 20146.5332 - val_loss: 672596672.0000 - val_rmse: 25934.4668\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417101120.0000 - rmse: 20423.0527 - val_loss: 819292672.0000 - val_rmse: 28623.2891\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379219424.0000 - rmse: 19473.5547 - val_loss: 646536064.0000 - val_rmse: 25427.0703\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437613440.0000 - rmse: 20919.2090 - val_loss: 1036674880.0000 - val_rmse: 32197.4336\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389109120.0000 - rmse: 19725.8457 - val_loss: 804424640.0000 - val_rmse: 28362.3789\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427872288.0000 - rmse: 20685.0723 - val_loss: 576456192.0000 - val_rmse: 24009.5000\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376719840.0000 - rmse: 19409.2695 - val_loss: 555929536.0000 - val_rmse: 23578.1562\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424728992.0000 - rmse: 20608.9531 - val_loss: 840798976.0000 - val_rmse: 28996.5332\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401794304.0000 - rmse: 20044.8047 - val_loss: 666157504.0000 - val_rmse: 25810.0254\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443832064.0000 - rmse: 21067.3223 - val_loss: 999461248.0000 - val_rmse: 31614.2559\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386357824.0000 - rmse: 19655.9844 - val_loss: 745362688.0000 - val_rmse: 27301.3320\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450099072.0000 - rmse: 21215.5371 - val_loss: 795382720.0000 - val_rmse: 28202.5273\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441186336.0000 - rmse: 21004.4336 - val_loss: 528053088.0000 - val_rmse: 22979.4043\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367576800.0000 - rmse: 19172.2891 - val_loss: 482033888.0000 - val_rmse: 21955.2695\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366447552.0000 - rmse: 19142.8184 - val_loss: 819929792.0000 - val_rmse: 28634.4160\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366082368.0000 - rmse: 19133.2754 - val_loss: 1550256128.0000 - val_rmse: 39373.2891\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354257696.0000 - rmse: 18821.7324 - val_loss: 571230656.0000 - val_rmse: 23900.4297\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373309440.0000 - rmse: 19321.2148 - val_loss: 846988032.0000 - val_rmse: 29103.0586\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363506016.0000 - rmse: 19065.8320 - val_loss: 470436224.0000 - val_rmse: 21689.5391\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405421536.0000 - rmse: 20135.0801 - val_loss: 997821376.0000 - val_rmse: 31588.3105\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403873120.0000 - rmse: 20096.5918 - val_loss: 641112832.0000 - val_rmse: 25320.2031\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346518848.0000 - rmse: 18615.0137 - val_loss: 886689280.0000 - val_rmse: 29777.3281\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358196640.0000 - rmse: 18926.0820 - val_loss: 510524448.0000 - val_rmse: 22594.7871\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356919808.0000 - rmse: 18892.3203 - val_loss: 524730400.0000 - val_rmse: 22906.9922\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332352800.0000 - rmse: 18230.5449 - val_loss: 865396288.0000 - val_rmse: 29417.6172\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388718080.0000 - rmse: 19715.9355 - val_loss: 942180864.0000 - val_rmse: 30694.9648\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417642112.0000 - rmse: 20436.2910 - val_loss: 1007254400.0000 - val_rmse: 31737.2715\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365903136.0000 - rmse: 19128.5938 - val_loss: 1000565824.0000 - val_rmse: 31631.7188\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384097600.0000 - rmse: 19598.4043 - val_loss: 1014360256.0000 - val_rmse: 31849.0234\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419073696.0000 - rmse: 20471.2871 - val_loss: 813505472.0000 - val_rmse: 28522.0156\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374224160.0000 - rmse: 19344.8711 - val_loss: 459201792.0000 - val_rmse: 21428.9922\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409959968.0000 - rmse: 20247.4648 - val_loss: 716752640.0000 - val_rmse: 26772.2363\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393215424.0000 - rmse: 19829.6562 - val_loss: 1024222208.0000 - val_rmse: 32003.4727\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356004160.0000 - rmse: 18868.0684 - val_loss: 689132160.0000 - val_rmse: 26251.3242\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354380864.0000 - rmse: 18825.0020 - val_loss: 1023743872.0000 - val_rmse: 31995.9980\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340254784.0000 - rmse: 18445.9922 - val_loss: 780393408.0000 - val_rmse: 27935.5195\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355785088.0000 - rmse: 18862.2637 - val_loss: 1011790720.0000 - val_rmse: 31808.6562\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408011072.0000 - rmse: 20199.2812 - val_loss: 1286229888.0000 - val_rmse: 35864.0469\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400196544.0000 - rmse: 20004.9121 - val_loss: 1103880704.0000 - val_rmse: 33224.6992\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356365216.0000 - rmse: 18877.6348 - val_loss: 861620608.0000 - val_rmse: 29353.3730\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382942080.0000 - rmse: 19568.9043 - val_loss: 893312576.0000 - val_rmse: 29888.3340\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355903648.0000 - rmse: 18865.4062 - val_loss: 793445248.0000 - val_rmse: 28168.1602\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339501312.0000 - rmse: 18425.5586 - val_loss: 1157718656.0000 - val_rmse: 34025.2617\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371365600.0000 - rmse: 19270.8457 - val_loss: 1057640128.0000 - val_rmse: 32521.3789\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316852768.0000 - rmse: 17800.3574 - val_loss: 805454976.0000 - val_rmse: 28380.5371\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358694016.0000 - rmse: 18939.2188 - val_loss: 890500800.0000 - val_rmse: 29841.2598\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313029920.0000 - rmse: 17692.6504 - val_loss: 1392542464.0000 - val_rmse: 37316.7852\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366648544.0000 - rmse: 19148.0664 - val_loss: 993373312.0000 - val_rmse: 31517.8262\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299481088.0000 - rmse: 17305.5176 - val_loss: 649610688.0000 - val_rmse: 25487.4590\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300801472.0000 - rmse: 17343.6270 - val_loss: 719515456.0000 - val_rmse: 26823.7832\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269139232.0000 - rmse: 16405.4609 - val_loss: 1097223424.0000 - val_rmse: 33124.3633\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343063552.0000 - rmse: 18521.9727 - val_loss: 859647296.0000 - val_rmse: 29319.7402\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333826400.0000 - rmse: 18270.9141 - val_loss: 670988032.0000 - val_rmse: 25903.4336\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306373184.0000 - rmse: 17503.5156 - val_loss: 587748224.0000 - val_rmse: 24243.5176\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310904512.0000 - rmse: 17632.4824 - val_loss: 1351378944.0000 - val_rmse: 36761.1055\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330826848.0000 - rmse: 18188.6426 - val_loss: 918737088.0000 - val_rmse: 30310.6758\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311475360.0000 - rmse: 17648.6641 - val_loss: 970404864.0000 - val_rmse: 31151.3203\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295322304.0000 - rmse: 17184.9414 - val_loss: 619934592.0000 - val_rmse: 24898.4824\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303500832.0000 - rmse: 17421.2734 - val_loss: 590424896.0000 - val_rmse: 24298.6582\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316700192.0000 - rmse: 17796.0684 - val_loss: 419371744.0000 - val_rmse: 20478.5664\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324164864.0000 - rmse: 18004.5781 - val_loss: 500470112.0000 - val_rmse: 22371.1855\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311173408.0000 - rmse: 17640.1016 - val_loss: 477061600.0000 - val_rmse: 21841.7383\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276587776.0000 - rmse: 16630.9258 - val_loss: 627901312.0000 - val_rmse: 25057.9570\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304286656.0000 - rmse: 17443.8105 - val_loss: 800166784.0000 - val_rmse: 28287.2188\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345696160.0000 - rmse: 18592.9062 - val_loss: 845152832.0000 - val_rmse: 29071.5117\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346211360.0000 - rmse: 18606.7500 - val_loss: 507014240.0000 - val_rmse: 22516.9727\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261514672.0000 - rmse: 16171.4111 - val_loss: 869552896.0000 - val_rmse: 29488.1797\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287504096.0000 - rmse: 16955.9434 - val_loss: 589114304.0000 - val_rmse: 24271.6758\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348416768.0000 - rmse: 18665.9199 - val_loss: 982111360.0000 - val_rmse: 31338.6543\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329690432.0000 - rmse: 18157.3770 - val_loss: 1166420224.0000 - val_rmse: 34152.8945\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325343008.0000 - rmse: 18037.2637 - val_loss: 1357118848.0000 - val_rmse: 36839.0938\n",
      "104/104 [==============================] - 0s 687us/step - loss: 989765568.0000 - rmse: 31460.5352\n",
      "[989765568.0, 31460.53515625]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,217\n",
      "Trainable params: 20,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,217\n",
      "Trainable params: 20,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 7871568384.0000 - rmse: 88721.8594 - val_loss: 1513744896.0000 - val_rmse: 38906.8750\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1915042688.0000 - rmse: 43761.1992 - val_loss: 1173304064.0000 - val_rmse: 34253.5273\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1640457600.0000 - rmse: 40502.5625 - val_loss: 1167980160.0000 - val_rmse: 34175.7266\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1530640768.0000 - rmse: 39123.4062 - val_loss: 943430656.0000 - val_rmse: 30715.3164\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1484205312.0000 - rmse: 38525.3867 - val_loss: 924487872.0000 - val_rmse: 30405.3926\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1381500800.0000 - rmse: 37168.5469 - val_loss: 970376704.0000 - val_rmse: 31150.8691\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1331206784.0000 - rmse: 36485.7070 - val_loss: 1102857088.0000 - val_rmse: 33209.2930\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1297042432.0000 - rmse: 36014.4727 - val_loss: 858396800.0000 - val_rmse: 29298.4102\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1233661824.0000 - rmse: 35123.5234 - val_loss: 999360192.0000 - val_rmse: 31612.6582\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1183949824.0000 - rmse: 34408.5703 - val_loss: 884864960.0000 - val_rmse: 29746.6797\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1204396672.0000 - rmse: 34704.4180 - val_loss: 1475259008.0000 - val_rmse: 38409.1016\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1110953088.0000 - rmse: 33330.9648 - val_loss: 839353088.0000 - val_rmse: 28971.5918\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1132260992.0000 - rmse: 33649.0859 - val_loss: 853253248.0000 - val_rmse: 29210.4980\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1140494464.0000 - rmse: 33771.2070 - val_loss: 817914624.0000 - val_rmse: 28599.2070\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1089098240.0000 - rmse: 33001.4883 - val_loss: 895929088.0000 - val_rmse: 29932.0742\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1079536896.0000 - rmse: 32856.3047 - val_loss: 1143355264.0000 - val_rmse: 33813.5352\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1065369792.0000 - rmse: 32640.0039 - val_loss: 948423872.0000 - val_rmse: 30796.4883\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1014553472.0000 - rmse: 31852.0547 - val_loss: 913940032.0000 - val_rmse: 30231.4414\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1021146112.0000 - rmse: 31955.3770 - val_loss: 858755136.0000 - val_rmse: 29304.5234\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 909477184.0000 - rmse: 30157.5391 - val_loss: 922774912.0000 - val_rmse: 30377.2109\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 893816192.0000 - rmse: 29896.7578 - val_loss: 855140416.0000 - val_rmse: 29242.7812\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 908889472.0000 - rmse: 30147.7930 - val_loss: 961364672.0000 - val_rmse: 31005.8809\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 948341056.0000 - rmse: 30795.1465 - val_loss: 802746240.0000 - val_rmse: 28332.7754\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 887739520.0000 - rmse: 29794.9570 - val_loss: 781329600.0000 - val_rmse: 27952.2734\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 827932928.0000 - rmse: 28773.8223 - val_loss: 920742016.0000 - val_rmse: 30343.7305\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 865516352.0000 - rmse: 29419.6602 - val_loss: 1111193984.0000 - val_rmse: 33334.5781\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 878944320.0000 - rmse: 29646.9941 - val_loss: 995065664.0000 - val_rmse: 31544.6602\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 815750528.0000 - rmse: 28561.3457 - val_loss: 985160256.0000 - val_rmse: 31387.2617\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 759072896.0000 - rmse: 27551.2773 - val_loss: 1002952704.0000 - val_rmse: 31669.4277\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 717255808.0000 - rmse: 26781.6328 - val_loss: 896904128.0000 - val_rmse: 29948.3574\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 732164928.0000 - rmse: 27058.5469 - val_loss: 885331008.0000 - val_rmse: 29754.5117\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 740396736.0000 - rmse: 27210.2324 - val_loss: 871871808.0000 - val_rmse: 29527.4746\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 702258560.0000 - rmse: 26500.1602 - val_loss: 879540928.0000 - val_rmse: 29657.0547\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 738466624.0000 - rmse: 27174.7422 - val_loss: 985115648.0000 - val_rmse: 31386.5527\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 755655872.0000 - rmse: 27489.1953 - val_loss: 1211091712.0000 - val_rmse: 34800.7422\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 743193984.0000 - rmse: 27261.5840 - val_loss: 1218712832.0000 - val_rmse: 34910.0664\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647235840.0000 - rmse: 25440.8281 - val_loss: 1046626240.0000 - val_rmse: 32351.6016\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698541696.0000 - rmse: 26429.9375 - val_loss: 1225078656.0000 - val_rmse: 35001.1250\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742740864.0000 - rmse: 27253.2734 - val_loss: 826772288.0000 - val_rmse: 28753.6484\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 804222016.0000 - rmse: 28358.8086 - val_loss: 689706880.0000 - val_rmse: 26262.2715\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648017728.0000 - rmse: 25456.1914 - val_loss: 1107464448.0000 - val_rmse: 33278.5898\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609291264.0000 - rmse: 24683.8242 - val_loss: 610448384.0000 - val_rmse: 24707.2520\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591160768.0000 - rmse: 24313.7969 - val_loss: 884815360.0000 - val_rmse: 29745.8457\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684773888.0000 - rmse: 26168.1855 - val_loss: 922214016.0000 - val_rmse: 30367.9746\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 719969856.0000 - rmse: 26832.2539 - val_loss: 809401728.0000 - val_rmse: 28449.9844\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 689107136.0000 - rmse: 26250.8496 - val_loss: 790917760.0000 - val_rmse: 28123.2598\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601555840.0000 - rmse: 24526.6348 - val_loss: 938352128.0000 - val_rmse: 30632.5312\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626575296.0000 - rmse: 25031.4863 - val_loss: 794100992.0000 - val_rmse: 28179.7969\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570391552.0000 - rmse: 23882.8711 - val_loss: 653212672.0000 - val_rmse: 25558.0254\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589367040.0000 - rmse: 24276.8828 - val_loss: 815997632.0000 - val_rmse: 28565.6719\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552814016.0000 - rmse: 23511.9980 - val_loss: 2060016768.0000 - val_rmse: 45387.4062\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 653228928.0000 - rmse: 25558.3438 - val_loss: 867841856.0000 - val_rmse: 29459.1543\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606122880.0000 - rmse: 24619.5625 - val_loss: 1026374016.0000 - val_rmse: 32037.0723\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565906176.0000 - rmse: 23788.7832 - val_loss: 1418353408.0000 - val_rmse: 37661.0312\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540472384.0000 - rmse: 23248.0625 - val_loss: 884315968.0000 - val_rmse: 29737.4512\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685945984.0000 - rmse: 26190.5703 - val_loss: 963651008.0000 - val_rmse: 31042.7285\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546725440.0000 - rmse: 23382.1602 - val_loss: 1152418048.0000 - val_rmse: 33947.2852\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563064640.0000 - rmse: 23728.9785 - val_loss: 1132558080.0000 - val_rmse: 33653.5000\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550813824.0000 - rmse: 23469.4219 - val_loss: 1514469376.0000 - val_rmse: 38916.1836\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615202560.0000 - rmse: 24803.2773 - val_loss: 811159104.0000 - val_rmse: 28480.8535\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523733536.0000 - rmse: 22885.2246 - val_loss: 933863936.0000 - val_rmse: 30559.1875\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477133824.0000 - rmse: 21843.3906 - val_loss: 803970240.0000 - val_rmse: 28354.3672\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530723904.0000 - rmse: 23037.4453 - val_loss: 1041442432.0000 - val_rmse: 32271.3867\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471219360.0000 - rmse: 21707.5840 - val_loss: 1089411456.0000 - val_rmse: 33006.2344\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491025952.0000 - rmse: 22159.1035 - val_loss: 870806208.0000 - val_rmse: 29509.4258\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454832288.0000 - rmse: 21326.7969 - val_loss: 1110281728.0000 - val_rmse: 33320.8867\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501010624.0000 - rmse: 22383.2676 - val_loss: 676270272.0000 - val_rmse: 26005.1953\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629418880.0000 - rmse: 25088.2188 - val_loss: 1000443008.0000 - val_rmse: 31629.7812\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508420096.0000 - rmse: 22548.1719 - val_loss: 774142848.0000 - val_rmse: 27823.4199\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512290848.0000 - rmse: 22633.8418 - val_loss: 962082816.0000 - val_rmse: 31017.4570\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442395424.0000 - rmse: 21033.1953 - val_loss: 958409728.0000 - val_rmse: 30958.1934\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555761728.0000 - rmse: 23574.5977 - val_loss: 952014016.0000 - val_rmse: 30854.7246\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562299648.0000 - rmse: 23712.8574 - val_loss: 915505088.0000 - val_rmse: 30257.3125\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533521664.0000 - rmse: 23098.0859 - val_loss: 726995584.0000 - val_rmse: 26962.8535\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482000288.0000 - rmse: 21954.5020 - val_loss: 1281634560.0000 - val_rmse: 35799.9258\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550922112.0000 - rmse: 23471.7285 - val_loss: 1052829952.0000 - val_rmse: 32447.3418\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480613120.0000 - rmse: 21922.8887 - val_loss: 1042370176.0000 - val_rmse: 32285.7578\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471981632.0000 - rmse: 21725.1367 - val_loss: 1697025920.0000 - val_rmse: 41194.9727\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540239232.0000 - rmse: 23243.0449 - val_loss: 1011935680.0000 - val_rmse: 31810.9355\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433356864.0000 - rmse: 20817.2227 - val_loss: 1237820544.0000 - val_rmse: 35182.6719\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438542208.0000 - rmse: 20941.3984 - val_loss: 870430720.0000 - val_rmse: 29503.0625\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488761856.0000 - rmse: 22107.9590 - val_loss: 827598336.0000 - val_rmse: 28768.0078\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474114496.0000 - rmse: 21774.1680 - val_loss: 1353792512.0000 - val_rmse: 36793.9180\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453058560.0000 - rmse: 21285.1719 - val_loss: 1095960960.0000 - val_rmse: 33105.3008\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454871488.0000 - rmse: 21327.7148 - val_loss: 1639236480.0000 - val_rmse: 40487.4844\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512655328.0000 - rmse: 22641.8926 - val_loss: 1023653376.0000 - val_rmse: 31994.5840\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510768672.0000 - rmse: 22600.1914 - val_loss: 871283520.0000 - val_rmse: 29517.5117\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485749216.0000 - rmse: 22039.7168 - val_loss: 1155265024.0000 - val_rmse: 33989.1875\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493332000.0000 - rmse: 22211.0762 - val_loss: 953052480.0000 - val_rmse: 30871.5488\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469739936.0000 - rmse: 21673.4824 - val_loss: 1454264448.0000 - val_rmse: 38134.8203\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560056832.0000 - rmse: 23665.5195 - val_loss: 1230143360.0000 - val_rmse: 35073.3984\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442976256.0000 - rmse: 21046.9980 - val_loss: 1035020160.0000 - val_rmse: 32171.7285\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484870112.0000 - rmse: 22019.7656 - val_loss: 1577938816.0000 - val_rmse: 39723.2773\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468990624.0000 - rmse: 21656.1914 - val_loss: 864965696.0000 - val_rmse: 29410.2988\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484321664.0000 - rmse: 22007.3066 - val_loss: 1051037696.0000 - val_rmse: 32419.7109\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487498752.0000 - rmse: 22079.3691 - val_loss: 881707072.0000 - val_rmse: 29693.5488\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507206848.0000 - rmse: 22521.2520 - val_loss: 1183412608.0000 - val_rmse: 34400.7656\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396145120.0000 - rmse: 19903.3926 - val_loss: 1499040384.0000 - val_rmse: 38717.4414\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437278816.0000 - rmse: 20911.2129 - val_loss: 1194830464.0000 - val_rmse: 34566.3164\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459858048.0000 - rmse: 21444.2988 - val_loss: 1492794496.0000 - val_rmse: 38636.6992\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534163776.0000 - rmse: 23111.9805 - val_loss: 1224277632.0000 - val_rmse: 34989.6758\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374752416.0000 - rmse: 19358.5215 - val_loss: 1359805440.0000 - val_rmse: 36875.5391\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427174624.0000 - rmse: 20668.2012 - val_loss: 894259904.0000 - val_rmse: 29904.1758\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356205632.0000 - rmse: 18873.4082 - val_loss: 1199741696.0000 - val_rmse: 34637.2891\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454109760.0000 - rmse: 21309.8496 - val_loss: 1899718528.0000 - val_rmse: 43585.7617\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419576224.0000 - rmse: 20483.5566 - val_loss: 935556480.0000 - val_rmse: 30586.8672\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393070944.0000 - rmse: 19826.0137 - val_loss: 864383232.0000 - val_rmse: 29400.3945\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428229024.0000 - rmse: 20693.6953 - val_loss: 1032350912.0000 - val_rmse: 32130.2129\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396117312.0000 - rmse: 19902.6953 - val_loss: 1357191424.0000 - val_rmse: 36840.0781\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352097952.0000 - rmse: 18764.2695 - val_loss: 886395328.0000 - val_rmse: 29772.3906\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478926816.0000 - rmse: 21884.3926 - val_loss: 1031750784.0000 - val_rmse: 32120.8770\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454757952.0000 - rmse: 21325.0547 - val_loss: 1736123392.0000 - val_rmse: 41666.8125\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399035296.0000 - rmse: 19975.8672 - val_loss: 1137843840.0000 - val_rmse: 33731.9375\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395143168.0000 - rmse: 19878.2070 - val_loss: 1084552960.0000 - val_rmse: 32932.5508\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378133920.0000 - rmse: 19445.6641 - val_loss: 809438784.0000 - val_rmse: 28450.6367\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358581728.0000 - rmse: 18936.2520 - val_loss: 1971199104.0000 - val_rmse: 44398.1836\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405984320.0000 - rmse: 20149.0527 - val_loss: 839250496.0000 - val_rmse: 28969.8203\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446273696.0000 - rmse: 21125.1895 - val_loss: 1457538432.0000 - val_rmse: 38177.7188\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389918496.0000 - rmse: 19746.3516 - val_loss: 1661311872.0000 - val_rmse: 40759.1953\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372305632.0000 - rmse: 19295.2207 - val_loss: 2433528576.0000 - val_rmse: 49330.8086\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387220768.0000 - rmse: 19677.9238 - val_loss: 1010916544.0000 - val_rmse: 31794.9141\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376996800.0000 - rmse: 19416.4043 - val_loss: 1583630336.0000 - val_rmse: 39794.8516\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410556192.0000 - rmse: 20262.1855 - val_loss: 1103883008.0000 - val_rmse: 33224.7344\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408070560.0000 - rmse: 20200.7539 - val_loss: 1562867328.0000 - val_rmse: 39533.1172\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394393696.0000 - rmse: 19859.3457 - val_loss: 1320413056.0000 - val_rmse: 36337.4883\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391791904.0000 - rmse: 19793.7324 - val_loss: 1783444480.0000 - val_rmse: 42230.8477\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361266464.0000 - rmse: 19007.0078 - val_loss: 1726229632.0000 - val_rmse: 41547.9180\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364153120.0000 - rmse: 19082.7949 - val_loss: 1261512704.0000 - val_rmse: 35517.7773\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397496608.0000 - rmse: 19937.3145 - val_loss: 1154730240.0000 - val_rmse: 33981.3203\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382483648.0000 - rmse: 19557.1875 - val_loss: 1654170368.0000 - val_rmse: 40671.4922\n",
      "104/104 [==============================] - 0s 643us/step - loss: 492365408.0000 - rmse: 22189.3047\n",
      "[492365408.0, 22189.3046875]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,217\n",
      "Trainable params: 20,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,217\n",
      "Trainable params: 20,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 7239025152.0000 - rmse: 85082.4609 - val_loss: 1348151424.0000 - val_rmse: 36717.1797\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1828760448.0000 - rmse: 42764.0078 - val_loss: 1627940736.0000 - val_rmse: 40347.7461\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1603857536.0000 - rmse: 40048.1914 - val_loss: 1101737472.0000 - val_rmse: 33192.4297\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1441357184.0000 - rmse: 37965.2109 - val_loss: 935381120.0000 - val_rmse: 30584.0000\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1379435520.0000 - rmse: 37140.7539 - val_loss: 907369728.0000 - val_rmse: 30122.5781\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1379047552.0000 - rmse: 37135.5312 - val_loss: 891402240.0000 - val_rmse: 29856.3594\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1309878784.0000 - rmse: 36192.2461 - val_loss: 1226413568.0000 - val_rmse: 35020.1875\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1268641792.0000 - rmse: 35618.0000 - val_loss: 870631552.0000 - val_rmse: 29506.4668\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1288907648.0000 - rmse: 35901.3594 - val_loss: 984102400.0000 - val_rmse: 31370.4062\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1176964352.0000 - rmse: 34306.9141 - val_loss: 1314958592.0000 - val_rmse: 36262.3594\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1124110208.0000 - rmse: 33527.7539 - val_loss: 904006208.0000 - val_rmse: 30066.6953\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1181573760.0000 - rmse: 34374.0273 - val_loss: 956011008.0000 - val_rmse: 30919.4277\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1110472576.0000 - rmse: 33323.7539 - val_loss: 879026816.0000 - val_rmse: 29648.3867\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1137771648.0000 - rmse: 33730.8711 - val_loss: 847169728.0000 - val_rmse: 29106.1797\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1105547648.0000 - rmse: 33249.7734 - val_loss: 1059759936.0000 - val_rmse: 32553.9551\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1120366720.0000 - rmse: 33471.8789 - val_loss: 853818880.0000 - val_rmse: 29220.1797\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1019180096.0000 - rmse: 31924.5996 - val_loss: 1038580288.0000 - val_rmse: 32227.0117\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1034875584.0000 - rmse: 32169.4824 - val_loss: 966226560.0000 - val_rmse: 31084.1855\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 975592384.0000 - rmse: 31234.4746 - val_loss: 911644224.0000 - val_rmse: 30193.4473\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 943802752.0000 - rmse: 30721.3730 - val_loss: 889431808.0000 - val_rmse: 29823.3438\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 903655040.0000 - rmse: 30060.8555 - val_loss: 977492352.0000 - val_rmse: 31264.8750\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 917907840.0000 - rmse: 30296.9941 - val_loss: 790799424.0000 - val_rmse: 28121.1562\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 871504256.0000 - rmse: 29521.2500 - val_loss: 1252324224.0000 - val_rmse: 35388.1914\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 876317184.0000 - rmse: 29602.6543 - val_loss: 735262336.0000 - val_rmse: 27115.7188\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 848315840.0000 - rmse: 29125.8613 - val_loss: 917062336.0000 - val_rmse: 30283.0371\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 818679744.0000 - rmse: 28612.5781 - val_loss: 864946624.0000 - val_rmse: 29409.9746\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 811906112.0000 - rmse: 28493.9668 - val_loss: 722609984.0000 - val_rmse: 26881.4043\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 797868672.0000 - rmse: 28246.5684 - val_loss: 715606336.0000 - val_rmse: 26750.8203\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 768429504.0000 - rmse: 27720.5605 - val_loss: 1088234368.0000 - val_rmse: 32988.3984\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 740512256.0000 - rmse: 27212.3555 - val_loss: 1001487424.0000 - val_rmse: 31646.2852\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 682242880.0000 - rmse: 26119.7773 - val_loss: 781356672.0000 - val_rmse: 27952.7559\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 731832064.0000 - rmse: 27052.3945 - val_loss: 875480192.0000 - val_rmse: 29588.5137\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 738180288.0000 - rmse: 27169.4727 - val_loss: 655744960.0000 - val_rmse: 25607.5156\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665860736.0000 - rmse: 25804.2773 - val_loss: 717079488.0000 - val_rmse: 26778.3398\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630726912.0000 - rmse: 25114.2754 - val_loss: 1034222656.0000 - val_rmse: 32159.3301\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 669699648.0000 - rmse: 25878.5566 - val_loss: 656405824.0000 - val_rmse: 25620.4180\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666868352.0000 - rmse: 25823.7930 - val_loss: 850528064.0000 - val_rmse: 29163.8145\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571448768.0000 - rmse: 23904.9922 - val_loss: 720386112.0000 - val_rmse: 26840.0078\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574346304.0000 - rmse: 23965.5215 - val_loss: 787863104.0000 - val_rmse: 28068.8984\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568886144.0000 - rmse: 23851.3340 - val_loss: 782021440.0000 - val_rmse: 27964.6465\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531436768.0000 - rmse: 23052.9121 - val_loss: 1728555264.0000 - val_rmse: 41575.8984\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525227936.0000 - rmse: 22917.8496 - val_loss: 955924352.0000 - val_rmse: 30918.0254\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555060352.0000 - rmse: 23559.7168 - val_loss: 739924992.0000 - val_rmse: 27201.5625\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512661920.0000 - rmse: 22642.0391 - val_loss: 894184640.0000 - val_rmse: 29902.9199\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574286144.0000 - rmse: 23964.2656 - val_loss: 686909632.0000 - val_rmse: 26208.9590\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550862272.0000 - rmse: 23470.4551 - val_loss: 991486592.0000 - val_rmse: 31487.8809\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465420544.0000 - rmse: 21573.6055 - val_loss: 1038271936.0000 - val_rmse: 32222.2246\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552534720.0000 - rmse: 23506.0547 - val_loss: 826068416.0000 - val_rmse: 28741.4062\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563957888.0000 - rmse: 23747.7969 - val_loss: 736193280.0000 - val_rmse: 27132.8828\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547405568.0000 - rmse: 23396.6992 - val_loss: 724564224.0000 - val_rmse: 26917.7305\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455498176.0000 - rmse: 21342.4023 - val_loss: 994530112.0000 - val_rmse: 31536.1699\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476856064.0000 - rmse: 21837.0352 - val_loss: 1609554432.0000 - val_rmse: 40119.2539\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512011456.0000 - rmse: 22627.6680 - val_loss: 916802176.0000 - val_rmse: 30278.7402\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527243168.0000 - rmse: 22961.7754 - val_loss: 1043203072.0000 - val_rmse: 32298.6543\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465156096.0000 - rmse: 21567.4766 - val_loss: 1007823808.0000 - val_rmse: 31746.2402\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465560864.0000 - rmse: 21576.8594 - val_loss: 1153746560.0000 - val_rmse: 33966.8438\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447540480.0000 - rmse: 21155.1504 - val_loss: 925283712.0000 - val_rmse: 30418.4766\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466131424.0000 - rmse: 21590.0762 - val_loss: 719421248.0000 - val_rmse: 26822.0293\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543948352.0000 - rmse: 23322.7012 - val_loss: 1003588736.0000 - val_rmse: 31679.4688\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405580608.0000 - rmse: 20139.0312 - val_loss: 1402906240.0000 - val_rmse: 37455.3906\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523188256.0000 - rmse: 22873.3086 - val_loss: 764316032.0000 - val_rmse: 27646.2637\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451387520.0000 - rmse: 21245.8809 - val_loss: 663291776.0000 - val_rmse: 25754.4512\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418831456.0000 - rmse: 20465.3730 - val_loss: 1073881088.0000 - val_rmse: 32770.1250\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450009312.0000 - rmse: 21213.4219 - val_loss: 1245673088.0000 - val_rmse: 35294.0938\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405930848.0000 - rmse: 20147.7227 - val_loss: 970470336.0000 - val_rmse: 31152.3730\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413553952.0000 - rmse: 20336.0234 - val_loss: 664040256.0000 - val_rmse: 25768.9766\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413811648.0000 - rmse: 20342.3594 - val_loss: 912190400.0000 - val_rmse: 30202.4883\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440550624.0000 - rmse: 20989.2969 - val_loss: 825491712.0000 - val_rmse: 28731.3711\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398683872.0000 - rmse: 19967.0703 - val_loss: 776283008.0000 - val_rmse: 27861.8555\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379048224.0000 - rmse: 19469.1602 - val_loss: 711689792.0000 - val_rmse: 26677.5137\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473710912.0000 - rmse: 21764.9004 - val_loss: 892077696.0000 - val_rmse: 29867.6680\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415670496.0000 - rmse: 20387.9980 - val_loss: 799153920.0000 - val_rmse: 28269.3086\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376910944.0000 - rmse: 19414.1914 - val_loss: 828024128.0000 - val_rmse: 28775.4082\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472007296.0000 - rmse: 21725.7285 - val_loss: 958849600.0000 - val_rmse: 30965.2949\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445169344.0000 - rmse: 21099.0371 - val_loss: 1002542848.0000 - val_rmse: 31662.9570\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400410848.0000 - rmse: 20010.2656 - val_loss: 855966208.0000 - val_rmse: 29256.9004\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366008672.0000 - rmse: 19131.3516 - val_loss: 851344064.0000 - val_rmse: 29177.7988\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348619904.0000 - rmse: 18671.3633 - val_loss: 1494235264.0000 - val_rmse: 38655.3398\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360392832.0000 - rmse: 18984.0137 - val_loss: 744672832.0000 - val_rmse: 27288.6934\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382133056.0000 - rmse: 19548.2207 - val_loss: 811634368.0000 - val_rmse: 28489.1973\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336692352.0000 - rmse: 18349.1777 - val_loss: 820794368.0000 - val_rmse: 28649.5098\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370751872.0000 - rmse: 19254.9160 - val_loss: 783436928.0000 - val_rmse: 27989.9434\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397486016.0000 - rmse: 19937.0488 - val_loss: 1005524928.0000 - val_rmse: 31710.0117\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375951552.0000 - rmse: 19389.4688 - val_loss: 808616128.0000 - val_rmse: 28436.1758\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333564224.0000 - rmse: 18263.7402 - val_loss: 1148169216.0000 - val_rmse: 33884.6445\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377854144.0000 - rmse: 19438.4707 - val_loss: 838638464.0000 - val_rmse: 28959.2559\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363323008.0000 - rmse: 19061.0312 - val_loss: 925301952.0000 - val_rmse: 30418.7754\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378441312.0000 - rmse: 19453.5664 - val_loss: 875685376.0000 - val_rmse: 29591.9785\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335570528.0000 - rmse: 18318.5820 - val_loss: 864126272.0000 - val_rmse: 29396.0234\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324943808.0000 - rmse: 18026.1973 - val_loss: 849166144.0000 - val_rmse: 29140.4531\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327817312.0000 - rmse: 18105.7246 - val_loss: 953667648.0000 - val_rmse: 30881.5098\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309915552.0000 - rmse: 17604.4180 - val_loss: 924250944.0000 - val_rmse: 30401.4961\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351954400.0000 - rmse: 18760.4473 - val_loss: 994442240.0000 - val_rmse: 31534.7773\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371549248.0000 - rmse: 19275.6094 - val_loss: 970014720.0000 - val_rmse: 31145.0586\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361087456.0000 - rmse: 19002.3008 - val_loss: 994869952.0000 - val_rmse: 31541.5586\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303781408.0000 - rmse: 17429.3242 - val_loss: 854463232.0000 - val_rmse: 29231.2031\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347700512.0000 - rmse: 18646.7266 - val_loss: 926762944.0000 - val_rmse: 30442.7812\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358159264.0000 - rmse: 18925.0957 - val_loss: 1120614016.0000 - val_rmse: 33475.5742\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311289568.0000 - rmse: 17643.3984 - val_loss: 764536000.0000 - val_rmse: 27650.2422\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373422368.0000 - rmse: 19324.1387 - val_loss: 850186240.0000 - val_rmse: 29157.9512\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289537824.0000 - rmse: 17015.8105 - val_loss: 835375040.0000 - val_rmse: 28902.8496\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378551136.0000 - rmse: 19456.3887 - val_loss: 1165802752.0000 - val_rmse: 34143.8555\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384740992.0000 - rmse: 19614.8145 - val_loss: 837130048.0000 - val_rmse: 28933.1992\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325308544.0000 - rmse: 18036.3105 - val_loss: 931278272.0000 - val_rmse: 30516.8496\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363927680.0000 - rmse: 19076.8867 - val_loss: 894118400.0000 - val_rmse: 29901.8125\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347071040.0000 - rmse: 18629.8418 - val_loss: 1018570752.0000 - val_rmse: 31915.0547\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374119872.0000 - rmse: 19342.1777 - val_loss: 1125643520.0000 - val_rmse: 33550.6094\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343933856.0000 - rmse: 18545.4512 - val_loss: 890328896.0000 - val_rmse: 29838.3789\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332286400.0000 - rmse: 18228.7246 - val_loss: 988474240.0000 - val_rmse: 31440.0098\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330310464.0000 - rmse: 18174.4434 - val_loss: 1183211520.0000 - val_rmse: 34397.8398\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379679168.0000 - rmse: 19485.3555 - val_loss: 938395072.0000 - val_rmse: 30633.2324\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321690624.0000 - rmse: 17935.7344 - val_loss: 1128107776.0000 - val_rmse: 33587.3164\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314323456.0000 - rmse: 17729.1680 - val_loss: 947909568.0000 - val_rmse: 30788.1387\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285360064.0000 - rmse: 16892.6035 - val_loss: 864169408.0000 - val_rmse: 29396.7559\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281827776.0000 - rmse: 16787.7266 - val_loss: 910928384.0000 - val_rmse: 30181.5898\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337842272.0000 - rmse: 18380.4844 - val_loss: 904074048.0000 - val_rmse: 30067.8242\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325147872.0000 - rmse: 18031.8555 - val_loss: 895955584.0000 - val_rmse: 29932.5176\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335361984.0000 - rmse: 18312.8887 - val_loss: 841511936.0000 - val_rmse: 29008.8242\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303464736.0000 - rmse: 17420.2363 - val_loss: 887075264.0000 - val_rmse: 29783.8066\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298478656.0000 - rmse: 17276.5312 - val_loss: 1002114816.0000 - val_rmse: 31656.1973\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317213056.0000 - rmse: 17810.4746 - val_loss: 1021050496.0000 - val_rmse: 31953.8809\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333326208.0000 - rmse: 18257.2227 - val_loss: 759934208.0000 - val_rmse: 27566.9023\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330650624.0000 - rmse: 18183.7988 - val_loss: 936657600.0000 - val_rmse: 30604.8633\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317584224.0000 - rmse: 17820.8906 - val_loss: 950933248.0000 - val_rmse: 30837.2031\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293255648.0000 - rmse: 17124.7051 - val_loss: 939786560.0000 - val_rmse: 30655.9375\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322148960.0000 - rmse: 17948.5059 - val_loss: 913209984.0000 - val_rmse: 30219.3613\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341840832.0000 - rmse: 18488.9355 - val_loss: 1028009216.0000 - val_rmse: 32062.5801\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327918528.0000 - rmse: 18108.5176 - val_loss: 1361632256.0000 - val_rmse: 36900.3008\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338049600.0000 - rmse: 18386.1211 - val_loss: 1380316544.0000 - val_rmse: 37152.6133\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332107968.0000 - rmse: 18223.8262 - val_loss: 1096093824.0000 - val_rmse: 33107.3086\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343430112.0000 - rmse: 18531.8652 - val_loss: 973309248.0000 - val_rmse: 31197.9043\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276762176.0000 - rmse: 16636.1699 - val_loss: 1104552960.0000 - val_rmse: 33234.8164\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285711456.0000 - rmse: 16903.0000 - val_loss: 1013848960.0000 - val_rmse: 31840.9941\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303436512.0000 - rmse: 17419.4277 - val_loss: 1050312576.0000 - val_rmse: 32408.5234\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308751200.0000 - rmse: 17571.3145 - val_loss: 1068148928.0000 - val_rmse: 32682.5469\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374212064.0000 - rmse: 19344.5605 - val_loss: 1203176704.0000 - val_rmse: 34686.8359\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318744928.0000 - rmse: 17853.4238 - val_loss: 1062472896.0000 - val_rmse: 32595.5938\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276762432.0000 - rmse: 16636.1777 - val_loss: 1255011584.0000 - val_rmse: 35426.1406\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312565408.0000 - rmse: 17679.5176 - val_loss: 1255324544.0000 - val_rmse: 35430.5586\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341366912.0000 - rmse: 18476.1133 - val_loss: 954576576.0000 - val_rmse: 30896.2207\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286721824.0000 - rmse: 16932.8594 - val_loss: 1125654912.0000 - val_rmse: 33550.7812\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300714848.0000 - rmse: 17341.1289 - val_loss: 1507320448.0000 - val_rmse: 38824.2266\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282136128.0000 - rmse: 16796.9062 - val_loss: 976964992.0000 - val_rmse: 31256.4375\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318560352.0000 - rmse: 17848.2559 - val_loss: 969742464.0000 - val_rmse: 31140.6855\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276889408.0000 - rmse: 16639.9922 - val_loss: 1047474496.0000 - val_rmse: 32364.7070\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304631072.0000 - rmse: 17453.6797 - val_loss: 909204864.0000 - val_rmse: 30153.0215\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321380992.0000 - rmse: 17927.1016 - val_loss: 829387264.0000 - val_rmse: 28799.0840\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285469760.0000 - rmse: 16895.8457 - val_loss: 970218752.0000 - val_rmse: 31148.3320\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321609792.0000 - rmse: 17933.4824 - val_loss: 935499200.0000 - val_rmse: 30585.9297\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287996320.0000 - rmse: 16970.4512 - val_loss: 946697856.0000 - val_rmse: 30768.4551\n",
      "104/104 [==============================] - 0s 653us/step - loss: 371564160.0000 - rmse: 19275.9980\n",
      "[371564160.0, 19275.998046875]\n",
      "[22670.779296875, 30280.705078125, 31460.53515625, 22189.3046875, 19275.998046875]\n",
      "25175.464453125\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "!python train.py kfold baseline\n",
    "#d 0.25 p 20 epoch 150 layer -1 (16,8)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 19:13:59.996620: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 19:13:59.996668: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 19:13:59.997047: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 19:14:00.210772: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 6480912896.0000 - rmse: 80504.1172 - val_loss: 1414950784.0000 - val_rmse: 37615.8320\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1892872448.0000 - rmse: 43507.1523 - val_loss: 927139648.0000 - val_rmse: 30448.9688\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1634601088.0000 - rmse: 40430.1992 - val_loss: 847219968.0000 - val_rmse: 29107.0430\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1494009472.0000 - rmse: 38652.4180 - val_loss: 906829632.0000 - val_rmse: 30113.6113\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1450097024.0000 - rmse: 38080.1406 - val_loss: 724620352.0000 - val_rmse: 26918.7734\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1373377536.0000 - rmse: 37059.1094 - val_loss: 689944128.0000 - val_rmse: 26266.7871\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1357541376.0000 - rmse: 36844.8281 - val_loss: 702726656.0000 - val_rmse: 26508.9922\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1345867008.0000 - rmse: 36686.0586 - val_loss: 979962880.0000 - val_rmse: 31304.3594\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1327725056.0000 - rmse: 36437.9609 - val_loss: 679922944.0000 - val_rmse: 26075.3320\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1238538368.0000 - rmse: 35192.8750 - val_loss: 709999680.0000 - val_rmse: 26645.8184\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1202578048.0000 - rmse: 34678.2070 - val_loss: 756006208.0000 - val_rmse: 27495.5664\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1211372160.0000 - rmse: 34804.7734 - val_loss: 650399360.0000 - val_rmse: 25502.9277\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1180693888.0000 - rmse: 34361.2266 - val_loss: 747034688.0000 - val_rmse: 27331.9355\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1182661376.0000 - rmse: 34389.8438 - val_loss: 687065088.0000 - val_rmse: 26211.9258\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1087176320.0000 - rmse: 32972.3555 - val_loss: 942186624.0000 - val_rmse: 30695.0586\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1108031872.0000 - rmse: 33287.1133 - val_loss: 760858944.0000 - val_rmse: 27583.6699\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1055838464.0000 - rmse: 32493.6680 - val_loss: 651511488.0000 - val_rmse: 25524.7227\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 998039424.0000 - rmse: 31591.7617 - val_loss: 600696576.0000 - val_rmse: 24509.1113\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 944853504.0000 - rmse: 30738.4668 - val_loss: 602222272.0000 - val_rmse: 24540.2168\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1083909760.0000 - rmse: 32922.7852 - val_loss: 757099200.0000 - val_rmse: 27515.4336\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 891252096.0000 - rmse: 29853.8457 - val_loss: 543730368.0000 - val_rmse: 23318.0273\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 913797632.0000 - rmse: 30229.0859 - val_loss: 974410112.0000 - val_rmse: 31215.5430\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 956508864.0000 - rmse: 30927.4746 - val_loss: 927118144.0000 - val_rmse: 30448.6152\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 874894720.0000 - rmse: 29578.6191 - val_loss: 517876032.0000 - val_rmse: 22756.8887\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 897871872.0000 - rmse: 29964.5059 - val_loss: 601153856.0000 - val_rmse: 24518.4375\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 736485376.0000 - rmse: 27138.2637 - val_loss: 1335239040.0000 - val_rmse: 36540.9219\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 857221568.0000 - rmse: 29278.3457 - val_loss: 492810720.0000 - val_rmse: 22199.3398\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 845151744.0000 - rmse: 29071.4941 - val_loss: 528709696.0000 - val_rmse: 22993.6855\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 890593472.0000 - rmse: 29842.8125 - val_loss: 1078295552.0000 - val_rmse: 32837.4102\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733808256.0000 - rmse: 27088.8945 - val_loss: 564284416.0000 - val_rmse: 23754.6680\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 779954560.0000 - rmse: 27927.6660 - val_loss: 438826784.0000 - val_rmse: 20948.1914\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 744273600.0000 - rmse: 27281.3770 - val_loss: 513748736.0000 - val_rmse: 22666.0254\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 765108224.0000 - rmse: 27660.5879 - val_loss: 393463616.0000 - val_rmse: 19835.9180\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 752343040.0000 - rmse: 27428.8711 - val_loss: 468477632.0000 - val_rmse: 21644.3438\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 745068096.0000 - rmse: 27295.9355 - val_loss: 695494720.0000 - val_rmse: 26372.2344\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 823014144.0000 - rmse: 28688.2227 - val_loss: 483192608.0000 - val_rmse: 21981.6426\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 837426240.0000 - rmse: 28938.3164 - val_loss: 349988160.0000 - val_rmse: 18707.9707\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 736813440.0000 - rmse: 27144.3086 - val_loss: 986127488.0000 - val_rmse: 31402.6660\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 745004480.0000 - rmse: 27294.7695 - val_loss: 421639136.0000 - val_rmse: 20533.8535\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706961280.0000 - rmse: 26588.7441 - val_loss: 1244781952.0000 - val_rmse: 35281.4688\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 701879808.0000 - rmse: 26493.0137 - val_loss: 396734048.0000 - val_rmse: 19918.1816\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587622720.0000 - rmse: 24240.9316 - val_loss: 540339392.0000 - val_rmse: 23245.2012\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687901952.0000 - rmse: 26227.8828 - val_loss: 1120547584.0000 - val_rmse: 33474.5820\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 770028160.0000 - rmse: 27749.3809 - val_loss: 762184768.0000 - val_rmse: 27607.6934\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 775490752.0000 - rmse: 27847.6328 - val_loss: 420861280.0000 - val_rmse: 20514.9023\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 688454464.0000 - rmse: 26238.4160 - val_loss: 345073600.0000 - val_rmse: 18576.1543\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649317312.0000 - rmse: 25481.7051 - val_loss: 1476068736.0000 - val_rmse: 38419.6406\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618979392.0000 - rmse: 24879.2969 - val_loss: 933790720.0000 - val_rmse: 30557.9902\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 719221184.0000 - rmse: 26818.2969 - val_loss: 726101696.0000 - val_rmse: 26946.2734\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 644341696.0000 - rmse: 25383.8848 - val_loss: 1199560320.0000 - val_rmse: 34634.6680\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593873472.0000 - rmse: 24369.5195 - val_loss: 800935936.0000 - val_rmse: 28300.8125\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680771840.0000 - rmse: 26091.6035 - val_loss: 589670976.0000 - val_rmse: 24283.1426\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539502400.0000 - rmse: 23227.1895 - val_loss: 517481152.0000 - val_rmse: 22748.2109\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583279616.0000 - rmse: 24151.1816 - val_loss: 337726144.0000 - val_rmse: 18377.3262\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540983744.0000 - rmse: 23259.0566 - val_loss: 354768320.0000 - val_rmse: 18835.2930\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513398496.0000 - rmse: 22658.2988 - val_loss: 1435549056.0000 - val_rmse: 37888.6367\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576875904.0000 - rmse: 24018.2402 - val_loss: 524238080.0000 - val_rmse: 22896.2461\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545817216.0000 - rmse: 23362.7305 - val_loss: 339857472.0000 - val_rmse: 18435.2246\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522138240.0000 - rmse: 22850.3438 - val_loss: 404783520.0000 - val_rmse: 20119.2324\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459212864.0000 - rmse: 21429.2520 - val_loss: 871780928.0000 - val_rmse: 29525.9375\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641920064.0000 - rmse: 25336.1406 - val_loss: 1580241280.0000 - val_rmse: 39752.2500\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622154880.0000 - rmse: 24943.0293 - val_loss: 646182848.0000 - val_rmse: 25420.1270\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572692928.0000 - rmse: 23931.0020 - val_loss: 723462848.0000 - val_rmse: 26897.2656\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 672463808.0000 - rmse: 25931.9062 - val_loss: 442337344.0000 - val_rmse: 21031.8184\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488091840.0000 - rmse: 22092.8008 - val_loss: 922067968.0000 - val_rmse: 30365.5684\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439274432.0000 - rmse: 20958.8730 - val_loss: 433070144.0000 - val_rmse: 20810.3359\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564774848.0000 - rmse: 23764.9922 - val_loss: 699403712.0000 - val_rmse: 26446.2422\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524089824.0000 - rmse: 22893.0059 - val_loss: 1208154240.0000 - val_rmse: 34758.5117\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 662706816.0000 - rmse: 25743.0898 - val_loss: 837412928.0000 - val_rmse: 28938.0879\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 671686848.0000 - rmse: 25916.9199 - val_loss: 448169920.0000 - val_rmse: 21170.0234\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506697728.0000 - rmse: 22509.9473 - val_loss: 625449984.0000 - val_rmse: 25008.9980\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486992000.0000 - rmse: 22067.8945 - val_loss: 624763264.0000 - val_rmse: 24995.2656\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497156640.0000 - rmse: 22297.0098 - val_loss: 456040768.0000 - val_rmse: 21355.1113\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495188992.0000 - rmse: 22252.8418 - val_loss: 417588096.0000 - val_rmse: 20434.9707\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589679424.0000 - rmse: 24283.3164 - val_loss: 753561728.0000 - val_rmse: 27451.0781\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516402240.0000 - rmse: 22724.4863 - val_loss: 539081728.0000 - val_rmse: 23218.1328\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500540256.0000 - rmse: 22372.7578 - val_loss: 1309381760.0000 - val_rmse: 36185.3789\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566846656.0000 - rmse: 23808.5410 - val_loss: 950266944.0000 - val_rmse: 30826.4004\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436907424.0000 - rmse: 20902.3301 - val_loss: 1010089088.0000 - val_rmse: 31781.8984\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471937120.0000 - rmse: 21724.1133 - val_loss: 428110080.0000 - val_rmse: 20690.8203\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463118368.0000 - rmse: 21520.1855 - val_loss: 947003136.0000 - val_rmse: 30773.4160\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555390848.0000 - rmse: 23566.7285 - val_loss: 375561728.0000 - val_rmse: 19379.4141\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478274560.0000 - rmse: 21869.4883 - val_loss: 1415348224.0000 - val_rmse: 37621.1133\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425550880.0000 - rmse: 20628.8828 - val_loss: 662296320.0000 - val_rmse: 25735.1191\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507072480.0000 - rmse: 22518.2676 - val_loss: 1009796864.0000 - val_rmse: 31777.3008\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566295424.0000 - rmse: 23796.9590 - val_loss: 452551520.0000 - val_rmse: 21273.2578\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472719360.0000 - rmse: 21742.1094 - val_loss: 581602816.0000 - val_rmse: 24116.4434\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469411552.0000 - rmse: 21665.9082 - val_loss: 992922688.0000 - val_rmse: 31510.6758\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444770752.0000 - rmse: 21089.5879 - val_loss: 474044192.0000 - val_rmse: 21772.5547\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440337184.0000 - rmse: 20984.2129 - val_loss: 917224000.0000 - val_rmse: 30285.7070\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498754336.0000 - rmse: 22332.8086 - val_loss: 1338037120.0000 - val_rmse: 36579.1914\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502767104.0000 - rmse: 22422.4688 - val_loss: 591138496.0000 - val_rmse: 24313.3398\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453466208.0000 - rmse: 21294.7441 - val_loss: 1304061824.0000 - val_rmse: 36111.7969\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409345472.0000 - rmse: 20232.2871 - val_loss: 920574208.0000 - val_rmse: 30340.9629\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425948192.0000 - rmse: 20638.5117 - val_loss: 678373056.0000 - val_rmse: 26045.5938\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427936800.0000 - rmse: 20686.6309 - val_loss: 489948800.0000 - val_rmse: 22134.7852\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529285376.0000 - rmse: 23006.2031 - val_loss: 1020856576.0000 - val_rmse: 31950.8438\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381549248.0000 - rmse: 19533.2832 - val_loss: 1112070016.0000 - val_rmse: 33347.7148\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465992608.0000 - rmse: 21586.8613 - val_loss: 1229167360.0000 - val_rmse: 35059.4844\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413153728.0000 - rmse: 20326.1836 - val_loss: 892862656.0000 - val_rmse: 29880.8047\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411241792.0000 - rmse: 20279.0977 - val_loss: 1525251840.0000 - val_rmse: 39054.4727\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483952832.0000 - rmse: 21998.9277 - val_loss: 1117090176.0000 - val_rmse: 33422.8984\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427699840.0000 - rmse: 20680.9023 - val_loss: 760312704.0000 - val_rmse: 27573.7676\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472144416.0000 - rmse: 21728.8828 - val_loss: 690366528.0000 - val_rmse: 26274.8262\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422751808.0000 - rmse: 20560.9277 - val_loss: 650070912.0000 - val_rmse: 25496.4863\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373178816.0000 - rmse: 19317.8359 - val_loss: 653588352.0000 - val_rmse: 25565.3750\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374478176.0000 - rmse: 19351.4375 - val_loss: 960245248.0000 - val_rmse: 30987.8242\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385348992.0000 - rmse: 19630.3066 - val_loss: 384006976.0000 - val_rmse: 19596.0938\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392161728.0000 - rmse: 19803.0742 - val_loss: 755582272.0000 - val_rmse: 27487.8574\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404150752.0000 - rmse: 20103.5000 - val_loss: 512014176.0000 - val_rmse: 22627.7305\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401341824.0000 - rmse: 20033.5176 - val_loss: 1407784704.0000 - val_rmse: 37520.4570\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394307648.0000 - rmse: 19857.1797 - val_loss: 477056448.0000 - val_rmse: 21841.6211\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373414848.0000 - rmse: 19323.9453 - val_loss: 674542592.0000 - val_rmse: 25971.9570\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393844256.0000 - rmse: 19845.5078 - val_loss: 604156416.0000 - val_rmse: 24579.5938\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407850272.0000 - rmse: 20195.3008 - val_loss: 558930880.0000 - val_rmse: 23641.7188\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362077920.0000 - rmse: 19028.3438 - val_loss: 587885504.0000 - val_rmse: 24246.3496\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395379392.0000 - rmse: 19884.1484 - val_loss: 1592280704.0000 - val_rmse: 39903.3906\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497653760.0000 - rmse: 22308.1543 - val_loss: 456892960.0000 - val_rmse: 21375.0547\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351348448.0000 - rmse: 18744.2891 - val_loss: 430475168.0000 - val_rmse: 20747.8945\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372560160.0000 - rmse: 19301.8164 - val_loss: 696001920.0000 - val_rmse: 26381.8477\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341816864.0000 - rmse: 18488.2871 - val_loss: 833464000.0000 - val_rmse: 28869.7773\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358037472.0000 - rmse: 18921.8789 - val_loss: 347597888.0000 - val_rmse: 18643.9766\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353571232.0000 - rmse: 18803.4902 - val_loss: 849833024.0000 - val_rmse: 29151.8965\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326301056.0000 - rmse: 18063.8027 - val_loss: 725099456.0000 - val_rmse: 26927.6699\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367807872.0000 - rmse: 19178.3145 - val_loss: 881530240.0000 - val_rmse: 29690.5723\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436058432.0000 - rmse: 20882.0098 - val_loss: 575252864.0000 - val_rmse: 23984.4277\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365825536.0000 - rmse: 19126.5645 - val_loss: 1289323136.0000 - val_rmse: 35907.1445\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410272384.0000 - rmse: 20255.1816 - val_loss: 1876862976.0000 - val_rmse: 43322.7773\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406520192.0000 - rmse: 20162.3438 - val_loss: 2013003136.0000 - val_rmse: 44866.5039\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345449952.0000 - rmse: 18586.2832 - val_loss: 446497472.0000 - val_rmse: 21130.4863\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361647904.0000 - rmse: 19017.0410 - val_loss: 595769472.0000 - val_rmse: 24408.3887\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321336768.0000 - rmse: 17925.8652 - val_loss: 1247174272.0000 - val_rmse: 35315.3555\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364299264.0000 - rmse: 19086.6230 - val_loss: 768242816.0000 - val_rmse: 27717.1914\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406957216.0000 - rmse: 20173.1797 - val_loss: 902094976.0000 - val_rmse: 30034.8945\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330747328.0000 - rmse: 18186.4590 - val_loss: 1347247616.0000 - val_rmse: 36704.8711\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323544768.0000 - rmse: 17987.3477 - val_loss: 572857664.0000 - val_rmse: 23934.4453\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324822848.0000 - rmse: 18022.8398 - val_loss: 745621760.0000 - val_rmse: 27306.0762\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337766688.0000 - rmse: 18378.4297 - val_loss: 871849792.0000 - val_rmse: 29527.0996\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369413152.0000 - rmse: 19220.1211 - val_loss: 810842368.0000 - val_rmse: 28475.2930\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306271136.0000 - rmse: 17500.6016 - val_loss: 1269074048.0000 - val_rmse: 35624.0664\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316608000.0000 - rmse: 17793.4805 - val_loss: 894941824.0000 - val_rmse: 29915.5781\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368436736.0000 - rmse: 19194.7051 - val_loss: 546467968.0000 - val_rmse: 23376.6543\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352552576.0000 - rmse: 18776.3809 - val_loss: 1082671488.0000 - val_rmse: 32903.9727\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348994656.0000 - rmse: 18681.3965 - val_loss: 676782400.0000 - val_rmse: 26015.0391\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304145056.0000 - rmse: 17439.7520 - val_loss: 636478784.0000 - val_rmse: 25228.5312\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365066688.0000 - rmse: 19106.7168 - val_loss: 744035392.0000 - val_rmse: 27277.0117\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343813600.0000 - rmse: 18542.2109 - val_loss: 549578304.0000 - val_rmse: 23443.0859\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378095136.0000 - rmse: 19444.6660 - val_loss: 477192864.0000 - val_rmse: 21844.7441\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401878560.0000 - rmse: 20046.9082 - val_loss: 994247616.0000 - val_rmse: 31531.6914\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370291424.0000 - rmse: 19242.9570 - val_loss: 396034624.0000 - val_rmse: 19900.6172\n",
      "104/104 [==============================] - 0s 710us/step - loss: 506327040.0000 - rmse: 22501.7109\n",
      "[506327040.0, 22501.7109375]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 6634277888.0000 - rmse: 81451.0781 - val_loss: 1257513600.0000 - val_rmse: 35461.4375\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1605611904.0000 - rmse: 40070.0859 - val_loss: 1084324608.0000 - val_rmse: 32929.0859\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1351381504.0000 - rmse: 36761.1406 - val_loss: 1048538688.0000 - val_rmse: 32381.1465\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1248844416.0000 - rmse: 35338.9922 - val_loss: 892639936.0000 - val_rmse: 29877.0820\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1185165696.0000 - rmse: 34426.2344 - val_loss: 962251328.0000 - val_rmse: 31020.1758\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1190088448.0000 - rmse: 34497.6602 - val_loss: 858949440.0000 - val_rmse: 29307.8398\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1048595776.0000 - rmse: 32382.0293 - val_loss: 835840960.0000 - val_rmse: 28910.9141\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1073543104.0000 - rmse: 32764.9668 - val_loss: 819332608.0000 - val_rmse: 28623.9863\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1019081344.0000 - rmse: 31923.0527 - val_loss: 813957312.0000 - val_rmse: 28529.9375\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1034022784.0000 - rmse: 32156.2246 - val_loss: 842670144.0000 - val_rmse: 29028.7812\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1048514048.0000 - rmse: 32380.7676 - val_loss: 782760512.0000 - val_rmse: 27977.8574\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 978493184.0000 - rmse: 31280.8750 - val_loss: 862258880.0000 - val_rmse: 29364.2441\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1000383360.0000 - rmse: 31628.8379 - val_loss: 832729216.0000 - val_rmse: 28857.0488\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 949793216.0000 - rmse: 30818.7148 - val_loss: 720097024.0000 - val_rmse: 26834.6230\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 862884928.0000 - rmse: 29374.9023 - val_loss: 770323648.0000 - val_rmse: 27754.7051\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 861139968.0000 - rmse: 29345.1855 - val_loss: 725202624.0000 - val_rmse: 26929.5859\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 865566528.0000 - rmse: 29420.5117 - val_loss: 827083136.0000 - val_rmse: 28759.0527\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 844389696.0000 - rmse: 29058.3848 - val_loss: 666989696.0000 - val_rmse: 25826.1445\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 820859264.0000 - rmse: 28650.6406 - val_loss: 702586624.0000 - val_rmse: 26506.3516\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 772625664.0000 - rmse: 27796.1445 - val_loss: 661538496.0000 - val_rmse: 25720.3906\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 777831680.0000 - rmse: 27889.6348 - val_loss: 647326464.0000 - val_rmse: 25442.6094\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 757404736.0000 - rmse: 27520.9863 - val_loss: 902218688.0000 - val_rmse: 30036.9551\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724516928.0000 - rmse: 26916.8516 - val_loss: 2148362240.0000 - val_rmse: 46350.4297\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 767759040.0000 - rmse: 27708.4648 - val_loss: 601287936.0000 - val_rmse: 24521.1738\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 726546496.0000 - rmse: 26954.5273 - val_loss: 717083776.0000 - val_rmse: 26778.4199\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 686733248.0000 - rmse: 26205.5957 - val_loss: 596307264.0000 - val_rmse: 24419.4004\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647247296.0000 - rmse: 25441.0547 - val_loss: 821573504.0000 - val_rmse: 28663.1035\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665003008.0000 - rmse: 25787.6523 - val_loss: 664036224.0000 - val_rmse: 25768.9004\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677397824.0000 - rmse: 26026.8672 - val_loss: 589446016.0000 - val_rmse: 24278.5078\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598481344.0000 - rmse: 24463.8770 - val_loss: 687909184.0000 - val_rmse: 26228.0215\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605373056.0000 - rmse: 24604.3301 - val_loss: 582136640.0000 - val_rmse: 24127.5078\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593373184.0000 - rmse: 24359.2520 - val_loss: 667919104.0000 - val_rmse: 25844.1309\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542547776.0000 - rmse: 23292.6543 - val_loss: 567552256.0000 - val_rmse: 23823.3555\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590406080.0000 - rmse: 24298.2715 - val_loss: 601400768.0000 - val_rmse: 24523.4746\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589583488.0000 - rmse: 24281.3398 - val_loss: 635709440.0000 - val_rmse: 25213.2793\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551836608.0000 - rmse: 23491.1992 - val_loss: 654406400.0000 - val_rmse: 25581.3672\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541180352.0000 - rmse: 23263.2832 - val_loss: 611950528.0000 - val_rmse: 24737.6348\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576262464.0000 - rmse: 24005.4668 - val_loss: 545537728.0000 - val_rmse: 23356.7480\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558166528.0000 - rmse: 23625.5449 - val_loss: 654252032.0000 - val_rmse: 25578.3496\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560165504.0000 - rmse: 23667.8164 - val_loss: 670435648.0000 - val_rmse: 25892.7715\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490323200.0000 - rmse: 22143.2422 - val_loss: 731598464.0000 - val_rmse: 27048.0762\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517946304.0000 - rmse: 22758.4336 - val_loss: 607651264.0000 - val_rmse: 24650.5820\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521521664.0000 - rmse: 22836.8477 - val_loss: 562191424.0000 - val_rmse: 23710.5762\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497656544.0000 - rmse: 22308.2148 - val_loss: 576191360.0000 - val_rmse: 24003.9863\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489423872.0000 - rmse: 22122.9238 - val_loss: 552814208.0000 - val_rmse: 23512.0020\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446929696.0000 - rmse: 21140.7109 - val_loss: 566069312.0000 - val_rmse: 23792.2090\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444938368.0000 - rmse: 21093.5625 - val_loss: 684333376.0000 - val_rmse: 26159.7656\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465413056.0000 - rmse: 21573.4336 - val_loss: 783671744.0000 - val_rmse: 27994.1367\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505327264.0000 - rmse: 22479.4844 - val_loss: 556894976.0000 - val_rmse: 23598.6230\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428223232.0000 - rmse: 20693.5547 - val_loss: 627632128.0000 - val_rmse: 25052.5859\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507892384.0000 - rmse: 22536.4668 - val_loss: 535519232.0000 - val_rmse: 23141.2871\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389645696.0000 - rmse: 19739.4434 - val_loss: 569453696.0000 - val_rmse: 23863.2285\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500538144.0000 - rmse: 22372.7090 - val_loss: 525897792.0000 - val_rmse: 22932.4609\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391902432.0000 - rmse: 19796.5254 - val_loss: 775936192.0000 - val_rmse: 27855.6309\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470973440.0000 - rmse: 21701.9199 - val_loss: 533930016.0000 - val_rmse: 23106.9258\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427071008.0000 - rmse: 20665.6953 - val_loss: 550826816.0000 - val_rmse: 23469.6992\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373729920.0000 - rmse: 19332.0938 - val_loss: 568521536.0000 - val_rmse: 23843.6875\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390337344.0000 - rmse: 19756.9570 - val_loss: 529764192.0000 - val_rmse: 23016.6055\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381129152.0000 - rmse: 19522.5293 - val_loss: 676206720.0000 - val_rmse: 26003.9727\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422806208.0000 - rmse: 20562.2520 - val_loss: 576063040.0000 - val_rmse: 24001.3125\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372846208.0000 - rmse: 19309.2246 - val_loss: 585775488.0000 - val_rmse: 24202.7988\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350050560.0000 - rmse: 18709.6367 - val_loss: 572829632.0000 - val_rmse: 23933.8574\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448912800.0000 - rmse: 21187.5625 - val_loss: 549866688.0000 - val_rmse: 23449.2344\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411896096.0000 - rmse: 20295.2227 - val_loss: 619873024.0000 - val_rmse: 24897.2461\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452727296.0000 - rmse: 21277.3887 - val_loss: 748795712.0000 - val_rmse: 27364.1309\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404616544.0000 - rmse: 20115.0820 - val_loss: 533961312.0000 - val_rmse: 23107.6016\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420002528.0000 - rmse: 20493.9609 - val_loss: 538070336.0000 - val_rmse: 23196.3438\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365253216.0000 - rmse: 19111.5977 - val_loss: 519779552.0000 - val_rmse: 22798.6738\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394246560.0000 - rmse: 19855.6426 - val_loss: 633675136.0000 - val_rmse: 25172.9023\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376551328.0000 - rmse: 19404.9297 - val_loss: 597864000.0000 - val_rmse: 24451.2578\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413708928.0000 - rmse: 20339.8340 - val_loss: 560801728.0000 - val_rmse: 23681.2520\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354330240.0000 - rmse: 18823.6602 - val_loss: 529518496.0000 - val_rmse: 23011.2676\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392261344.0000 - rmse: 19805.5879 - val_loss: 563025472.0000 - val_rmse: 23728.1582\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387235680.0000 - rmse: 19678.3047 - val_loss: 661306880.0000 - val_rmse: 25715.8848\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381485600.0000 - rmse: 19531.6562 - val_loss: 611262144.0000 - val_rmse: 24723.7148\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355978848.0000 - rmse: 18867.3984 - val_loss: 573624128.0000 - val_rmse: 23950.4492\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383258208.0000 - rmse: 19576.9805 - val_loss: 562482560.0000 - val_rmse: 23716.7129\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339822368.0000 - rmse: 18434.2695 - val_loss: 786948160.0000 - val_rmse: 28052.5938\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368762368.0000 - rmse: 19203.1855 - val_loss: 581773888.0000 - val_rmse: 24119.9883\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346466144.0000 - rmse: 18613.5996 - val_loss: 568755264.0000 - val_rmse: 23848.5898\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351097376.0000 - rmse: 18737.5918 - val_loss: 594067456.0000 - val_rmse: 24373.5000\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342668224.0000 - rmse: 18511.2988 - val_loss: 547447424.0000 - val_rmse: 23397.5938\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402294848.0000 - rmse: 20057.2871 - val_loss: 649400384.0000 - val_rmse: 25483.3340\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351563808.0000 - rmse: 18750.0332 - val_loss: 541055168.0000 - val_rmse: 23260.5918\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378983808.0000 - rmse: 19467.5059 - val_loss: 500508032.0000 - val_rmse: 22372.0352\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328052384.0000 - rmse: 18112.2168 - val_loss: 707487616.0000 - val_rmse: 26598.6387\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351640832.0000 - rmse: 18752.0820 - val_loss: 516362912.0000 - val_rmse: 22723.6211\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331233664.0000 - rmse: 18199.8242 - val_loss: 805428096.0000 - val_rmse: 28380.0645\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345986880.0000 - rmse: 18600.7227 - val_loss: 589429376.0000 - val_rmse: 24278.1660\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329106016.0000 - rmse: 18141.2773 - val_loss: 522272800.0000 - val_rmse: 22853.2871\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359555040.0000 - rmse: 18961.9355 - val_loss: 1362350976.0000 - val_rmse: 36910.0391\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299287712.0000 - rmse: 17299.9316 - val_loss: 540860864.0000 - val_rmse: 23256.4141\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340529344.0000 - rmse: 18453.4355 - val_loss: 538166720.0000 - val_rmse: 23198.4199\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337655392.0000 - rmse: 18375.4004 - val_loss: 861242688.0000 - val_rmse: 29346.9375\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292840544.0000 - rmse: 17112.5820 - val_loss: 555272512.0000 - val_rmse: 23564.2207\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306531456.0000 - rmse: 17508.0371 - val_loss: 883932672.0000 - val_rmse: 29731.0059\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365585184.0000 - rmse: 19120.2812 - val_loss: 550750400.0000 - val_rmse: 23468.0703\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352736096.0000 - rmse: 18781.2656 - val_loss: 530610720.0000 - val_rmse: 23034.9863\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325092768.0000 - rmse: 18030.3281 - val_loss: 588302016.0000 - val_rmse: 24254.9375\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288026112.0000 - rmse: 16971.3320 - val_loss: 601153856.0000 - val_rmse: 24518.4375\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313505088.0000 - rmse: 17706.0742 - val_loss: 583545920.0000 - val_rmse: 24156.6934\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297753024.0000 - rmse: 17255.5195 - val_loss: 668819904.0000 - val_rmse: 25861.5508\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318832992.0000 - rmse: 17855.8926 - val_loss: 508804224.0000 - val_rmse: 22556.6875\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311672288.0000 - rmse: 17654.2422 - val_loss: 514189664.0000 - val_rmse: 22675.7500\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318852256.0000 - rmse: 17856.4336 - val_loss: 572879040.0000 - val_rmse: 23934.8926\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302584128.0000 - rmse: 17394.9434 - val_loss: 507026240.0000 - val_rmse: 22517.2422\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381963872.0000 - rmse: 19543.8945 - val_loss: 687398144.0000 - val_rmse: 26218.2773\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324896608.0000 - rmse: 18024.8867 - val_loss: 531722368.0000 - val_rmse: 23059.1055\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343952352.0000 - rmse: 18545.9512 - val_loss: 684638848.0000 - val_rmse: 26165.6035\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347577568.0000 - rmse: 18643.4297 - val_loss: 514499712.0000 - val_rmse: 22682.5859\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351999072.0000 - rmse: 18761.6387 - val_loss: 503397888.0000 - val_rmse: 22436.5293\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298327008.0000 - rmse: 17272.1426 - val_loss: 795621568.0000 - val_rmse: 28206.7617\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305441024.0000 - rmse: 17476.8711 - val_loss: 580788096.0000 - val_rmse: 24099.5449\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273360928.0000 - rmse: 16533.6289 - val_loss: 708687488.0000 - val_rmse: 26621.1836\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338172064.0000 - rmse: 18389.4531 - val_loss: 518956736.0000 - val_rmse: 22780.6211\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321684928.0000 - rmse: 17935.5742 - val_loss: 552625920.0000 - val_rmse: 23507.9941\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321431296.0000 - rmse: 17928.5059 - val_loss: 566738432.0000 - val_rmse: 23806.2676\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369013344.0000 - rmse: 19209.7188 - val_loss: 729832192.0000 - val_rmse: 27015.4062\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312001696.0000 - rmse: 17663.5684 - val_loss: 601510528.0000 - val_rmse: 24525.7109\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336773568.0000 - rmse: 18351.3906 - val_loss: 615517248.0000 - val_rmse: 24809.6191\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312197312.0000 - rmse: 17669.1035 - val_loss: 588619840.0000 - val_rmse: 24261.4883\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330090144.0000 - rmse: 18168.3809 - val_loss: 583511488.0000 - val_rmse: 24155.9805\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298928768.0000 - rmse: 17289.5547 - val_loss: 624025920.0000 - val_rmse: 24980.5117\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284514656.0000 - rmse: 16867.5605 - val_loss: 594266112.0000 - val_rmse: 24377.5723\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298743264.0000 - rmse: 17284.1895 - val_loss: 547742400.0000 - val_rmse: 23403.8965\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321914624.0000 - rmse: 17941.9785 - val_loss: 517410016.0000 - val_rmse: 22746.6484\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268393088.0000 - rmse: 16382.7041 - val_loss: 714163200.0000 - val_rmse: 26723.8320\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279585280.0000 - rmse: 16720.8027 - val_loss: 546539008.0000 - val_rmse: 23378.1738\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276136288.0000 - rmse: 16617.3496 - val_loss: 600172864.0000 - val_rmse: 24498.4238\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290388672.0000 - rmse: 17040.7910 - val_loss: 543915584.0000 - val_rmse: 23321.9961\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332739264.0000 - rmse: 18241.1406 - val_loss: 670459264.0000 - val_rmse: 25893.2285\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273974976.0000 - rmse: 16552.1875 - val_loss: 551681408.0000 - val_rmse: 23487.8984\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271525376.0000 - rmse: 16478.0254 - val_loss: 542078272.0000 - val_rmse: 23282.5723\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317943968.0000 - rmse: 17830.9824 - val_loss: 657866880.0000 - val_rmse: 25648.9141\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302455200.0000 - rmse: 17391.2363 - val_loss: 507699328.0000 - val_rmse: 22532.1816\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307564832.0000 - rmse: 17537.5254 - val_loss: 527472928.0000 - val_rmse: 22966.7773\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316114752.0000 - rmse: 17779.6152 - val_loss: 559667840.0000 - val_rmse: 23657.3008\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422841568.0000 - rmse: 20563.1113 - val_loss: 535975136.0000 - val_rmse: 23151.1348\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301819168.0000 - rmse: 17372.9414 - val_loss: 675161984.0000 - val_rmse: 25983.8789\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261751424.0000 - rmse: 16178.7295 - val_loss: 642590208.0000 - val_rmse: 25349.3633\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313229184.0000 - rmse: 17698.2812 - val_loss: 593902272.0000 - val_rmse: 24370.1094\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330923776.0000 - rmse: 18191.3105 - val_loss: 657546112.0000 - val_rmse: 25642.6621\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329784256.0000 - rmse: 18159.9609 - val_loss: 584026112.0000 - val_rmse: 24166.6309\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299447360.0000 - rmse: 17304.5469 - val_loss: 530183520.0000 - val_rmse: 23025.7129\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286965856.0000 - rmse: 16940.0664 - val_loss: 674947264.0000 - val_rmse: 25979.7461\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310141632.0000 - rmse: 17610.8379 - val_loss: 874649856.0000 - val_rmse: 29574.4805\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293954656.0000 - rmse: 17145.1016 - val_loss: 523445472.0000 - val_rmse: 22878.9297\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305543328.0000 - rmse: 17479.7969 - val_loss: 561550464.0000 - val_rmse: 23697.0527\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275101152.0000 - rmse: 16586.1699 - val_loss: 516411936.0000 - val_rmse: 22724.6973\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300626240.0000 - rmse: 17338.5762 - val_loss: 540616960.0000 - val_rmse: 23251.1680\n",
      "104/104 [==============================] - 0s 662us/step - loss: 846385856.0000 - rmse: 29092.7109\n",
      "[846385856.0, 29092.7109375]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 5443072000.0000 - rmse: 73777.1797 - val_loss: 1387123072.0000 - val_rmse: 37244.1016\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1485350912.0000 - rmse: 38540.2500 - val_loss: 1108160896.0000 - val_rmse: 33289.0508\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1363285888.0000 - rmse: 36922.7031 - val_loss: 996071168.0000 - val_rmse: 31560.5957\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1258788480.0000 - rmse: 35479.4102 - val_loss: 1095854080.0000 - val_rmse: 33103.6875\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1201745792.0000 - rmse: 34666.2070 - val_loss: 1082721536.0000 - val_rmse: 32904.7344\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1055241024.0000 - rmse: 32484.4727 - val_loss: 1019800192.0000 - val_rmse: 31934.3105\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1051620864.0000 - rmse: 32428.7051 - val_loss: 926436864.0000 - val_rmse: 30437.4258\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1023502208.0000 - rmse: 31992.2207 - val_loss: 1002965120.0000 - val_rmse: 31669.6250\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1019820736.0000 - rmse: 31934.6328 - val_loss: 873273216.0000 - val_rmse: 29551.1973\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1075180544.0000 - rmse: 32789.9453 - val_loss: 966564736.0000 - val_rmse: 31089.6250\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 942887424.0000 - rmse: 30706.4727 - val_loss: 888159936.0000 - val_rmse: 29802.0117\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 960796672.0000 - rmse: 30996.7188 - val_loss: 1429569152.0000 - val_rmse: 37809.6445\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 938109888.0000 - rmse: 30628.5801 - val_loss: 1263416832.0000 - val_rmse: 35544.5742\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 903859264.0000 - rmse: 30064.2520 - val_loss: 944893056.0000 - val_rmse: 30739.1133\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 951249728.0000 - rmse: 30842.3359 - val_loss: 869870336.0000 - val_rmse: 29493.5645\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 953026176.0000 - rmse: 30871.1211 - val_loss: 918201664.0000 - val_rmse: 30301.8418\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 966507328.0000 - rmse: 31088.7012 - val_loss: 874377408.0000 - val_rmse: 29569.8730\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 878533824.0000 - rmse: 29640.0703 - val_loss: 963391360.0000 - val_rmse: 31038.5469\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 834603200.0000 - rmse: 28889.5000 - val_loss: 849629952.0000 - val_rmse: 29148.4121\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 878169152.0000 - rmse: 29633.9180 - val_loss: 773976384.0000 - val_rmse: 27820.4316\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 854578944.0000 - rmse: 29233.1816 - val_loss: 864868224.0000 - val_rmse: 29408.6406\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 849573760.0000 - rmse: 29147.4473 - val_loss: 1086393472.0000 - val_rmse: 32960.4844\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 827143616.0000 - rmse: 28760.1055 - val_loss: 818942656.0000 - val_rmse: 28617.1719\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 754249600.0000 - rmse: 27463.6055 - val_loss: 990886080.0000 - val_rmse: 31478.3438\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 759463360.0000 - rmse: 27558.3613 - val_loss: 785293824.0000 - val_rmse: 28023.0918\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 774558848.0000 - rmse: 27830.8965 - val_loss: 773051520.0000 - val_rmse: 27803.8047\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 766217152.0000 - rmse: 27680.6270 - val_loss: 1254084224.0000 - val_rmse: 35413.0508\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751966080.0000 - rmse: 27422.0000 - val_loss: 924378816.0000 - val_rmse: 30403.5996\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685431872.0000 - rmse: 26180.7520 - val_loss: 2162107392.0000 - val_rmse: 46498.4648\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 714357056.0000 - rmse: 26727.4590 - val_loss: 814609152.0000 - val_rmse: 28541.3574\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 725045760.0000 - rmse: 26926.6719 - val_loss: 756097024.0000 - val_rmse: 27497.2188\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724753664.0000 - rmse: 26921.2500 - val_loss: 1124463872.0000 - val_rmse: 33533.0273\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 744923968.0000 - rmse: 27293.2949 - val_loss: 760362432.0000 - val_rmse: 27574.6699\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 639192384.0000 - rmse: 25282.2539 - val_loss: 683636160.0000 - val_rmse: 26146.4355\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677237824.0000 - rmse: 26023.7930 - val_loss: 661247552.0000 - val_rmse: 25714.7344\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 657645696.0000 - rmse: 25644.6016 - val_loss: 714305472.0000 - val_rmse: 26726.4922\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632027968.0000 - rmse: 25140.1660 - val_loss: 726422720.0000 - val_rmse: 26952.2305\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606349888.0000 - rmse: 24624.1719 - val_loss: 808551744.0000 - val_rmse: 28435.0430\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 688027904.0000 - rmse: 26230.2852 - val_loss: 1050151424.0000 - val_rmse: 32406.0391\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558994496.0000 - rmse: 23643.0645 - val_loss: 1116750976.0000 - val_rmse: 33417.8242\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651688512.0000 - rmse: 25528.1914 - val_loss: 870543168.0000 - val_rmse: 29504.9688\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654235392.0000 - rmse: 25578.0234 - val_loss: 530465024.0000 - val_rmse: 23031.8262\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570952192.0000 - rmse: 23894.6055 - val_loss: 633850560.0000 - val_rmse: 25176.3867\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587469824.0000 - rmse: 24237.7773 - val_loss: 1063813952.0000 - val_rmse: 32616.1602\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642020288.0000 - rmse: 25338.1172 - val_loss: 838426048.0000 - val_rmse: 28955.5879\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571228736.0000 - rmse: 23900.3926 - val_loss: 681471168.0000 - val_rmse: 26105.0020\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560511616.0000 - rmse: 23675.1270 - val_loss: 1024625216.0000 - val_rmse: 32009.7676\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561015552.0000 - rmse: 23685.7676 - val_loss: 447495008.0000 - val_rmse: 21154.0762\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604611328.0000 - rmse: 24588.8438 - val_loss: 499241984.0000 - val_rmse: 22343.7227\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577022400.0000 - rmse: 24021.2891 - val_loss: 681618688.0000 - val_rmse: 26107.8281\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561031488.0000 - rmse: 23686.0996 - val_loss: 586007872.0000 - val_rmse: 24207.5996\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520616608.0000 - rmse: 22817.0254 - val_loss: 478104064.0000 - val_rmse: 21865.5898\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596786304.0000 - rmse: 24429.2090 - val_loss: 574303872.0000 - val_rmse: 23964.6367\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535800160.0000 - rmse: 23147.3574 - val_loss: 678763392.0000 - val_rmse: 26053.0859\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509687904.0000 - rmse: 22576.2695 - val_loss: 871206976.0000 - val_rmse: 29516.2148\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566700160.0000 - rmse: 23805.4648 - val_loss: 819641216.0000 - val_rmse: 28629.3770\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526510432.0000 - rmse: 22945.8145 - val_loss: 906953664.0000 - val_rmse: 30115.6719\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 528954304.0000 - rmse: 22999.0059 - val_loss: 550357952.0000 - val_rmse: 23459.7070\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495702688.0000 - rmse: 22264.3809 - val_loss: 1285836416.0000 - val_rmse: 35858.5586\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488854848.0000 - rmse: 22110.0625 - val_loss: 844902272.0000 - val_rmse: 29067.2012\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543776448.0000 - rmse: 23319.0137 - val_loss: 834504064.0000 - val_rmse: 28887.7832\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527967552.0000 - rmse: 22977.5449 - val_loss: 1111260928.0000 - val_rmse: 33335.5820\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562991488.0000 - rmse: 23727.4395 - val_loss: 696897600.0000 - val_rmse: 26398.8164\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485225248.0000 - rmse: 22027.8281 - val_loss: 500407872.0000 - val_rmse: 22369.7969\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441888768.0000 - rmse: 21021.1504 - val_loss: 700436928.0000 - val_rmse: 26465.7695\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544799552.0000 - rmse: 23340.9414 - val_loss: 658548736.0000 - val_rmse: 25662.2051\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516009536.0000 - rmse: 22715.8438 - val_loss: 628035328.0000 - val_rmse: 25060.6309\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477652448.0000 - rmse: 21855.2598 - val_loss: 617481856.0000 - val_rmse: 24849.1816\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470178976.0000 - rmse: 21683.6094 - val_loss: 528656128.0000 - val_rmse: 22992.5234\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497540704.0000 - rmse: 22305.6191 - val_loss: 524937824.0000 - val_rmse: 22911.5215\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424577952.0000 - rmse: 20605.2891 - val_loss: 505820416.0000 - val_rmse: 22490.4512\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516662752.0000 - rmse: 22730.2168 - val_loss: 581559616.0000 - val_rmse: 24115.5449\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462733440.0000 - rmse: 21511.2402 - val_loss: 612183680.0000 - val_rmse: 24742.3457\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490878720.0000 - rmse: 22155.7793 - val_loss: 812013248.0000 - val_rmse: 28495.8457\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422998784.0000 - rmse: 20566.9336 - val_loss: 525238784.0000 - val_rmse: 22918.0879\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484530560.0000 - rmse: 22012.0547 - val_loss: 693509504.0000 - val_rmse: 26334.5664\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420831648.0000 - rmse: 20514.1816 - val_loss: 670748992.0000 - val_rmse: 25898.8203\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446233920.0000 - rmse: 21124.2480 - val_loss: 580223872.0000 - val_rmse: 24087.8359\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428967680.0000 - rmse: 20711.5332 - val_loss: 482989152.0000 - val_rmse: 21977.0137\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457832864.0000 - rmse: 21397.0293 - val_loss: 535482304.0000 - val_rmse: 23140.4883\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419609024.0000 - rmse: 20484.3574 - val_loss: 689712576.0000 - val_rmse: 26262.3789\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440794656.0000 - rmse: 20995.1094 - val_loss: 809649152.0000 - val_rmse: 28454.3340\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485992224.0000 - rmse: 22045.2305 - val_loss: 794575936.0000 - val_rmse: 28188.2227\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455669984.0000 - rmse: 21346.4277 - val_loss: 577802624.0000 - val_rmse: 24037.5234\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405901888.0000 - rmse: 20147.0059 - val_loss: 932843328.0000 - val_rmse: 30542.4844\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429182176.0000 - rmse: 20716.7109 - val_loss: 815501952.0000 - val_rmse: 28556.9941\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427143744.0000 - rmse: 20667.4531 - val_loss: 633139392.0000 - val_rmse: 25162.2617\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432134816.0000 - rmse: 20787.8516 - val_loss: 414399104.0000 - val_rmse: 20356.7949\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390712288.0000 - rmse: 19766.4434 - val_loss: 438023552.0000 - val_rmse: 20929.0098\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404722464.0000 - rmse: 20117.7129 - val_loss: 746281280.0000 - val_rmse: 27318.1465\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395299488.0000 - rmse: 19882.1387 - val_loss: 426737792.0000 - val_rmse: 20657.6309\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381748768.0000 - rmse: 19538.3906 - val_loss: 652349888.0000 - val_rmse: 25541.1406\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381554208.0000 - rmse: 19533.4102 - val_loss: 526749344.0000 - val_rmse: 22951.0195\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439534016.0000 - rmse: 20965.0645 - val_loss: 500474656.0000 - val_rmse: 22371.2891\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396517952.0000 - rmse: 19912.7559 - val_loss: 522764992.0000 - val_rmse: 22864.0547\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412823648.0000 - rmse: 20318.0605 - val_loss: 600084992.0000 - val_rmse: 24496.6289\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367644064.0000 - rmse: 19174.0449 - val_loss: 615039104.0000 - val_rmse: 24799.9785\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399077696.0000 - rmse: 19976.9277 - val_loss: 602256640.0000 - val_rmse: 24540.9160\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372905152.0000 - rmse: 19310.7480 - val_loss: 484418080.0000 - val_rmse: 22009.4980\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333084512.0000 - rmse: 18250.6016 - val_loss: 469391680.0000 - val_rmse: 21665.4453\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349112480.0000 - rmse: 18684.5508 - val_loss: 699263168.0000 - val_rmse: 26443.5840\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354543616.0000 - rmse: 18829.3262 - val_loss: 742378752.0000 - val_rmse: 27246.6250\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359124064.0000 - rmse: 18950.5684 - val_loss: 843294336.0000 - val_rmse: 29039.5293\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417241632.0000 - rmse: 20426.4922 - val_loss: 794690432.0000 - val_rmse: 28190.2539\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340054688.0000 - rmse: 18440.5703 - val_loss: 441411200.0000 - val_rmse: 21009.7871\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387880288.0000 - rmse: 19694.6738 - val_loss: 836502016.0000 - val_rmse: 28922.3418\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350139008.0000 - rmse: 18712.0000 - val_loss: 530739520.0000 - val_rmse: 23037.7852\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373956896.0000 - rmse: 19337.9648 - val_loss: 589959616.0000 - val_rmse: 24289.0820\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314289984.0000 - rmse: 17728.2246 - val_loss: 608899520.0000 - val_rmse: 24675.8867\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386892704.0000 - rmse: 19669.5879 - val_loss: 643348480.0000 - val_rmse: 25364.3125\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274148480.0000 - rmse: 16557.4277 - val_loss: 519375552.0000 - val_rmse: 22789.8125\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352699456.0000 - rmse: 18780.2930 - val_loss: 634729536.0000 - val_rmse: 25193.8359\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343998752.0000 - rmse: 18547.2012 - val_loss: 564736000.0000 - val_rmse: 23764.1738\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363013888.0000 - rmse: 19052.9199 - val_loss: 482093984.0000 - val_rmse: 21956.6367\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335719232.0000 - rmse: 18322.6387 - val_loss: 648338624.0000 - val_rmse: 25462.4941\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301464800.0000 - rmse: 17362.7422 - val_loss: 606376960.0000 - val_rmse: 24624.7207\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389768320.0000 - rmse: 19742.5508 - val_loss: 1833040640.0000 - val_rmse: 42814.0234\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326960128.0000 - rmse: 18082.0352 - val_loss: 642687744.0000 - val_rmse: 25351.2832\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364572384.0000 - rmse: 19093.7773 - val_loss: 696141632.0000 - val_rmse: 26384.4941\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330969728.0000 - rmse: 18192.5742 - val_loss: 488756544.0000 - val_rmse: 22107.8359\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489769248.0000 - rmse: 22130.7285 - val_loss: 835538432.0000 - val_rmse: 28905.6816\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321855552.0000 - rmse: 17940.3320 - val_loss: 560424704.0000 - val_rmse: 23673.2871\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392338496.0000 - rmse: 19807.5352 - val_loss: 544181440.0000 - val_rmse: 23327.6973\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368314464.0000 - rmse: 19191.5215 - val_loss: 481306816.0000 - val_rmse: 21938.7051\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334895360.0000 - rmse: 18300.1445 - val_loss: 513555008.0000 - val_rmse: 22661.7500\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331978016.0000 - rmse: 18220.2617 - val_loss: 603845504.0000 - val_rmse: 24573.2676\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341414464.0000 - rmse: 18477.4023 - val_loss: 879839872.0000 - val_rmse: 29662.0938\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305053536.0000 - rmse: 17465.7793 - val_loss: 561533184.0000 - val_rmse: 23696.6895\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338077280.0000 - rmse: 18386.8770 - val_loss: 397965632.0000 - val_rmse: 19949.0742\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383017664.0000 - rmse: 19570.8359 - val_loss: 496957088.0000 - val_rmse: 22292.5312\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275439776.0000 - rmse: 16596.3750 - val_loss: 610945472.0000 - val_rmse: 24717.3105\n",
      "104/104 [==============================] - 0s 667us/step - loss: 795265024.0000 - rmse: 28200.4434\n",
      "[795265024.0, 28200.443359375]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 6844017664.0000 - rmse: 82728.5781 - val_loss: 1409244672.0000 - val_rmse: 37539.9062\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1777308800.0000 - rmse: 42158.1406 - val_loss: 1086896256.0000 - val_rmse: 32968.1094\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1647978880.0000 - rmse: 40595.3047 - val_loss: 968827840.0000 - val_rmse: 31126.0000\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1427098496.0000 - rmse: 37776.9570 - val_loss: 904048576.0000 - val_rmse: 30067.4004\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1432306560.0000 - rmse: 37845.8242 - val_loss: 972123456.0000 - val_rmse: 31178.8945\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1349068032.0000 - rmse: 36729.6602 - val_loss: 837090944.0000 - val_rmse: 28932.5234\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1269721600.0000 - rmse: 35633.1523 - val_loss: 863669120.0000 - val_rmse: 29388.2480\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1214700160.0000 - rmse: 34852.5508 - val_loss: 853361344.0000 - val_rmse: 29212.3496\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1260538880.0000 - rmse: 35504.0703 - val_loss: 836662464.0000 - val_rmse: 28925.1191\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1161753472.0000 - rmse: 34084.5039 - val_loss: 1385705472.0000 - val_rmse: 37225.0664\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1077896192.0000 - rmse: 32831.3281 - val_loss: 855708608.0000 - val_rmse: 29252.4980\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1070815296.0000 - rmse: 32723.3145 - val_loss: 863826688.0000 - val_rmse: 29390.9277\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1063918144.0000 - rmse: 32617.7578 - val_loss: 1127278080.0000 - val_rmse: 33574.9609\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1053678016.0000 - rmse: 32460.4062 - val_loss: 833919744.0000 - val_rmse: 28877.6680\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1033404352.0000 - rmse: 32146.6055 - val_loss: 959838912.0000 - val_rmse: 30981.2676\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 996227328.0000 - rmse: 31563.0684 - val_loss: 822546176.0000 - val_rmse: 28680.0664\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1008258880.0000 - rmse: 31753.0918 - val_loss: 1276954112.0000 - val_rmse: 35734.4961\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 974184192.0000 - rmse: 31211.9238 - val_loss: 861814848.0000 - val_rmse: 29356.6836\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 884884672.0000 - rmse: 29747.0117 - val_loss: 815913280.0000 - val_rmse: 28564.1934\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 855525312.0000 - rmse: 29249.3652 - val_loss: 751995776.0000 - val_rmse: 27422.5410\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 847488640.0000 - rmse: 29111.6562 - val_loss: 749000000.0000 - val_rmse: 27367.8652\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 900445568.0000 - rmse: 30007.4258 - val_loss: 735654464.0000 - val_rmse: 27122.9512\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 865585984.0000 - rmse: 29420.8398 - val_loss: 719325376.0000 - val_rmse: 26820.2422\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 737610432.0000 - rmse: 27158.9824 - val_loss: 759384192.0000 - val_rmse: 27556.9258\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 845596416.0000 - rmse: 29079.1406 - val_loss: 766126528.0000 - val_rmse: 27678.9902\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751490752.0000 - rmse: 27413.3320 - val_loss: 717069568.0000 - val_rmse: 26778.1543\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 752095168.0000 - rmse: 27424.3535 - val_loss: 810715712.0000 - val_rmse: 28473.0684\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 739061568.0000 - rmse: 27185.6855 - val_loss: 710088128.0000 - val_rmse: 26647.4785\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729786112.0000 - rmse: 27014.5527 - val_loss: 797779648.0000 - val_rmse: 28244.9941\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 650518656.0000 - rmse: 25505.2656 - val_loss: 1034304512.0000 - val_rmse: 32160.6055\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 745521216.0000 - rmse: 27304.2344 - val_loss: 804987520.0000 - val_rmse: 28372.3008\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656892672.0000 - rmse: 25629.9180 - val_loss: 801205568.0000 - val_rmse: 28305.5742\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 643961408.0000 - rmse: 25376.3945 - val_loss: 752918720.0000 - val_rmse: 27439.3652\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 660617472.0000 - rmse: 25702.4805 - val_loss: 613732928.0000 - val_rmse: 24773.6328\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 697740544.0000 - rmse: 26414.7773 - val_loss: 699114048.0000 - val_rmse: 26440.7656\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610936768.0000 - rmse: 24717.1348 - val_loss: 876675008.0000 - val_rmse: 29608.6973\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517182016.0000 - rmse: 22741.6348 - val_loss: 1031504640.0000 - val_rmse: 32117.0449\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576868288.0000 - rmse: 24018.0820 - val_loss: 985306112.0000 - val_rmse: 31389.5859\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691069632.0000 - rmse: 26288.2031 - val_loss: 832495296.0000 - val_rmse: 28852.9941\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612363072.0000 - rmse: 24745.9707 - val_loss: 684816768.0000 - val_rmse: 26169.0039\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548022400.0000 - rmse: 23409.8770 - val_loss: 645230336.0000 - val_rmse: 25401.3848\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584783936.0000 - rmse: 24182.3066 - val_loss: 792392512.0000 - val_rmse: 28149.4668\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607021888.0000 - rmse: 24637.8145 - val_loss: 562403520.0000 - val_rmse: 23715.0469\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594624768.0000 - rmse: 24384.9277 - val_loss: 603759552.0000 - val_rmse: 24571.5176\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541423296.0000 - rmse: 23268.5039 - val_loss: 759983872.0000 - val_rmse: 27567.8047\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541781696.0000 - rmse: 23276.2051 - val_loss: 981439168.0000 - val_rmse: 31327.9297\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486928864.0000 - rmse: 22066.4648 - val_loss: 844350720.0000 - val_rmse: 29057.7129\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547644544.0000 - rmse: 23401.8047 - val_loss: 649831680.0000 - val_rmse: 25491.7969\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498828320.0000 - rmse: 22334.4648 - val_loss: 814796288.0000 - val_rmse: 28544.6348\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523015008.0000 - rmse: 22869.5215 - val_loss: 917377600.0000 - val_rmse: 30288.2383\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557542976.0000 - rmse: 23612.3477 - val_loss: 637381376.0000 - val_rmse: 25246.4102\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499617664.0000 - rmse: 22352.1289 - val_loss: 562680128.0000 - val_rmse: 23720.8789\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484487648.0000 - rmse: 22011.0801 - val_loss: 640144448.0000 - val_rmse: 25301.0742\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426360768.0000 - rmse: 20648.5039 - val_loss: 691987072.0000 - val_rmse: 26305.6465\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506268704.0000 - rmse: 22500.4141 - val_loss: 621378176.0000 - val_rmse: 24927.4590\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511366240.0000 - rmse: 22613.4082 - val_loss: 692749120.0000 - val_rmse: 26320.1270\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514976096.0000 - rmse: 22693.0840 - val_loss: 1453838208.0000 - val_rmse: 38129.2266\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492831008.0000 - rmse: 22199.7969 - val_loss: 536423776.0000 - val_rmse: 23160.8242\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450774304.0000 - rmse: 21231.4453 - val_loss: 1294985216.0000 - val_rmse: 35985.9023\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547061312.0000 - rmse: 23389.3398 - val_loss: 608846848.0000 - val_rmse: 24674.8203\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535774592.0000 - rmse: 23146.8027 - val_loss: 701124928.0000 - val_rmse: 26478.7617\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482976160.0000 - rmse: 21976.7148 - val_loss: 839184448.0000 - val_rmse: 28968.6797\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477258624.0000 - rmse: 21846.2480 - val_loss: 921169856.0000 - val_rmse: 30350.7793\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504279136.0000 - rmse: 22456.1582 - val_loss: 577814144.0000 - val_rmse: 24037.7656\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504709888.0000 - rmse: 22465.7500 - val_loss: 786188992.0000 - val_rmse: 28039.0605\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473744768.0000 - rmse: 21765.6777 - val_loss: 970372736.0000 - val_rmse: 31150.8047\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475323808.0000 - rmse: 21801.9219 - val_loss: 563850880.0000 - val_rmse: 23745.5410\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414102144.0000 - rmse: 20349.5000 - val_loss: 582727680.0000 - val_rmse: 24139.7520\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414730976.0000 - rmse: 20364.9434 - val_loss: 722233024.0000 - val_rmse: 26874.3926\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408005888.0000 - rmse: 20199.1523 - val_loss: 700305024.0000 - val_rmse: 26463.2773\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440461472.0000 - rmse: 20987.1738 - val_loss: 637780608.0000 - val_rmse: 25254.3164\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410704800.0000 - rmse: 20265.8516 - val_loss: 977480640.0000 - val_rmse: 31264.6855\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426985920.0000 - rmse: 20663.6367 - val_loss: 537851008.0000 - val_rmse: 23191.6133\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372765216.0000 - rmse: 19307.1270 - val_loss: 820834688.0000 - val_rmse: 28650.2129\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389407616.0000 - rmse: 19733.4141 - val_loss: 571084032.0000 - val_rmse: 23897.3613\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416821024.0000 - rmse: 20416.1934 - val_loss: 605923712.0000 - val_rmse: 24615.5176\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387587328.0000 - rmse: 19687.2383 - val_loss: 984558144.0000 - val_rmse: 31377.6699\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362696736.0000 - rmse: 19044.5977 - val_loss: 1746720640.0000 - val_rmse: 41793.7852\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442831744.0000 - rmse: 21043.5684 - val_loss: 902510720.0000 - val_rmse: 30041.8164\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392256128.0000 - rmse: 19805.4551 - val_loss: 868617536.0000 - val_rmse: 29472.3164\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444446816.0000 - rmse: 21081.9062 - val_loss: 827006144.0000 - val_rmse: 28757.7129\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363997504.0000 - rmse: 19078.7168 - val_loss: 536827264.0000 - val_rmse: 23169.5332\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407908224.0000 - rmse: 20196.7363 - val_loss: 638078336.0000 - val_rmse: 25260.2129\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393830208.0000 - rmse: 19845.1543 - val_loss: 726862528.0000 - val_rmse: 26960.3887\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321842144.0000 - rmse: 17939.9590 - val_loss: 1511172224.0000 - val_rmse: 38873.7969\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469339840.0000 - rmse: 21664.2520 - val_loss: 945556096.0000 - val_rmse: 30749.8965\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330148256.0000 - rmse: 18169.9805 - val_loss: 703971648.0000 - val_rmse: 26532.4648\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398981056.0000 - rmse: 19974.5098 - val_loss: 831833536.0000 - val_rmse: 28841.5254\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378585280.0000 - rmse: 19457.2656 - val_loss: 849039616.0000 - val_rmse: 29138.2852\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351373856.0000 - rmse: 18744.9668 - val_loss: 718690368.0000 - val_rmse: 26808.3984\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376416320.0000 - rmse: 19401.4492 - val_loss: 592377344.0000 - val_rmse: 24338.8008\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368630784.0000 - rmse: 19199.7598 - val_loss: 614321792.0000 - val_rmse: 24785.5137\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355982464.0000 - rmse: 18867.4961 - val_loss: 599210560.0000 - val_rmse: 24478.7773\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435896320.0000 - rmse: 20878.1270 - val_loss: 642051904.0000 - val_rmse: 25338.7422\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419650464.0000 - rmse: 20485.3691 - val_loss: 842516992.0000 - val_rmse: 29026.1426\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344714496.0000 - rmse: 18566.4883 - val_loss: 859594944.0000 - val_rmse: 29318.8477\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422108992.0000 - rmse: 20545.2891 - val_loss: 1001200320.0000 - val_rmse: 31641.7500\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362441248.0000 - rmse: 19037.8887 - val_loss: 874841856.0000 - val_rmse: 29577.7266\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340614208.0000 - rmse: 18455.7344 - val_loss: 506756640.0000 - val_rmse: 22511.2539\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338216480.0000 - rmse: 18390.6602 - val_loss: 1079191808.0000 - val_rmse: 32851.0547\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340272832.0000 - rmse: 18446.4824 - val_loss: 744984320.0000 - val_rmse: 27294.4004\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346256800.0000 - rmse: 18607.9766 - val_loss: 1013777472.0000 - val_rmse: 31839.8730\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398881312.0000 - rmse: 19972.0117 - val_loss: 781939392.0000 - val_rmse: 27963.1797\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375436672.0000 - rmse: 19376.1875 - val_loss: 1184893056.0000 - val_rmse: 34422.2773\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362102784.0000 - rmse: 19028.9961 - val_loss: 945825024.0000 - val_rmse: 30754.2676\n",
      "104/104 [==============================] - 0s 685us/step - loss: 327946432.0000 - rmse: 18109.2871\n",
      "[327946432.0, 18109.287109375]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 6349662208.0000 - rmse: 79684.7656 - val_loss: 1298701824.0000 - val_rmse: 36037.5039\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1894648704.0000 - rmse: 43527.5625 - val_loss: 1310840832.0000 - val_rmse: 36205.5352\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1649405824.0000 - rmse: 40612.8789 - val_loss: 1451361152.0000 - val_rmse: 38096.7344\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1523827200.0000 - rmse: 39036.2305 - val_loss: 944443200.0000 - val_rmse: 30731.7949\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1406524544.0000 - rmse: 37503.6602 - val_loss: 901941632.0000 - val_rmse: 30032.3438\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1346187136.0000 - rmse: 36690.4219 - val_loss: 975724288.0000 - val_rmse: 31236.5859\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1323005056.0000 - rmse: 36373.1367 - val_loss: 1023455424.0000 - val_rmse: 31991.4902\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1239888384.0000 - rmse: 35212.0508 - val_loss: 1071484800.0000 - val_rmse: 32733.5430\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1211586816.0000 - rmse: 34807.8555 - val_loss: 811802304.0000 - val_rmse: 28492.1445\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1111858560.0000 - rmse: 33344.5430 - val_loss: 818164224.0000 - val_rmse: 28603.5703\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1114699392.0000 - rmse: 33387.1133 - val_loss: 829256576.0000 - val_rmse: 28796.8145\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1053762304.0000 - rmse: 32461.7051 - val_loss: 821221504.0000 - val_rmse: 28656.9629\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1015306624.0000 - rmse: 31863.8770 - val_loss: 912591680.0000 - val_rmse: 30209.1328\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1026637760.0000 - rmse: 32041.1895 - val_loss: 750996224.0000 - val_rmse: 27404.3105\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 987780032.0000 - rmse: 31428.9688 - val_loss: 876859520.0000 - val_rmse: 29611.8145\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 927242304.0000 - rmse: 30450.6543 - val_loss: 829258432.0000 - val_rmse: 28796.8457\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 971333248.0000 - rmse: 31166.2188 - val_loss: 1241358080.0000 - val_rmse: 35232.9102\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 943216064.0000 - rmse: 30711.8223 - val_loss: 778090688.0000 - val_rmse: 27894.2773\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 959737280.0000 - rmse: 30979.6270 - val_loss: 849663104.0000 - val_rmse: 29148.9805\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 819311424.0000 - rmse: 28623.6172 - val_loss: 834597056.0000 - val_rmse: 28889.3926\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 805459968.0000 - rmse: 28380.6270 - val_loss: 796539776.0000 - val_rmse: 28223.0371\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 822350848.0000 - rmse: 28676.6602 - val_loss: 724363904.0000 - val_rmse: 26914.0098\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 840953216.0000 - rmse: 28999.1914 - val_loss: 748524992.0000 - val_rmse: 27359.1836\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 863075200.0000 - rmse: 29378.1406 - val_loss: 762655360.0000 - val_rmse: 27616.2148\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800818112.0000 - rmse: 28298.7285 - val_loss: 837534464.0000 - val_rmse: 28940.1875\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782834816.0000 - rmse: 27979.1855 - val_loss: 1005476544.0000 - val_rmse: 31709.2500\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 777093696.0000 - rmse: 27876.3984 - val_loss: 812713984.0000 - val_rmse: 28508.1387\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 728495808.0000 - rmse: 26990.6602 - val_loss: 944680000.0000 - val_rmse: 30735.6465\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637805888.0000 - rmse: 25254.8184 - val_loss: 757264192.0000 - val_rmse: 27518.4336\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 763040448.0000 - rmse: 27623.1875 - val_loss: 785442048.0000 - val_rmse: 28025.7383\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591347008.0000 - rmse: 24317.6270 - val_loss: 825754048.0000 - val_rmse: 28735.9336\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573198208.0000 - rmse: 23941.5586 - val_loss: 640047168.0000 - val_rmse: 25299.1543\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 700802048.0000 - rmse: 26472.6641 - val_loss: 734200768.0000 - val_rmse: 27096.1367\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634159616.0000 - rmse: 25182.5254 - val_loss: 739480256.0000 - val_rmse: 27193.3848\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 658232192.0000 - rmse: 25656.0352 - val_loss: 963359936.0000 - val_rmse: 31038.0410\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 613260928.0000 - rmse: 24764.1055 - val_loss: 682135680.0000 - val_rmse: 26117.7266\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676792192.0000 - rmse: 26015.2285 - val_loss: 607095808.0000 - val_rmse: 24639.3145\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537982528.0000 - rmse: 23194.4512 - val_loss: 632048320.0000 - val_rmse: 25140.5703\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524004096.0000 - rmse: 22891.1348 - val_loss: 910967360.0000 - val_rmse: 30182.2363\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548761664.0000 - rmse: 23425.6621 - val_loss: 1042466624.0000 - val_rmse: 32287.2520\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598237376.0000 - rmse: 24458.8906 - val_loss: 770436608.0000 - val_rmse: 27756.7383\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545962880.0000 - rmse: 23365.8477 - val_loss: 1074141696.0000 - val_rmse: 32774.0977\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630871808.0000 - rmse: 25117.1602 - val_loss: 783734208.0000 - val_rmse: 27995.2539\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586635328.0000 - rmse: 24220.5547 - val_loss: 623039872.0000 - val_rmse: 24960.7656\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518470944.0000 - rmse: 22769.9570 - val_loss: 893230720.0000 - val_rmse: 29886.9648\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573947200.0000 - rmse: 23957.1953 - val_loss: 918783040.0000 - val_rmse: 30311.4336\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544064128.0000 - rmse: 23325.1816 - val_loss: 698961216.0000 - val_rmse: 26437.8750\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552114880.0000 - rmse: 23497.1250 - val_loss: 674966592.0000 - val_rmse: 25980.1191\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 581850816.0000 - rmse: 24121.5840 - val_loss: 725023680.0000 - val_rmse: 26926.2637\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515721120.0000 - rmse: 22709.4941 - val_loss: 774909760.0000 - val_rmse: 27837.1992\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548161728.0000 - rmse: 23412.8535 - val_loss: 695180992.0000 - val_rmse: 26366.2832\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548452352.0000 - rmse: 23419.0586 - val_loss: 1004668288.0000 - val_rmse: 31696.5020\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529766144.0000 - rmse: 23016.6484 - val_loss: 624442880.0000 - val_rmse: 24988.8555\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470415264.0000 - rmse: 21689.0586 - val_loss: 630174656.0000 - val_rmse: 25103.2773\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534436864.0000 - rmse: 23117.8906 - val_loss: 682918592.0000 - val_rmse: 26132.7090\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517395680.0000 - rmse: 22746.3340 - val_loss: 1615098752.0000 - val_rmse: 40188.2930\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420967552.0000 - rmse: 20517.4941 - val_loss: 1007449984.0000 - val_rmse: 31740.3535\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515092160.0000 - rmse: 22695.6426 - val_loss: 944692032.0000 - val_rmse: 30735.8418\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493805120.0000 - rmse: 22221.7246 - val_loss: 801231552.0000 - val_rmse: 28306.0332\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531747648.0000 - rmse: 23059.6543 - val_loss: 586302848.0000 - val_rmse: 24213.6895\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556009792.0000 - rmse: 23579.8594 - val_loss: 640601024.0000 - val_rmse: 25310.0977\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465146752.0000 - rmse: 21567.2598 - val_loss: 772055936.0000 - val_rmse: 27785.8926\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466919424.0000 - rmse: 21608.3184 - val_loss: 786908096.0000 - val_rmse: 28051.8809\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454267712.0000 - rmse: 21313.5566 - val_loss: 705679744.0000 - val_rmse: 26564.6328\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522905408.0000 - rmse: 22867.1230 - val_loss: 920540544.0000 - val_rmse: 30340.4082\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416682208.0000 - rmse: 20412.7949 - val_loss: 640360320.0000 - val_rmse: 25305.3418\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454320896.0000 - rmse: 21314.8047 - val_loss: 711601024.0000 - val_rmse: 26675.8516\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435287584.0000 - rmse: 20863.5449 - val_loss: 946609216.0000 - val_rmse: 30767.0156\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451470304.0000 - rmse: 21247.8281 - val_loss: 656487040.0000 - val_rmse: 25622.0039\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437714304.0000 - rmse: 20921.6230 - val_loss: 1113693696.0000 - val_rmse: 33372.0508\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423029792.0000 - rmse: 20567.6875 - val_loss: 863029952.0000 - val_rmse: 29377.3711\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408503808.0000 - rmse: 20211.4746 - val_loss: 724203328.0000 - val_rmse: 26911.0254\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500641216.0000 - rmse: 22375.0137 - val_loss: 572219712.0000 - val_rmse: 23921.1152\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470093344.0000 - rmse: 21681.6348 - val_loss: 901688896.0000 - val_rmse: 30028.1348\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441141312.0000 - rmse: 21003.3652 - val_loss: 789331904.0000 - val_rmse: 28095.0508\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411361312.0000 - rmse: 20282.0430 - val_loss: 831440000.0000 - val_rmse: 28834.6992\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445600320.0000 - rmse: 21109.2461 - val_loss: 701550848.0000 - val_rmse: 26486.8047\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406585376.0000 - rmse: 20163.9609 - val_loss: 981243136.0000 - val_rmse: 31324.8008\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469429024.0000 - rmse: 21666.3086 - val_loss: 804507904.0000 - val_rmse: 28363.8477\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379634496.0000 - rmse: 19484.2109 - val_loss: 808289984.0000 - val_rmse: 28430.4375\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470580192.0000 - rmse: 21692.8594 - val_loss: 863449344.0000 - val_rmse: 29384.5059\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397809088.0000 - rmse: 19945.1504 - val_loss: 647045568.0000 - val_rmse: 25437.0898\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383639584.0000 - rmse: 19586.7188 - val_loss: 670726784.0000 - val_rmse: 25898.3926\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459449856.0000 - rmse: 21434.7812 - val_loss: 690777856.0000 - val_rmse: 26282.6523\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430636352.0000 - rmse: 20751.7773 - val_loss: 640045568.0000 - val_rmse: 25299.1211\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357745216.0000 - rmse: 18914.1543 - val_loss: 694021312.0000 - val_rmse: 26344.2832\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373040160.0000 - rmse: 19314.2461 - val_loss: 555167488.0000 - val_rmse: 23561.9922\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493249184.0000 - rmse: 22209.2129 - val_loss: 726510912.0000 - val_rmse: 26953.8652\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378719488.0000 - rmse: 19460.7148 - val_loss: 713768448.0000 - val_rmse: 26716.4434\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369855840.0000 - rmse: 19231.6348 - val_loss: 795111488.0000 - val_rmse: 28197.7207\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443600736.0000 - rmse: 21061.8301 - val_loss: 1002244864.0000 - val_rmse: 31658.2520\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503216480.0000 - rmse: 22432.4863 - val_loss: 665873728.0000 - val_rmse: 25804.5293\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461074912.0000 - rmse: 21472.6543 - val_loss: 769032768.0000 - val_rmse: 27731.4395\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397480096.0000 - rmse: 19936.9004 - val_loss: 735706368.0000 - val_rmse: 27123.9082\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337023744.0000 - rmse: 18358.2051 - val_loss: 799022720.0000 - val_rmse: 28266.9883\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405830016.0000 - rmse: 20145.2207 - val_loss: 792695872.0000 - val_rmse: 28154.8555\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358529888.0000 - rmse: 18934.8848 - val_loss: 746570560.0000 - val_rmse: 27323.4414\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384459808.0000 - rmse: 19607.6465 - val_loss: 701502976.0000 - val_rmse: 26485.9023\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387106624.0000 - rmse: 19675.0254 - val_loss: 1347245312.0000 - val_rmse: 36704.8398\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382080064.0000 - rmse: 19546.8652 - val_loss: 616729088.0000 - val_rmse: 24834.0312\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384038400.0000 - rmse: 19596.8965 - val_loss: 789477312.0000 - val_rmse: 28097.6387\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356514048.0000 - rmse: 18881.5781 - val_loss: 1265916416.0000 - val_rmse: 35579.7188\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380323584.0000 - rmse: 19501.8848 - val_loss: 673195392.0000 - val_rmse: 25946.0059\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395665472.0000 - rmse: 19891.3398 - val_loss: 789123648.0000 - val_rmse: 28091.3418\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375235776.0000 - rmse: 19371.0020 - val_loss: 713743936.0000 - val_rmse: 26715.9863\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328947200.0000 - rmse: 18136.9004 - val_loss: 967610560.0000 - val_rmse: 31106.4395\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321580736.0000 - rmse: 17932.6719 - val_loss: 819029184.0000 - val_rmse: 28618.6855\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375780288.0000 - rmse: 19385.0508 - val_loss: 680680576.0000 - val_rmse: 26089.8555\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352522016.0000 - rmse: 18775.5684 - val_loss: 819269504.0000 - val_rmse: 28622.8828\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318935744.0000 - rmse: 17858.7695 - val_loss: 820122880.0000 - val_rmse: 28637.7871\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306906720.0000 - rmse: 17518.7500 - val_loss: 878648832.0000 - val_rmse: 29642.0117\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341837824.0000 - rmse: 18488.8555 - val_loss: 1073911040.0000 - val_rmse: 32770.5781\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382889152.0000 - rmse: 19567.5527 - val_loss: 723294144.0000 - val_rmse: 26894.1270\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322838784.0000 - rmse: 17967.7129 - val_loss: 717963072.0000 - val_rmse: 26794.8320\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384812544.0000 - rmse: 19616.6387 - val_loss: 788676672.0000 - val_rmse: 28083.3848\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347925120.0000 - rmse: 18652.7480 - val_loss: 676104320.0000 - val_rmse: 26002.0059\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373685920.0000 - rmse: 19330.9551 - val_loss: 834592448.0000 - val_rmse: 28889.3105\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339159520.0000 - rmse: 18416.2812 - val_loss: 733595392.0000 - val_rmse: 27084.9668\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340052256.0000 - rmse: 18440.5039 - val_loss: 805511424.0000 - val_rmse: 28381.5332\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349756896.0000 - rmse: 18701.7852 - val_loss: 675061440.0000 - val_rmse: 25981.9414\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313959488.0000 - rmse: 17718.9004 - val_loss: 792725952.0000 - val_rmse: 28155.3887\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306441728.0000 - rmse: 17505.4746 - val_loss: 706189696.0000 - val_rmse: 26574.2305\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321124320.0000 - rmse: 17919.9414 - val_loss: 667206784.0000 - val_rmse: 25830.3438\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318566752.0000 - rmse: 17848.4375 - val_loss: 702386240.0000 - val_rmse: 26502.5684\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317490720.0000 - rmse: 17818.2676 - val_loss: 748228224.0000 - val_rmse: 27353.7598\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311198016.0000 - rmse: 17640.8027 - val_loss: 762211840.0000 - val_rmse: 27608.1836\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313253632.0000 - rmse: 17698.9727 - val_loss: 754295936.0000 - val_rmse: 27464.4453\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375959072.0000 - rmse: 19389.6641 - val_loss: 888829696.0000 - val_rmse: 29813.2441\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365128928.0000 - rmse: 19108.3457 - val_loss: 674879744.0000 - val_rmse: 25978.4473\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315512384.0000 - rmse: 17762.6641 - val_loss: 630568512.0000 - val_rmse: 25111.1211\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345396544.0000 - rmse: 18584.8457 - val_loss: 699948352.0000 - val_rmse: 26456.5371\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326185920.0000 - rmse: 18060.6172 - val_loss: 707159872.0000 - val_rmse: 26592.4746\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287309472.0000 - rmse: 16950.2031 - val_loss: 674299456.0000 - val_rmse: 25967.2734\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311671008.0000 - rmse: 17654.2051 - val_loss: 690269952.0000 - val_rmse: 26272.9863\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286790560.0000 - rmse: 16934.8926 - val_loss: 641171584.0000 - val_rmse: 25321.3652\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320595008.0000 - rmse: 17905.1660 - val_loss: 742491008.0000 - val_rmse: 27248.6875\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330245568.0000 - rmse: 18172.6582 - val_loss: 793699776.0000 - val_rmse: 28172.6777\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308092160.0000 - rmse: 17552.5527 - val_loss: 733494208.0000 - val_rmse: 27083.0977\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381533376.0000 - rmse: 19532.8770 - val_loss: 807669120.0000 - val_rmse: 28419.5195\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364422304.0000 - rmse: 19089.8457 - val_loss: 874043776.0000 - val_rmse: 29564.2305\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318478528.0000 - rmse: 17845.9648 - val_loss: 736750400.0000 - val_rmse: 27143.1445\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297202272.0000 - rmse: 17239.5547 - val_loss: 731824704.0000 - val_rmse: 27052.2578\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292473152.0000 - rmse: 17101.8438 - val_loss: 691319104.0000 - val_rmse: 26292.9453\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313335072.0000 - rmse: 17701.2695 - val_loss: 939364864.0000 - val_rmse: 30649.0586\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310728224.0000 - rmse: 17627.4824 - val_loss: 678339008.0000 - val_rmse: 26044.9414\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296752672.0000 - rmse: 17226.5098 - val_loss: 754347328.0000 - val_rmse: 27465.3809\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316899040.0000 - rmse: 17801.6562 - val_loss: 651987776.0000 - val_rmse: 25534.0488\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282346848.0000 - rmse: 16803.1777 - val_loss: 716698176.0000 - val_rmse: 26771.2188\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391014400.0000 - rmse: 19774.0820 - val_loss: 764559680.0000 - val_rmse: 27650.6719\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311595424.0000 - rmse: 17652.0625 - val_loss: 740485696.0000 - val_rmse: 27211.8672\n",
      "104/104 [==============================] - 0s 691us/step - loss: 474665280.0000 - rmse: 21786.8145\n",
      "[474665280.0, 21786.814453125]\n",
      "[22501.7109375, 29092.7109375, 28200.443359375, 18109.287109375, 21786.814453125]\n",
      "23938.193359375\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "!python train.py kfold light"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 19:28:17.336703: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 19:28:17.336743: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 19:28:17.337061: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 24,065\n",
      "Trainable params: 24,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 24,065\n",
      "Trainable params: 24,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 19:28:17.531287: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 18809147392.0000 - rmse: 137146.4375 - val_loss: 5441876992.0000 - val_rmse: 73769.0781\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 3265982720.0000 - rmse: 57148.7773 - val_loss: 1603885952.0000 - val_rmse: 40048.5469\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2151898880.0000 - rmse: 46388.5625 - val_loss: 1168654208.0000 - val_rmse: 34185.5859\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1867882240.0000 - rmse: 43219.0039 - val_loss: 1004064320.0000 - val_rmse: 31686.9746\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1787832960.0000 - rmse: 42282.7734 - val_loss: 933256256.0000 - val_rmse: 30549.2422\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1691158272.0000 - rmse: 41123.6953 - val_loss: 889534592.0000 - val_rmse: 29825.0664\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1706446080.0000 - rmse: 41309.1523 - val_loss: 868811968.0000 - val_rmse: 29475.6172\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1547933440.0000 - rmse: 39343.7852 - val_loss: 849837184.0000 - val_rmse: 29151.9668\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1467778304.0000 - rmse: 38311.5938 - val_loss: 819500864.0000 - val_rmse: 28626.9258\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1515420928.0000 - rmse: 38928.4062 - val_loss: 875139264.0000 - val_rmse: 29582.7520\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1471516928.0000 - rmse: 38360.3555 - val_loss: 781724096.0000 - val_rmse: 27959.3301\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1412572800.0000 - rmse: 37584.2109 - val_loss: 742704704.0000 - val_rmse: 27252.6094\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1370022656.0000 - rmse: 37013.8164 - val_loss: 741361792.0000 - val_rmse: 27227.9590\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1364281472.0000 - rmse: 36936.1797 - val_loss: 727455744.0000 - val_rmse: 26971.3867\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1311052416.0000 - rmse: 36208.4570 - val_loss: 785475456.0000 - val_rmse: 28026.3359\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1364440960.0000 - rmse: 36938.3398 - val_loss: 672962240.0000 - val_rmse: 25941.5156\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1325570688.0000 - rmse: 36408.3867 - val_loss: 710904192.0000 - val_rmse: 26662.7871\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1291528448.0000 - rmse: 35937.8398 - val_loss: 679420864.0000 - val_rmse: 26065.7031\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1338966784.0000 - rmse: 36591.8945 - val_loss: 717250048.0000 - val_rmse: 26781.5234\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1264772224.0000 - rmse: 35563.6367 - val_loss: 742052736.0000 - val_rmse: 27240.6445\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1223562368.0000 - rmse: 34979.4570 - val_loss: 689783040.0000 - val_rmse: 26263.7207\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1157526400.0000 - rmse: 34022.4414 - val_loss: 685392832.0000 - val_rmse: 26180.0078\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1174502528.0000 - rmse: 34271.0156 - val_loss: 665546560.0000 - val_rmse: 25798.1895\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1153520640.0000 - rmse: 33963.5195 - val_loss: 667914496.0000 - val_rmse: 25844.0410\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1234576512.0000 - rmse: 35136.5391 - val_loss: 632343744.0000 - val_rmse: 25146.4453\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1211020672.0000 - rmse: 34799.7227 - val_loss: 646099264.0000 - val_rmse: 25418.4824\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1104115328.0000 - rmse: 33228.2305 - val_loss: 609386816.0000 - val_rmse: 24685.7617\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1086573312.0000 - rmse: 32963.2109 - val_loss: 593787456.0000 - val_rmse: 24367.7539\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1144877056.0000 - rmse: 33836.0312 - val_loss: 619804864.0000 - val_rmse: 24895.8809\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1082227584.0000 - rmse: 32897.2266 - val_loss: 606882944.0000 - val_rmse: 24634.9941\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1017871744.0000 - rmse: 31904.1016 - val_loss: 570895488.0000 - val_rmse: 23893.4199\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1033041728.0000 - rmse: 32140.9668 - val_loss: 610107328.0000 - val_rmse: 24700.3516\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1056302144.0000 - rmse: 32500.8027 - val_loss: 633050048.0000 - val_rmse: 25160.4863\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1055613568.0000 - rmse: 32490.2070 - val_loss: 580190592.0000 - val_rmse: 24087.1465\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1005204224.0000 - rmse: 31704.9551 - val_loss: 713497344.0000 - val_rmse: 26711.3711\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1005725056.0000 - rmse: 31713.1680 - val_loss: 538000512.0000 - val_rmse: 23194.8379\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 983556608.0000 - rmse: 31361.7070 - val_loss: 515474048.0000 - val_rmse: 22704.0527\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1008931008.0000 - rmse: 31763.6738 - val_loss: 569693376.0000 - val_rmse: 23868.2500\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 984163264.0000 - rmse: 31371.3770 - val_loss: 534382528.0000 - val_rmse: 23116.7148\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1015343296.0000 - rmse: 31864.4512 - val_loss: 555648000.0000 - val_rmse: 23572.1875\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 966409856.0000 - rmse: 31087.1328 - val_loss: 569967168.0000 - val_rmse: 23873.9844\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 981321920.0000 - rmse: 31326.0586 - val_loss: 513341600.0000 - val_rmse: 22657.0430\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 907739968.0000 - rmse: 30128.7227 - val_loss: 648259264.0000 - val_rmse: 25460.9355\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 883939200.0000 - rmse: 29731.1152 - val_loss: 483709280.0000 - val_rmse: 21993.3926\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 950343232.0000 - rmse: 30827.6367 - val_loss: 506791296.0000 - val_rmse: 22512.0254\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 859503488.0000 - rmse: 29317.2891 - val_loss: 466774400.0000 - val_rmse: 21604.9629\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 906764800.0000 - rmse: 30112.5352 - val_loss: 429026944.0000 - val_rmse: 20712.9648\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 966421632.0000 - rmse: 31087.3223 - val_loss: 446503008.0000 - val_rmse: 21130.6172\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1017601920.0000 - rmse: 31899.8730 - val_loss: 465145888.0000 - val_rmse: 21567.2402\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 879558912.0000 - rmse: 29657.3594 - val_loss: 451930912.0000 - val_rmse: 21258.6660\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 902070784.0000 - rmse: 30034.4941 - val_loss: 547705024.0000 - val_rmse: 23403.0977\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 943065728.0000 - rmse: 30709.3750 - val_loss: 475277632.0000 - val_rmse: 21800.8633\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1014828352.0000 - rmse: 31856.3711 - val_loss: 436974976.0000 - val_rmse: 20903.9473\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 976783232.0000 - rmse: 31253.5312 - val_loss: 469695648.0000 - val_rmse: 21672.4629\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 869473856.0000 - rmse: 29486.8418 - val_loss: 465877568.0000 - val_rmse: 21584.1973\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 846599360.0000 - rmse: 29096.3809 - val_loss: 537794048.0000 - val_rmse: 23190.3867\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 927053504.0000 - rmse: 30447.5527 - val_loss: 464363520.0000 - val_rmse: 21549.0957\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 762843136.0000 - rmse: 27619.6152 - val_loss: 398560576.0000 - val_rmse: 19963.9824\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 890427584.0000 - rmse: 29840.0332 - val_loss: 393573248.0000 - val_rmse: 19838.6797\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 889234112.0000 - rmse: 29820.0293 - val_loss: 378437792.0000 - val_rmse: 19453.4785\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 816217792.0000 - rmse: 28569.5254 - val_loss: 437216896.0000 - val_rmse: 20909.7324\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 791824128.0000 - rmse: 28139.3691 - val_loss: 414299520.0000 - val_rmse: 20354.3496\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 822768000.0000 - rmse: 28683.9336 - val_loss: 423874752.0000 - val_rmse: 20588.2188\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 807413504.0000 - rmse: 28415.0234 - val_loss: 437482848.0000 - val_rmse: 20916.0898\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 814344256.0000 - rmse: 28536.7168 - val_loss: 373794400.0000 - val_rmse: 19333.7637\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 783026816.0000 - rmse: 27982.6172 - val_loss: 493008992.0000 - val_rmse: 22203.8066\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 844004928.0000 - rmse: 29051.7637 - val_loss: 404804448.0000 - val_rmse: 20119.7520\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800439104.0000 - rmse: 28292.0332 - val_loss: 375515872.0000 - val_rmse: 19378.2324\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 816708096.0000 - rmse: 28578.1055 - val_loss: 361489952.0000 - val_rmse: 19012.8887\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 795723968.0000 - rmse: 28208.5801 - val_loss: 427974816.0000 - val_rmse: 20687.5527\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 804514944.0000 - rmse: 28363.9727 - val_loss: 392229024.0000 - val_rmse: 19804.7734\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 764599040.0000 - rmse: 27651.3848 - val_loss: 358564608.0000 - val_rmse: 18935.8027\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 709236864.0000 - rmse: 26631.5020 - val_loss: 385405216.0000 - val_rmse: 19631.7402\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751094656.0000 - rmse: 27406.1055 - val_loss: 402339968.0000 - val_rmse: 20058.4141\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 731566144.0000 - rmse: 27047.4785 - val_loss: 415206080.0000 - val_rmse: 20376.6055\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742040960.0000 - rmse: 27240.4297 - val_loss: 395002688.0000 - val_rmse: 19874.6738\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 798726848.0000 - rmse: 28261.7559 - val_loss: 459084096.0000 - val_rmse: 21426.2480\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 774024832.0000 - rmse: 27821.3027 - val_loss: 337493504.0000 - val_rmse: 18370.9961\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729007488.0000 - rmse: 27000.1387 - val_loss: 403585280.0000 - val_rmse: 20089.4316\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 773689408.0000 - rmse: 27815.2734 - val_loss: 359023872.0000 - val_rmse: 18947.9258\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 743494592.0000 - rmse: 27267.0977 - val_loss: 336081536.0000 - val_rmse: 18332.5273\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 709739392.0000 - rmse: 26640.9336 - val_loss: 340596928.0000 - val_rmse: 18455.2676\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 765858816.0000 - rmse: 27674.1543 - val_loss: 377158944.0000 - val_rmse: 19420.5801\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 789150464.0000 - rmse: 28091.8223 - val_loss: 349118080.0000 - val_rmse: 18684.7012\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 669766720.0000 - rmse: 25879.8516 - val_loss: 368281120.0000 - val_rmse: 19190.6523\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666773504.0000 - rmse: 25821.9570 - val_loss: 373994464.0000 - val_rmse: 19338.9355\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 714331264.0000 - rmse: 26726.9766 - val_loss: 438519424.0000 - val_rmse: 20940.8555\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 705584896.0000 - rmse: 26562.8477 - val_loss: 402176416.0000 - val_rmse: 20054.3359\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641727104.0000 - rmse: 25332.3340 - val_loss: 336686912.0000 - val_rmse: 18349.0312\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 728724736.0000 - rmse: 26994.9023 - val_loss: 330915456.0000 - val_rmse: 18191.0820\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684198336.0000 - rmse: 26157.1855 - val_loss: 305996096.0000 - val_rmse: 17492.7441\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 786511936.0000 - rmse: 28044.8203 - val_loss: 345479840.0000 - val_rmse: 18587.0879\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 764212608.0000 - rmse: 27644.3965 - val_loss: 304453824.0000 - val_rmse: 17448.6055\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 675350208.0000 - rmse: 25987.5020 - val_loss: 388859168.0000 - val_rmse: 19719.5117\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 734159168.0000 - rmse: 27095.3711 - val_loss: 340632096.0000 - val_rmse: 18456.2207\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730971200.0000 - rmse: 27036.4785 - val_loss: 394302304.0000 - val_rmse: 19857.0469\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685578752.0000 - rmse: 26183.5586 - val_loss: 335137216.0000 - val_rmse: 18306.7539\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 645747520.0000 - rmse: 25411.5625 - val_loss: 356795520.0000 - val_rmse: 18889.0312\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 766439872.0000 - rmse: 27684.6504 - val_loss: 309876800.0000 - val_rmse: 17603.3184\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 714301760.0000 - rmse: 26726.4238 - val_loss: 305830720.0000 - val_rmse: 17488.0156\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691443520.0000 - rmse: 26295.3145 - val_loss: 340342848.0000 - val_rmse: 18448.3828\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 625785920.0000 - rmse: 25015.7129 - val_loss: 296069728.0000 - val_rmse: 17206.6777\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647969216.0000 - rmse: 25455.2402 - val_loss: 299191264.0000 - val_rmse: 17297.1465\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640821696.0000 - rmse: 25314.4570 - val_loss: 302458592.0000 - val_rmse: 17391.3359\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642171136.0000 - rmse: 25341.0957 - val_loss: 291197024.0000 - val_rmse: 17064.4961\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 660645568.0000 - rmse: 25703.0273 - val_loss: 307631008.0000 - val_rmse: 17539.4121\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 678931968.0000 - rmse: 26056.3223 - val_loss: 290980320.0000 - val_rmse: 17058.1445\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 708091456.0000 - rmse: 26609.9883 - val_loss: 340764000.0000 - val_rmse: 18459.7949\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652969088.0000 - rmse: 25553.2598 - val_loss: 358516576.0000 - val_rmse: 18934.5332\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649593984.0000 - rmse: 25487.1328 - val_loss: 360349568.0000 - val_rmse: 18982.8750\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 600104448.0000 - rmse: 24497.0293 - val_loss: 391740192.0000 - val_rmse: 19792.4277\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687939712.0000 - rmse: 26228.6055 - val_loss: 326546400.0000 - val_rmse: 18070.5957\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 678999040.0000 - rmse: 26057.6094 - val_loss: 281191520.0000 - val_rmse: 16768.7656\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656662464.0000 - rmse: 25625.4258 - val_loss: 318801344.0000 - val_rmse: 17855.0098\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 657219392.0000 - rmse: 25636.2910 - val_loss: 315076288.0000 - val_rmse: 17750.3887\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 617621760.0000 - rmse: 24851.9980 - val_loss: 399535040.0000 - val_rmse: 19988.3730\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597799616.0000 - rmse: 24449.9414 - val_loss: 333710176.0000 - val_rmse: 18267.7363\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623983488.0000 - rmse: 24979.6621 - val_loss: 301165824.0000 - val_rmse: 17354.1309\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599581056.0000 - rmse: 24486.3438 - val_loss: 284664256.0000 - val_rmse: 16871.9961\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651572160.0000 - rmse: 25525.9121 - val_loss: 294362080.0000 - val_rmse: 17156.9844\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 669659456.0000 - rmse: 25877.7793 - val_loss: 276998304.0000 - val_rmse: 16643.2656\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652224384.0000 - rmse: 25538.6836 - val_loss: 324048224.0000 - val_rmse: 18001.3398\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665790912.0000 - rmse: 25802.9238 - val_loss: 294542880.0000 - val_rmse: 17162.2520\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 658299200.0000 - rmse: 25657.3418 - val_loss: 308348032.0000 - val_rmse: 17559.8418\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 659879488.0000 - rmse: 25688.1191 - val_loss: 309947168.0000 - val_rmse: 17605.3164\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 608623168.0000 - rmse: 24670.2891 - val_loss: 275764160.0000 - val_rmse: 16606.1484\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 617318720.0000 - rmse: 24845.9004 - val_loss: 271652512.0000 - val_rmse: 16481.8848\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642829504.0000 - rmse: 25354.0820 - val_loss: 279654048.0000 - val_rmse: 16722.8594\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 627384320.0000 - rmse: 25047.6406 - val_loss: 291138336.0000 - val_rmse: 17062.7754\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624325568.0000 - rmse: 24986.5078 - val_loss: 316066240.0000 - val_rmse: 17778.2520\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568671360.0000 - rmse: 23846.8320 - val_loss: 284973600.0000 - val_rmse: 16881.1602\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 636065152.0000 - rmse: 25220.3320 - val_loss: 284491584.0000 - val_rmse: 16866.8789\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570798144.0000 - rmse: 23891.3828 - val_loss: 297510784.0000 - val_rmse: 17248.5000\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583204352.0000 - rmse: 24149.6250 - val_loss: 268953728.0000 - val_rmse: 16399.8086\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569618688.0000 - rmse: 23866.6855 - val_loss: 300940640.0000 - val_rmse: 17347.6406\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 645067776.0000 - rmse: 25398.1836 - val_loss: 275323488.0000 - val_rmse: 16592.8750\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535166048.0000 - rmse: 23133.6562 - val_loss: 330929952.0000 - val_rmse: 18191.4805\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552197568.0000 - rmse: 23498.8848 - val_loss: 269679456.0000 - val_rmse: 16421.9199\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621389952.0000 - rmse: 24927.6953 - val_loss: 342800064.0000 - val_rmse: 18514.8613\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585789568.0000 - rmse: 24203.0898 - val_loss: 269169536.0000 - val_rmse: 16406.3867\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651948992.0000 - rmse: 25533.2910 - val_loss: 252919440.0000 - val_rmse: 15903.4414\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568662464.0000 - rmse: 23846.6445 - val_loss: 336542752.0000 - val_rmse: 18345.1016\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586388544.0000 - rmse: 24215.4609 - val_loss: 296032224.0000 - val_rmse: 17205.5879\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566525632.0000 - rmse: 23801.7988 - val_loss: 260458304.0000 - val_rmse: 16138.7207\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640538368.0000 - rmse: 25308.8594 - val_loss: 261393136.0000 - val_rmse: 16167.6572\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529265440.0000 - rmse: 23005.7695 - val_loss: 320279872.0000 - val_rmse: 17896.3652\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583787328.0000 - rmse: 24161.6914 - val_loss: 254155696.0000 - val_rmse: 15942.2617\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548962368.0000 - rmse: 23429.9453 - val_loss: 338625696.0000 - val_rmse: 18401.7852\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547756160.0000 - rmse: 23404.1914 - val_loss: 281982976.0000 - val_rmse: 16792.3496\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610174656.0000 - rmse: 24701.7129 - val_loss: 280879424.0000 - val_rmse: 16759.4570\n",
      "104/104 [==============================] - 0s 730us/step - loss: 451508512.0000 - rmse: 21248.7305\n",
      "[451508512.0, 21248.73046875]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 24,065\n",
      "Trainable params: 24,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 24,065\n",
      "Trainable params: 24,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 17373771776.0000 - rmse: 131809.6094 - val_loss: 3767042560.0000 - val_rmse: 61376.2383\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2801081856.0000 - rmse: 52925.2461 - val_loss: 1499437056.0000 - val_rmse: 38722.5664\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1753186432.0000 - rmse: 41871.0703 - val_loss: 1220409728.0000 - val_rmse: 34934.3633\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1554263680.0000 - rmse: 39424.1523 - val_loss: 1156258176.0000 - val_rmse: 34003.7969\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1512802048.0000 - rmse: 38894.7578 - val_loss: 1127235712.0000 - val_rmse: 33574.3320\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1455799680.0000 - rmse: 38154.9414 - val_loss: 1112235776.0000 - val_rmse: 33350.1992\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1346707840.0000 - rmse: 36697.5195 - val_loss: 1085148544.0000 - val_rmse: 32941.5938\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1293246848.0000 - rmse: 35961.7422 - val_loss: 1058671744.0000 - val_rmse: 32537.2363\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1305488128.0000 - rmse: 36131.5391 - val_loss: 1054526016.0000 - val_rmse: 32473.4668\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1251586816.0000 - rmse: 35377.7734 - val_loss: 1107891456.0000 - val_rmse: 33285.0039\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1271252864.0000 - rmse: 35654.6328 - val_loss: 939913792.0000 - val_rmse: 30658.0137\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1253216000.0000 - rmse: 35400.7891 - val_loss: 928411776.0000 - val_rmse: 30469.8496\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1203373952.0000 - rmse: 34689.6797 - val_loss: 880179136.0000 - val_rmse: 29667.8125\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1218660096.0000 - rmse: 34909.3125 - val_loss: 884988672.0000 - val_rmse: 29748.7598\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1182811904.0000 - rmse: 34392.0312 - val_loss: 845133376.0000 - val_rmse: 29071.1777\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1101703680.0000 - rmse: 33191.9219 - val_loss: 822437376.0000 - val_rmse: 28678.1699\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1114654464.0000 - rmse: 33386.4414 - val_loss: 824700736.0000 - val_rmse: 28717.6035\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1028378368.0000 - rmse: 32068.3398 - val_loss: 810413120.0000 - val_rmse: 28467.7559\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1073980800.0000 - rmse: 32771.6445 - val_loss: 807286848.0000 - val_rmse: 28412.7930\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1059134528.0000 - rmse: 32544.3477 - val_loss: 761912896.0000 - val_rmse: 27602.7695\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1047624512.0000 - rmse: 32367.0273 - val_loss: 768910592.0000 - val_rmse: 27729.2363\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1076449280.0000 - rmse: 32809.2852 - val_loss: 740455808.0000 - val_rmse: 27211.3184\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1051058240.0000 - rmse: 32420.0293 - val_loss: 744195456.0000 - val_rmse: 27279.9453\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1036526144.0000 - rmse: 32195.1270 - val_loss: 725462016.0000 - val_rmse: 26934.4023\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 993271040.0000 - rmse: 31516.2031 - val_loss: 783697792.0000 - val_rmse: 27994.6035\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 973737536.0000 - rmse: 31204.7676 - val_loss: 715741120.0000 - val_rmse: 26753.3379\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 959582848.0000 - rmse: 30977.1348 - val_loss: 737677440.0000 - val_rmse: 27160.2188\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 951643200.0000 - rmse: 30848.7148 - val_loss: 755810752.0000 - val_rmse: 27492.0117\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 956535488.0000 - rmse: 30927.9082 - val_loss: 674364096.0000 - val_rmse: 25968.5215\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 948623168.0000 - rmse: 30799.7266 - val_loss: 683076544.0000 - val_rmse: 26135.7324\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 915257472.0000 - rmse: 30253.2227 - val_loss: 672551296.0000 - val_rmse: 25933.5938\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 914429120.0000 - rmse: 30239.5293 - val_loss: 707715904.0000 - val_rmse: 26602.9297\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 898483648.0000 - rmse: 29974.7168 - val_loss: 657920320.0000 - val_rmse: 25649.9570\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 930544896.0000 - rmse: 30504.8340 - val_loss: 675051200.0000 - val_rmse: 25981.7480\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 908617984.0000 - rmse: 30143.2910 - val_loss: 668979200.0000 - val_rmse: 25864.6328\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 920620416.0000 - rmse: 30341.7266 - val_loss: 642352896.0000 - val_rmse: 25344.6816\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 817893312.0000 - rmse: 28598.8340 - val_loss: 630204544.0000 - val_rmse: 25103.8750\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 861014528.0000 - rmse: 29343.0488 - val_loss: 629879872.0000 - val_rmse: 25097.4082\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 845818752.0000 - rmse: 29082.9629 - val_loss: 664444160.0000 - val_rmse: 25776.8145\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 809104256.0000 - rmse: 28444.7578 - val_loss: 634731904.0000 - val_rmse: 25193.8867\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 810214080.0000 - rmse: 28464.2598 - val_loss: 627297216.0000 - val_rmse: 25045.9023\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 790506368.0000 - rmse: 28115.9453 - val_loss: 609273088.0000 - val_rmse: 24683.4570\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 846469760.0000 - rmse: 29094.1543 - val_loss: 623689024.0000 - val_rmse: 24973.7676\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 819141184.0000 - rmse: 28620.6426 - val_loss: 590293696.0000 - val_rmse: 24295.9609\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 814006656.0000 - rmse: 28530.8027 - val_loss: 586663808.0000 - val_rmse: 24221.1445\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 799719424.0000 - rmse: 28279.3105 - val_loss: 586033920.0000 - val_rmse: 24208.1367\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 816620928.0000 - rmse: 28576.5801 - val_loss: 586803968.0000 - val_rmse: 24224.0371\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 785355840.0000 - rmse: 28024.2012 - val_loss: 581658560.0000 - val_rmse: 24117.5977\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721533056.0000 - rmse: 26861.3672 - val_loss: 613936576.0000 - val_rmse: 24777.7441\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 755834816.0000 - rmse: 27492.4492 - val_loss: 606565696.0000 - val_rmse: 24628.5547\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 789438976.0000 - rmse: 28096.9570 - val_loss: 588693184.0000 - val_rmse: 24263.0000\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 818585216.0000 - rmse: 28610.9277 - val_loss: 663490560.0000 - val_rmse: 25758.3105\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 694930816.0000 - rmse: 26361.5410 - val_loss: 603772736.0000 - val_rmse: 24571.7871\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 780156032.0000 - rmse: 27931.2734 - val_loss: 562740352.0000 - val_rmse: 23722.1484\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 689904960.0000 - rmse: 26266.0410 - val_loss: 584155200.0000 - val_rmse: 24169.3027\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 720735424.0000 - rmse: 26846.5156 - val_loss: 539891072.0000 - val_rmse: 23235.5566\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 695424960.0000 - rmse: 26370.9121 - val_loss: 539442240.0000 - val_rmse: 23225.8965\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 734313472.0000 - rmse: 27098.2188 - val_loss: 541657728.0000 - val_rmse: 23273.5410\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687955200.0000 - rmse: 26228.9004 - val_loss: 585815232.0000 - val_rmse: 24203.6211\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 744054336.0000 - rmse: 27277.3594 - val_loss: 542833600.0000 - val_rmse: 23298.7891\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 709477952.0000 - rmse: 26636.0273 - val_loss: 527539680.0000 - val_rmse: 22968.2324\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 727158784.0000 - rmse: 26965.8828 - val_loss: 537697280.0000 - val_rmse: 23188.3008\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668987456.0000 - rmse: 25864.7910 - val_loss: 526184928.0000 - val_rmse: 22938.7207\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692839616.0000 - rmse: 26321.8477 - val_loss: 545598016.0000 - val_rmse: 23358.0391\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 722152960.0000 - rmse: 26872.9043 - val_loss: 553232320.0000 - val_rmse: 23520.8906\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698475136.0000 - rmse: 26428.6797 - val_loss: 530378048.0000 - val_rmse: 23029.9375\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 673727232.0000 - rmse: 25956.2559 - val_loss: 516522368.0000 - val_rmse: 22727.1289\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 713824960.0000 - rmse: 26717.5020 - val_loss: 501172576.0000 - val_rmse: 22386.8848\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 703753728.0000 - rmse: 26528.3574 - val_loss: 504828928.0000 - val_rmse: 22468.3984\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 720807616.0000 - rmse: 26847.8613 - val_loss: 520406688.0000 - val_rmse: 22812.4238\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685053888.0000 - rmse: 26173.5332 - val_loss: 520188384.0000 - val_rmse: 22807.6387\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 633392128.0000 - rmse: 25167.2832 - val_loss: 565894208.0000 - val_rmse: 23788.5312\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 714909760.0000 - rmse: 26737.7969 - val_loss: 533175456.0000 - val_rmse: 23090.5918\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656532288.0000 - rmse: 25622.8867 - val_loss: 485266560.0000 - val_rmse: 22028.7676\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680666112.0000 - rmse: 26089.5781 - val_loss: 496778688.0000 - val_rmse: 22288.5332\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 633062848.0000 - rmse: 25160.7402 - val_loss: 469835808.0000 - val_rmse: 21675.6953\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597865920.0000 - rmse: 24451.2969 - val_loss: 506514496.0000 - val_rmse: 22505.8770\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582301056.0000 - rmse: 24130.9141 - val_loss: 467010976.0000 - val_rmse: 21610.4375\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584900416.0000 - rmse: 24184.7148 - val_loss: 479769728.0000 - val_rmse: 21903.6465\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622236928.0000 - rmse: 24944.6777 - val_loss: 550341184.0000 - val_rmse: 23459.3516\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628314368.0000 - rmse: 25066.1992 - val_loss: 464485984.0000 - val_rmse: 21551.9375\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611229184.0000 - rmse: 24723.0488 - val_loss: 467294720.0000 - val_rmse: 21617.0000\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 635040256.0000 - rmse: 25200.0059 - val_loss: 466109024.0000 - val_rmse: 21589.5586\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642052288.0000 - rmse: 25338.7500 - val_loss: 522080608.0000 - val_rmse: 22849.0840\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620154368.0000 - rmse: 24902.8984 - val_loss: 476032736.0000 - val_rmse: 21818.1738\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568755904.0000 - rmse: 23848.6035 - val_loss: 467830912.0000 - val_rmse: 21629.3984\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543425152.0000 - rmse: 23311.4805 - val_loss: 455219520.0000 - val_rmse: 21335.8730\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575582912.0000 - rmse: 23991.3086 - val_loss: 451021568.0000 - val_rmse: 21237.2676\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582252736.0000 - rmse: 24129.9141 - val_loss: 448955136.0000 - val_rmse: 21188.5605\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564998144.0000 - rmse: 23769.6895 - val_loss: 500654592.0000 - val_rmse: 22375.3125\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 613865600.0000 - rmse: 24776.3105 - val_loss: 454561696.0000 - val_rmse: 21320.4531\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590381056.0000 - rmse: 24297.7578 - val_loss: 463421024.0000 - val_rmse: 21527.2168\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563384640.0000 - rmse: 23735.7246 - val_loss: 452304256.0000 - val_rmse: 21267.4453\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588312448.0000 - rmse: 24255.1523 - val_loss: 436675872.0000 - val_rmse: 20896.7910\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597512128.0000 - rmse: 24444.0605 - val_loss: 478315552.0000 - val_rmse: 21870.4258\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571104256.0000 - rmse: 23897.7871 - val_loss: 467693024.0000 - val_rmse: 21626.2109\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630841920.0000 - rmse: 25116.5664 - val_loss: 419077120.0000 - val_rmse: 20471.3730\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558761152.0000 - rmse: 23638.1289 - val_loss: 455529696.0000 - val_rmse: 21343.1426\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571937344.0000 - rmse: 23915.2109 - val_loss: 406993696.0000 - val_rmse: 20174.0840\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570099072.0000 - rmse: 23876.7480 - val_loss: 421101312.0000 - val_rmse: 20520.7539\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531089376.0000 - rmse: 23045.3770 - val_loss: 420018752.0000 - val_rmse: 20494.3594\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554974208.0000 - rmse: 23557.8906 - val_loss: 427764608.0000 - val_rmse: 20682.4707\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561896384.0000 - rmse: 23704.3535 - val_loss: 428678656.0000 - val_rmse: 20704.5566\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567638208.0000 - rmse: 23825.1582 - val_loss: 420278944.0000 - val_rmse: 20500.7051\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587134336.0000 - rmse: 24230.8555 - val_loss: 447150912.0000 - val_rmse: 21145.9434\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569683712.0000 - rmse: 23868.0488 - val_loss: 423038720.0000 - val_rmse: 20567.9043\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551469376.0000 - rmse: 23483.3848 - val_loss: 406789664.0000 - val_rmse: 20169.0273\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562149696.0000 - rmse: 23709.6953 - val_loss: 412154784.0000 - val_rmse: 20301.5957\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554490944.0000 - rmse: 23547.6309 - val_loss: 411519040.0000 - val_rmse: 20285.9316\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515217152.0000 - rmse: 22698.3945 - val_loss: 407905408.0000 - val_rmse: 20196.6680\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507538432.0000 - rmse: 22528.6133 - val_loss: 421586880.0000 - val_rmse: 20532.5801\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546740608.0000 - rmse: 23382.4844 - val_loss: 393088224.0000 - val_rmse: 19826.4531\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540517120.0000 - rmse: 23249.0234 - val_loss: 394493632.0000 - val_rmse: 19861.8633\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541340992.0000 - rmse: 23266.7363 - val_loss: 404182848.0000 - val_rmse: 20104.2988\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452041056.0000 - rmse: 21261.2578 - val_loss: 405762784.0000 - val_rmse: 20143.5547\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549784192.0000 - rmse: 23447.4766 - val_loss: 400225472.0000 - val_rmse: 20005.6367\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538223360.0000 - rmse: 23199.6406 - val_loss: 407258880.0000 - val_rmse: 20180.6562\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544607168.0000 - rmse: 23336.8203 - val_loss: 447738304.0000 - val_rmse: 21159.8281\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468734720.0000 - rmse: 21650.2812 - val_loss: 410460288.0000 - val_rmse: 20259.8203\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483807232.0000 - rmse: 21995.6191 - val_loss: 420562112.0000 - val_rmse: 20507.6113\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571212480.0000 - rmse: 23900.0527 - val_loss: 392413216.0000 - val_rmse: 19809.4219\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523560672.0000 - rmse: 22881.4473 - val_loss: 421177632.0000 - val_rmse: 20522.6133\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530586752.0000 - rmse: 23034.4688 - val_loss: 415086208.0000 - val_rmse: 20373.6641\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500498368.0000 - rmse: 22371.8203 - val_loss: 401831616.0000 - val_rmse: 20045.7383\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545743296.0000 - rmse: 23361.1484 - val_loss: 378490912.0000 - val_rmse: 19454.8438\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509638688.0000 - rmse: 22575.1777 - val_loss: 393306080.0000 - val_rmse: 19831.9453\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511467424.0000 - rmse: 22615.6465 - val_loss: 396269280.0000 - val_rmse: 19906.5137\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474444128.0000 - rmse: 21781.7383 - val_loss: 388580288.0000 - val_rmse: 19712.4395\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516837600.0000 - rmse: 22734.0625 - val_loss: 368678848.0000 - val_rmse: 19201.0117\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521553664.0000 - rmse: 22837.5488 - val_loss: 381655392.0000 - val_rmse: 19536.0020\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542236608.0000 - rmse: 23285.9746 - val_loss: 446739520.0000 - val_rmse: 21136.2129\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476890464.0000 - rmse: 21837.8223 - val_loss: 387953216.0000 - val_rmse: 19696.5273\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521180800.0000 - rmse: 22829.3848 - val_loss: 382865600.0000 - val_rmse: 19566.9512\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489959200.0000 - rmse: 22135.0215 - val_loss: 365504512.0000 - val_rmse: 19118.1719\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440859008.0000 - rmse: 20996.6426 - val_loss: 372199104.0000 - val_rmse: 19292.4629\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499620768.0000 - rmse: 22352.1992 - val_loss: 381597920.0000 - val_rmse: 19534.5312\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474560544.0000 - rmse: 21784.4102 - val_loss: 378448736.0000 - val_rmse: 19453.7598\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489741440.0000 - rmse: 22130.1035 - val_loss: 367533376.0000 - val_rmse: 19171.1602\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552897664.0000 - rmse: 23513.7754 - val_loss: 371951040.0000 - val_rmse: 19286.0332\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422477024.0000 - rmse: 20554.2461 - val_loss: 363018848.0000 - val_rmse: 19053.0527\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499361248.0000 - rmse: 22346.3926 - val_loss: 380565920.0000 - val_rmse: 19508.0996\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483777920.0000 - rmse: 21994.9512 - val_loss: 374059360.0000 - val_rmse: 19340.6152\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448133152.0000 - rmse: 21169.1562 - val_loss: 377012448.0000 - val_rmse: 19416.8086\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484548896.0000 - rmse: 22012.4707 - val_loss: 371502976.0000 - val_rmse: 19274.4121\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477593856.0000 - rmse: 21853.9199 - val_loss: 366749760.0000 - val_rmse: 19150.7109\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534741600.0000 - rmse: 23124.4805 - val_loss: 350234816.0000 - val_rmse: 18714.5625\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465886592.0000 - rmse: 21584.4062 - val_loss: 348794080.0000 - val_rmse: 18676.0293\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478940096.0000 - rmse: 21884.6992 - val_loss: 342587360.0000 - val_rmse: 18509.1152\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454066752.0000 - rmse: 21308.8418 - val_loss: 357909376.0000 - val_rmse: 18918.4922\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461621376.0000 - rmse: 21485.3750 - val_loss: 354164800.0000 - val_rmse: 18819.2676\n",
      "104/104 [==============================] - 0s 651us/step - loss: 829152448.0000 - rmse: 28795.0078\n",
      "[829152448.0, 28795.0078125]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 24,065\n",
      "Trainable params: 24,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 24,065\n",
      "Trainable params: 24,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 18959597568.0000 - rmse: 137693.8594 - val_loss: 6488815104.0000 - val_rmse: 80553.1797\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 3546964736.0000 - rmse: 59556.3984 - val_loss: 1834111616.0000 - val_rmse: 42826.5312\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1879821184.0000 - rmse: 43356.9062 - val_loss: 1354621696.0000 - val_rmse: 36805.1875\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1611619072.0000 - rmse: 40144.9766 - val_loss: 1290858624.0000 - val_rmse: 35928.5195\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1551816320.0000 - rmse: 39393.1016 - val_loss: 1224284416.0000 - val_rmse: 34989.7773\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1414781824.0000 - rmse: 37613.5859 - val_loss: 1217065728.0000 - val_rmse: 34886.4688\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1423692800.0000 - rmse: 37731.8555 - val_loss: 1139198848.0000 - val_rmse: 33752.0195\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1416619648.0000 - rmse: 37638.0078 - val_loss: 1141151872.0000 - val_rmse: 33780.9414\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1318531328.0000 - rmse: 36311.5859 - val_loss: 1070144256.0000 - val_rmse: 32713.0586\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1280299392.0000 - rmse: 35781.2734 - val_loss: 1034058368.0000 - val_rmse: 32156.7773\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1212956288.0000 - rmse: 34827.5234 - val_loss: 1140682112.0000 - val_rmse: 33773.9844\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1190090240.0000 - rmse: 34497.6836 - val_loss: 1267199744.0000 - val_rmse: 35597.7500\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1162483840.0000 - rmse: 34095.2188 - val_loss: 972128064.0000 - val_rmse: 31178.9688\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1160943488.0000 - rmse: 34072.6211 - val_loss: 973469888.0000 - val_rmse: 31200.4785\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1147512960.0000 - rmse: 33874.9609 - val_loss: 1012778112.0000 - val_rmse: 31824.1758\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1146096768.0000 - rmse: 33854.0508 - val_loss: 991102080.0000 - val_rmse: 31481.7734\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1076770944.0000 - rmse: 32814.1875 - val_loss: 879426176.0000 - val_rmse: 29655.1211\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1059853120.0000 - rmse: 32555.3848 - val_loss: 852476800.0000 - val_rmse: 29197.2051\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1037854848.0000 - rmse: 32215.7539 - val_loss: 866777728.0000 - val_rmse: 29441.0898\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1056420480.0000 - rmse: 32502.6230 - val_loss: 933370496.0000 - val_rmse: 30551.1133\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 975267520.0000 - rmse: 31229.2734 - val_loss: 872445440.0000 - val_rmse: 29537.1875\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 920577024.0000 - rmse: 30341.0117 - val_loss: 803072960.0000 - val_rmse: 28338.5410\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 924086848.0000 - rmse: 30398.7969 - val_loss: 794126592.0000 - val_rmse: 28180.2520\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 963079552.0000 - rmse: 31033.5234 - val_loss: 770099776.0000 - val_rmse: 27750.6719\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 919394688.0000 - rmse: 30321.5215 - val_loss: 760457408.0000 - val_rmse: 27576.3926\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 919223360.0000 - rmse: 30318.6973 - val_loss: 941902528.0000 - val_rmse: 30690.4297\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 913769856.0000 - rmse: 30228.6270 - val_loss: 796753408.0000 - val_rmse: 28226.8203\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 844974592.0000 - rmse: 29068.4473 - val_loss: 798593920.0000 - val_rmse: 28259.4043\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 894063168.0000 - rmse: 29900.8887 - val_loss: 771291136.0000 - val_rmse: 27772.1289\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 889065792.0000 - rmse: 29817.2070 - val_loss: 711646912.0000 - val_rmse: 26676.7109\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 895370240.0000 - rmse: 29922.7383 - val_loss: 716673280.0000 - val_rmse: 26770.7539\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 872587392.0000 - rmse: 29539.5898 - val_loss: 696604416.0000 - val_rmse: 26393.2637\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 787868160.0000 - rmse: 28068.9902 - val_loss: 671651904.0000 - val_rmse: 25916.2480\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 836256960.0000 - rmse: 28918.1074 - val_loss: 653967744.0000 - val_rmse: 25572.7930\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 892840640.0000 - rmse: 29880.4395 - val_loss: 717415296.0000 - val_rmse: 26784.6094\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 810231744.0000 - rmse: 28464.5703 - val_loss: 621323008.0000 - val_rmse: 24926.3516\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 815406912.0000 - rmse: 28555.3301 - val_loss: 621031616.0000 - val_rmse: 24920.5059\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 799871744.0000 - rmse: 28282.0039 - val_loss: 635089920.0000 - val_rmse: 25200.9902\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 821659776.0000 - rmse: 28664.6094 - val_loss: 590334144.0000 - val_rmse: 24296.7930\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 891459200.0000 - rmse: 29857.3145 - val_loss: 599219392.0000 - val_rmse: 24478.9590\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 816092416.0000 - rmse: 28567.3320 - val_loss: 603280256.0000 - val_rmse: 24561.7637\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 762826496.0000 - rmse: 27619.3145 - val_loss: 593031168.0000 - val_rmse: 24352.2305\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 828201664.0000 - rmse: 28778.4922 - val_loss: 606789312.0000 - val_rmse: 24633.0938\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 780708224.0000 - rmse: 27941.1562 - val_loss: 593772480.0000 - val_rmse: 24367.4473\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 767739840.0000 - rmse: 27708.1191 - val_loss: 603790848.0000 - val_rmse: 24572.1562\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 787651136.0000 - rmse: 28065.1230 - val_loss: 596342080.0000 - val_rmse: 24420.1172\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 774192576.0000 - rmse: 27824.3164 - val_loss: 751869888.0000 - val_rmse: 27420.2461\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 788123904.0000 - rmse: 28073.5449 - val_loss: 606940992.0000 - val_rmse: 24636.1719\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 708371520.0000 - rmse: 26615.2500 - val_loss: 553560512.0000 - val_rmse: 23527.8672\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 732371200.0000 - rmse: 27062.3574 - val_loss: 621302976.0000 - val_rmse: 24925.9492\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 778804608.0000 - rmse: 27907.0703 - val_loss: 555335488.0000 - val_rmse: 23565.5566\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 722988160.0000 - rmse: 26888.4395 - val_loss: 666993792.0000 - val_rmse: 25826.2227\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 773755904.0000 - rmse: 27816.4688 - val_loss: 525459712.0000 - val_rmse: 22922.9082\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 734660160.0000 - rmse: 27104.6152 - val_loss: 524349056.0000 - val_rmse: 22898.6699\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718760384.0000 - rmse: 26809.7070 - val_loss: 511019648.0000 - val_rmse: 22605.7441\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 681523008.0000 - rmse: 26105.9961 - val_loss: 588584192.0000 - val_rmse: 24260.7539\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 697487872.0000 - rmse: 26409.9961 - val_loss: 619214464.0000 - val_rmse: 24884.0195\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 755431744.0000 - rmse: 27485.1191 - val_loss: 538676672.0000 - val_rmse: 23209.4082\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 689569728.0000 - rmse: 26259.6602 - val_loss: 549216576.0000 - val_rmse: 23435.3711\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676423744.0000 - rmse: 26008.1484 - val_loss: 547854016.0000 - val_rmse: 23406.2812\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742658816.0000 - rmse: 27251.7676 - val_loss: 499301888.0000 - val_rmse: 22345.0645\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 711200832.0000 - rmse: 26668.3496 - val_loss: 519246400.0000 - val_rmse: 22786.9785\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 699569024.0000 - rmse: 26449.3672 - val_loss: 522141568.0000 - val_rmse: 22850.4180\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721403072.0000 - rmse: 26858.9473 - val_loss: 513523904.0000 - val_rmse: 22661.0664\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 682850240.0000 - rmse: 26131.4043 - val_loss: 534944288.0000 - val_rmse: 23128.8633\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721395968.0000 - rmse: 26858.8164 - val_loss: 499178592.0000 - val_rmse: 22342.3047\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685810560.0000 - rmse: 26187.9844 - val_loss: 476763232.0000 - val_rmse: 21834.9082\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680540672.0000 - rmse: 26087.1738 - val_loss: 488159904.0000 - val_rmse: 22094.3418\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730144064.0000 - rmse: 27021.1777 - val_loss: 491461056.0000 - val_rmse: 22168.9219\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 682719936.0000 - rmse: 26128.9102 - val_loss: 467025152.0000 - val_rmse: 21610.7656\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652439104.0000 - rmse: 25542.8867 - val_loss: 483916160.0000 - val_rmse: 21998.0938\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 682833408.0000 - rmse: 26131.0820 - val_loss: 478864416.0000 - val_rmse: 21882.9707\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 699712832.0000 - rmse: 26452.0859 - val_loss: 490877856.0000 - val_rmse: 22155.7637\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 658410176.0000 - rmse: 25659.5039 - val_loss: 466549376.0000 - val_rmse: 21599.7539\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656352960.0000 - rmse: 25619.3867 - val_loss: 488751872.0000 - val_rmse: 22107.7324\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661990272.0000 - rmse: 25729.1719 - val_loss: 565943104.0000 - val_rmse: 23789.5586\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 650651136.0000 - rmse: 25507.8633 - val_loss: 450816448.0000 - val_rmse: 21232.4395\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 669512640.0000 - rmse: 25874.9414 - val_loss: 531068480.0000 - val_rmse: 23044.9238\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 682269504.0000 - rmse: 26120.2891 - val_loss: 460478656.0000 - val_rmse: 21458.7656\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 670714304.0000 - rmse: 25898.1523 - val_loss: 453150464.0000 - val_rmse: 21287.3320\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 625406016.0000 - rmse: 25008.1191 - val_loss: 433046944.0000 - val_rmse: 20809.7793\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 653687424.0000 - rmse: 25567.3125 - val_loss: 473000288.0000 - val_rmse: 21748.5703\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648371136.0000 - rmse: 25463.1328 - val_loss: 425862464.0000 - val_rmse: 20636.4355\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649036864.0000 - rmse: 25476.2012 - val_loss: 429732224.0000 - val_rmse: 20729.9844\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665501952.0000 - rmse: 25797.3242 - val_loss: 496635744.0000 - val_rmse: 22285.3262\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649646016.0000 - rmse: 25488.1543 - val_loss: 471946272.0000 - val_rmse: 21724.3242\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 603715712.0000 - rmse: 24570.6270 - val_loss: 448526912.0000 - val_rmse: 21178.4531\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618939776.0000 - rmse: 24878.5000 - val_loss: 435682336.0000 - val_rmse: 20873.0059\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629742144.0000 - rmse: 25094.6641 - val_loss: 440432128.0000 - val_rmse: 20986.4746\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656001920.0000 - rmse: 25612.5352 - val_loss: 411956544.0000 - val_rmse: 20296.7129\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584909312.0000 - rmse: 24184.8984 - val_loss: 407190304.0000 - val_rmse: 20178.9570\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573155904.0000 - rmse: 23940.6738 - val_loss: 436654944.0000 - val_rmse: 20896.2910\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616049408.0000 - rmse: 24820.3418 - val_loss: 491363008.0000 - val_rmse: 22166.7090\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 603571072.0000 - rmse: 24567.6836 - val_loss: 424413024.0000 - val_rmse: 20601.2871\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609567232.0000 - rmse: 24689.4160 - val_loss: 552425088.0000 - val_rmse: 23503.7246\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654962368.0000 - rmse: 25592.2324 - val_loss: 403005728.0000 - val_rmse: 20075.0020\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649224640.0000 - rmse: 25479.8867 - val_loss: 405459232.0000 - val_rmse: 20136.0176\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584314944.0000 - rmse: 24172.6074 - val_loss: 414103488.0000 - val_rmse: 20349.5332\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601967232.0000 - rmse: 24535.0215 - val_loss: 406308512.0000 - val_rmse: 20157.0957\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585474816.0000 - rmse: 24196.5859 - val_loss: 386057952.0000 - val_rmse: 19648.3574\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587865344.0000 - rmse: 24245.9355 - val_loss: 421606048.0000 - val_rmse: 20533.0469\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 643135616.0000 - rmse: 25360.1191 - val_loss: 400450432.0000 - val_rmse: 20011.2578\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642844800.0000 - rmse: 25354.3848 - val_loss: 409290112.0000 - val_rmse: 20230.9199\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576797824.0000 - rmse: 24016.6152 - val_loss: 368312640.0000 - val_rmse: 19191.4727\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555433472.0000 - rmse: 23567.6367 - val_loss: 409818720.0000 - val_rmse: 20243.9805\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556601856.0000 - rmse: 23592.4102 - val_loss: 384233408.0000 - val_rmse: 19601.8730\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610899968.0000 - rmse: 24716.3906 - val_loss: 394540064.0000 - val_rmse: 19863.0332\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570344256.0000 - rmse: 23881.8809 - val_loss: 421702400.0000 - val_rmse: 20535.3945\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 528107264.0000 - rmse: 22980.5840 - val_loss: 371388192.0000 - val_rmse: 19271.4355\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586860672.0000 - rmse: 24225.2070 - val_loss: 381488352.0000 - val_rmse: 19531.7266\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590107520.0000 - rmse: 24292.1289 - val_loss: 378220704.0000 - val_rmse: 19447.8965\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579812032.0000 - rmse: 24079.2871 - val_loss: 426703200.0000 - val_rmse: 20656.7949\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557298560.0000 - rmse: 23607.1719 - val_loss: 361969344.0000 - val_rmse: 19025.4922\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555100416.0000 - rmse: 23560.5684 - val_loss: 375219360.0000 - val_rmse: 19370.5801\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530044256.0000 - rmse: 23022.6895 - val_loss: 369672832.0000 - val_rmse: 19226.8770\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520444832.0000 - rmse: 22813.2598 - val_loss: 354007040.0000 - val_rmse: 18815.0742\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586760128.0000 - rmse: 24223.1328 - val_loss: 446658912.0000 - val_rmse: 21134.3066\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583300608.0000 - rmse: 24151.6172 - val_loss: 403583488.0000 - val_rmse: 20089.3867\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 625923904.0000 - rmse: 25018.4707 - val_loss: 371779424.0000 - val_rmse: 19281.5820\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569185984.0000 - rmse: 23857.6191 - val_loss: 421953440.0000 - val_rmse: 20541.5059\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564972992.0000 - rmse: 23769.1602 - val_loss: 408127712.0000 - val_rmse: 20202.1719\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530769984.0000 - rmse: 23038.4453 - val_loss: 350428096.0000 - val_rmse: 18719.7246\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514370432.0000 - rmse: 22679.7363 - val_loss: 360930496.0000 - val_rmse: 18998.1699\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542206592.0000 - rmse: 23285.3301 - val_loss: 353359648.0000 - val_rmse: 18797.8633\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522020032.0000 - rmse: 22847.7578 - val_loss: 355610496.0000 - val_rmse: 18857.6367\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584551232.0000 - rmse: 24177.4941 - val_loss: 351913536.0000 - val_rmse: 18759.3594\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569086592.0000 - rmse: 23855.5352 - val_loss: 379570752.0000 - val_rmse: 19482.5762\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574767296.0000 - rmse: 23974.3047 - val_loss: 365826336.0000 - val_rmse: 19126.5879\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607351296.0000 - rmse: 24644.4980 - val_loss: 347839968.0000 - val_rmse: 18650.4688\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587700544.0000 - rmse: 24242.5352 - val_loss: 364805856.0000 - val_rmse: 19099.8906\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556942976.0000 - rmse: 23599.6387 - val_loss: 353864960.0000 - val_rmse: 18811.2988\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549007680.0000 - rmse: 23430.9121 - val_loss: 338083040.0000 - val_rmse: 18387.0352\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527468160.0000 - rmse: 22966.6758 - val_loss: 386065664.0000 - val_rmse: 19648.5547\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477946400.0000 - rmse: 21861.9844 - val_loss: 345806368.0000 - val_rmse: 18595.8691\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549142720.0000 - rmse: 23433.7949 - val_loss: 347519008.0000 - val_rmse: 18641.8613\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520182688.0000 - rmse: 22807.5137 - val_loss: 343751008.0000 - val_rmse: 18540.5234\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554122560.0000 - rmse: 23539.8086 - val_loss: 337163008.0000 - val_rmse: 18361.9980\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554153728.0000 - rmse: 23540.4707 - val_loss: 335311072.0000 - val_rmse: 18311.5020\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560287744.0000 - rmse: 23670.3984 - val_loss: 324539616.0000 - val_rmse: 18014.9824\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567742720.0000 - rmse: 23827.3516 - val_loss: 516002720.0000 - val_rmse: 22715.6934\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541390656.0000 - rmse: 23267.8027 - val_loss: 341062912.0000 - val_rmse: 18467.8887\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524993088.0000 - rmse: 22912.7285 - val_loss: 343613984.0000 - val_rmse: 18536.8281\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548544640.0000 - rmse: 23421.0293 - val_loss: 367169536.0000 - val_rmse: 19161.6680\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513704960.0000 - rmse: 22665.0605 - val_loss: 320780224.0000 - val_rmse: 17910.3379\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513294560.0000 - rmse: 22656.0059 - val_loss: 322649344.0000 - val_rmse: 17962.4434\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540103424.0000 - rmse: 23240.1250 - val_loss: 401713600.0000 - val_rmse: 20042.7949\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574663808.0000 - rmse: 23972.1465 - val_loss: 325779328.0000 - val_rmse: 18049.3574\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538745152.0000 - rmse: 23210.8848 - val_loss: 336122368.0000 - val_rmse: 18333.6406\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590423424.0000 - rmse: 24298.6309 - val_loss: 367451392.0000 - val_rmse: 19169.0215\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512977536.0000 - rmse: 22649.0078 - val_loss: 318775520.0000 - val_rmse: 17854.2852\n",
      "104/104 [==============================] - 0s 655us/step - loss: 859888384.0000 - rmse: 29323.8535\n",
      "[859888384.0, 29323.853515625]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 24,065\n",
      "Trainable params: 24,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 24,065\n",
      "Trainable params: 24,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 16790873088.0000 - rmse: 129579.6016 - val_loss: 3637084672.0000 - val_rmse: 60308.2461\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2779657216.0000 - rmse: 52722.4531 - val_loss: 1562102656.0000 - val_rmse: 39523.4453\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2014564224.0000 - rmse: 44883.8984 - val_loss: 1225417600.0000 - val_rmse: 35005.9648\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1807288064.0000 - rmse: 42512.2109 - val_loss: 1174976640.0000 - val_rmse: 34277.9336\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1640842112.0000 - rmse: 40507.3086 - val_loss: 1094204416.0000 - val_rmse: 33078.7617\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1640464128.0000 - rmse: 40502.6445 - val_loss: 1088089856.0000 - val_rmse: 32986.2070\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1513116160.0000 - rmse: 38898.7930 - val_loss: 1031882880.0000 - val_rmse: 32122.9336\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1485287168.0000 - rmse: 38539.4219 - val_loss: 1006393728.0000 - val_rmse: 31723.7090\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1513689856.0000 - rmse: 38906.1680 - val_loss: 962607744.0000 - val_rmse: 31025.9199\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1473198080.0000 - rmse: 38382.2617 - val_loss: 1168580480.0000 - val_rmse: 34184.5078\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1448035072.0000 - rmse: 38053.0547 - val_loss: 931205696.0000 - val_rmse: 30515.6641\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1405970304.0000 - rmse: 37496.2695 - val_loss: 919318400.0000 - val_rmse: 30320.2637\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1337398272.0000 - rmse: 36570.4570 - val_loss: 988605376.0000 - val_rmse: 31442.0957\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1281643904.0000 - rmse: 35800.0547 - val_loss: 881745792.0000 - val_rmse: 29694.2051\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1270021120.0000 - rmse: 35637.3555 - val_loss: 893163200.0000 - val_rmse: 29885.8359\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1187986176.0000 - rmse: 34467.1758 - val_loss: 894696960.0000 - val_rmse: 29911.4863\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1280254208.0000 - rmse: 35780.6406 - val_loss: 931066176.0000 - val_rmse: 30513.3770\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1263820416.0000 - rmse: 35550.2539 - val_loss: 860076992.0000 - val_rmse: 29327.0684\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1118055424.0000 - rmse: 33437.3359 - val_loss: 872798016.0000 - val_rmse: 29543.1543\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1219709696.0000 - rmse: 34924.3438 - val_loss: 846483200.0000 - val_rmse: 29094.3848\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1135411584.0000 - rmse: 33695.8672 - val_loss: 843232128.0000 - val_rmse: 29038.4590\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1111636096.0000 - rmse: 33341.2070 - val_loss: 837934848.0000 - val_rmse: 28947.1035\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1196856320.0000 - rmse: 34595.6094 - val_loss: 877899392.0000 - val_rmse: 29629.3672\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1098812288.0000 - rmse: 33148.3359 - val_loss: 802325888.0000 - val_rmse: 28325.3574\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1095365120.0000 - rmse: 33096.3008 - val_loss: 859561600.0000 - val_rmse: 29318.2812\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1039978880.0000 - rmse: 32248.7031 - val_loss: 914420864.0000 - val_rmse: 30239.3926\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1078319232.0000 - rmse: 32837.7695 - val_loss: 864953728.0000 - val_rmse: 29410.0957\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1038110912.0000 - rmse: 32219.7285 - val_loss: 967126656.0000 - val_rmse: 31098.6602\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1069129984.0000 - rmse: 32697.5527 - val_loss: 864417408.0000 - val_rmse: 29400.9766\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1178740608.0000 - rmse: 34332.7930 - val_loss: 804522560.0000 - val_rmse: 28364.1074\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1012805696.0000 - rmse: 31824.6074 - val_loss: 772440000.0000 - val_rmse: 27792.8047\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 985434176.0000 - rmse: 31391.6250 - val_loss: 747738304.0000 - val_rmse: 27344.8047\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1052031296.0000 - rmse: 32435.0312 - val_loss: 879632640.0000 - val_rmse: 29658.6016\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 964701184.0000 - rmse: 31059.6387 - val_loss: 768915968.0000 - val_rmse: 27729.3340\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 983061248.0000 - rmse: 31353.8066 - val_loss: 722398016.0000 - val_rmse: 26877.4629\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1034296832.0000 - rmse: 32160.4863 - val_loss: 845189376.0000 - val_rmse: 29072.1406\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 952448000.0000 - rmse: 30861.7559 - val_loss: 756737536.0000 - val_rmse: 27508.8633\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 958942912.0000 - rmse: 30966.8027 - val_loss: 741953024.0000 - val_rmse: 27238.8145\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 977227968.0000 - rmse: 31260.6465 - val_loss: 721963968.0000 - val_rmse: 26869.3867\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 977526976.0000 - rmse: 31265.4277 - val_loss: 776824896.0000 - val_rmse: 27871.5781\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 970415232.0000 - rmse: 31151.4883 - val_loss: 760638976.0000 - val_rmse: 27579.6836\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 933534272.0000 - rmse: 30553.7930 - val_loss: 692748224.0000 - val_rmse: 26320.1113\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 953793152.0000 - rmse: 30883.5410 - val_loss: 689390272.0000 - val_rmse: 26256.2422\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 909375296.0000 - rmse: 30155.8496 - val_loss: 716444544.0000 - val_rmse: 26766.4824\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 866857856.0000 - rmse: 29442.4492 - val_loss: 708054976.0000 - val_rmse: 26609.3027\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 881363712.0000 - rmse: 29687.7695 - val_loss: 723938816.0000 - val_rmse: 26906.1113\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 865861632.0000 - rmse: 29425.5273 - val_loss: 675192192.0000 - val_rmse: 25984.4609\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 872435648.0000 - rmse: 29537.0215 - val_loss: 627605632.0000 - val_rmse: 25052.0586\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 818653376.0000 - rmse: 28612.1191 - val_loss: 697212224.0000 - val_rmse: 26404.7773\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 859943808.0000 - rmse: 29324.7988 - val_loss: 634494912.0000 - val_rmse: 25189.1816\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 838235840.0000 - rmse: 28952.3027 - val_loss: 627513984.0000 - val_rmse: 25050.2285\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 909279424.0000 - rmse: 30154.2598 - val_loss: 608948544.0000 - val_rmse: 24676.8828\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 886876800.0000 - rmse: 29780.4766 - val_loss: 665965632.0000 - val_rmse: 25806.3105\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 856701376.0000 - rmse: 29269.4609 - val_loss: 609684032.0000 - val_rmse: 24691.7812\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 826612800.0000 - rmse: 28750.8750 - val_loss: 662194624.0000 - val_rmse: 25733.1426\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 766775744.0000 - rmse: 27690.7168 - val_loss: 663524992.0000 - val_rmse: 25758.9785\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 803925248.0000 - rmse: 28353.5762 - val_loss: 616661120.0000 - val_rmse: 24832.6621\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 827716096.0000 - rmse: 28770.0547 - val_loss: 642859712.0000 - val_rmse: 25354.6777\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 820105344.0000 - rmse: 28637.4824 - val_loss: 646837696.0000 - val_rmse: 25433.0039\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 847830592.0000 - rmse: 29117.5312 - val_loss: 547984256.0000 - val_rmse: 23409.0645\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 815356288.0000 - rmse: 28554.4434 - val_loss: 601518656.0000 - val_rmse: 24525.8770\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 874728576.0000 - rmse: 29575.8105 - val_loss: 583122880.0000 - val_rmse: 24147.9375\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 772098816.0000 - rmse: 27786.6660 - val_loss: 607296000.0000 - val_rmse: 24643.3770\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 719599424.0000 - rmse: 26825.3496 - val_loss: 573353856.0000 - val_rmse: 23944.8086\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 713474496.0000 - rmse: 26710.9434 - val_loss: 585101376.0000 - val_rmse: 24188.8691\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 758447168.0000 - rmse: 27539.9199 - val_loss: 609898176.0000 - val_rmse: 24696.1172\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 754608640.0000 - rmse: 27470.1406 - val_loss: 589026752.0000 - val_rmse: 24269.8730\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 791471552.0000 - rmse: 28133.1035 - val_loss: 677548224.0000 - val_rmse: 26029.7559\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 705903424.0000 - rmse: 26568.8438 - val_loss: 593432896.0000 - val_rmse: 24360.4785\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 759975680.0000 - rmse: 27567.6562 - val_loss: 618494528.0000 - val_rmse: 24869.5508\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721897472.0000 - rmse: 26868.1504 - val_loss: 540102912.0000 - val_rmse: 23240.1152\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 725536000.0000 - rmse: 26935.7754 - val_loss: 546005696.0000 - val_rmse: 23366.7656\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 761493632.0000 - rmse: 27595.1738 - val_loss: 518251872.0000 - val_rmse: 22765.1465\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 694242112.0000 - rmse: 26348.4746 - val_loss: 617122688.0000 - val_rmse: 24841.9551\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 693852608.0000 - rmse: 26341.0820 - val_loss: 511579904.0000 - val_rmse: 22618.1328\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724339776.0000 - rmse: 26913.5605 - val_loss: 504581984.0000 - val_rmse: 22462.9023\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 701406848.0000 - rmse: 26484.0859 - val_loss: 494472896.0000 - val_rmse: 22236.7461\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 737696896.0000 - rmse: 27160.5762 - val_loss: 493112896.0000 - val_rmse: 22206.1445\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 711266880.0000 - rmse: 26669.5879 - val_loss: 483949184.0000 - val_rmse: 21998.8457\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 752599296.0000 - rmse: 27433.5430 - val_loss: 490152896.0000 - val_rmse: 22139.3965\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666552384.0000 - rmse: 25817.6758 - val_loss: 501850304.0000 - val_rmse: 22402.0156\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 683774784.0000 - rmse: 26149.0879 - val_loss: 526597696.0000 - val_rmse: 22947.7168\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640265600.0000 - rmse: 25303.4707 - val_loss: 505367232.0000 - val_rmse: 22480.3750\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 694149056.0000 - rmse: 26346.7090 - val_loss: 511556000.0000 - val_rmse: 22617.6035\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 681709248.0000 - rmse: 26109.5625 - val_loss: 562772224.0000 - val_rmse: 23722.8203\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 675701312.0000 - rmse: 25994.2559 - val_loss: 495855296.0000 - val_rmse: 22267.8086\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 669268096.0000 - rmse: 25870.2168 - val_loss: 483054880.0000 - val_rmse: 21978.5098\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 678598400.0000 - rmse: 26049.9219 - val_loss: 524851488.0000 - val_rmse: 22909.6367\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 717547840.0000 - rmse: 26787.0840 - val_loss: 500217312.0000 - val_rmse: 22365.5391\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 693313664.0000 - rmse: 26330.8496 - val_loss: 491377440.0000 - val_rmse: 22167.0352\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 655071488.0000 - rmse: 25594.3652 - val_loss: 508435136.0000 - val_rmse: 22548.5059\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640294016.0000 - rmse: 25304.0312 - val_loss: 482933728.0000 - val_rmse: 21975.7539\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578822080.0000 - rmse: 24058.7207 - val_loss: 463784672.0000 - val_rmse: 21535.6602\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 722741504.0000 - rmse: 26883.8516 - val_loss: 493550208.0000 - val_rmse: 22215.9902\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620442560.0000 - rmse: 24908.6836 - val_loss: 455573984.0000 - val_rmse: 21344.1797\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574058240.0000 - rmse: 23959.5117 - val_loss: 477730336.0000 - val_rmse: 21857.0430\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 702758528.0000 - rmse: 26509.5938 - val_loss: 448289632.0000 - val_rmse: 21172.8516\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 619330432.0000 - rmse: 24886.3496 - val_loss: 459003968.0000 - val_rmse: 21424.3770\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584360768.0000 - rmse: 24173.5547 - val_loss: 450962784.0000 - val_rmse: 21235.8848\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 608943296.0000 - rmse: 24676.7773 - val_loss: 503616576.0000 - val_rmse: 22441.4023\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 635460992.0000 - rmse: 25208.3516 - val_loss: 457532160.0000 - val_rmse: 21390.0020\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537508416.0000 - rmse: 23184.2285 - val_loss: 498969376.0000 - val_rmse: 22337.6230\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 644189760.0000 - rmse: 25380.8945 - val_loss: 498196192.0000 - val_rmse: 22320.3086\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 701467328.0000 - rmse: 26485.2285 - val_loss: 463872640.0000 - val_rmse: 21537.7031\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589094208.0000 - rmse: 24271.2637 - val_loss: 437376544.0000 - val_rmse: 20913.5488\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642720576.0000 - rmse: 25351.9336 - val_loss: 517822016.0000 - val_rmse: 22755.7031\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535090912.0000 - rmse: 23132.0312 - val_loss: 506700224.0000 - val_rmse: 22510.0020\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593583488.0000 - rmse: 24363.5684 - val_loss: 513645888.0000 - val_rmse: 22663.7578\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646166016.0000 - rmse: 25419.7949 - val_loss: 468858080.0000 - val_rmse: 21653.1309\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546198976.0000 - rmse: 23370.9004 - val_loss: 439701376.0000 - val_rmse: 20969.0566\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587002432.0000 - rmse: 24228.1328 - val_loss: 435222592.0000 - val_rmse: 20861.9883\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 617794496.0000 - rmse: 24855.4727 - val_loss: 439461664.0000 - val_rmse: 20963.3418\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585584512.0000 - rmse: 24198.8535 - val_loss: 451484992.0000 - val_rmse: 21248.1758\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551347200.0000 - rmse: 23480.7832 - val_loss: 492862400.0000 - val_rmse: 22200.5039\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614243008.0000 - rmse: 24783.9258 - val_loss: 448042048.0000 - val_rmse: 21167.0039\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582721152.0000 - rmse: 24139.6172 - val_loss: 516890912.0000 - val_rmse: 22735.2344\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587456512.0000 - rmse: 24237.5020 - val_loss: 427297568.0000 - val_rmse: 20671.1777\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587179648.0000 - rmse: 24231.7891 - val_loss: 451389568.0000 - val_rmse: 21245.9297\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562749440.0000 - rmse: 23722.3398 - val_loss: 472635168.0000 - val_rmse: 21740.1738\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612328064.0000 - rmse: 24745.2637 - val_loss: 425587168.0000 - val_rmse: 20629.7637\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569240256.0000 - rmse: 23858.7559 - val_loss: 445754304.0000 - val_rmse: 21112.8945\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651861376.0000 - rmse: 25531.5762 - val_loss: 450444096.0000 - val_rmse: 21223.6680\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522990976.0000 - rmse: 22868.9961 - val_loss: 478908160.0000 - val_rmse: 21883.9707\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529852032.0000 - rmse: 23018.5156 - val_loss: 449713120.0000 - val_rmse: 21206.4414\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549293376.0000 - rmse: 23437.0078 - val_loss: 522700512.0000 - val_rmse: 22862.6445\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502305792.0000 - rmse: 22412.1797 - val_loss: 426048576.0000 - val_rmse: 20640.9434\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524252736.0000 - rmse: 22896.5664 - val_loss: 430327648.0000 - val_rmse: 20744.3398\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 581592768.0000 - rmse: 24116.2344 - val_loss: 423779264.0000 - val_rmse: 20585.9004\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 528715456.0000 - rmse: 22993.8125 - val_loss: 448711584.0000 - val_rmse: 21182.8125\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534391552.0000 - rmse: 23116.9102 - val_loss: 441794752.0000 - val_rmse: 21018.9141\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541585600.0000 - rmse: 23271.9922 - val_loss: 443548352.0000 - val_rmse: 21060.5879\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547217472.0000 - rmse: 23392.6797 - val_loss: 445279520.0000 - val_rmse: 21101.6465\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536530144.0000 - rmse: 23163.1211 - val_loss: 475348736.0000 - val_rmse: 21802.4941\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588884480.0000 - rmse: 24266.9414 - val_loss: 518577696.0000 - val_rmse: 22772.3008\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573435328.0000 - rmse: 23946.5098 - val_loss: 447856448.0000 - val_rmse: 21162.6191\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506575840.0000 - rmse: 22507.2402 - val_loss: 434357504.0000 - val_rmse: 20841.2461\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554389312.0000 - rmse: 23545.4727 - val_loss: 436768096.0000 - val_rmse: 20898.9980\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554247872.0000 - rmse: 23542.4688 - val_loss: 448429792.0000 - val_rmse: 21176.1602\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549397888.0000 - rmse: 23439.2383 - val_loss: 453841504.0000 - val_rmse: 21303.5566\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468218176.0000 - rmse: 21638.3496 - val_loss: 397658656.0000 - val_rmse: 19941.3809\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520804576.0000 - rmse: 22821.1426 - val_loss: 470953088.0000 - val_rmse: 21701.4531\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515231936.0000 - rmse: 22698.7207 - val_loss: 454209760.0000 - val_rmse: 21312.1973\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516527936.0000 - rmse: 22727.2500 - val_loss: 467042720.0000 - val_rmse: 21611.1719\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491263104.0000 - rmse: 22164.4551 - val_loss: 407300832.0000 - val_rmse: 20181.6953\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505158784.0000 - rmse: 22475.7383 - val_loss: 404692192.0000 - val_rmse: 20116.9629\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535003648.0000 - rmse: 23130.1465 - val_loss: 445320992.0000 - val_rmse: 21102.6309\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504044832.0000 - rmse: 22450.9434 - val_loss: 438513312.0000 - val_rmse: 20940.7090\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522261088.0000 - rmse: 22853.0332 - val_loss: 431272000.0000 - val_rmse: 20767.0898\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569976576.0000 - rmse: 23874.1816 - val_loss: 424187744.0000 - val_rmse: 20595.8184\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477760960.0000 - rmse: 21857.7441 - val_loss: 497777632.0000 - val_rmse: 22310.9316\n",
      "104/104 [==============================] - 0s 658us/step - loss: 568004608.0000 - rmse: 23832.8477\n",
      "[568004608.0, 23832.84765625]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 24,065\n",
      "Trainable params: 24,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 24,065\n",
      "Trainable params: 24,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 17062579200.0000 - rmse: 130623.8047 - val_loss: 3033216256.0000 - val_rmse: 55074.6445\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2940399360.0000 - rmse: 54225.4492 - val_loss: 1474478976.0000 - val_rmse: 38398.9453\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2048988672.0000 - rmse: 45265.7578 - val_loss: 1177744512.0000 - val_rmse: 34318.2812\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1838506240.0000 - rmse: 42877.8047 - val_loss: 1102173312.0000 - val_rmse: 33198.9961\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1758503168.0000 - rmse: 41934.5117 - val_loss: 1117681536.0000 - val_rmse: 33431.7461\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1702801920.0000 - rmse: 41265.0195 - val_loss: 1018478336.0000 - val_rmse: 31913.6074\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1569785856.0000 - rmse: 39620.5234 - val_loss: 1062632448.0000 - val_rmse: 32598.0430\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1542788864.0000 - rmse: 39278.3516 - val_loss: 981415680.0000 - val_rmse: 31327.5547\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1505001856.0000 - rmse: 38794.3516 - val_loss: 947747648.0000 - val_rmse: 30785.5098\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1527784448.0000 - rmse: 39086.8828 - val_loss: 914141824.0000 - val_rmse: 30234.7793\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1438824576.0000 - rmse: 37931.8398 - val_loss: 896148288.0000 - val_rmse: 29935.7363\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1457555840.0000 - rmse: 38177.9492 - val_loss: 887982528.0000 - val_rmse: 29799.0352\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1444508928.0000 - rmse: 38006.6953 - val_loss: 884069248.0000 - val_rmse: 29733.3027\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1386297216.0000 - rmse: 37233.0117 - val_loss: 835573120.0000 - val_rmse: 28906.2812\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1265670400.0000 - rmse: 35576.2617 - val_loss: 814769728.0000 - val_rmse: 28544.1719\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1217996032.0000 - rmse: 34899.8008 - val_loss: 778129600.0000 - val_rmse: 27894.9746\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1310156416.0000 - rmse: 36196.0820 - val_loss: 901954944.0000 - val_rmse: 30032.5645\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1132652800.0000 - rmse: 33654.9062 - val_loss: 752371520.0000 - val_rmse: 27429.3906\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1194284032.0000 - rmse: 34558.4141 - val_loss: 799083840.0000 - val_rmse: 28268.0703\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1144861824.0000 - rmse: 33835.8086 - val_loss: 760363264.0000 - val_rmse: 27574.6855\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1110787456.0000 - rmse: 33328.4766 - val_loss: 807963136.0000 - val_rmse: 28424.6914\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1063224832.0000 - rmse: 32607.1289 - val_loss: 748593344.0000 - val_rmse: 27360.4336\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1093925632.0000 - rmse: 33074.5469 - val_loss: 741898752.0000 - val_rmse: 27237.8184\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1088863232.0000 - rmse: 32997.9258 - val_loss: 733719872.0000 - val_rmse: 27087.2637\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1089700224.0000 - rmse: 33010.6094 - val_loss: 730955328.0000 - val_rmse: 27036.1855\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1137068800.0000 - rmse: 33720.4492 - val_loss: 822642880.0000 - val_rmse: 28681.7520\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1030598592.0000 - rmse: 32102.9375 - val_loss: 753474432.0000 - val_rmse: 27449.4883\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1081415296.0000 - rmse: 32884.8789 - val_loss: 698865792.0000 - val_rmse: 26436.0703\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1063466752.0000 - rmse: 32610.8379 - val_loss: 663364544.0000 - val_rmse: 25755.8652\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 992471680.0000 - rmse: 31503.5176 - val_loss: 674738112.0000 - val_rmse: 25975.7207\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 984364096.0000 - rmse: 31374.5781 - val_loss: 671916416.0000 - val_rmse: 25921.3496\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 972156288.0000 - rmse: 31179.4219 - val_loss: 867323712.0000 - val_rmse: 29450.3594\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1010950912.0000 - rmse: 31795.4551 - val_loss: 759035264.0000 - val_rmse: 27550.5938\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1044922624.0000 - rmse: 32325.2637 - val_loss: 655846848.0000 - val_rmse: 25609.5078\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 918008576.0000 - rmse: 30298.6562 - val_loss: 641445824.0000 - val_rmse: 25326.7812\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1030431296.0000 - rmse: 32100.3320 - val_loss: 618174400.0000 - val_rmse: 24863.1133\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 967094912.0000 - rmse: 31098.1504 - val_loss: 646327232.0000 - val_rmse: 25422.9668\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1015313088.0000 - rmse: 31863.9785 - val_loss: 634494144.0000 - val_rmse: 25189.1680\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 960174912.0000 - rmse: 30986.6895 - val_loss: 606543424.0000 - val_rmse: 24628.1016\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 901049728.0000 - rmse: 30017.4902 - val_loss: 632602688.0000 - val_rmse: 25151.5938\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 914076416.0000 - rmse: 30233.6973 - val_loss: 683723968.0000 - val_rmse: 26148.1152\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 839913536.0000 - rmse: 28981.2617 - val_loss: 704776512.0000 - val_rmse: 26547.6270\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 822467456.0000 - rmse: 28678.6934 - val_loss: 599985600.0000 - val_rmse: 24494.6035\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 829175744.0000 - rmse: 28795.4121 - val_loss: 588432256.0000 - val_rmse: 24257.6230\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 861134400.0000 - rmse: 29345.0918 - val_loss: 585647168.0000 - val_rmse: 24200.1484\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 902040448.0000 - rmse: 30033.9883 - val_loss: 616372992.0000 - val_rmse: 24826.8594\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 849595136.0000 - rmse: 29147.8145 - val_loss: 552615744.0000 - val_rmse: 23507.7812\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 828617600.0000 - rmse: 28785.7188 - val_loss: 542722624.0000 - val_rmse: 23296.4082\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 901658688.0000 - rmse: 30027.6328 - val_loss: 533842976.0000 - val_rmse: 23105.0430\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 794154496.0000 - rmse: 28180.7461 - val_loss: 593002688.0000 - val_rmse: 24351.6465\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 760175872.0000 - rmse: 27571.2871 - val_loss: 529229536.0000 - val_rmse: 23004.9902\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 877200640.0000 - rmse: 29617.5723 - val_loss: 538869184.0000 - val_rmse: 23213.5566\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 875856192.0000 - rmse: 29594.8672 - val_loss: 555019264.0000 - val_rmse: 23558.8477\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 771418432.0000 - rmse: 27774.4199 - val_loss: 517013024.0000 - val_rmse: 22737.9199\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 797555648.0000 - rmse: 28241.0273 - val_loss: 534822144.0000 - val_rmse: 23126.2227\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 780977792.0000 - rmse: 27945.9805 - val_loss: 560891712.0000 - val_rmse: 23683.1523\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 798080064.0000 - rmse: 28250.3105 - val_loss: 504224064.0000 - val_rmse: 22454.9336\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733830144.0000 - rmse: 27089.2988 - val_loss: 532143936.0000 - val_rmse: 23068.2461\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 824306304.0000 - rmse: 28710.7344 - val_loss: 504170784.0000 - val_rmse: 22453.7480\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 765827776.0000 - rmse: 27673.5938 - val_loss: 498417888.0000 - val_rmse: 22325.2754\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 777106880.0000 - rmse: 27876.6367 - val_loss: 529591200.0000 - val_rmse: 23012.8477\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 719637824.0000 - rmse: 26826.0664 - val_loss: 510191456.0000 - val_rmse: 22587.4180\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 713327040.0000 - rmse: 26708.1836 - val_loss: 490966816.0000 - val_rmse: 22157.7715\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 798449344.0000 - rmse: 28256.8457 - val_loss: 488446432.0000 - val_rmse: 22100.8242\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 839164864.0000 - rmse: 28968.3418 - val_loss: 495141280.0000 - val_rmse: 22251.7695\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 723725952.0000 - rmse: 26902.1543 - val_loss: 551870720.0000 - val_rmse: 23491.9297\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 720427392.0000 - rmse: 26840.7793 - val_loss: 480716896.0000 - val_rmse: 21925.2578\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 755353600.0000 - rmse: 27483.6973 - val_loss: 482211680.0000 - val_rmse: 21959.3184\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730966080.0000 - rmse: 27036.3848 - val_loss: 523802112.0000 - val_rmse: 22886.7227\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 758449408.0000 - rmse: 27539.9609 - val_loss: 464540384.0000 - val_rmse: 21553.1992\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800375232.0000 - rmse: 28290.9043 - val_loss: 510555648.0000 - val_rmse: 22595.4785\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 753957760.0000 - rmse: 27458.2910 - val_loss: 476487104.0000 - val_rmse: 21828.5840\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 664090112.0000 - rmse: 25769.9453 - val_loss: 495243584.0000 - val_rmse: 22254.0684\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651970560.0000 - rmse: 25533.7148 - val_loss: 476553760.0000 - val_rmse: 21830.1113\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 746609344.0000 - rmse: 27324.1523 - val_loss: 444503328.0000 - val_rmse: 21083.2480\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 699819840.0000 - rmse: 26454.1074 - val_loss: 462786016.0000 - val_rmse: 21512.4609\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680593216.0000 - rmse: 26088.1816 - val_loss: 459138240.0000 - val_rmse: 21427.5117\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 767865088.0000 - rmse: 27710.3789 - val_loss: 447601248.0000 - val_rmse: 21156.5879\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680368768.0000 - rmse: 26083.8789 - val_loss: 480566528.0000 - val_rmse: 21921.8281\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 725258048.0000 - rmse: 26930.6152 - val_loss: 444473440.0000 - val_rmse: 21082.5391\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 683300096.0000 - rmse: 26140.0098 - val_loss: 451155008.0000 - val_rmse: 21240.4102\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 699363392.0000 - rmse: 26445.4805 - val_loss: 444061312.0000 - val_rmse: 21072.7617\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661859584.0000 - rmse: 25726.6309 - val_loss: 506195968.0000 - val_rmse: 22498.7988\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 688957184.0000 - rmse: 26247.9941 - val_loss: 451744320.0000 - val_rmse: 21254.2773\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 703382912.0000 - rmse: 26521.3672 - val_loss: 436697152.0000 - val_rmse: 20897.3008\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 719302784.0000 - rmse: 26819.8203 - val_loss: 449142720.0000 - val_rmse: 21192.9883\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 669569280.0000 - rmse: 25876.0371 - val_loss: 431869568.0000 - val_rmse: 20781.4727\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698992576.0000 - rmse: 26438.4668 - val_loss: 429135904.0000 - val_rmse: 20715.5957\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 697129536.0000 - rmse: 26403.2109 - val_loss: 450416800.0000 - val_rmse: 21223.0254\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 738093312.0000 - rmse: 27167.8730 - val_loss: 505535776.0000 - val_rmse: 22484.1230\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 681451904.0000 - rmse: 26104.6328 - val_loss: 424692128.0000 - val_rmse: 20608.0605\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 699700160.0000 - rmse: 26451.8457 - val_loss: 428277376.0000 - val_rmse: 20694.8633\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634978624.0000 - rmse: 25198.7812 - val_loss: 419333632.0000 - val_rmse: 20477.6367\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652605696.0000 - rmse: 25546.1484 - val_loss: 429817824.0000 - val_rmse: 20732.0488\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 660056320.0000 - rmse: 25691.5605 - val_loss: 474141824.0000 - val_rmse: 21774.7988\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 669792064.0000 - rmse: 25880.3418 - val_loss: 479446976.0000 - val_rmse: 21896.2773\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 697782464.0000 - rmse: 26415.5723 - val_loss: 424991008.0000 - val_rmse: 20615.3105\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 683832512.0000 - rmse: 26150.1914 - val_loss: 426448864.0000 - val_rmse: 20650.6387\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612774720.0000 - rmse: 24754.2871 - val_loss: 423399296.0000 - val_rmse: 20576.6680\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666862848.0000 - rmse: 25823.6875 - val_loss: 428534400.0000 - val_rmse: 20701.0723\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 643619456.0000 - rmse: 25369.6562 - val_loss: 455554528.0000 - val_rmse: 21343.7227\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622749440.0000 - rmse: 24954.9473 - val_loss: 423186720.0000 - val_rmse: 20571.5020\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620643712.0000 - rmse: 24912.7227 - val_loss: 433511488.0000 - val_rmse: 20820.9395\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621728640.0000 - rmse: 24934.4863 - val_loss: 411732768.0000 - val_rmse: 20291.1992\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577767360.0000 - rmse: 24036.7910 - val_loss: 413782656.0000 - val_rmse: 20341.6484\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612954816.0000 - rmse: 24757.9238 - val_loss: 424327808.0000 - val_rmse: 20599.2188\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 690578624.0000 - rmse: 26278.8633 - val_loss: 410881312.0000 - val_rmse: 20270.2070\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579623168.0000 - rmse: 24075.3652 - val_loss: 426123776.0000 - val_rmse: 20642.7656\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569142784.0000 - rmse: 23856.7129 - val_loss: 418933280.0000 - val_rmse: 20467.8594\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616728384.0000 - rmse: 24834.0176 - val_loss: 454008288.0000 - val_rmse: 21307.4707\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637353536.0000 - rmse: 25245.8613 - val_loss: 406806656.0000 - val_rmse: 20169.4492\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648872320.0000 - rmse: 25472.9727 - val_loss: 413121088.0000 - val_rmse: 20325.3809\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 619911872.0000 - rmse: 24898.0293 - val_loss: 423481344.0000 - val_rmse: 20578.6621\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564856448.0000 - rmse: 23766.7090 - val_loss: 412491776.0000 - val_rmse: 20309.8926\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620593856.0000 - rmse: 24911.7207 - val_loss: 427913760.0000 - val_rmse: 20686.0762\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656706688.0000 - rmse: 25626.2891 - val_loss: 392022272.0000 - val_rmse: 19799.5527\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576278464.0000 - rmse: 24005.8008 - val_loss: 491722112.0000 - val_rmse: 22174.8086\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591335936.0000 - rmse: 24317.4004 - val_loss: 411864576.0000 - val_rmse: 20294.4473\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610923264.0000 - rmse: 24716.8613 - val_loss: 426773824.0000 - val_rmse: 20658.5059\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585147392.0000 - rmse: 24189.8203 - val_loss: 416662624.0000 - val_rmse: 20412.3164\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618452928.0000 - rmse: 24868.7129 - val_loss: 460986784.0000 - val_rmse: 21470.6035\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620004288.0000 - rmse: 24899.8848 - val_loss: 423008064.0000 - val_rmse: 20567.1602\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651248320.0000 - rmse: 25519.5684 - val_loss: 439396544.0000 - val_rmse: 20961.7871\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 655702784.0000 - rmse: 25606.6934 - val_loss: 385472640.0000 - val_rmse: 19633.4570\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598233280.0000 - rmse: 24458.8086 - val_loss: 490543584.0000 - val_rmse: 22148.2188\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605580352.0000 - rmse: 24608.5430 - val_loss: 397402048.0000 - val_rmse: 19934.9453\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574413632.0000 - rmse: 23966.9277 - val_loss: 399177952.0000 - val_rmse: 19979.4375\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532894336.0000 - rmse: 23084.5039 - val_loss: 401436512.0000 - val_rmse: 20035.8809\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568997824.0000 - rmse: 23853.6758 - val_loss: 405276064.0000 - val_rmse: 20131.4688\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 595419904.0000 - rmse: 24401.2266 - val_loss: 401911232.0000 - val_rmse: 20047.7246\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 657346368.0000 - rmse: 25638.7676 - val_loss: 420454048.0000 - val_rmse: 20504.9766\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612463232.0000 - rmse: 24747.9941 - val_loss: 410708672.0000 - val_rmse: 20265.9492\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 595054208.0000 - rmse: 24393.7324 - val_loss: 430118976.0000 - val_rmse: 20739.3105\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549671296.0000 - rmse: 23445.0703 - val_loss: 422269952.0000 - val_rmse: 20549.2090\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 581978688.0000 - rmse: 24124.2344 - val_loss: 390948256.0000 - val_rmse: 19772.4121\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549386688.0000 - rmse: 23439.0000 - val_loss: 405525920.0000 - val_rmse: 20137.6738\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621963968.0000 - rmse: 24939.2051 - val_loss: 386459840.0000 - val_rmse: 19658.5820\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564777152.0000 - rmse: 23765.0410 - val_loss: 430952000.0000 - val_rmse: 20759.3828\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597061888.0000 - rmse: 24434.8496 - val_loss: 407612608.0000 - val_rmse: 20189.4180\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563191808.0000 - rmse: 23731.6621 - val_loss: 417183872.0000 - val_rmse: 20425.0801\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 581447552.0000 - rmse: 24113.2227 - val_loss: 388475776.0000 - val_rmse: 19709.7891\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540009408.0000 - rmse: 23238.1016 - val_loss: 438136224.0000 - val_rmse: 20931.7031\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560404288.0000 - rmse: 23672.8594 - val_loss: 376559712.0000 - val_rmse: 19405.1465\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604642368.0000 - rmse: 24589.4766 - val_loss: 414652064.0000 - val_rmse: 20363.0078\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652080256.0000 - rmse: 25535.8613 - val_loss: 418487168.0000 - val_rmse: 20456.9590\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548312640.0000 - rmse: 23416.0762 - val_loss: 414532448.0000 - val_rmse: 20360.0703\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532734880.0000 - rmse: 23081.0508 - val_loss: 392899712.0000 - val_rmse: 19821.6973\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588047168.0000 - rmse: 24249.6836 - val_loss: 395322240.0000 - val_rmse: 19882.7129\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646783104.0000 - rmse: 25431.9316 - val_loss: 390569472.0000 - val_rmse: 19762.8301\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560404480.0000 - rmse: 23672.8633 - val_loss: 406297344.0000 - val_rmse: 20156.8184\n",
      "104/104 [==============================] - 0s 647us/step - loss: 326443968.0000 - rmse: 18067.7598\n",
      "[326443968.0, 18067.759765625]\n",
      "[21248.73046875, 28795.0078125, 29323.853515625, 23832.84765625, 18067.759765625]\n",
      "24253.63984375\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "!python train.py kfold light\n",
    "## layer+1 d 0.3"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 19:38:03.702326: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 19:38:03.702363: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 19:38:03.702677: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 20,961\n",
      "Trainable params: 20,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 20,961\n",
      "Trainable params: 20,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 19:38:03.891719: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 9411990528.0000 - rmse: 97015.4141 - val_loss: 1344367744.0000 - val_rmse: 36665.6211\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1992233216.0000 - rmse: 44634.4414 - val_loss: 1056953920.0000 - val_rmse: 32510.8281\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1816627328.0000 - rmse: 42621.9102 - val_loss: 916514240.0000 - val_rmse: 30273.9863\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1765806720.0000 - rmse: 42021.5039 - val_loss: 856155072.0000 - val_rmse: 29260.1270\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1608349696.0000 - rmse: 40104.2344 - val_loss: 1052831040.0000 - val_rmse: 32447.3574\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1565027968.0000 - rmse: 39560.4336 - val_loss: 1472622976.0000 - val_rmse: 38374.7695\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1485116160.0000 - rmse: 38537.2031 - val_loss: 752717440.0000 - val_rmse: 27435.6973\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1574294528.0000 - rmse: 39677.3789 - val_loss: 684810112.0000 - val_rmse: 26168.8770\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1631353856.0000 - rmse: 40390.0234 - val_loss: 655864704.0000 - val_rmse: 25609.8555\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1393759744.0000 - rmse: 37333.0938 - val_loss: 721355776.0000 - val_rmse: 26858.0664\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1323296640.0000 - rmse: 36377.1445 - val_loss: 642059520.0000 - val_rmse: 25338.8926\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1386717568.0000 - rmse: 37238.6562 - val_loss: 735139968.0000 - val_rmse: 27113.4648\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1309706112.0000 - rmse: 36189.8633 - val_loss: 626995392.0000 - val_rmse: 25039.8770\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1221875968.0000 - rmse: 34955.3438 - val_loss: 566294208.0000 - val_rmse: 23796.9375\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1255374848.0000 - rmse: 35431.2695 - val_loss: 677272256.0000 - val_rmse: 26024.4551\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1240339712.0000 - rmse: 35218.4570 - val_loss: 574073664.0000 - val_rmse: 23959.8340\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1328795136.0000 - rmse: 36452.6406 - val_loss: 549137792.0000 - val_rmse: 23433.6895\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1254743424.0000 - rmse: 35422.3594 - val_loss: 584779392.0000 - val_rmse: 24182.2129\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1236964096.0000 - rmse: 35170.5000 - val_loss: 525333312.0000 - val_rmse: 22920.1504\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1117116032.0000 - rmse: 33423.2852 - val_loss: 634402560.0000 - val_rmse: 25187.3496\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1188193792.0000 - rmse: 34470.1875 - val_loss: 540833728.0000 - val_rmse: 23255.8320\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1199470336.0000 - rmse: 34633.3711 - val_loss: 540373056.0000 - val_rmse: 23245.9258\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1104950656.0000 - rmse: 33240.7969 - val_loss: 468526368.0000 - val_rmse: 21645.4707\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1176785792.0000 - rmse: 34304.3125 - val_loss: 561705408.0000 - val_rmse: 23700.3242\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1064153728.0000 - rmse: 32621.3691 - val_loss: 521522848.0000 - val_rmse: 22836.8750\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1091651968.0000 - rmse: 33040.1562 - val_loss: 493627264.0000 - val_rmse: 22217.7246\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1079507072.0000 - rmse: 32855.8516 - val_loss: 508217088.0000 - val_rmse: 22543.6699\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1115630720.0000 - rmse: 33401.0586 - val_loss: 482230080.0000 - val_rmse: 21959.7383\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1018782720.0000 - rmse: 31918.3750 - val_loss: 435405984.0000 - val_rmse: 20866.3848\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 969843584.0000 - rmse: 31142.3125 - val_loss: 434599392.0000 - val_rmse: 20847.0469\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1047241280.0000 - rmse: 32361.1074 - val_loss: 601320000.0000 - val_rmse: 24521.8262\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1040235712.0000 - rmse: 32252.6855 - val_loss: 409027424.0000 - val_rmse: 20224.4258\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1047085248.0000 - rmse: 32358.6973 - val_loss: 389273312.0000 - val_rmse: 19730.0098\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 946077888.0000 - rmse: 30758.3789 - val_loss: 475693056.0000 - val_rmse: 21810.3887\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 953687744.0000 - rmse: 30881.8359 - val_loss: 404407552.0000 - val_rmse: 20109.8867\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 909993984.0000 - rmse: 30166.1074 - val_loss: 403957920.0000 - val_rmse: 20098.7051\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 981427520.0000 - rmse: 31327.7441 - val_loss: 459399744.0000 - val_rmse: 21433.6133\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 937549120.0000 - rmse: 30619.4238 - val_loss: 391167680.0000 - val_rmse: 19777.9590\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 921706880.0000 - rmse: 30359.6250 - val_loss: 396292768.0000 - val_rmse: 19907.1035\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1013961536.0000 - rmse: 31842.7617 - val_loss: 429181056.0000 - val_rmse: 20716.6855\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 923180992.0000 - rmse: 30383.8945 - val_loss: 580786880.0000 - val_rmse: 24099.5195\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 927184832.0000 - rmse: 30449.7090 - val_loss: 463754912.0000 - val_rmse: 21534.9688\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 892271232.0000 - rmse: 29870.9102 - val_loss: 405361984.0000 - val_rmse: 20133.6035\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 988813696.0000 - rmse: 31445.4082 - val_loss: 383526720.0000 - val_rmse: 19583.8379\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 926689344.0000 - rmse: 30441.5723 - val_loss: 405604224.0000 - val_rmse: 20139.6191\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 917433088.0000 - rmse: 30289.1582 - val_loss: 352051904.0000 - val_rmse: 18763.0469\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 885324864.0000 - rmse: 29754.4082 - val_loss: 349303776.0000 - val_rmse: 18689.6699\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 868468480.0000 - rmse: 29469.7891 - val_loss: 512344992.0000 - val_rmse: 22635.0391\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 867208896.0000 - rmse: 29448.4102 - val_loss: 394046816.0000 - val_rmse: 19850.6133\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 838890944.0000 - rmse: 28963.6133 - val_loss: 337118336.0000 - val_rmse: 18360.7832\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 918987648.0000 - rmse: 30314.8086 - val_loss: 453704480.0000 - val_rmse: 21300.3398\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 859584768.0000 - rmse: 29318.6758 - val_loss: 349448576.0000 - val_rmse: 18693.5430\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 906552320.0000 - rmse: 30109.0078 - val_loss: 363623168.0000 - val_rmse: 19068.9062\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800065984.0000 - rmse: 28285.4375 - val_loss: 393034016.0000 - val_rmse: 19825.0859\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 816360320.0000 - rmse: 28572.0195 - val_loss: 343728288.0000 - val_rmse: 18539.9102\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 924564288.0000 - rmse: 30406.6484 - val_loss: 334416128.0000 - val_rmse: 18287.0488\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 877671360.0000 - rmse: 29625.5195 - val_loss: 379973152.0000 - val_rmse: 19492.9004\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 846270784.0000 - rmse: 29090.7344 - val_loss: 347225664.0000 - val_rmse: 18633.9922\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 826461376.0000 - rmse: 28748.2422 - val_loss: 359668576.0000 - val_rmse: 18964.9297\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 749697984.0000 - rmse: 27380.6133 - val_loss: 470602240.0000 - val_rmse: 21693.3691\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 754859392.0000 - rmse: 27474.7051 - val_loss: 380390112.0000 - val_rmse: 19503.5918\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 825662080.0000 - rmse: 28734.3359 - val_loss: 342560064.0000 - val_rmse: 18508.3789\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 786883840.0000 - rmse: 28051.4492 - val_loss: 312228288.0000 - val_rmse: 17669.9824\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 821654784.0000 - rmse: 28664.5215 - val_loss: 322558272.0000 - val_rmse: 17959.9082\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 828394944.0000 - rmse: 28781.8516 - val_loss: 356998880.0000 - val_rmse: 18894.4141\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 823744768.0000 - rmse: 28700.9551 - val_loss: 591553856.0000 - val_rmse: 24321.8809\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 731850752.0000 - rmse: 27052.7402 - val_loss: 373132512.0000 - val_rmse: 19316.6387\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 781210176.0000 - rmse: 27950.1367 - val_loss: 393353792.0000 - val_rmse: 19833.1484\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 777319552.0000 - rmse: 27880.4512 - val_loss: 669393280.0000 - val_rmse: 25872.6348\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 881019392.0000 - rmse: 29681.9707 - val_loss: 469271232.0000 - val_rmse: 21662.6699\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 707136384.0000 - rmse: 26592.0352 - val_loss: 436723424.0000 - val_rmse: 20897.9277\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 819491648.0000 - rmse: 28626.7637 - val_loss: 419350720.0000 - val_rmse: 20478.0547\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 766541440.0000 - rmse: 27686.4844 - val_loss: 431970464.0000 - val_rmse: 20783.8984\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 659530368.0000 - rmse: 25681.3242 - val_loss: 309935200.0000 - val_rmse: 17604.9766\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 771203072.0000 - rmse: 27770.5430 - val_loss: 352303488.0000 - val_rmse: 18769.7500\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 752451584.0000 - rmse: 27430.8516 - val_loss: 505649280.0000 - val_rmse: 22486.6465\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 727483264.0000 - rmse: 26971.8984 - val_loss: 301882784.0000 - val_rmse: 17374.7734\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706844480.0000 - rmse: 26586.5469 - val_loss: 315186240.0000 - val_rmse: 17753.4844\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656261568.0000 - rmse: 25617.6035 - val_loss: 327511584.0000 - val_rmse: 18097.2812\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 770384064.0000 - rmse: 27755.7930 - val_loss: 375122816.0000 - val_rmse: 19368.0879\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 750143488.0000 - rmse: 27388.7480 - val_loss: 296947488.0000 - val_rmse: 17232.1641\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718116224.0000 - rmse: 26797.6914 - val_loss: 315825536.0000 - val_rmse: 17771.4805\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 727243328.0000 - rmse: 26967.4492 - val_loss: 305687648.0000 - val_rmse: 17483.9258\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 682552768.0000 - rmse: 26125.7109 - val_loss: 396811392.0000 - val_rmse: 19920.1250\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721989568.0000 - rmse: 26869.8633 - val_loss: 300960224.0000 - val_rmse: 17348.2051\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 726239488.0000 - rmse: 26948.8301 - val_loss: 330846304.0000 - val_rmse: 18189.1816\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730216512.0000 - rmse: 27022.5195 - val_loss: 354192768.0000 - val_rmse: 18820.0098\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676808640.0000 - rmse: 26015.5469 - val_loss: 429825952.0000 - val_rmse: 20732.2441\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680469248.0000 - rmse: 26085.8047 - val_loss: 411706048.0000 - val_rmse: 20290.5410\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 639501568.0000 - rmse: 25288.3691 - val_loss: 309678368.0000 - val_rmse: 17597.6816\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634915328.0000 - rmse: 25197.5254 - val_loss: 352700032.0000 - val_rmse: 18780.3105\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 662671360.0000 - rmse: 25742.4043 - val_loss: 328575968.0000 - val_rmse: 18126.6641\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615559872.0000 - rmse: 24810.4785 - val_loss: 333316192.0000 - val_rmse: 18256.9492\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629619904.0000 - rmse: 25092.2285 - val_loss: 463022176.0000 - val_rmse: 21517.9492\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651254720.0000 - rmse: 25519.6934 - val_loss: 943212096.0000 - val_rmse: 30711.7578\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 655055744.0000 - rmse: 25594.0566 - val_loss: 293892640.0000 - val_rmse: 17143.2969\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 638266176.0000 - rmse: 25263.9297 - val_loss: 451047872.0000 - val_rmse: 21237.8867\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623803072.0000 - rmse: 24976.0508 - val_loss: 344926944.0000 - val_rmse: 18572.2090\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 603352256.0000 - rmse: 24563.2305 - val_loss: 324381984.0000 - val_rmse: 18010.6074\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 679956160.0000 - rmse: 26075.9688 - val_loss: 327637088.0000 - val_rmse: 18100.7480\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626672064.0000 - rmse: 25033.4180 - val_loss: 314509568.0000 - val_rmse: 17734.4180\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 672779712.0000 - rmse: 25937.9980 - val_loss: 328306912.0000 - val_rmse: 18119.2422\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691240960.0000 - rmse: 26291.4609 - val_loss: 338346464.0000 - val_rmse: 18394.1973\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580243520.0000 - rmse: 24088.2441 - val_loss: 344084352.0000 - val_rmse: 18549.5117\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618509632.0000 - rmse: 24869.8535 - val_loss: 470481920.0000 - val_rmse: 21690.5957\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647488064.0000 - rmse: 25445.7871 - val_loss: 516574464.0000 - val_rmse: 22728.2754\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 627628864.0000 - rmse: 25052.5215 - val_loss: 425905344.0000 - val_rmse: 20637.4746\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 653751552.0000 - rmse: 25568.5664 - val_loss: 481120768.0000 - val_rmse: 21934.4648\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572541504.0000 - rmse: 23927.8398 - val_loss: 547263936.0000 - val_rmse: 23393.6738\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 670447936.0000 - rmse: 25893.0098 - val_loss: 413414016.0000 - val_rmse: 20332.5859\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 613376384.0000 - rmse: 24766.4375 - val_loss: 308114400.0000 - val_rmse: 17553.1875\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561171520.0000 - rmse: 23689.0586 - val_loss: 308776544.0000 - val_rmse: 17572.0391\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685335680.0000 - rmse: 26178.9160 - val_loss: 337955712.0000 - val_rmse: 18383.5723\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572613312.0000 - rmse: 23929.3398 - val_loss: 338203680.0000 - val_rmse: 18390.3145\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 726103168.0000 - rmse: 26946.3008 - val_loss: 355886432.0000 - val_rmse: 18864.9531\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 635633472.0000 - rmse: 25211.7734 - val_loss: 298168032.0000 - val_rmse: 17267.5430\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 627487360.0000 - rmse: 25049.6973 - val_loss: 334420288.0000 - val_rmse: 18287.1621\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525172704.0000 - rmse: 22916.6465 - val_loss: 465498496.0000 - val_rmse: 21575.4141\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637330368.0000 - rmse: 25245.4023 - val_loss: 291886848.0000 - val_rmse: 17084.6973\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598521472.0000 - rmse: 24464.6992 - val_loss: 528730432.0000 - val_rmse: 22994.1387\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579226496.0000 - rmse: 24067.1250 - val_loss: 388946368.0000 - val_rmse: 19721.7227\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572895552.0000 - rmse: 23935.2363 - val_loss: 380046304.0000 - val_rmse: 19494.7754\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612687232.0000 - rmse: 24752.5195 - val_loss: 355851904.0000 - val_rmse: 18864.0371\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541188800.0000 - rmse: 23263.4648 - val_loss: 345277536.0000 - val_rmse: 18581.6445\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632121920.0000 - rmse: 25142.0352 - val_loss: 396680736.0000 - val_rmse: 19916.8457\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582256640.0000 - rmse: 24129.9941 - val_loss: 298288608.0000 - val_rmse: 17271.0332\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 627469888.0000 - rmse: 25049.3496 - val_loss: 361889248.0000 - val_rmse: 19023.3867\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533363168.0000 - rmse: 23094.6562 - val_loss: 354412800.0000 - val_rmse: 18825.8555\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590635200.0000 - rmse: 24302.9883 - val_loss: 486209888.0000 - val_rmse: 22050.1680\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535462144.0000 - rmse: 23140.0547 - val_loss: 531488384.0000 - val_rmse: 23054.0312\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476389088.0000 - rmse: 21826.3398 - val_loss: 387665184.0000 - val_rmse: 19689.2148\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517353088.0000 - rmse: 22745.3965 - val_loss: 351574304.0000 - val_rmse: 18750.3145\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599886912.0000 - rmse: 24492.5898 - val_loss: 385812960.0000 - val_rmse: 19642.1230\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555077504.0000 - rmse: 23560.0820 - val_loss: 435131136.0000 - val_rmse: 20859.7969\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620881664.0000 - rmse: 24917.4980 - val_loss: 352375040.0000 - val_rmse: 18771.6543\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572617664.0000 - rmse: 23929.4316 - val_loss: 348022432.0000 - val_rmse: 18655.3594\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572902912.0000 - rmse: 23935.3906 - val_loss: 621965696.0000 - val_rmse: 24939.2402\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573018368.0000 - rmse: 23937.8027 - val_loss: 456860032.0000 - val_rmse: 21374.2852\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503302368.0000 - rmse: 22434.4023 - val_loss: 333194720.0000 - val_rmse: 18253.6230\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517952288.0000 - rmse: 22758.5645 - val_loss: 318256960.0000 - val_rmse: 17839.7578\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523656928.0000 - rmse: 22883.5508 - val_loss: 394272256.0000 - val_rmse: 19856.2910\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535330560.0000 - rmse: 23137.2109 - val_loss: 388335680.0000 - val_rmse: 19706.2344\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577920448.0000 - rmse: 24039.9766 - val_loss: 417573120.0000 - val_rmse: 20434.6055\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623694272.0000 - rmse: 24973.8711 - val_loss: 315180832.0000 - val_rmse: 17753.3320\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559089600.0000 - rmse: 23645.0762 - val_loss: 337135264.0000 - val_rmse: 18361.2441\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556632832.0000 - rmse: 23593.0684 - val_loss: 347990048.0000 - val_rmse: 18654.4922\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515287648.0000 - rmse: 22699.9473 - val_loss: 350291584.0000 - val_rmse: 18716.0781\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555517120.0000 - rmse: 23569.4102 - val_loss: 440385248.0000 - val_rmse: 20985.3574\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534070784.0000 - rmse: 23109.9707 - val_loss: 359643552.0000 - val_rmse: 18964.2695\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510947936.0000 - rmse: 22604.1582 - val_loss: 379334976.0000 - val_rmse: 19476.5234\n",
      "104/104 [==============================] - 0s 687us/step - loss: 478534784.0000 - rmse: 21875.4375\n",
      "[478534784.0, 21875.4375]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 20,961\n",
      "Trainable params: 20,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 20,961\n",
      "Trainable params: 20,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 8465716736.0000 - rmse: 92009.3281 - val_loss: 1384087680.0000 - val_rmse: 37203.3281\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1690741632.0000 - rmse: 41118.6289 - val_loss: 1166325120.0000 - val_rmse: 34151.5039\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1640298752.0000 - rmse: 40500.6016 - val_loss: 1176647424.0000 - val_rmse: 34302.2930\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1523726080.0000 - rmse: 39034.9336 - val_loss: 1112771840.0000 - val_rmse: 33358.2344\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1472007296.0000 - rmse: 38366.7461 - val_loss: 1031947648.0000 - val_rmse: 32123.9414\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1401793920.0000 - rmse: 37440.5391 - val_loss: 959335296.0000 - val_rmse: 30973.1387\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1364571392.0000 - rmse: 36940.1055 - val_loss: 1278541312.0000 - val_rmse: 35756.6953\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1291392768.0000 - rmse: 35935.9531 - val_loss: 933872256.0000 - val_rmse: 30559.3242\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1330450176.0000 - rmse: 36475.3359 - val_loss: 1048382848.0000 - val_rmse: 32378.7402\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1220060160.0000 - rmse: 34929.3594 - val_loss: 835270592.0000 - val_rmse: 28901.0488\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1194872064.0000 - rmse: 34566.9219 - val_loss: 834345280.0000 - val_rmse: 28885.0352\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1229363584.0000 - rmse: 35062.2812 - val_loss: 799839360.0000 - val_rmse: 28281.4316\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1159331968.0000 - rmse: 34048.9648 - val_loss: 806233408.0000 - val_rmse: 28394.2500\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1199028224.0000 - rmse: 34626.9883 - val_loss: 948719744.0000 - val_rmse: 30801.2949\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1086927616.0000 - rmse: 32968.5859 - val_loss: 738695360.0000 - val_rmse: 27178.9512\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1078729728.0000 - rmse: 32844.0195 - val_loss: 728257088.0000 - val_rmse: 26986.2383\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1137835648.0000 - rmse: 33731.8203 - val_loss: 721280000.0000 - val_rmse: 26856.6562\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1143252992.0000 - rmse: 33812.0234 - val_loss: 698112896.0000 - val_rmse: 26421.8262\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1073136832.0000 - rmse: 32758.7676 - val_loss: 744959040.0000 - val_rmse: 27293.9375\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 964225408.0000 - rmse: 31051.9785 - val_loss: 704951872.0000 - val_rmse: 26550.9297\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1051193984.0000 - rmse: 32422.1211 - val_loss: 740874304.0000 - val_rmse: 27219.0059\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 968576832.0000 - rmse: 31121.9668 - val_loss: 738183552.0000 - val_rmse: 27169.5332\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 901882240.0000 - rmse: 30031.3535 - val_loss: 688689088.0000 - val_rmse: 26242.8867\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 966454912.0000 - rmse: 31087.8574 - val_loss: 675651264.0000 - val_rmse: 25993.2930\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 936374016.0000 - rmse: 30600.2285 - val_loss: 673206400.0000 - val_rmse: 25946.2207\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1005932352.0000 - rmse: 31716.4375 - val_loss: 751240704.0000 - val_rmse: 27408.7715\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 962619136.0000 - rmse: 31026.1035 - val_loss: 648184448.0000 - val_rmse: 25459.4668\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 943890432.0000 - rmse: 30722.8008 - val_loss: 721394816.0000 - val_rmse: 26858.7949\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 940906432.0000 - rmse: 30674.1973 - val_loss: 630855680.0000 - val_rmse: 25116.8398\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 849187072.0000 - rmse: 29140.8145 - val_loss: 608013376.0000 - val_rmse: 24657.9277\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 872284928.0000 - rmse: 29534.4707 - val_loss: 641618944.0000 - val_rmse: 25330.1992\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 840004416.0000 - rmse: 28982.8301 - val_loss: 609259456.0000 - val_rmse: 24683.1816\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 920481600.0000 - rmse: 30339.4395 - val_loss: 613809792.0000 - val_rmse: 24775.1855\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 853814976.0000 - rmse: 29220.1133 - val_loss: 606520640.0000 - val_rmse: 24627.6406\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 875855488.0000 - rmse: 29594.8555 - val_loss: 592233472.0000 - val_rmse: 24335.8477\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 839256960.0000 - rmse: 28969.9316 - val_loss: 597349824.0000 - val_rmse: 24440.7402\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 919106688.0000 - rmse: 30316.7715 - val_loss: 585299648.0000 - val_rmse: 24192.9668\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 886439424.0000 - rmse: 29773.1328 - val_loss: 574102208.0000 - val_rmse: 23960.4297\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 872394304.0000 - rmse: 29536.3223 - val_loss: 570383872.0000 - val_rmse: 23882.7109\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 859267904.0000 - rmse: 29313.2715 - val_loss: 588711360.0000 - val_rmse: 24263.3750\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 810637440.0000 - rmse: 28471.6953 - val_loss: 587878976.0000 - val_rmse: 24246.2148\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 882875392.0000 - rmse: 29713.2188 - val_loss: 539824320.0000 - val_rmse: 23234.1191\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 834633728.0000 - rmse: 28890.0273 - val_loss: 548325248.0000 - val_rmse: 23416.3457\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 736611584.0000 - rmse: 27140.5898 - val_loss: 545065792.0000 - val_rmse: 23346.6445\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 853603968.0000 - rmse: 29216.5020 - val_loss: 537104576.0000 - val_rmse: 23175.5176\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800040320.0000 - rmse: 28284.9844 - val_loss: 567367168.0000 - val_rmse: 23819.4707\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 783449664.0000 - rmse: 27990.1699 - val_loss: 545912832.0000 - val_rmse: 23364.7773\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 835833792.0000 - rmse: 28910.7910 - val_loss: 559056896.0000 - val_rmse: 23644.3848\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 744754048.0000 - rmse: 27290.1816 - val_loss: 522985536.0000 - val_rmse: 22868.8770\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782660288.0000 - rmse: 27976.0664 - val_loss: 508099680.0000 - val_rmse: 22541.0664\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 815953792.0000 - rmse: 28564.9043 - val_loss: 498804896.0000 - val_rmse: 22333.9414\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782612992.0000 - rmse: 27975.2207 - val_loss: 534599232.0000 - val_rmse: 23121.4023\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 763275712.0000 - rmse: 27627.4453 - val_loss: 486369728.0000 - val_rmse: 22053.7910\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724708992.0000 - rmse: 26920.4199 - val_loss: 501098880.0000 - val_rmse: 22385.2383\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 739452224.0000 - rmse: 27192.8711 - val_loss: 535629664.0000 - val_rmse: 23143.6738\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 759359744.0000 - rmse: 27556.4824 - val_loss: 493310464.0000 - val_rmse: 22210.5938\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 747925568.0000 - rmse: 27348.2285 - val_loss: 523894336.0000 - val_rmse: 22888.7383\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 689469568.0000 - rmse: 26257.7520 - val_loss: 491592576.0000 - val_rmse: 22171.8867\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 703588160.0000 - rmse: 26525.2363 - val_loss: 536338592.0000 - val_rmse: 23158.9844\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 723395712.0000 - rmse: 26896.0176 - val_loss: 474983168.0000 - val_rmse: 21794.1094\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 725128000.0000 - rmse: 26928.2012 - val_loss: 570064448.0000 - val_rmse: 23876.0215\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 764804992.0000 - rmse: 27655.1074 - val_loss: 465319808.0000 - val_rmse: 21571.2734\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 673496512.0000 - rmse: 25951.8105 - val_loss: 596849088.0000 - val_rmse: 24430.4941\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 686145344.0000 - rmse: 26194.3770 - val_loss: 458101536.0000 - val_rmse: 21403.3066\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692698944.0000 - rmse: 26319.1738 - val_loss: 469006240.0000 - val_rmse: 21656.5527\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 710043776.0000 - rmse: 26646.6465 - val_loss: 484763904.0000 - val_rmse: 22017.3555\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 714020352.0000 - rmse: 26721.1602 - val_loss: 488721280.0000 - val_rmse: 22107.0410\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 671213504.0000 - rmse: 25907.7891 - val_loss: 451831392.0000 - val_rmse: 21256.3262\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 690434560.0000 - rmse: 26276.1211 - val_loss: 436875264.0000 - val_rmse: 20901.5605\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691019904.0000 - rmse: 26287.2578 - val_loss: 436686784.0000 - val_rmse: 20897.0527\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665926720.0000 - rmse: 25805.5566 - val_loss: 486731200.0000 - val_rmse: 22061.9863\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 679741504.0000 - rmse: 26071.8535 - val_loss: 429904128.0000 - val_rmse: 20734.1289\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687214080.0000 - rmse: 26214.7676 - val_loss: 477159136.0000 - val_rmse: 21843.9727\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729969728.0000 - rmse: 27017.9512 - val_loss: 449355104.0000 - val_rmse: 21197.9980\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540371840.0000 - rmse: 23245.9004 - val_loss: 429792832.0000 - val_rmse: 20731.4453\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610709568.0000 - rmse: 24712.5391 - val_loss: 421727872.0000 - val_rmse: 20536.0137\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676184960.0000 - rmse: 26003.5566 - val_loss: 457462112.0000 - val_rmse: 21388.3633\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647693376.0000 - rmse: 25449.8203 - val_loss: 451780448.0000 - val_rmse: 21255.1270\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654550720.0000 - rmse: 25584.1895 - val_loss: 436541536.0000 - val_rmse: 20893.5762\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 644289536.0000 - rmse: 25382.8594 - val_loss: 533864832.0000 - val_rmse: 23105.5156\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575225728.0000 - rmse: 23983.8633 - val_loss: 431030240.0000 - val_rmse: 20761.2676\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649305728.0000 - rmse: 25481.4785 - val_loss: 395089760.0000 - val_rmse: 19876.8652\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544854848.0000 - rmse: 23342.1270 - val_loss: 414349792.0000 - val_rmse: 20355.5840\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629291456.0000 - rmse: 25085.6816 - val_loss: 504211232.0000 - val_rmse: 22454.6484\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576881152.0000 - rmse: 24018.3496 - val_loss: 420900416.0000 - val_rmse: 20515.8574\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604859328.0000 - rmse: 24593.8887 - val_loss: 484809312.0000 - val_rmse: 22018.3867\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 673641216.0000 - rmse: 25954.5996 - val_loss: 416083808.0000 - val_rmse: 20398.1328\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588772736.0000 - rmse: 24264.6387 - val_loss: 408337376.0000 - val_rmse: 20207.3594\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582811968.0000 - rmse: 24141.4980 - val_loss: 456006560.0000 - val_rmse: 21354.3105\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547646976.0000 - rmse: 23401.8574 - val_loss: 423120352.0000 - val_rmse: 20569.8887\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516053888.0000 - rmse: 22716.8203 - val_loss: 544372672.0000 - val_rmse: 23331.7949\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538084800.0000 - rmse: 23196.6543 - val_loss: 424924352.0000 - val_rmse: 20613.6934\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536276736.0000 - rmse: 23157.6504 - val_loss: 485373344.0000 - val_rmse: 22031.1895\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567570240.0000 - rmse: 23823.7324 - val_loss: 539745536.0000 - val_rmse: 23232.4238\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596077312.0000 - rmse: 24414.6953 - val_loss: 400063136.0000 - val_rmse: 20001.5781\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615806016.0000 - rmse: 24815.4395 - val_loss: 465096736.0000 - val_rmse: 21566.1016\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566419904.0000 - rmse: 23799.5781 - val_loss: 486417184.0000 - val_rmse: 22054.8672\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591724096.0000 - rmse: 24325.3789 - val_loss: 384344096.0000 - val_rmse: 19604.6953\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546255424.0000 - rmse: 23372.1074 - val_loss: 446872544.0000 - val_rmse: 21139.3594\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540996480.0000 - rmse: 23259.3301 - val_loss: 420240864.0000 - val_rmse: 20499.7773\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539243328.0000 - rmse: 23221.6133 - val_loss: 437868608.0000 - val_rmse: 20925.3105\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536781824.0000 - rmse: 23168.5527 - val_loss: 474011584.0000 - val_rmse: 21771.8066\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553337856.0000 - rmse: 23523.1348 - val_loss: 412070784.0000 - val_rmse: 20299.5273\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545753984.0000 - rmse: 23361.3789 - val_loss: 402387520.0000 - val_rmse: 20059.5996\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545257216.0000 - rmse: 23350.7441 - val_loss: 392699616.0000 - val_rmse: 19816.6504\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493825920.0000 - rmse: 22222.1934 - val_loss: 417714656.0000 - val_rmse: 20438.0684\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557256256.0000 - rmse: 23606.2754 - val_loss: 385535872.0000 - val_rmse: 19635.0684\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482056544.0000 - rmse: 21955.7852 - val_loss: 408759904.0000 - val_rmse: 20217.8125\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538702336.0000 - rmse: 23209.9629 - val_loss: 459868064.0000 - val_rmse: 21444.5352\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488628032.0000 - rmse: 22104.9316 - val_loss: 412208960.0000 - val_rmse: 20302.9297\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540235584.0000 - rmse: 23242.9688 - val_loss: 446112768.0000 - val_rmse: 21121.3809\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489183360.0000 - rmse: 22117.4902 - val_loss: 394056320.0000 - val_rmse: 19850.8516\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533854624.0000 - rmse: 23105.2949 - val_loss: 452088864.0000 - val_rmse: 21262.3809\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504620736.0000 - rmse: 22463.7656 - val_loss: 416521792.0000 - val_rmse: 20408.8652\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537721472.0000 - rmse: 23188.8223 - val_loss: 470327072.0000 - val_rmse: 21687.0254\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459123136.0000 - rmse: 21427.1582 - val_loss: 401477920.0000 - val_rmse: 20036.9141\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500755648.0000 - rmse: 22377.5703 - val_loss: 415079904.0000 - val_rmse: 20373.5098\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494098528.0000 - rmse: 22228.3281 - val_loss: 434837600.0000 - val_rmse: 20852.7598\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477810976.0000 - rmse: 21858.8887 - val_loss: 411090048.0000 - val_rmse: 20275.3555\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414134144.0000 - rmse: 20350.2852 - val_loss: 479110016.0000 - val_rmse: 21888.5820\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490043680.0000 - rmse: 22136.9297 - val_loss: 453955360.0000 - val_rmse: 21306.2285\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510810848.0000 - rmse: 22601.1250 - val_loss: 433581440.0000 - val_rmse: 20822.6191\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474719488.0000 - rmse: 21788.0586 - val_loss: 397398208.0000 - val_rmse: 19934.8496\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498307488.0000 - rmse: 22322.8027 - val_loss: 421039296.0000 - val_rmse: 20519.2422\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469650048.0000 - rmse: 21671.4102 - val_loss: 379031680.0000 - val_rmse: 19468.7363\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469999008.0000 - rmse: 21679.4609 - val_loss: 498188928.0000 - val_rmse: 22320.1465\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467560000.0000 - rmse: 21623.1367 - val_loss: 441729920.0000 - val_rmse: 21017.3711\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476986720.0000 - rmse: 21840.0254 - val_loss: 464759296.0000 - val_rmse: 21558.2773\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461182304.0000 - rmse: 21475.1562 - val_loss: 396422144.0000 - val_rmse: 19910.3535\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477299168.0000 - rmse: 21847.1777 - val_loss: 406793120.0000 - val_rmse: 20169.1133\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466967264.0000 - rmse: 21609.4258 - val_loss: 424072128.0000 - val_rmse: 20593.0117\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497244256.0000 - rmse: 22298.9746 - val_loss: 371957824.0000 - val_rmse: 19286.2090\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518942336.0000 - rmse: 22780.3066 - val_loss: 399268128.0000 - val_rmse: 19981.6953\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485432992.0000 - rmse: 22032.5430 - val_loss: 420427936.0000 - val_rmse: 20504.3398\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424965440.0000 - rmse: 20614.6895 - val_loss: 429257344.0000 - val_rmse: 20718.5273\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419040384.0000 - rmse: 20470.4766 - val_loss: 390515360.0000 - val_rmse: 19761.4609\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424989344.0000 - rmse: 20615.2695 - val_loss: 424395232.0000 - val_rmse: 20600.8555\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459242048.0000 - rmse: 21429.9336 - val_loss: 373285440.0000 - val_rmse: 19320.5957\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460163520.0000 - rmse: 21451.4219 - val_loss: 401046112.0000 - val_rmse: 20026.1348\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442795200.0000 - rmse: 21042.6992 - val_loss: 380643936.0000 - val_rmse: 19510.0977\n",
      "104/104 [==============================] - 0s 703us/step - loss: 899627584.0000 - rmse: 29993.7930\n",
      "[899627584.0, 29993.79296875]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 20,961\n",
      "Trainable params: 20,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 20,961\n",
      "Trainable params: 20,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 10522949632.0000 - rmse: 102581.4297 - val_loss: 1625906304.0000 - val_rmse: 40322.5273\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1822922112.0000 - rmse: 42695.6914 - val_loss: 1359165696.0000 - val_rmse: 36866.8633\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1627752320.0000 - rmse: 40345.4141 - val_loss: 1215713792.0000 - val_rmse: 34867.0859\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1519475200.0000 - rmse: 38980.4453 - val_loss: 1164849024.0000 - val_rmse: 34129.8828\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1366198656.0000 - rmse: 36962.1250 - val_loss: 1145513344.0000 - val_rmse: 33845.4336\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1356068864.0000 - rmse: 36824.8398 - val_loss: 1182703104.0000 - val_rmse: 34390.4492\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1339709568.0000 - rmse: 36602.0430 - val_loss: 1014574016.0000 - val_rmse: 31852.3789\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1243515392.0000 - rmse: 35263.5117 - val_loss: 959469440.0000 - val_rmse: 30975.3047\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1154310016.0000 - rmse: 33975.1367 - val_loss: 1509710848.0000 - val_rmse: 38854.9961\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1205066624.0000 - rmse: 34714.0703 - val_loss: 932729984.0000 - val_rmse: 30540.6289\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1065096896.0000 - rmse: 32635.8223 - val_loss: 874053440.0000 - val_rmse: 29564.3945\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1014011840.0000 - rmse: 31843.5527 - val_loss: 830894208.0000 - val_rmse: 28825.2363\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1004966080.0000 - rmse: 31701.1992 - val_loss: 801736640.0000 - val_rmse: 28314.9551\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 946760000.0000 - rmse: 30769.4648 - val_loss: 881090944.0000 - val_rmse: 29683.1758\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1028692736.0000 - rmse: 32073.2402 - val_loss: 799169600.0000 - val_rmse: 28269.5879\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1014281536.0000 - rmse: 31847.7871 - val_loss: 729367936.0000 - val_rmse: 27006.8125\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 928230400.0000 - rmse: 30466.8730 - val_loss: 822507328.0000 - val_rmse: 28679.3887\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1007659200.0000 - rmse: 31743.6484 - val_loss: 762901248.0000 - val_rmse: 27620.6680\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 955176192.0000 - rmse: 30905.9258 - val_loss: 636128064.0000 - val_rmse: 25221.5801\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 872070912.0000 - rmse: 29530.8477 - val_loss: 860250368.0000 - val_rmse: 29330.0254\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 876736192.0000 - rmse: 29609.7305 - val_loss: 617338432.0000 - val_rmse: 24846.2969\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 887396736.0000 - rmse: 29789.2051 - val_loss: 685406336.0000 - val_rmse: 26180.2656\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 933191168.0000 - rmse: 30548.1777 - val_loss: 583343488.0000 - val_rmse: 24152.5039\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 874675648.0000 - rmse: 29574.9160 - val_loss: 584541312.0000 - val_rmse: 24177.2891\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 813055936.0000 - rmse: 28514.1348 - val_loss: 593021504.0000 - val_rmse: 24352.0332\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 812798720.0000 - rmse: 28509.6250 - val_loss: 1131951872.0000 - val_rmse: 33644.4922\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 793425728.0000 - rmse: 28167.8145 - val_loss: 621327488.0000 - val_rmse: 24926.4414\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 832202304.0000 - rmse: 28847.9160 - val_loss: 652603264.0000 - val_rmse: 25546.1016\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 801297216.0000 - rmse: 28307.1934 - val_loss: 900667328.0000 - val_rmse: 30011.1191\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 779849088.0000 - rmse: 27925.7773 - val_loss: 666347200.0000 - val_rmse: 25813.7012\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 739796160.0000 - rmse: 27199.1934 - val_loss: 583938368.0000 - val_rmse: 24164.8164\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 794652864.0000 - rmse: 28189.5879 - val_loss: 563841472.0000 - val_rmse: 23745.3457\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 824221312.0000 - rmse: 28709.2539 - val_loss: 501341664.0000 - val_rmse: 22390.6602\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 785373120.0000 - rmse: 28024.5098 - val_loss: 465050656.0000 - val_rmse: 21565.0332\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 743817920.0000 - rmse: 27273.0254 - val_loss: 525430016.0000 - val_rmse: 22922.2598\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 739580352.0000 - rmse: 27195.2266 - val_loss: 496134432.0000 - val_rmse: 22274.0762\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 790931968.0000 - rmse: 28123.5137 - val_loss: 525438176.0000 - val_rmse: 22922.4395\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730393344.0000 - rmse: 27025.7910 - val_loss: 516993600.0000 - val_rmse: 22737.4941\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 727229760.0000 - rmse: 26967.1973 - val_loss: 597584192.0000 - val_rmse: 24445.5352\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 707092480.0000 - rmse: 26591.2109 - val_loss: 506210912.0000 - val_rmse: 22499.1309\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 767573120.0000 - rmse: 27705.1094 - val_loss: 526648480.0000 - val_rmse: 22948.8223\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 716085120.0000 - rmse: 26759.7676 - val_loss: 644957504.0000 - val_rmse: 25396.0137\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 771748864.0000 - rmse: 27780.3691 - val_loss: 505498144.0000 - val_rmse: 22483.2852\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 700283136.0000 - rmse: 26462.8633 - val_loss: 505982048.0000 - val_rmse: 22494.0449\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 809458624.0000 - rmse: 28450.9863 - val_loss: 416413472.0000 - val_rmse: 20406.2109\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704802496.0000 - rmse: 26548.1172 - val_loss: 614500032.0000 - val_rmse: 24789.1113\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 710574976.0000 - rmse: 26656.6113 - val_loss: 553316672.0000 - val_rmse: 23522.6836\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 681639424.0000 - rmse: 26108.2246 - val_loss: 642050816.0000 - val_rmse: 25338.7207\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 743712960.0000 - rmse: 27271.1016 - val_loss: 494613536.0000 - val_rmse: 22239.9082\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 681776832.0000 - rmse: 26110.8574 - val_loss: 445230208.0000 - val_rmse: 21100.4785\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 683109184.0000 - rmse: 26136.3574 - val_loss: 430300736.0000 - val_rmse: 20743.6914\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 705688512.0000 - rmse: 26564.7988 - val_loss: 478097376.0000 - val_rmse: 21865.4375\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704602624.0000 - rmse: 26544.3516 - val_loss: 383613824.0000 - val_rmse: 19586.0625\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 683942656.0000 - rmse: 26152.2969 - val_loss: 496438080.0000 - val_rmse: 22280.8906\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 673029824.0000 - rmse: 25942.8184 - val_loss: 377782464.0000 - val_rmse: 19436.6270\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 673612928.0000 - rmse: 25954.0547 - val_loss: 502732960.0000 - val_rmse: 22421.7070\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 707813504.0000 - rmse: 26604.7656 - val_loss: 385395776.0000 - val_rmse: 19631.5000\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648395520.0000 - rmse: 25463.6113 - val_loss: 768774528.0000 - val_rmse: 27726.7832\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 672505472.0000 - rmse: 25932.7109 - val_loss: 443776864.0000 - val_rmse: 21066.0117\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 635843776.0000 - rmse: 25215.9434 - val_loss: 367512864.0000 - val_rmse: 19170.6250\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610428352.0000 - rmse: 24706.8477 - val_loss: 490254400.0000 - val_rmse: 22141.6895\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 764242048.0000 - rmse: 27644.9277 - val_loss: 418442048.0000 - val_rmse: 20455.8555\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 619186304.0000 - rmse: 24883.4551 - val_loss: 448857664.0000 - val_rmse: 21186.2617\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654139392.0000 - rmse: 25576.1484 - val_loss: 463798144.0000 - val_rmse: 21535.9727\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 600811072.0000 - rmse: 24511.4473 - val_loss: 366675712.0000 - val_rmse: 19148.7793\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622783552.0000 - rmse: 24955.6309 - val_loss: 407598720.0000 - val_rmse: 20189.0742\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665054720.0000 - rmse: 25788.6543 - val_loss: 386306464.0000 - val_rmse: 19654.6797\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634114368.0000 - rmse: 25181.6270 - val_loss: 417072032.0000 - val_rmse: 20422.3418\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 613049600.0000 - rmse: 24759.8379 - val_loss: 421339840.0000 - val_rmse: 20526.5645\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629630784.0000 - rmse: 25092.4453 - val_loss: 352137152.0000 - val_rmse: 18765.3184\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 659993152.0000 - rmse: 25690.3320 - val_loss: 349921088.0000 - val_rmse: 18706.1777\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622351744.0000 - rmse: 24946.9785 - val_loss: 345795840.0000 - val_rmse: 18595.5859\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640746688.0000 - rmse: 25312.9746 - val_loss: 394914592.0000 - val_rmse: 19872.4590\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 758784128.0000 - rmse: 27546.0371 - val_loss: 381027072.0000 - val_rmse: 19519.9141\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 645043968.0000 - rmse: 25397.7148 - val_loss: 672042304.0000 - val_rmse: 25923.7793\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 711951936.0000 - rmse: 26682.4277 - val_loss: 627649664.0000 - val_rmse: 25052.9375\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 603650240.0000 - rmse: 24569.2949 - val_loss: 370683104.0000 - val_rmse: 19253.1328\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661502976.0000 - rmse: 25719.6992 - val_loss: 606498752.0000 - val_rmse: 24627.1953\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606736064.0000 - rmse: 24632.0137 - val_loss: 390233472.0000 - val_rmse: 19754.3281\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589307392.0000 - rmse: 24275.6543 - val_loss: 345491136.0000 - val_rmse: 18587.3926\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 627198272.0000 - rmse: 25043.9277 - val_loss: 451236800.0000 - val_rmse: 21242.3359\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610820480.0000 - rmse: 24714.7832 - val_loss: 383038528.0000 - val_rmse: 19571.3691\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589404736.0000 - rmse: 24277.6602 - val_loss: 600543744.0000 - val_rmse: 24505.9941\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624546304.0000 - rmse: 24990.9238 - val_loss: 449069952.0000 - val_rmse: 21191.2715\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615654976.0000 - rmse: 24812.3965 - val_loss: 425472800.0000 - val_rmse: 20626.9922\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550069504.0000 - rmse: 23453.5605 - val_loss: 403845760.0000 - val_rmse: 20095.9141\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614913344.0000 - rmse: 24797.4473 - val_loss: 369903456.0000 - val_rmse: 19232.8750\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606750784.0000 - rmse: 24632.3125 - val_loss: 324692224.0000 - val_rmse: 18019.2188\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630672512.0000 - rmse: 25113.1934 - val_loss: 342535872.0000 - val_rmse: 18507.7246\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 608179840.0000 - rmse: 24661.3027 - val_loss: 475010016.0000 - val_rmse: 21794.7246\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560231232.0000 - rmse: 23669.2051 - val_loss: 408611840.0000 - val_rmse: 20214.1504\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 663340096.0000 - rmse: 25755.3906 - val_loss: 521642368.0000 - val_rmse: 22839.4922\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592419456.0000 - rmse: 24339.6680 - val_loss: 321544480.0000 - val_rmse: 17931.6621\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568312256.0000 - rmse: 23839.3008 - val_loss: 343096128.0000 - val_rmse: 18522.8535\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 740216128.0000 - rmse: 27206.9141 - val_loss: 583818432.0000 - val_rmse: 24162.3359\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561176000.0000 - rmse: 23689.1543 - val_loss: 424349824.0000 - val_rmse: 20599.7539\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591464896.0000 - rmse: 24320.0508 - val_loss: 326599616.0000 - val_rmse: 18072.0664\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553964352.0000 - rmse: 23536.4473 - val_loss: 488613408.0000 - val_rmse: 22104.6016\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579499456.0000 - rmse: 24072.7949 - val_loss: 384119744.0000 - val_rmse: 19598.9727\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532835328.0000 - rmse: 23083.2266 - val_loss: 446792544.0000 - val_rmse: 21137.4688\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 608368512.0000 - rmse: 24665.1270 - val_loss: 402446624.0000 - val_rmse: 20061.0723\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533941600.0000 - rmse: 23107.1758 - val_loss: 422789344.0000 - val_rmse: 20561.8418\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552660736.0000 - rmse: 23508.7383 - val_loss: 480875520.0000 - val_rmse: 21928.8750\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609202496.0000 - rmse: 24682.0273 - val_loss: 351307520.0000 - val_rmse: 18743.1992\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651826432.0000 - rmse: 25530.8926 - val_loss: 356129856.0000 - val_rmse: 18871.4023\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519030688.0000 - rmse: 22782.2441 - val_loss: 436081248.0000 - val_rmse: 20882.5586\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585218560.0000 - rmse: 24191.2910 - val_loss: 456429248.0000 - val_rmse: 21364.2051\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597731456.0000 - rmse: 24448.5469 - val_loss: 377327840.0000 - val_rmse: 19424.9277\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524350464.0000 - rmse: 22898.6992 - val_loss: 318438752.0000 - val_rmse: 17844.8516\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518746592.0000 - rmse: 22776.0098 - val_loss: 371648256.0000 - val_rmse: 19278.1816\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500486688.0000 - rmse: 22371.5605 - val_loss: 435420576.0000 - val_rmse: 20866.7344\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533278592.0000 - rmse: 23092.8262 - val_loss: 334519008.0000 - val_rmse: 18289.8613\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557038592.0000 - rmse: 23601.6641 - val_loss: 421155008.0000 - val_rmse: 20522.0605\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546463232.0000 - rmse: 23376.5527 - val_loss: 721264960.0000 - val_rmse: 26856.3770\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523484320.0000 - rmse: 22879.7793 - val_loss: 504602112.0000 - val_rmse: 22463.3496\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594198784.0000 - rmse: 24376.1934 - val_loss: 322716384.0000 - val_rmse: 17964.3086\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527708960.0000 - rmse: 22971.9160 - val_loss: 293774688.0000 - val_rmse: 17139.8574\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623024128.0000 - rmse: 24960.4512 - val_loss: 700810624.0000 - val_rmse: 26472.8281\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487823808.0000 - rmse: 22086.7344 - val_loss: 333916416.0000 - val_rmse: 18273.3809\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556523008.0000 - rmse: 23590.7402 - val_loss: 426101856.0000 - val_rmse: 20642.2344\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551604992.0000 - rmse: 23486.2715 - val_loss: 316416288.0000 - val_rmse: 17788.0938\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537189952.0000 - rmse: 23177.3594 - val_loss: 611104320.0000 - val_rmse: 24720.5234\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592620096.0000 - rmse: 24343.7891 - val_loss: 296543008.0000 - val_rmse: 17220.4238\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552724736.0000 - rmse: 23510.0977 - val_loss: 311923392.0000 - val_rmse: 17661.3535\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571950144.0000 - rmse: 23915.4785 - val_loss: 406201312.0000 - val_rmse: 20154.4375\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505472352.0000 - rmse: 22482.7129 - val_loss: 321641792.0000 - val_rmse: 17934.3750\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537185216.0000 - rmse: 23177.2559 - val_loss: 382909952.0000 - val_rmse: 19568.0859\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582628800.0000 - rmse: 24137.7051 - val_loss: 303967360.0000 - val_rmse: 17434.6582\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469172128.0000 - rmse: 21660.3809 - val_loss: 300204832.0000 - val_rmse: 17326.4199\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475544512.0000 - rmse: 21806.9824 - val_loss: 352503840.0000 - val_rmse: 18775.0859\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492274528.0000 - rmse: 22187.2598 - val_loss: 448497440.0000 - val_rmse: 21177.7578\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511497504.0000 - rmse: 22616.3105 - val_loss: 330312672.0000 - val_rmse: 18174.5059\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458970944.0000 - rmse: 21423.6074 - val_loss: 358687840.0000 - val_rmse: 18939.0566\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 617353536.0000 - rmse: 24846.5996 - val_loss: 487196160.0000 - val_rmse: 22072.5195\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507402688.0000 - rmse: 22525.6016 - val_loss: 322315456.0000 - val_rmse: 17953.1465\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522424160.0000 - rmse: 22856.5996 - val_loss: 393215072.0000 - val_rmse: 19829.6504\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558900736.0000 - rmse: 23641.0820 - val_loss: 380492960.0000 - val_rmse: 19506.2285\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469212672.0000 - rmse: 21661.3184 - val_loss: 408696928.0000 - val_rmse: 20216.2539\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510456384.0000 - rmse: 22593.2812 - val_loss: 420860352.0000 - val_rmse: 20514.8809\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464886944.0000 - rmse: 21561.2363 - val_loss: 597353472.0000 - val_rmse: 24440.8164\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516483776.0000 - rmse: 22726.2793 - val_loss: 403401664.0000 - val_rmse: 20084.8613\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532321920.0000 - rmse: 23072.1035 - val_loss: 881996544.0000 - val_rmse: 29698.4258\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661323904.0000 - rmse: 25716.2188 - val_loss: 428736704.0000 - val_rmse: 20705.9590\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535586016.0000 - rmse: 23142.7305 - val_loss: 335788128.0000 - val_rmse: 18324.5234\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518057824.0000 - rmse: 22760.8828 - val_loss: 290626208.0000 - val_rmse: 17047.7617\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436622272.0000 - rmse: 20895.5078 - val_loss: 289534624.0000 - val_rmse: 17015.7168\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529808224.0000 - rmse: 23017.5625 - val_loss: 389913504.0000 - val_rmse: 19746.2285\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485031648.0000 - rmse: 22023.4336 - val_loss: 1022838464.0000 - val_rmse: 31981.8457\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574879744.0000 - rmse: 23976.6504 - val_loss: 344054016.0000 - val_rmse: 18548.6934\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507635648.0000 - rmse: 22530.7715 - val_loss: 462373632.0000 - val_rmse: 21502.8750\n",
      "104/104 [==============================] - 0s 673us/step - loss: 899894080.0000 - rmse: 29998.2344\n",
      "[899894080.0, 29998.234375]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 20,961\n",
      "Trainable params: 20,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 20,961\n",
      "Trainable params: 20,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 11753222144.0000 - rmse: 108412.2812 - val_loss: 1662610816.0000 - val_rmse: 40775.1250\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2031777536.0000 - rmse: 45075.2422 - val_loss: 1225131904.0000 - val_rmse: 35001.8828\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1842586112.0000 - rmse: 42925.3555 - val_loss: 1185514752.0000 - val_rmse: 34431.3047\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1756321792.0000 - rmse: 41908.4922 - val_loss: 1029655424.0000 - val_rmse: 32088.2441\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1614996224.0000 - rmse: 40187.0156 - val_loss: 981472640.0000 - val_rmse: 31328.4629\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1539340416.0000 - rmse: 39234.4297 - val_loss: 918589888.0000 - val_rmse: 30308.2480\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1509683072.0000 - rmse: 38854.6406 - val_loss: 882356352.0000 - val_rmse: 29704.4844\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1409276416.0000 - rmse: 37540.3320 - val_loss: 835327232.0000 - val_rmse: 28902.0273\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1439267968.0000 - rmse: 37937.6836 - val_loss: 876367360.0000 - val_rmse: 29603.5020\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1350522880.0000 - rmse: 36749.4609 - val_loss: 898214656.0000 - val_rmse: 29970.2305\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1272062592.0000 - rmse: 35665.9883 - val_loss: 865588928.0000 - val_rmse: 29420.8926\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1214452864.0000 - rmse: 34849.0000 - val_loss: 792401088.0000 - val_rmse: 28149.6191\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1203333120.0000 - rmse: 34689.0938 - val_loss: 783115456.0000 - val_rmse: 27984.1992\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1162702848.0000 - rmse: 34098.4297 - val_loss: 694586624.0000 - val_rmse: 26355.0117\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1252100352.0000 - rmse: 35385.0312 - val_loss: 760476160.0000 - val_rmse: 27576.7324\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1092226176.0000 - rmse: 33048.8438 - val_loss: 844054784.0000 - val_rmse: 29052.6211\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1178451328.0000 - rmse: 34328.5781 - val_loss: 716744576.0000 - val_rmse: 26772.0859\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1108384768.0000 - rmse: 33292.4141 - val_loss: 723876992.0000 - val_rmse: 26904.9629\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1069745536.0000 - rmse: 32706.9648 - val_loss: 712431872.0000 - val_rmse: 26691.4199\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1181437312.0000 - rmse: 34372.0430 - val_loss: 624269760.0000 - val_rmse: 24985.3906\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1014360704.0000 - rmse: 31849.0293 - val_loss: 617497984.0000 - val_rmse: 24849.5059\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1082724736.0000 - rmse: 32904.7812 - val_loss: 630038016.0000 - val_rmse: 25100.5586\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1035140288.0000 - rmse: 32173.5957 - val_loss: 621653312.0000 - val_rmse: 24932.9766\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 964227264.0000 - rmse: 31052.0098 - val_loss: 574225280.0000 - val_rmse: 23962.9980\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 946316032.0000 - rmse: 30762.2500 - val_loss: 575953984.0000 - val_rmse: 23999.0410\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1043942656.0000 - rmse: 32310.1016 - val_loss: 646561024.0000 - val_rmse: 25427.5645\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 928186048.0000 - rmse: 30466.1465 - val_loss: 563220800.0000 - val_rmse: 23732.2734\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 958844160.0000 - rmse: 30965.2090 - val_loss: 605826304.0000 - val_rmse: 24613.5391\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 999230592.0000 - rmse: 31610.6094 - val_loss: 609127808.0000 - val_rmse: 24680.5156\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 893721408.0000 - rmse: 29895.1738 - val_loss: 612858688.0000 - val_rmse: 24755.9824\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 980645440.0000 - rmse: 31315.2598 - val_loss: 644723712.0000 - val_rmse: 25391.4102\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 944737152.0000 - rmse: 30736.5762 - val_loss: 593700416.0000 - val_rmse: 24365.9688\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 820438080.0000 - rmse: 28643.2910 - val_loss: 610353600.0000 - val_rmse: 24705.3359\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 883773952.0000 - rmse: 29728.3359 - val_loss: 717462784.0000 - val_rmse: 26785.4961\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 900811648.0000 - rmse: 30013.5254 - val_loss: 500353376.0000 - val_rmse: 22368.5801\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 867887104.0000 - rmse: 29459.9238 - val_loss: 503591072.0000 - val_rmse: 22440.8340\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 922681792.0000 - rmse: 30375.6777 - val_loss: 557791680.0000 - val_rmse: 23617.6133\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 865582720.0000 - rmse: 29420.7871 - val_loss: 511500896.0000 - val_rmse: 22616.3848\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 757862592.0000 - rmse: 27529.3047 - val_loss: 657900032.0000 - val_rmse: 25649.5625\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 843819328.0000 - rmse: 29048.5684 - val_loss: 605828224.0000 - val_rmse: 24613.5781\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 807164864.0000 - rmse: 28410.6465 - val_loss: 558140416.0000 - val_rmse: 23624.9961\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 891029056.0000 - rmse: 29850.1094 - val_loss: 532428448.0000 - val_rmse: 23074.4102\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 881061440.0000 - rmse: 29682.6797 - val_loss: 494565856.0000 - val_rmse: 22238.8359\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 826022272.0000 - rmse: 28740.6035 - val_loss: 520296704.0000 - val_rmse: 22810.0137\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 824378496.0000 - rmse: 28711.9922 - val_loss: 484066752.0000 - val_rmse: 22001.5176\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 801175680.0000 - rmse: 28305.0469 - val_loss: 530474464.0000 - val_rmse: 23032.0312\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 869177856.0000 - rmse: 29481.8223 - val_loss: 488350176.0000 - val_rmse: 22098.6465\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 743247488.0000 - rmse: 27262.5664 - val_loss: 497539136.0000 - val_rmse: 22305.5859\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 777318720.0000 - rmse: 27880.4355 - val_loss: 493911648.0000 - val_rmse: 22224.1230\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 891603328.0000 - rmse: 29859.7266 - val_loss: 564792000.0000 - val_rmse: 23765.3535\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 799453760.0000 - rmse: 28274.6133 - val_loss: 482597120.0000 - val_rmse: 21968.0938\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 768894208.0000 - rmse: 27728.9414 - val_loss: 532761920.0000 - val_rmse: 23081.6367\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 758877888.0000 - rmse: 27547.7383 - val_loss: 621263424.0000 - val_rmse: 24925.1562\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 798803520.0000 - rmse: 28263.1133 - val_loss: 434832320.0000 - val_rmse: 20852.6328\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 760936192.0000 - rmse: 27585.0723 - val_loss: 477868032.0000 - val_rmse: 21860.1934\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 807179328.0000 - rmse: 28410.9023 - val_loss: 453499584.0000 - val_rmse: 21295.5293\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 845748352.0000 - rmse: 29081.7520 - val_loss: 443532832.0000 - val_rmse: 21060.2188\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730761536.0000 - rmse: 27032.6016 - val_loss: 532552640.0000 - val_rmse: 23077.1016\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 887078784.0000 - rmse: 29783.8672 - val_loss: 449947360.0000 - val_rmse: 21211.9629\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 722343936.0000 - rmse: 26876.4570 - val_loss: 426990400.0000 - val_rmse: 20663.7461\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 819809856.0000 - rmse: 28632.3223 - val_loss: 495234304.0000 - val_rmse: 22253.8613\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665788096.0000 - rmse: 25802.8691 - val_loss: 414205184.0000 - val_rmse: 20352.0312\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 683029440.0000 - rmse: 26134.8320 - val_loss: 439615776.0000 - val_rmse: 20967.0156\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704101248.0000 - rmse: 26534.9062 - val_loss: 400334400.0000 - val_rmse: 20008.3574\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 719570880.0000 - rmse: 26824.8184 - val_loss: 406528608.0000 - val_rmse: 20162.5547\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 812047040.0000 - rmse: 28496.4395 - val_loss: 454941280.0000 - val_rmse: 21329.3535\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724022784.0000 - rmse: 26907.6719 - val_loss: 416562016.0000 - val_rmse: 20409.8516\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 761979008.0000 - rmse: 27603.9668 - val_loss: 500990016.0000 - val_rmse: 22382.8066\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 662533504.0000 - rmse: 25739.7266 - val_loss: 440640576.0000 - val_rmse: 20991.4414\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 752862976.0000 - rmse: 27438.3496 - val_loss: 473173024.0000 - val_rmse: 21752.5410\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687212096.0000 - rmse: 26214.7305 - val_loss: 513270144.0000 - val_rmse: 22655.4668\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641906048.0000 - rmse: 25335.8652 - val_loss: 425159840.0000 - val_rmse: 20619.4043\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 752545152.0000 - rmse: 27432.5566 - val_loss: 422591840.0000 - val_rmse: 20557.0391\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 674164416.0000 - rmse: 25964.6758 - val_loss: 421189664.0000 - val_rmse: 20522.9062\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 820961792.0000 - rmse: 28652.4316 - val_loss: 450031488.0000 - val_rmse: 21213.9453\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 743033472.0000 - rmse: 27258.6406 - val_loss: 408885728.0000 - val_rmse: 20220.9238\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 738086144.0000 - rmse: 27167.7402 - val_loss: 429387328.0000 - val_rmse: 20721.6641\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 699046912.0000 - rmse: 26439.4961 - val_loss: 484702976.0000 - val_rmse: 22015.9707\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 639525056.0000 - rmse: 25288.8320 - val_loss: 473747328.0000 - val_rmse: 21765.7383\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 603516992.0000 - rmse: 24566.5820 - val_loss: 487181728.0000 - val_rmse: 22072.1934\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656291392.0000 - rmse: 25618.1855 - val_loss: 461267488.0000 - val_rmse: 21477.1387\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649159744.0000 - rmse: 25478.6133 - val_loss: 949900480.0000 - val_rmse: 30820.4551\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566975872.0000 - rmse: 23811.2559 - val_loss: 719543936.0000 - val_rmse: 26824.3164\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 631398080.0000 - rmse: 25127.6367 - val_loss: 416220384.0000 - val_rmse: 20401.4805\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 653260288.0000 - rmse: 25558.9570 - val_loss: 476468032.0000 - val_rmse: 21828.1484\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593708800.0000 - rmse: 24366.1406 - val_loss: 1106466944.0000 - val_rmse: 33263.5977\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591477632.0000 - rmse: 24320.3125 - val_loss: 957488384.0000 - val_rmse: 30943.3086\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654770560.0000 - rmse: 25588.4844 - val_loss: 455840736.0000 - val_rmse: 21350.4277\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652390976.0000 - rmse: 25541.9453 - val_loss: 467065216.0000 - val_rmse: 21611.6914\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 639661824.0000 - rmse: 25291.5371 - val_loss: 445750240.0000 - val_rmse: 21112.7988\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611066304.0000 - rmse: 24719.7559 - val_loss: 596053952.0000 - val_rmse: 24414.2168\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607694912.0000 - rmse: 24651.4688 - val_loss: 423668800.0000 - val_rmse: 20583.2168\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593268160.0000 - rmse: 24357.0977 - val_loss: 498819200.0000 - val_rmse: 22334.2598\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576898432.0000 - rmse: 24018.7109 - val_loss: 545175232.0000 - val_rmse: 23348.9883\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622387392.0000 - rmse: 24947.6934 - val_loss: 424208032.0000 - val_rmse: 20596.3105\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573160704.0000 - rmse: 23940.7754 - val_loss: 585821056.0000 - val_rmse: 24203.7402\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 603058496.0000 - rmse: 24557.2500 - val_loss: 441621024.0000 - val_rmse: 21014.7812\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573032192.0000 - rmse: 23938.0898 - val_loss: 861672448.0000 - val_rmse: 29354.2578\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 949308544.0000 - rmse: 30810.8516 - val_loss: 434050432.0000 - val_rmse: 20833.8770\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626391040.0000 - rmse: 25027.8047 - val_loss: 427310336.0000 - val_rmse: 20671.4863\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552779392.0000 - rmse: 23511.2617 - val_loss: 473713024.0000 - val_rmse: 21764.9492\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504731552.0000 - rmse: 22466.2305 - val_loss: 571836736.0000 - val_rmse: 23913.1074\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574064640.0000 - rmse: 23959.6465 - val_loss: 954742784.0000 - val_rmse: 30898.9121\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563707840.0000 - rmse: 23742.5332 - val_loss: 943635264.0000 - val_rmse: 30718.6465\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 672909248.0000 - rmse: 25940.4941 - val_loss: 1182641920.0000 - val_rmse: 34389.5625\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536951104.0000 - rmse: 23172.2051 - val_loss: 1299928960.0000 - val_rmse: 36054.5273\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567877056.0000 - rmse: 23830.1719 - val_loss: 651575424.0000 - val_rmse: 25525.9746\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554340352.0000 - rmse: 23544.4336 - val_loss: 528502016.0000 - val_rmse: 22989.1719\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571911040.0000 - rmse: 23914.6621 - val_loss: 468496960.0000 - val_rmse: 21644.7910\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489142976.0000 - rmse: 22116.5762 - val_loss: 524207424.0000 - val_rmse: 22895.5762\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584557248.0000 - rmse: 24177.6191 - val_loss: 525807008.0000 - val_rmse: 22930.4824\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583753280.0000 - rmse: 24160.9863 - val_loss: 754315712.0000 - val_rmse: 27464.8086\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542975616.0000 - rmse: 23301.8379 - val_loss: 461210976.0000 - val_rmse: 21475.8223\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513450112.0000 - rmse: 22659.4375 - val_loss: 508204608.0000 - val_rmse: 22543.3945\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555501568.0000 - rmse: 23569.0801 - val_loss: 429098432.0000 - val_rmse: 20714.6914\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548644608.0000 - rmse: 23423.1641 - val_loss: 2266730496.0000 - val_rmse: 47610.1914\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615573952.0000 - rmse: 24810.7637 - val_loss: 420750944.0000 - val_rmse: 20512.2148\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611027776.0000 - rmse: 24718.9766 - val_loss: 2034501632.0000 - val_rmse: 45105.4492\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 722800960.0000 - rmse: 26884.9570 - val_loss: 1004755712.0000 - val_rmse: 31697.8809\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531308736.0000 - rmse: 23050.1348 - val_loss: 609008960.0000 - val_rmse: 24678.1074\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571711296.0000 - rmse: 23910.4844 - val_loss: 509362144.0000 - val_rmse: 22569.0527\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733736000.0000 - rmse: 27087.5625 - val_loss: 610023872.0000 - val_rmse: 24698.6621\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513904576.0000 - rmse: 22669.4629 - val_loss: 579268800.0000 - val_rmse: 24068.0039\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491232032.0000 - rmse: 22163.7559 - val_loss: 827247296.0000 - val_rmse: 28761.9062\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491916224.0000 - rmse: 22179.1836 - val_loss: 1839450752.0000 - val_rmse: 42888.8203\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496786720.0000 - rmse: 22288.7129 - val_loss: 4192875520.0000 - val_rmse: 64752.4180\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1053051712.0000 - rmse: 32450.7578 - val_loss: 1492575744.0000 - val_rmse: 38633.8672\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488230048.0000 - rmse: 22095.9277 - val_loss: 976126592.0000 - val_rmse: 31243.0254\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447274848.0000 - rmse: 21148.8730 - val_loss: 924049216.0000 - val_rmse: 30398.1777\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 990184960.0000 - rmse: 31467.2051 - val_loss: 1015058176.0000 - val_rmse: 31859.9785\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483384512.0000 - rmse: 21986.0078 - val_loss: 915557312.0000 - val_rmse: 30258.1777\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434190208.0000 - rmse: 20837.2305 - val_loss: 541614656.0000 - val_rmse: 23272.6152\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472016832.0000 - rmse: 21725.9492 - val_loss: 3660508928.0000 - val_rmse: 60502.1406\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480417440.0000 - rmse: 21918.4277 - val_loss: 1717995648.0000 - val_rmse: 41448.7109\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495272704.0000 - rmse: 22254.7227 - val_loss: 541405376.0000 - val_rmse: 23268.1191\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540301824.0000 - rmse: 23244.3926 - val_loss: 519698048.0000 - val_rmse: 22796.8867\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1096633472.0000 - rmse: 33115.4570 - val_loss: 469479264.0000 - val_rmse: 21667.4707\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532540448.0000 - rmse: 23076.8379 - val_loss: 491440128.0000 - val_rmse: 22168.4492\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465872320.0000 - rmse: 21584.0762 - val_loss: 851014720.0000 - val_rmse: 29172.1562\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498824384.0000 - rmse: 22334.3770 - val_loss: 1414746880.0000 - val_rmse: 37613.1211\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461946976.0000 - rmse: 21492.9512 - val_loss: 1460293888.0000 - val_rmse: 38213.7930\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401746816.0000 - rmse: 20043.6230 - val_loss: 3980320256.0000 - val_rmse: 63089.7812\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558488256.0000 - rmse: 23632.3555 - val_loss: 495647264.0000 - val_rmse: 22263.1367\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421670176.0000 - rmse: 20534.6094 - val_loss: 642062336.0000 - val_rmse: 25338.9492\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420119584.0000 - rmse: 20496.8184 - val_loss: 481079616.0000 - val_rmse: 21933.5273\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 818299520.0000 - rmse: 28605.9355 - val_loss: 585458112.0000 - val_rmse: 24196.2422\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444967648.0000 - rmse: 21094.2559 - val_loss: 1014244608.0000 - val_rmse: 31847.2070\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470950624.0000 - rmse: 21701.3965 - val_loss: 856094464.0000 - val_rmse: 29259.0918\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 714116800.0000 - rmse: 26722.9648 - val_loss: 802809920.0000 - val_rmse: 28333.9004\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435946080.0000 - rmse: 20879.3223 - val_loss: 507806496.0000 - val_rmse: 22534.5625\n",
      "104/104 [==============================] - 0s 721us/step - loss: 550741184.0000 - rmse: 23467.8750\n",
      "[550741184.0, 23467.875]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 20,961\n",
      "Trainable params: 20,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 20,961\n",
      "Trainable params: 20,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 8353521664.0000 - rmse: 91397.6016 - val_loss: 1346970496.0000 - val_rmse: 36701.0977\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2041695872.0000 - rmse: 45185.1289 - val_loss: 1266739456.0000 - val_rmse: 35591.2852\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1992326656.0000 - rmse: 44635.4883 - val_loss: 1084116096.0000 - val_rmse: 32925.9180\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1789493632.0000 - rmse: 42302.4062 - val_loss: 1004038208.0000 - val_rmse: 31686.5625\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1585409280.0000 - rmse: 39817.1992 - val_loss: 923462336.0000 - val_rmse: 30388.5234\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1631683072.0000 - rmse: 40394.0977 - val_loss: 889173888.0000 - val_rmse: 29819.0195\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1441458816.0000 - rmse: 37966.5469 - val_loss: 899039296.0000 - val_rmse: 29983.9844\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1349031808.0000 - rmse: 36729.1680 - val_loss: 836708864.0000 - val_rmse: 28925.9199\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1485254272.0000 - rmse: 38538.9961 - val_loss: 745908160.0000 - val_rmse: 27311.3184\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1430486784.0000 - rmse: 37821.7773 - val_loss: 720018752.0000 - val_rmse: 26833.1660\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1271847552.0000 - rmse: 35662.9727 - val_loss: 722176192.0000 - val_rmse: 26873.3359\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1198262656.0000 - rmse: 34615.9297 - val_loss: 1184195456.0000 - val_rmse: 34412.1406\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1222074752.0000 - rmse: 34958.1875 - val_loss: 672366464.0000 - val_rmse: 25930.0293\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1266214528.0000 - rmse: 35583.9102 - val_loss: 926798016.0000 - val_rmse: 30443.3574\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1138359552.0000 - rmse: 33739.5859 - val_loss: 619235776.0000 - val_rmse: 24884.4492\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1087345152.0000 - rmse: 32974.9180 - val_loss: 685864832.0000 - val_rmse: 26189.0215\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1163728768.0000 - rmse: 34113.4688 - val_loss: 605927616.0000 - val_rmse: 24615.5977\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1044552192.0000 - rmse: 32319.5332 - val_loss: 697908672.0000 - val_rmse: 26417.9609\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1095767296.0000 - rmse: 33102.3750 - val_loss: 633191104.0000 - val_rmse: 25163.2891\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1040959552.0000 - rmse: 32263.9043 - val_loss: 586346944.0000 - val_rmse: 24214.6016\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1048349632.0000 - rmse: 32378.2285 - val_loss: 628761984.0000 - val_rmse: 25075.1270\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1080452992.0000 - rmse: 32870.2461 - val_loss: 562765504.0000 - val_rmse: 23722.6797\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1100366848.0000 - rmse: 33171.7773 - val_loss: 600647040.0000 - val_rmse: 24508.1016\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1057367744.0000 - rmse: 32517.1914 - val_loss: 567310400.0000 - val_rmse: 23818.2793\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 956523456.0000 - rmse: 30927.7129 - val_loss: 619234432.0000 - val_rmse: 24884.4219\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1004807296.0000 - rmse: 31698.6953 - val_loss: 554684736.0000 - val_rmse: 23551.7461\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 981542976.0000 - rmse: 31329.5859 - val_loss: 519700672.0000 - val_rmse: 22796.9453\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1014919424.0000 - rmse: 31857.8008 - val_loss: 531892448.0000 - val_rmse: 23062.7930\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 871561024.0000 - rmse: 29522.2129 - val_loss: 537283264.0000 - val_rmse: 23179.3711\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 960872000.0000 - rmse: 30997.9355 - val_loss: 731064768.0000 - val_rmse: 27038.2090\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 956407104.0000 - rmse: 30925.8320 - val_loss: 654772416.0000 - val_rmse: 25588.5215\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 996196480.0000 - rmse: 31562.5801 - val_loss: 511362976.0000 - val_rmse: 22613.3359\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 967385600.0000 - rmse: 31102.8223 - val_loss: 497927968.0000 - val_rmse: 22314.2988\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 969545920.0000 - rmse: 31137.5332 - val_loss: 521149280.0000 - val_rmse: 22828.6934\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 894626560.0000 - rmse: 29910.3086 - val_loss: 629946176.0000 - val_rmse: 25098.7285\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 906280192.0000 - rmse: 30104.4883 - val_loss: 564163776.0000 - val_rmse: 23752.1328\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 944015296.0000 - rmse: 30724.8320 - val_loss: 500993312.0000 - val_rmse: 22382.8809\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 875490560.0000 - rmse: 29588.6895 - val_loss: 488883264.0000 - val_rmse: 22110.7051\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 880796416.0000 - rmse: 29678.2148 - val_loss: 485210784.0000 - val_rmse: 22027.5000\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 924551296.0000 - rmse: 30406.4355 - val_loss: 688188352.0000 - val_rmse: 26233.3438\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 908570816.0000 - rmse: 30142.5078 - val_loss: 490892384.0000 - val_rmse: 22156.0918\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 880782592.0000 - rmse: 29677.9824 - val_loss: 632384640.0000 - val_rmse: 25147.2598\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 815213056.0000 - rmse: 28551.9355 - val_loss: 467289888.0000 - val_rmse: 21616.8887\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 894094976.0000 - rmse: 29901.4219 - val_loss: 457387232.0000 - val_rmse: 21386.6133\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 876354944.0000 - rmse: 29603.2930 - val_loss: 513181088.0000 - val_rmse: 22653.5000\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 886209088.0000 - rmse: 29769.2637 - val_loss: 495732320.0000 - val_rmse: 22265.0469\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 908340096.0000 - rmse: 30138.6816 - val_loss: 526225088.0000 - val_rmse: 22939.5957\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 873781312.0000 - rmse: 29559.7930 - val_loss: 446181632.0000 - val_rmse: 21123.0117\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 918913152.0000 - rmse: 30313.5801 - val_loss: 785072768.0000 - val_rmse: 28019.1504\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 809008576.0000 - rmse: 28443.0762 - val_loss: 470818016.0000 - val_rmse: 21698.3418\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 829650496.0000 - rmse: 28803.6543 - val_loss: 482119808.0000 - val_rmse: 21957.2266\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 831048576.0000 - rmse: 28827.9141 - val_loss: 675455808.0000 - val_rmse: 25989.5332\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 833305344.0000 - rmse: 28867.0293 - val_loss: 448898656.0000 - val_rmse: 21187.2285\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 801874496.0000 - rmse: 28317.3887 - val_loss: 517152064.0000 - val_rmse: 22740.9785\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 859387968.0000 - rmse: 29315.3203 - val_loss: 452299808.0000 - val_rmse: 21267.3398\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 747035904.0000 - rmse: 27331.9570 - val_loss: 440486240.0000 - val_rmse: 20987.7637\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 768810176.0000 - rmse: 27727.4258 - val_loss: 477880800.0000 - val_rmse: 21860.4844\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 836266688.0000 - rmse: 28918.2754 - val_loss: 465120928.0000 - val_rmse: 21566.6621\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 815165376.0000 - rmse: 28551.1016 - val_loss: 465159872.0000 - val_rmse: 21567.5645\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 783359808.0000 - rmse: 27988.5664 - val_loss: 477716768.0000 - val_rmse: 21856.7324\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 771465152.0000 - rmse: 27775.2617 - val_loss: 436601472.0000 - val_rmse: 20895.0098\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 760076288.0000 - rmse: 27569.4805 - val_loss: 435385792.0000 - val_rmse: 20865.9004\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 745736768.0000 - rmse: 27308.1816 - val_loss: 467082688.0000 - val_rmse: 21612.0957\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 895600128.0000 - rmse: 29926.5781 - val_loss: 598669824.0000 - val_rmse: 24467.7305\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 705741504.0000 - rmse: 26565.7949 - val_loss: 434900640.0000 - val_rmse: 20854.2715\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 796655616.0000 - rmse: 28225.0879 - val_loss: 487329440.0000 - val_rmse: 22075.5391\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 788933312.0000 - rmse: 28087.9570 - val_loss: 459887744.0000 - val_rmse: 21444.9941\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 768975936.0000 - rmse: 27730.4160 - val_loss: 640432448.0000 - val_rmse: 25306.7676\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800412544.0000 - rmse: 28291.5625 - val_loss: 489500192.0000 - val_rmse: 22124.6504\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 717635072.0000 - rmse: 26788.7109 - val_loss: 416199168.0000 - val_rmse: 20400.9609\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 794894656.0000 - rmse: 28193.8770 - val_loss: 468467968.0000 - val_rmse: 21644.1211\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 753187072.0000 - rmse: 27444.2539 - val_loss: 400439360.0000 - val_rmse: 20010.9805\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 878684736.0000 - rmse: 29642.6172 - val_loss: 471448160.0000 - val_rmse: 21712.8574\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 806589632.0000 - rmse: 28400.5215 - val_loss: 413230880.0000 - val_rmse: 20328.0820\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 766439552.0000 - rmse: 27684.6445 - val_loss: 386701536.0000 - val_rmse: 19664.7285\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 688875520.0000 - rmse: 26246.4375 - val_loss: 498257696.0000 - val_rmse: 22321.6875\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 756854016.0000 - rmse: 27510.9805 - val_loss: 417245408.0000 - val_rmse: 20426.5859\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 792578624.0000 - rmse: 28152.7734 - val_loss: 483055072.0000 - val_rmse: 21978.5137\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692679360.0000 - rmse: 26318.8027 - val_loss: 394252000.0000 - val_rmse: 19855.7793\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 738294592.0000 - rmse: 27171.5762 - val_loss: 514962048.0000 - val_rmse: 22692.7754\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 746914432.0000 - rmse: 27329.7344 - val_loss: 434651744.0000 - val_rmse: 20848.3027\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733627520.0000 - rmse: 27085.5586 - val_loss: 455604256.0000 - val_rmse: 21344.8887\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730331584.0000 - rmse: 27024.6484 - val_loss: 540144000.0000 - val_rmse: 23240.9980\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 779075072.0000 - rmse: 27911.9160 - val_loss: 447520896.0000 - val_rmse: 21154.6895\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 804670400.0000 - rmse: 28366.7129 - val_loss: 443428608.0000 - val_rmse: 21057.7441\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 679007552.0000 - rmse: 26057.7734 - val_loss: 399807712.0000 - val_rmse: 19995.1914\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677526208.0000 - rmse: 26029.3340 - val_loss: 493761120.0000 - val_rmse: 22220.7363\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730288896.0000 - rmse: 27023.8574 - val_loss: 414940576.0000 - val_rmse: 20370.0898\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 675761152.0000 - rmse: 25995.4062 - val_loss: 444634432.0000 - val_rmse: 21086.3574\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 815646720.0000 - rmse: 28559.5293 - val_loss: 445258304.0000 - val_rmse: 21101.1445\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 699560896.0000 - rmse: 26449.2129 - val_loss: 385812384.0000 - val_rmse: 19642.1074\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687672704.0000 - rmse: 26223.5137 - val_loss: 543579648.0000 - val_rmse: 23314.7949\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 708348608.0000 - rmse: 26614.8184 - val_loss: 395129024.0000 - val_rmse: 19877.8535\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665914688.0000 - rmse: 25805.3223 - val_loss: 401582496.0000 - val_rmse: 20039.5234\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 719234368.0000 - rmse: 26818.5449 - val_loss: 437140576.0000 - val_rmse: 20907.9062\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 682680192.0000 - rmse: 26128.1504 - val_loss: 400429600.0000 - val_rmse: 20010.7363\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 701689856.0000 - rmse: 26489.4297 - val_loss: 573076224.0000 - val_rmse: 23939.0098\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 650551808.0000 - rmse: 25505.9180 - val_loss: 448572224.0000 - val_rmse: 21179.5234\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704037248.0000 - rmse: 26533.7012 - val_loss: 449693216.0000 - val_rmse: 21205.9707\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648070080.0000 - rmse: 25457.2207 - val_loss: 411314592.0000 - val_rmse: 20280.8926\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 752337984.0000 - rmse: 27428.7793 - val_loss: 485284416.0000 - val_rmse: 22029.1719\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698186240.0000 - rmse: 26423.2148 - val_loss: 507226432.0000 - val_rmse: 22521.6875\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668791360.0000 - rmse: 25861.0000 - val_loss: 448742880.0000 - val_rmse: 21183.5527\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629960192.0000 - rmse: 25099.0078 - val_loss: 672590720.0000 - val_rmse: 25934.3535\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610677824.0000 - rmse: 24711.8965 - val_loss: 470326528.0000 - val_rmse: 21687.0137\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601655168.0000 - rmse: 24528.6602 - val_loss: 453889600.0000 - val_rmse: 21304.6855\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 700866112.0000 - rmse: 26473.8770 - val_loss: 477199776.0000 - val_rmse: 21844.9023\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609908288.0000 - rmse: 24696.3223 - val_loss: 422775616.0000 - val_rmse: 20561.5078\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698908864.0000 - rmse: 26436.8848 - val_loss: 397163776.0000 - val_rmse: 19928.9688\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593144576.0000 - rmse: 24354.5605 - val_loss: 398642272.0000 - val_rmse: 19966.0273\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576669696.0000 - rmse: 24013.9473 - val_loss: 477338048.0000 - val_rmse: 21848.0664\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604946880.0000 - rmse: 24595.6680 - val_loss: 495999808.0000 - val_rmse: 22271.0527\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590333568.0000 - rmse: 24296.7812 - val_loss: 480654304.0000 - val_rmse: 21923.8301\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567881728.0000 - rmse: 23830.2695 - val_loss: 618306880.0000 - val_rmse: 24865.7773\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642110016.0000 - rmse: 25339.8906 - val_loss: 419476768.0000 - val_rmse: 20481.1328\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 633347776.0000 - rmse: 25166.4023 - val_loss: 393804864.0000 - val_rmse: 19844.5176\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 659923264.0000 - rmse: 25688.9707 - val_loss: 402786784.0000 - val_rmse: 20069.5488\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591109184.0000 - rmse: 24312.7363 - val_loss: 420508544.0000 - val_rmse: 20506.3047\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596708352.0000 - rmse: 24427.6152 - val_loss: 688646272.0000 - val_rmse: 26242.0703\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599886912.0000 - rmse: 24492.5898 - val_loss: 677863168.0000 - val_rmse: 26035.8047\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541882560.0000 - rmse: 23278.3711 - val_loss: 543016960.0000 - val_rmse: 23302.7246\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548873280.0000 - rmse: 23428.0449 - val_loss: 513370720.0000 - val_rmse: 22657.6855\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503239552.0000 - rmse: 22433.0020 - val_loss: 390188384.0000 - val_rmse: 19753.1875\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556138432.0000 - rmse: 23582.5879 - val_loss: 454846336.0000 - val_rmse: 21327.1270\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607344000.0000 - rmse: 24644.3496 - val_loss: 441030720.0000 - val_rmse: 21000.7305\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504607264.0000 - rmse: 22463.4648 - val_loss: 482922304.0000 - val_rmse: 21975.4941\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652053184.0000 - rmse: 25535.3320 - val_loss: 415779456.0000 - val_rmse: 20390.6699\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557707200.0000 - rmse: 23615.8262 - val_loss: 487058720.0000 - val_rmse: 22069.4062\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536593984.0000 - rmse: 23164.4980 - val_loss: 649097344.0000 - val_rmse: 25477.3887\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524070112.0000 - rmse: 22892.5781 - val_loss: 391491232.0000 - val_rmse: 19786.1367\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525809312.0000 - rmse: 22930.5332 - val_loss: 523368608.0000 - val_rmse: 22877.2500\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487838944.0000 - rmse: 22087.0762 - val_loss: 468581056.0000 - val_rmse: 21646.7324\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555930944.0000 - rmse: 23578.1875 - val_loss: 415428640.0000 - val_rmse: 20382.0664\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540615872.0000 - rmse: 23251.1484 - val_loss: 548978752.0000 - val_rmse: 23430.2949\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 608025088.0000 - rmse: 24658.1641 - val_loss: 432956576.0000 - val_rmse: 20807.6094\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534464224.0000 - rmse: 23118.4824 - val_loss: 418992320.0000 - val_rmse: 20469.3027\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559379392.0000 - rmse: 23651.2031 - val_loss: 682247808.0000 - val_rmse: 26119.8730\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537173440.0000 - rmse: 23177.0020 - val_loss: 396695008.0000 - val_rmse: 19917.2031\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465537216.0000 - rmse: 21576.3105 - val_loss: 407797952.0000 - val_rmse: 20194.0078\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521163296.0000 - rmse: 22829.0020 - val_loss: 593535552.0000 - val_rmse: 24362.5859\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489388064.0000 - rmse: 22122.1172 - val_loss: 418485152.0000 - val_rmse: 20456.9102\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442902240.0000 - rmse: 21045.2422 - val_loss: 530079552.0000 - val_rmse: 23023.4570\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488372160.0000 - rmse: 22099.1445 - val_loss: 597426752.0000 - val_rmse: 24442.3145\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480020992.0000 - rmse: 21909.3809 - val_loss: 420221024.0000 - val_rmse: 20499.2930\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492311456.0000 - rmse: 22188.0918 - val_loss: 479069504.0000 - val_rmse: 21887.6562\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490797280.0000 - rmse: 22153.9453 - val_loss: 428805440.0000 - val_rmse: 20707.6172\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545891776.0000 - rmse: 23364.3262 - val_loss: 425272000.0000 - val_rmse: 20622.1250\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451765824.0000 - rmse: 21254.7832 - val_loss: 432912512.0000 - val_rmse: 20806.5488\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446550304.0000 - rmse: 21131.7363 - val_loss: 456463200.0000 - val_rmse: 21365.0000\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475072000.0000 - rmse: 21796.1465 - val_loss: 435653792.0000 - val_rmse: 20872.3203\n",
      "104/104 [==============================] - 0s 681us/step - loss: 406772992.0000 - rmse: 20168.6133\n",
      "[406772992.0, 20168.61328125]\n",
      "[21875.4375, 29993.79296875, 29998.234375, 23467.875, 20168.61328125]\n",
      "25100.790625\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "!python train.py kfold baseline\n",
    "## layer - 1 (16 8) d 0.3"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 19:43:07.887760: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 19:43:07.887798: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 19:43:07.888113: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 19:43:08.091079: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 7093364736.0000 - rmse: 84222.1172 - val_loss: 1213022336.0000 - val_rmse: 34828.4688\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1847987584.0000 - rmse: 42988.2266 - val_loss: 1076752640.0000 - val_rmse: 32813.9102\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1649797376.0000 - rmse: 40617.6992 - val_loss: 867457472.0000 - val_rmse: 29452.6309\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1604030336.0000 - rmse: 40050.3477 - val_loss: 779369088.0000 - val_rmse: 27917.1836\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1464173568.0000 - rmse: 38264.5195 - val_loss: 763021248.0000 - val_rmse: 27622.8379\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1422267904.0000 - rmse: 37712.9688 - val_loss: 772010816.0000 - val_rmse: 27785.0820\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1345801088.0000 - rmse: 36685.1602 - val_loss: 878149888.0000 - val_rmse: 29633.5938\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1386072192.0000 - rmse: 37229.9922 - val_loss: 678805632.0000 - val_rmse: 26053.8984\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1286380032.0000 - rmse: 35866.1406 - val_loss: 666435328.0000 - val_rmse: 25815.4082\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1290253568.0000 - rmse: 35920.1016 - val_loss: 711883264.0000 - val_rmse: 26681.1406\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1311257216.0000 - rmse: 36211.2852 - val_loss: 702718912.0000 - val_rmse: 26508.8457\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1236568960.0000 - rmse: 35164.8828 - val_loss: 834336640.0000 - val_rmse: 28884.8867\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1156512512.0000 - rmse: 34007.5352 - val_loss: 645477888.0000 - val_rmse: 25406.2578\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1262404096.0000 - rmse: 35530.3281 - val_loss: 692697280.0000 - val_rmse: 26319.1426\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1087533824.0000 - rmse: 32977.7773 - val_loss: 964943360.0000 - val_rmse: 31063.5371\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1174838016.0000 - rmse: 34275.9102 - val_loss: 854756928.0000 - val_rmse: 29236.2266\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1125427200.0000 - rmse: 33547.3867 - val_loss: 701249408.0000 - val_rmse: 26481.1133\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1063365632.0000 - rmse: 32609.2871 - val_loss: 923621568.0000 - val_rmse: 30391.1426\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1052112960.0000 - rmse: 32436.2910 - val_loss: 677449856.0000 - val_rmse: 26027.8672\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1099765632.0000 - rmse: 33162.7148 - val_loss: 530126368.0000 - val_rmse: 23024.4727\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 999211520.0000 - rmse: 31610.3066 - val_loss: 596914496.0000 - val_rmse: 24431.8340\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1033578944.0000 - rmse: 32149.3223 - val_loss: 1301642496.0000 - val_rmse: 36078.2812\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 970935616.0000 - rmse: 31159.8398 - val_loss: 769834688.0000 - val_rmse: 27745.8945\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 966987776.0000 - rmse: 31096.4277 - val_loss: 618803968.0000 - val_rmse: 24875.7715\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 986924736.0000 - rmse: 31415.3574 - val_loss: 884039936.0000 - val_rmse: 29732.8066\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 887707200.0000 - rmse: 29794.4141 - val_loss: 786499904.0000 - val_rmse: 28044.6055\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 967011072.0000 - rmse: 31096.8008 - val_loss: 598225216.0000 - val_rmse: 24458.6426\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 868106560.0000 - rmse: 29463.6445 - val_loss: 958802368.0000 - val_rmse: 30964.5332\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 845548224.0000 - rmse: 29078.3125 - val_loss: 534430112.0000 - val_rmse: 23117.7441\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1020123520.0000 - rmse: 31939.3711 - val_loss: 617416064.0000 - val_rmse: 24847.8574\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 865670976.0000 - rmse: 29422.2871 - val_loss: 718777152.0000 - val_rmse: 26810.0195\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 858453056.0000 - rmse: 29299.3691 - val_loss: 928906496.0000 - val_rmse: 30477.9668\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 815067136.0000 - rmse: 28549.3789 - val_loss: 1223673600.0000 - val_rmse: 34981.0469\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751349504.0000 - rmse: 27410.7539 - val_loss: 595160000.0000 - val_rmse: 24395.9004\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 872680640.0000 - rmse: 29541.1660 - val_loss: 1163115264.0000 - val_rmse: 34104.4766\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 791608512.0000 - rmse: 28135.5371 - val_loss: 646534848.0000 - val_rmse: 25427.0488\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 762395264.0000 - rmse: 27611.5039 - val_loss: 659015104.0000 - val_rmse: 25671.2891\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 847894016.0000 - rmse: 29118.6172 - val_loss: 459594400.0000 - val_rmse: 21438.1523\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 720204416.0000 - rmse: 26836.6250 - val_loss: 632812480.0000 - val_rmse: 25155.7637\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 752499008.0000 - rmse: 27431.7148 - val_loss: 734004608.0000 - val_rmse: 27092.5176\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742412032.0000 - rmse: 27247.2383 - val_loss: 572308736.0000 - val_rmse: 23922.9746\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 659439616.0000 - rmse: 25679.5566 - val_loss: 972416000.0000 - val_rmse: 31183.5820\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628690304.0000 - rmse: 25073.6953 - val_loss: 1931439616.0000 - val_rmse: 43948.1484\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611707264.0000 - rmse: 24732.7168 - val_loss: 724526272.0000 - val_rmse: 26917.0254\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 772715648.0000 - rmse: 27797.7637 - val_loss: 509240160.0000 - val_rmse: 22566.3496\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687522688.0000 - rmse: 26220.6523 - val_loss: 423801824.0000 - val_rmse: 20586.4473\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615726080.0000 - rmse: 24813.8262 - val_loss: 736042432.0000 - val_rmse: 27130.1016\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 714521152.0000 - rmse: 26730.5293 - val_loss: 1190216064.0000 - val_rmse: 34499.5078\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620444096.0000 - rmse: 24908.7148 - val_loss: 591319616.0000 - val_rmse: 24317.0625\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626066368.0000 - rmse: 25021.3164 - val_loss: 729878080.0000 - val_rmse: 27016.2539\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588228992.0000 - rmse: 24253.4297 - val_loss: 1352484608.0000 - val_rmse: 36776.1406\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514900832.0000 - rmse: 22691.4258 - val_loss: 726406080.0000 - val_rmse: 26951.9199\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569600320.0000 - rmse: 23866.3008 - val_loss: 790858304.0000 - val_rmse: 28122.2031\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584392192.0000 - rmse: 24174.2051 - val_loss: 460936032.0000 - val_rmse: 21469.4199\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526450592.0000 - rmse: 22944.5117 - val_loss: 501989216.0000 - val_rmse: 22405.1152\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587332032.0000 - rmse: 24234.9336 - val_loss: 539356928.0000 - val_rmse: 23224.0586\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546073728.0000 - rmse: 23368.2188 - val_loss: 847842880.0000 - val_rmse: 29117.7422\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563456832.0000 - rmse: 23737.2441 - val_loss: 942246656.0000 - val_rmse: 30696.0352\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518538944.0000 - rmse: 22771.4492 - val_loss: 969516800.0000 - val_rmse: 31137.0645\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485554400.0000 - rmse: 22035.2969 - val_loss: 665238848.0000 - val_rmse: 25792.2227\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517662944.0000 - rmse: 22752.2070 - val_loss: 691698560.0000 - val_rmse: 26300.1621\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502146720.0000 - rmse: 22408.6289 - val_loss: 381313184.0000 - val_rmse: 19527.2422\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561646784.0000 - rmse: 23699.0859 - val_loss: 874316416.0000 - val_rmse: 29568.8398\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489253120.0000 - rmse: 22119.0664 - val_loss: 747594112.0000 - val_rmse: 27342.1680\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474851744.0000 - rmse: 21791.0938 - val_loss: 734820416.0000 - val_rmse: 27107.5703\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 528854496.0000 - rmse: 22996.8359 - val_loss: 910635712.0000 - val_rmse: 30176.7422\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579513920.0000 - rmse: 24073.0938 - val_loss: 739805440.0000 - val_rmse: 27199.3652\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552606720.0000 - rmse: 23507.5879 - val_loss: 1157230848.0000 - val_rmse: 34018.0977\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539784896.0000 - rmse: 23233.2695 - val_loss: 994171264.0000 - val_rmse: 31530.4824\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491056160.0000 - rmse: 22159.7871 - val_loss: 831603200.0000 - val_rmse: 28837.5312\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502770400.0000 - rmse: 22422.5430 - val_loss: 863318016.0000 - val_rmse: 29382.2734\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518866240.0000 - rmse: 22778.6328 - val_loss: 726864576.0000 - val_rmse: 26960.4258\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527328992.0000 - rmse: 22963.6445 - val_loss: 1772622208.0000 - val_rmse: 42102.5195\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445969760.0000 - rmse: 21117.9961 - val_loss: 569262528.0000 - val_rmse: 23859.2227\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472638432.0000 - rmse: 21740.2480 - val_loss: 1454789888.0000 - val_rmse: 38141.7070\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470321120.0000 - rmse: 21686.8887 - val_loss: 820547520.0000 - val_rmse: 28645.2012\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440397280.0000 - rmse: 20985.6426 - val_loss: 691030272.0000 - val_rmse: 26287.4551\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496475808.0000 - rmse: 22281.7363 - val_loss: 1068597312.0000 - val_rmse: 32689.4062\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440018592.0000 - rmse: 20976.6191 - val_loss: 737261120.0000 - val_rmse: 27152.5508\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522406464.0000 - rmse: 22856.2129 - val_loss: 901299264.0000 - val_rmse: 30021.6445\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527693984.0000 - rmse: 22971.5898 - val_loss: 1116929920.0000 - val_rmse: 33420.5000\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472078784.0000 - rmse: 21727.3730 - val_loss: 2979339008.0000 - val_rmse: 54583.3203\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446500512.0000 - rmse: 21130.5586 - val_loss: 552771968.0000 - val_rmse: 23511.1016\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448381376.0000 - rmse: 21175.0156 - val_loss: 934050560.0000 - val_rmse: 30562.2402\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442088480.0000 - rmse: 21025.8984 - val_loss: 1425883520.0000 - val_rmse: 37760.8711\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454233088.0000 - rmse: 21312.7441 - val_loss: 928268224.0000 - val_rmse: 30467.4922\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501302848.0000 - rmse: 22389.7930 - val_loss: 583226432.0000 - val_rmse: 24150.0801\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466811936.0000 - rmse: 21605.8320 - val_loss: 1037679872.0000 - val_rmse: 32213.0371\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436786560.0000 - rmse: 20899.4375 - val_loss: 987218880.0000 - val_rmse: 31420.0391\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416428032.0000 - rmse: 20406.5664 - val_loss: 765170176.0000 - val_rmse: 27661.7070\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428314848.0000 - rmse: 20695.7695 - val_loss: 684735552.0000 - val_rmse: 26167.4531\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450495072.0000 - rmse: 21224.8691 - val_loss: 810429504.0000 - val_rmse: 28468.0410\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474673248.0000 - rmse: 21786.9941 - val_loss: 499427616.0000 - val_rmse: 22347.8750\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459964448.0000 - rmse: 21446.7812 - val_loss: 609400896.0000 - val_rmse: 24686.0430\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409127360.0000 - rmse: 20226.8945 - val_loss: 683501120.0000 - val_rmse: 26143.8516\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427741920.0000 - rmse: 20681.9219 - val_loss: 762816768.0000 - val_rmse: 27619.1387\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456555392.0000 - rmse: 21367.1562 - val_loss: 578757376.0000 - val_rmse: 24057.3750\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394849248.0000 - rmse: 19870.8125 - val_loss: 1289147520.0000 - val_rmse: 35904.6992\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427422048.0000 - rmse: 20674.1855 - val_loss: 1358411008.0000 - val_rmse: 36856.6289\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376863232.0000 - rmse: 19412.9629 - val_loss: 1195365632.0000 - val_rmse: 34574.0547\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451209376.0000 - rmse: 21241.6895 - val_loss: 966608640.0000 - val_rmse: 31090.3301\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367994432.0000 - rmse: 19183.1816 - val_loss: 713853056.0000 - val_rmse: 26718.0293\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380833792.0000 - rmse: 19514.9629 - val_loss: 1071878656.0000 - val_rmse: 32739.5586\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399609056.0000 - rmse: 19990.2227 - val_loss: 1664555904.0000 - val_rmse: 40798.9688\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418842688.0000 - rmse: 20465.6465 - val_loss: 826263168.0000 - val_rmse: 28744.7930\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386480672.0000 - rmse: 19659.1113 - val_loss: 983134208.0000 - val_rmse: 31354.9707\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396604032.0000 - rmse: 19914.9199 - val_loss: 1176607232.0000 - val_rmse: 34301.7070\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359786752.0000 - rmse: 18968.0430 - val_loss: 1355033344.0000 - val_rmse: 36810.7773\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353438400.0000 - rmse: 18799.9570 - val_loss: 830676544.0000 - val_rmse: 28821.4590\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463004448.0000 - rmse: 21517.5371 - val_loss: 689303296.0000 - val_rmse: 26254.5859\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342421984.0000 - rmse: 18504.6484 - val_loss: 2295595008.0000 - val_rmse: 47912.3672\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412621472.0000 - rmse: 20313.0840 - val_loss: 785559168.0000 - val_rmse: 28027.8281\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389065408.0000 - rmse: 19724.7402 - val_loss: 875447936.0000 - val_rmse: 29587.9688\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377721792.0000 - rmse: 19435.0645 - val_loss: 1651411584.0000 - val_rmse: 40637.5625\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421910688.0000 - rmse: 20540.4648 - val_loss: 928954624.0000 - val_rmse: 30478.7578\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375148640.0000 - rmse: 19368.7500 - val_loss: 942291264.0000 - val_rmse: 30696.7617\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386467360.0000 - rmse: 19658.7715 - val_loss: 1794727040.0000 - val_rmse: 42364.2188\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366325760.0000 - rmse: 19139.6387 - val_loss: 974328512.0000 - val_rmse: 31214.2344\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389822624.0000 - rmse: 19743.9238 - val_loss: 743074752.0000 - val_rmse: 27259.3965\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406167904.0000 - rmse: 20153.6074 - val_loss: 1561183104.0000 - val_rmse: 39511.8047\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410226816.0000 - rmse: 20254.0547 - val_loss: 1142842624.0000 - val_rmse: 33805.9570\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448509504.0000 - rmse: 21178.0410 - val_loss: 938438208.0000 - val_rmse: 30633.9375\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397277600.0000 - rmse: 19931.8223 - val_loss: 1278946432.0000 - val_rmse: 35762.3555\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383816032.0000 - rmse: 19591.2207 - val_loss: 676762944.0000 - val_rmse: 26014.6660\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394103392.0000 - rmse: 19852.0371 - val_loss: 1191152512.0000 - val_rmse: 34513.0781\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414154784.0000 - rmse: 20350.7910 - val_loss: 759480064.0000 - val_rmse: 27558.6641\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418962016.0000 - rmse: 20468.5605 - val_loss: 937624704.0000 - val_rmse: 30620.6582\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357120640.0000 - rmse: 18897.6328 - val_loss: 881459456.0000 - val_rmse: 29689.3809\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403163744.0000 - rmse: 20078.9375 - val_loss: 893164864.0000 - val_rmse: 29885.8633\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465721632.0000 - rmse: 21580.5840 - val_loss: 991551936.0000 - val_rmse: 31488.9180\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420978080.0000 - rmse: 20517.7480 - val_loss: 707320960.0000 - val_rmse: 26595.5039\n",
      "104/104 [==============================] - 0s 649us/step - loss: 396156160.0000 - rmse: 19903.6719\n",
      "[396156160.0, 19903.671875]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 6591286784.0000 - rmse: 81186.7422 - val_loss: 1352221056.0000 - val_rmse: 36772.5586\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1629317376.0000 - rmse: 40364.8047 - val_loss: 1442913792.0000 - val_rmse: 37985.7070\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1340718464.0000 - rmse: 36615.8242 - val_loss: 961355520.0000 - val_rmse: 31005.7344\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1344913536.0000 - rmse: 36673.0625 - val_loss: 990745472.0000 - val_rmse: 31476.1094\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1198975360.0000 - rmse: 34626.2227 - val_loss: 917081600.0000 - val_rmse: 30283.3555\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1212485504.0000 - rmse: 34820.7617 - val_loss: 940381696.0000 - val_rmse: 30665.6445\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1179879168.0000 - rmse: 34349.3711 - val_loss: 844607040.0000 - val_rmse: 29062.1230\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1132838656.0000 - rmse: 33657.6680 - val_loss: 830830592.0000 - val_rmse: 28824.1328\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1185384064.0000 - rmse: 34429.4062 - val_loss: 823531328.0000 - val_rmse: 28697.2363\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1050531776.0000 - rmse: 32411.9082 - val_loss: 1027589376.0000 - val_rmse: 32056.0352\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1081081216.0000 - rmse: 32879.8008 - val_loss: 842712384.0000 - val_rmse: 29029.5098\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1111975680.0000 - rmse: 33346.3008 - val_loss: 800507392.0000 - val_rmse: 28293.2383\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1028696704.0000 - rmse: 32073.3027 - val_loss: 807824896.0000 - val_rmse: 28422.2598\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1043121920.0000 - rmse: 32297.3965 - val_loss: 811324736.0000 - val_rmse: 28483.7617\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 951392768.0000 - rmse: 30844.6562 - val_loss: 1396298368.0000 - val_rmse: 37367.0742\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 935166400.0000 - rmse: 30580.4902 - val_loss: 791390336.0000 - val_rmse: 28131.6602\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 988366016.0000 - rmse: 31438.2852 - val_loss: 810360960.0000 - val_rmse: 28466.8398\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 916153984.0000 - rmse: 30268.0332 - val_loss: 860972544.0000 - val_rmse: 29342.3340\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 983675712.0000 - rmse: 31363.6055 - val_loss: 789300864.0000 - val_rmse: 28094.4980\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 920474240.0000 - rmse: 30339.3164 - val_loss: 770444992.0000 - val_rmse: 27756.8906\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 904570176.0000 - rmse: 30076.0723 - val_loss: 770388608.0000 - val_rmse: 27755.8750\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 944441984.0000 - rmse: 30731.7754 - val_loss: 783757248.0000 - val_rmse: 27995.6641\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 843539008.0000 - rmse: 29043.7422 - val_loss: 747719296.0000 - val_rmse: 27344.4570\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 917290176.0000 - rmse: 30286.7988 - val_loss: 901733568.0000 - val_rmse: 30028.8789\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 834876672.0000 - rmse: 28894.2324 - val_loss: 1254837632.0000 - val_rmse: 35423.6875\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 840795840.0000 - rmse: 28996.4805 - val_loss: 733215680.0000 - val_rmse: 27077.9551\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 812532992.0000 - rmse: 28504.9648 - val_loss: 700384704.0000 - val_rmse: 26464.7832\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 822907328.0000 - rmse: 28686.3613 - val_loss: 717393280.0000 - val_rmse: 26784.1992\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 828297984.0000 - rmse: 28780.1660 - val_loss: 714232832.0000 - val_rmse: 26725.1348\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 833625408.0000 - rmse: 28872.5723 - val_loss: 701592256.0000 - val_rmse: 26487.5859\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 836471168.0000 - rmse: 28921.8105 - val_loss: 700457536.0000 - val_rmse: 26466.1562\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 822394240.0000 - rmse: 28677.4160 - val_loss: 656655040.0000 - val_rmse: 25625.2812\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692870720.0000 - rmse: 26322.4375 - val_loss: 676866880.0000 - val_rmse: 26016.6660\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 738440704.0000 - rmse: 27174.2656 - val_loss: 663728064.0000 - val_rmse: 25762.9199\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 740359872.0000 - rmse: 27209.5547 - val_loss: 728210048.0000 - val_rmse: 26985.3672\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 744101568.0000 - rmse: 27278.2246 - val_loss: 739233600.0000 - val_rmse: 27188.8477\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 667277952.0000 - rmse: 25831.7227 - val_loss: 755743424.0000 - val_rmse: 27490.7871\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 707816256.0000 - rmse: 26604.8145 - val_loss: 655718400.0000 - val_rmse: 25606.9961\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642855040.0000 - rmse: 25354.5840 - val_loss: 664051008.0000 - val_rmse: 25769.1875\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676999360.0000 - rmse: 26019.2109 - val_loss: 608339456.0000 - val_rmse: 24664.5371\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 660076992.0000 - rmse: 25691.9629 - val_loss: 775196608.0000 - val_rmse: 27842.3535\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 643151936.0000 - rmse: 25360.4395 - val_loss: 851426560.0000 - val_rmse: 29179.2148\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 639851840.0000 - rmse: 25295.2930 - val_loss: 698104896.0000 - val_rmse: 26421.6738\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620216192.0000 - rmse: 24904.1406 - val_loss: 675574144.0000 - val_rmse: 25991.8066\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626545472.0000 - rmse: 25030.8906 - val_loss: 585307776.0000 - val_rmse: 24193.1328\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604788160.0000 - rmse: 24592.4414 - val_loss: 608259264.0000 - val_rmse: 24662.9121\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567683264.0000 - rmse: 23826.1016 - val_loss: 608797824.0000 - val_rmse: 24673.8281\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628067776.0000 - rmse: 25061.2793 - val_loss: 573729920.0000 - val_rmse: 23952.6602\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 595564480.0000 - rmse: 24404.1875 - val_loss: 613458944.0000 - val_rmse: 24768.1035\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531699840.0000 - rmse: 23058.6152 - val_loss: 608948608.0000 - val_rmse: 24676.8828\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577045056.0000 - rmse: 24021.7617 - val_loss: 529737440.0000 - val_rmse: 23016.0234\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 528624896.0000 - rmse: 22991.8418 - val_loss: 686875456.0000 - val_rmse: 26208.3086\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553926080.0000 - rmse: 23535.6328 - val_loss: 584753536.0000 - val_rmse: 24181.6777\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513230528.0000 - rmse: 22654.5898 - val_loss: 924770880.0000 - val_rmse: 30410.0449\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524655232.0000 - rmse: 22905.3535 - val_loss: 605199680.0000 - val_rmse: 24600.8047\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517661952.0000 - rmse: 22752.1836 - val_loss: 580691392.0000 - val_rmse: 24097.5371\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550369472.0000 - rmse: 23459.9551 - val_loss: 591426752.0000 - val_rmse: 24319.2656\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486863200.0000 - rmse: 22064.9746 - val_loss: 647965440.0000 - val_rmse: 25455.1641\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463638400.0000 - rmse: 21532.2637 - val_loss: 593267008.0000 - val_rmse: 24357.0723\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511715008.0000 - rmse: 22621.1172 - val_loss: 516464864.0000 - val_rmse: 22725.8613\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538290304.0000 - rmse: 23201.0840 - val_loss: 641504320.0000 - val_rmse: 25327.9355\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468329632.0000 - rmse: 21640.9219 - val_loss: 576515072.0000 - val_rmse: 24010.7285\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499522656.0000 - rmse: 22350.0039 - val_loss: 608070592.0000 - val_rmse: 24659.0859\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468057120.0000 - rmse: 21634.6270 - val_loss: 582011968.0000 - val_rmse: 24124.9238\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449502688.0000 - rmse: 21201.4766 - val_loss: 556461632.0000 - val_rmse: 23589.4395\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441189984.0000 - rmse: 21004.5215 - val_loss: 539305472.0000 - val_rmse: 23222.9473\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429415872.0000 - rmse: 20722.3477 - val_loss: 531964320.0000 - val_rmse: 23064.3496\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449785472.0000 - rmse: 21208.1445 - val_loss: 497870240.0000 - val_rmse: 22313.0059\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435097344.0000 - rmse: 20858.9844 - val_loss: 527779456.0000 - val_rmse: 22973.4473\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443931040.0000 - rmse: 21069.6699 - val_loss: 483673888.0000 - val_rmse: 21992.5859\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423286144.0000 - rmse: 20573.9180 - val_loss: 580134080.0000 - val_rmse: 24085.9727\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430881504.0000 - rmse: 20757.6816 - val_loss: 596897536.0000 - val_rmse: 24431.4844\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441039840.0000 - rmse: 21000.9473 - val_loss: 597263168.0000 - val_rmse: 24438.9688\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441431040.0000 - rmse: 21010.2578 - val_loss: 558273600.0000 - val_rmse: 23627.8125\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420622432.0000 - rmse: 20509.0801 - val_loss: 701958016.0000 - val_rmse: 26494.4902\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420376064.0000 - rmse: 20503.0742 - val_loss: 728077888.0000 - val_rmse: 26982.9180\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348555552.0000 - rmse: 18669.6406 - val_loss: 520935328.0000 - val_rmse: 22824.0059\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414039552.0000 - rmse: 20347.9609 - val_loss: 540027264.0000 - val_rmse: 23238.4844\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439031680.0000 - rmse: 20953.0820 - val_loss: 554451072.0000 - val_rmse: 23546.7832\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417553024.0000 - rmse: 20434.1113 - val_loss: 623415552.0000 - val_rmse: 24968.2891\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480156576.0000 - rmse: 21912.4746 - val_loss: 563174528.0000 - val_rmse: 23731.2949\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390839360.0000 - rmse: 19769.6562 - val_loss: 859730048.0000 - val_rmse: 29321.1523\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423889056.0000 - rmse: 20588.5645 - val_loss: 525382944.0000 - val_rmse: 22921.2324\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389388192.0000 - rmse: 19732.9199 - val_loss: 621292928.0000 - val_rmse: 24925.7480\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375954624.0000 - rmse: 19389.5469 - val_loss: 531467104.0000 - val_rmse: 23053.5684\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386249440.0000 - rmse: 19653.2266 - val_loss: 572516352.0000 - val_rmse: 23927.3125\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353482848.0000 - rmse: 18801.1367 - val_loss: 563739584.0000 - val_rmse: 23743.1992\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367758976.0000 - rmse: 19177.0391 - val_loss: 528389792.0000 - val_rmse: 22986.7285\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357878816.0000 - rmse: 18917.6836 - val_loss: 569661056.0000 - val_rmse: 23867.5742\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422653760.0000 - rmse: 20558.5430 - val_loss: 982281792.0000 - val_rmse: 31341.3750\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336952160.0000 - rmse: 18356.2559 - val_loss: 587189376.0000 - val_rmse: 24231.9902\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376150496.0000 - rmse: 19394.5977 - val_loss: 651586816.0000 - val_rmse: 25526.1992\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389279552.0000 - rmse: 19730.1660 - val_loss: 502903360.0000 - val_rmse: 22425.5059\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398057504.0000 - rmse: 19951.3770 - val_loss: 806893248.0000 - val_rmse: 28405.8672\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362786208.0000 - rmse: 19046.9473 - val_loss: 607632000.0000 - val_rmse: 24650.1914\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342752960.0000 - rmse: 18513.5879 - val_loss: 589653568.0000 - val_rmse: 24282.7812\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340483104.0000 - rmse: 18452.1816 - val_loss: 560721344.0000 - val_rmse: 23679.5547\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332075200.0000 - rmse: 18222.9297 - val_loss: 607758784.0000 - val_rmse: 24652.7637\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356500960.0000 - rmse: 18881.2305 - val_loss: 518395840.0000 - val_rmse: 22768.3066\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378801472.0000 - rmse: 19462.8203 - val_loss: 461654976.0000 - val_rmse: 21486.1562\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355027072.0000 - rmse: 18842.1602 - val_loss: 535327680.0000 - val_rmse: 23137.1465\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357794528.0000 - rmse: 18915.4570 - val_loss: 585062400.0000 - val_rmse: 24188.0625\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389016096.0000 - rmse: 19723.4902 - val_loss: 536300864.0000 - val_rmse: 23158.1699\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372214624.0000 - rmse: 19292.8613 - val_loss: 607949184.0000 - val_rmse: 24656.6211\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368328128.0000 - rmse: 19191.8750 - val_loss: 556881792.0000 - val_rmse: 23598.3398\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395378048.0000 - rmse: 19884.1133 - val_loss: 759420032.0000 - val_rmse: 27557.5742\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334634624.0000 - rmse: 18293.0195 - val_loss: 488110688.0000 - val_rmse: 22093.2246\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353464352.0000 - rmse: 18800.6465 - val_loss: 555967616.0000 - val_rmse: 23578.9648\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340618560.0000 - rmse: 18455.8516 - val_loss: 502025856.0000 - val_rmse: 22405.9316\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405916896.0000 - rmse: 20147.3770 - val_loss: 576889664.0000 - val_rmse: 24018.5254\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373275200.0000 - rmse: 19320.3301 - val_loss: 558251392.0000 - val_rmse: 23627.3438\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323205120.0000 - rmse: 17977.9043 - val_loss: 573736128.0000 - val_rmse: 23952.7891\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350275712.0000 - rmse: 18715.6504 - val_loss: 677253504.0000 - val_rmse: 26024.0938\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371950560.0000 - rmse: 19286.0176 - val_loss: 590033728.0000 - val_rmse: 24290.6055\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314704992.0000 - rmse: 17739.9258 - val_loss: 536968512.0000 - val_rmse: 23172.5781\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318731936.0000 - rmse: 17853.0625 - val_loss: 554137024.0000 - val_rmse: 23540.1133\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341326976.0000 - rmse: 18475.0352 - val_loss: 554010560.0000 - val_rmse: 23537.4277\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377336736.0000 - rmse: 19425.1523 - val_loss: 523466976.0000 - val_rmse: 22879.3984\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366267488.0000 - rmse: 19138.1133 - val_loss: 591549824.0000 - val_rmse: 24321.7969\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311545984.0000 - rmse: 17650.6621 - val_loss: 519589888.0000 - val_rmse: 22794.5117\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338923936.0000 - rmse: 18409.8848 - val_loss: 551072448.0000 - val_rmse: 23474.9316\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434132032.0000 - rmse: 20835.8320 - val_loss: 591142656.0000 - val_rmse: 24313.4238\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317336864.0000 - rmse: 17813.9473 - val_loss: 547375360.0000 - val_rmse: 23396.0508\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378805888.0000 - rmse: 19462.9355 - val_loss: 607132992.0000 - val_rmse: 24640.0684\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319894464.0000 - rmse: 17885.5918 - val_loss: 629322880.0000 - val_rmse: 25086.3066\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356222720.0000 - rmse: 18873.8594 - val_loss: 735445120.0000 - val_rmse: 27119.0918\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294236736.0000 - rmse: 17153.3281 - val_loss: 516340160.0000 - val_rmse: 22723.1172\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337935328.0000 - rmse: 18383.0156 - val_loss: 640265472.0000 - val_rmse: 25303.4668\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317722368.0000 - rmse: 17824.7656 - val_loss: 538931456.0000 - val_rmse: 23214.8965\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316437824.0000 - rmse: 17788.6934 - val_loss: 676728960.0000 - val_rmse: 26014.0117\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360819584.0000 - rmse: 18995.2480 - val_loss: 511625344.0000 - val_rmse: 22619.1348\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322881408.0000 - rmse: 17968.8984 - val_loss: 638096704.0000 - val_rmse: 25260.5742\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331819968.0000 - rmse: 18215.9238 - val_loss: 599138112.0000 - val_rmse: 24477.2949\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331934400.0000 - rmse: 18219.0625 - val_loss: 495840736.0000 - val_rmse: 22267.4805\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335280960.0000 - rmse: 18310.6758 - val_loss: 984527488.0000 - val_rmse: 31377.1797\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310398368.0000 - rmse: 17618.1250 - val_loss: 562504320.0000 - val_rmse: 23717.1699\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336421408.0000 - rmse: 18341.7910 - val_loss: 577746944.0000 - val_rmse: 24036.3652\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301946144.0000 - rmse: 17376.5957 - val_loss: 556910144.0000 - val_rmse: 23598.9395\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323094144.0000 - rmse: 17974.8184 - val_loss: 601248960.0000 - val_rmse: 24520.3789\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348890880.0000 - rmse: 18678.6191 - val_loss: 564188608.0000 - val_rmse: 23752.6523\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310684416.0000 - rmse: 17626.2402 - val_loss: 518598848.0000 - val_rmse: 22772.7617\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286729792.0000 - rmse: 16933.0938 - val_loss: 481799232.0000 - val_rmse: 21949.9238\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296911904.0000 - rmse: 17231.1289 - val_loss: 579731520.0000 - val_rmse: 24077.6113\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344441760.0000 - rmse: 18559.1406 - val_loss: 553875392.0000 - val_rmse: 23534.5566\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293405344.0000 - rmse: 17129.0762 - val_loss: 580712192.0000 - val_rmse: 24097.9688\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338559360.0000 - rmse: 18399.9785 - val_loss: 603942720.0000 - val_rmse: 24575.2441\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332200640.0000 - rmse: 18226.3711 - val_loss: 539825664.0000 - val_rmse: 23234.1465\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272346848.0000 - rmse: 16502.9297 - val_loss: 503762432.0000 - val_rmse: 22444.6504\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318161280.0000 - rmse: 17837.0742 - val_loss: 572231360.0000 - val_rmse: 23921.3574\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274291264.0000 - rmse: 16561.7383 - val_loss: 990839936.0000 - val_rmse: 31477.6094\n",
      "104/104 [==============================] - 0s 757us/step - loss: 1550510976.0000 - rmse: 39376.5273\n",
      "[1550510976.0, 39376.52734375]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 5731416576.0000 - rmse: 75706.1172 - val_loss: 1470010880.0000 - val_rmse: 38340.7227\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1580325888.0000 - rmse: 39753.3125 - val_loss: 1184863232.0000 - val_rmse: 34421.8438\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1458892160.0000 - rmse: 38195.4453 - val_loss: 1054146560.0000 - val_rmse: 32467.6230\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1289036032.0000 - rmse: 35903.1484 - val_loss: 1149480576.0000 - val_rmse: 33903.9922\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1211989888.0000 - rmse: 34813.6445 - val_loss: 971641024.0000 - val_rmse: 31171.1562\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1169315584.0000 - rmse: 34195.2578 - val_loss: 941981760.0000 - val_rmse: 30691.7188\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1131500288.0000 - rmse: 33637.7812 - val_loss: 917235648.0000 - val_rmse: 30285.8965\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1107975808.0000 - rmse: 33286.2695 - val_loss: 906676928.0000 - val_rmse: 30111.0762\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1141668736.0000 - rmse: 33788.5859 - val_loss: 987764608.0000 - val_rmse: 31428.7227\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1044212992.0000 - rmse: 32314.2852 - val_loss: 1068457088.0000 - val_rmse: 32687.2617\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1043181504.0000 - rmse: 32298.3203 - val_loss: 982932800.0000 - val_rmse: 31351.7598\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1083793536.0000 - rmse: 32921.0195 - val_loss: 966615680.0000 - val_rmse: 31090.4434\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1025896768.0000 - rmse: 32029.6230 - val_loss: 926810752.0000 - val_rmse: 30443.5664\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1022476096.0000 - rmse: 31976.1777 - val_loss: 896321472.0000 - val_rmse: 29938.6289\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1021625344.0000 - rmse: 31962.8750 - val_loss: 882180416.0000 - val_rmse: 29701.5215\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 939468288.0000 - rmse: 30650.7461 - val_loss: 862743552.0000 - val_rmse: 29372.4961\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1002613696.0000 - rmse: 31664.0762 - val_loss: 872556544.0000 - val_rmse: 29539.0684\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 933389120.0000 - rmse: 30551.4180 - val_loss: 1015247296.0000 - val_rmse: 31862.9453\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 981472000.0000 - rmse: 31328.4531 - val_loss: 975570816.0000 - val_rmse: 31234.1289\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 972394496.0000 - rmse: 31183.2402 - val_loss: 970064704.0000 - val_rmse: 31145.8613\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 959422336.0000 - rmse: 30974.5430 - val_loss: 1018314112.0000 - val_rmse: 31911.0352\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 938999232.0000 - rmse: 30643.0938 - val_loss: 941983296.0000 - val_rmse: 30691.7461\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 936557760.0000 - rmse: 30603.2305 - val_loss: 987322176.0000 - val_rmse: 31421.6836\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 857610112.0000 - rmse: 29284.9805 - val_loss: 1119192704.0000 - val_rmse: 33454.3359\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 864325824.0000 - rmse: 29399.4160 - val_loss: 1306813440.0000 - val_rmse: 36149.8750\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 983655872.0000 - rmse: 31363.2891 - val_loss: 1391777280.0000 - val_rmse: 37306.5312\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 851708352.0000 - rmse: 29184.0430 - val_loss: 1446580480.0000 - val_rmse: 38033.9375\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 906188672.0000 - rmse: 30102.9688 - val_loss: 1689371264.0000 - val_rmse: 41101.9609\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 884655104.0000 - rmse: 29743.1523 - val_loss: 951224192.0000 - val_rmse: 30841.9219\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 892228160.0000 - rmse: 29870.1895 - val_loss: 1015019712.0000 - val_rmse: 31859.3730\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 892221440.0000 - rmse: 29870.0742 - val_loss: 917383552.0000 - val_rmse: 30288.3398\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 916852864.0000 - rmse: 30279.5781 - val_loss: 1039307456.0000 - val_rmse: 32238.2910\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 772139392.0000 - rmse: 27787.3965 - val_loss: 984738560.0000 - val_rmse: 31380.5430\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 823099904.0000 - rmse: 28689.7168 - val_loss: 1490436096.0000 - val_rmse: 38606.1680\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 767445120.0000 - rmse: 27702.8008 - val_loss: 1081193472.0000 - val_rmse: 32881.5078\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 773977920.0000 - rmse: 27820.4570 - val_loss: 1038160512.0000 - val_rmse: 32220.4980\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 836118336.0000 - rmse: 28915.7109 - val_loss: 1056385984.0000 - val_rmse: 32502.0918\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 789962752.0000 - rmse: 28106.2754 - val_loss: 829234368.0000 - val_rmse: 28796.4277\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 755969984.0000 - rmse: 27494.9082 - val_loss: 1205830272.0000 - val_rmse: 34725.0664\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733795648.0000 - rmse: 27088.6621 - val_loss: 983377088.0000 - val_rmse: 31358.8398\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 709847616.0000 - rmse: 26642.9648 - val_loss: 1032999936.0000 - val_rmse: 32140.3164\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 713211456.0000 - rmse: 26706.0176 - val_loss: 1209827200.0000 - val_rmse: 34782.5703\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 708861888.0000 - rmse: 26624.4609 - val_loss: 1004892928.0000 - val_rmse: 31700.0469\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 810990784.0000 - rmse: 28477.9004 - val_loss: 1084779392.0000 - val_rmse: 32935.9883\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 773179520.0000 - rmse: 27806.1055 - val_loss: 1066152896.0000 - val_rmse: 32651.9961\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 712284992.0000 - rmse: 26688.6660 - val_loss: 1044018048.0000 - val_rmse: 32311.2676\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 707291008.0000 - rmse: 26594.9434 - val_loss: 768597312.0000 - val_rmse: 27723.5879\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676774912.0000 - rmse: 26014.8965 - val_loss: 959291840.0000 - val_rmse: 30972.4375\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 686256640.0000 - rmse: 26196.5000 - val_loss: 843889792.0000 - val_rmse: 29049.7793\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634019968.0000 - rmse: 25179.7520 - val_loss: 1010424896.0000 - val_rmse: 31787.1797\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 722887168.0000 - rmse: 26886.5605 - val_loss: 1045399360.0000 - val_rmse: 32332.6367\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582009152.0000 - rmse: 24124.8652 - val_loss: 1062016640.0000 - val_rmse: 32588.5977\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601638144.0000 - rmse: 24528.3125 - val_loss: 950387648.0000 - val_rmse: 30828.3555\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668196928.0000 - rmse: 25849.5020 - val_loss: 1037252352.0000 - val_rmse: 32206.4023\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601974848.0000 - rmse: 24535.1758 - val_loss: 974538816.0000 - val_rmse: 31217.6035\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 603303744.0000 - rmse: 24562.2402 - val_loss: 956485376.0000 - val_rmse: 30927.0977\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623513600.0000 - rmse: 24970.2539 - val_loss: 856009280.0000 - val_rmse: 29257.6367\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 670783872.0000 - rmse: 25899.4941 - val_loss: 702523264.0000 - val_rmse: 26505.1562\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 697178560.0000 - rmse: 26404.1387 - val_loss: 678024896.0000 - val_rmse: 26038.9102\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 653036480.0000 - rmse: 25554.5781 - val_loss: 1163342208.0000 - val_rmse: 34107.8047\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 635511552.0000 - rmse: 25209.3535 - val_loss: 753162688.0000 - val_rmse: 27443.8105\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606453184.0000 - rmse: 24626.2676 - val_loss: 713551488.0000 - val_rmse: 26712.3848\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565038976.0000 - rmse: 23770.5449 - val_loss: 1006894784.0000 - val_rmse: 31731.6055\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512664320.0000 - rmse: 22642.0918 - val_loss: 1182517760.0000 - val_rmse: 34387.7539\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 608104832.0000 - rmse: 24659.7793 - val_loss: 761832384.0000 - val_rmse: 27601.3086\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505273120.0000 - rmse: 22478.2812 - val_loss: 1616400384.0000 - val_rmse: 40204.4805\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586196352.0000 - rmse: 24211.4902 - val_loss: 1424654336.0000 - val_rmse: 37744.5938\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592586112.0000 - rmse: 24343.0918 - val_loss: 588288384.0000 - val_rmse: 24254.6562\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538840640.0000 - rmse: 23212.9375 - val_loss: 910057344.0000 - val_rmse: 30167.1543\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515047136.0000 - rmse: 22694.6504 - val_loss: 765891712.0000 - val_rmse: 27674.7461\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512787008.0000 - rmse: 22644.7988 - val_loss: 626275520.0000 - val_rmse: 25025.4980\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493962560.0000 - rmse: 22225.2656 - val_loss: 646853568.0000 - val_rmse: 25433.3145\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480840192.0000 - rmse: 21928.0664 - val_loss: 1081307264.0000 - val_rmse: 32883.2344\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548428096.0000 - rmse: 23418.5410 - val_loss: 592171456.0000 - val_rmse: 24334.5742\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612416832.0000 - rmse: 24747.0566 - val_loss: 812763456.0000 - val_rmse: 28509.0059\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584391744.0000 - rmse: 24174.1953 - val_loss: 505905728.0000 - val_rmse: 22492.3477\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549579264.0000 - rmse: 23443.1035 - val_loss: 593884288.0000 - val_rmse: 24369.7402\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582376768.0000 - rmse: 24132.4844 - val_loss: 1198742400.0000 - val_rmse: 34622.8594\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580318080.0000 - rmse: 24089.7891 - val_loss: 632937920.0000 - val_rmse: 25158.2559\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469311488.0000 - rmse: 21663.5977 - val_loss: 1514301696.0000 - val_rmse: 38914.0273\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559676864.0000 - rmse: 23657.4902 - val_loss: 747865024.0000 - val_rmse: 27347.1211\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449941632.0000 - rmse: 21211.8262 - val_loss: 624148160.0000 - val_rmse: 24982.9570\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496637440.0000 - rmse: 22285.3613 - val_loss: 1223142528.0000 - val_rmse: 34973.4531\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494023200.0000 - rmse: 22226.6328 - val_loss: 597750848.0000 - val_rmse: 24448.9434\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466854624.0000 - rmse: 21606.8145 - val_loss: 582866112.0000 - val_rmse: 24142.6191\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432890368.0000 - rmse: 20806.0156 - val_loss: 788237760.0000 - val_rmse: 28075.5703\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424324256.0000 - rmse: 20599.1309 - val_loss: 887218816.0000 - val_rmse: 29786.2188\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497593216.0000 - rmse: 22306.7969 - val_loss: 893068992.0000 - val_rmse: 29884.2598\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467831232.0000 - rmse: 21629.4043 - val_loss: 1120071936.0000 - val_rmse: 33467.4766\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430543072.0000 - rmse: 20749.5312 - val_loss: 506078112.0000 - val_rmse: 22496.1797\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474999296.0000 - rmse: 21794.4785 - val_loss: 570354176.0000 - val_rmse: 23882.0879\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430854656.0000 - rmse: 20757.0371 - val_loss: 479158464.0000 - val_rmse: 21889.6875\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446045856.0000 - rmse: 21119.7969 - val_loss: 1128649600.0000 - val_rmse: 33595.3789\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530347680.0000 - rmse: 23029.2754 - val_loss: 1439307392.0000 - val_rmse: 37938.2031\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483318976.0000 - rmse: 21984.5176 - val_loss: 1118578304.0000 - val_rmse: 33445.1523\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489334144.0000 - rmse: 22120.8965 - val_loss: 931900096.0000 - val_rmse: 30527.0371\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516917408.0000 - rmse: 22735.8164 - val_loss: 801188928.0000 - val_rmse: 28305.2793\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391540832.0000 - rmse: 19787.3887 - val_loss: 472087232.0000 - val_rmse: 21727.5664\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393696768.0000 - rmse: 19841.7930 - val_loss: 796391680.0000 - val_rmse: 28220.4102\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422544608.0000 - rmse: 20555.8887 - val_loss: 750542976.0000 - val_rmse: 27396.0371\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446441856.0000 - rmse: 21129.1699 - val_loss: 867096000.0000 - val_rmse: 29446.4922\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447490176.0000 - rmse: 21153.9609 - val_loss: 617885632.0000 - val_rmse: 24857.3047\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434923904.0000 - rmse: 20854.8281 - val_loss: 1193035776.0000 - val_rmse: 34540.3516\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398750272.0000 - rmse: 19968.7305 - val_loss: 422979712.0000 - val_rmse: 20566.4707\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463828544.0000 - rmse: 21536.6758 - val_loss: 655481280.0000 - val_rmse: 25602.3672\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434204352.0000 - rmse: 20837.5664 - val_loss: 474582528.0000 - val_rmse: 21784.9121\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416740416.0000 - rmse: 20414.2188 - val_loss: 594738112.0000 - val_rmse: 24387.2500\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452482080.0000 - rmse: 21271.6250 - val_loss: 450479296.0000 - val_rmse: 21224.4980\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373384160.0000 - rmse: 19323.1484 - val_loss: 717674432.0000 - val_rmse: 26789.4453\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414288800.0000 - rmse: 20354.0840 - val_loss: 993490240.0000 - val_rmse: 31519.6777\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447152384.0000 - rmse: 21145.9746 - val_loss: 611230528.0000 - val_rmse: 24723.0762\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477211040.0000 - rmse: 21845.1602 - val_loss: 1221564800.0000 - val_rmse: 34950.8906\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463779296.0000 - rmse: 21535.5352 - val_loss: 418852544.0000 - val_rmse: 20465.8848\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458418208.0000 - rmse: 21410.7012 - val_loss: 1018613952.0000 - val_rmse: 31915.7324\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407397568.0000 - rmse: 20184.0898 - val_loss: 722134720.0000 - val_rmse: 26872.5645\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385851136.0000 - rmse: 19643.0918 - val_loss: 459210272.0000 - val_rmse: 21429.1914\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423901920.0000 - rmse: 20588.8770 - val_loss: 1032092800.0000 - val_rmse: 32126.2012\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415705376.0000 - rmse: 20388.8496 - val_loss: 758744000.0000 - val_rmse: 27545.3066\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410317952.0000 - rmse: 20256.3066 - val_loss: 602157440.0000 - val_rmse: 24538.8965\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382136192.0000 - rmse: 19548.3027 - val_loss: 497161664.0000 - val_rmse: 22297.1211\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384947936.0000 - rmse: 19620.0898 - val_loss: 820254336.0000 - val_rmse: 28640.0820\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414816608.0000 - rmse: 20367.0449 - val_loss: 505050624.0000 - val_rmse: 22473.3301\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360894272.0000 - rmse: 18997.2148 - val_loss: 563163200.0000 - val_rmse: 23731.0586\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378674880.0000 - rmse: 19459.5684 - val_loss: 1148595072.0000 - val_rmse: 33890.9297\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403271808.0000 - rmse: 20081.6270 - val_loss: 710859392.0000 - val_rmse: 26661.9473\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353195936.0000 - rmse: 18793.5078 - val_loss: 664733568.0000 - val_rmse: 25782.4258\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374439936.0000 - rmse: 19350.4492 - val_loss: 1365510528.0000 - val_rmse: 36952.8125\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350025088.0000 - rmse: 18708.9551 - val_loss: 975149120.0000 - val_rmse: 31227.3750\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351073408.0000 - rmse: 18736.9512 - val_loss: 701490624.0000 - val_rmse: 26485.6680\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397672832.0000 - rmse: 19941.7344 - val_loss: 545419008.0000 - val_rmse: 23354.2070\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418642176.0000 - rmse: 20460.7461 - val_loss: 668501824.0000 - val_rmse: 25855.4023\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341342496.0000 - rmse: 18475.4531 - val_loss: 1470273536.0000 - val_rmse: 38344.1445\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377716832.0000 - rmse: 19434.9375 - val_loss: 1382879616.0000 - val_rmse: 37187.0859\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424669824.0000 - rmse: 20607.5176 - val_loss: 743297280.0000 - val_rmse: 27263.4785\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368475968.0000 - rmse: 19195.7266 - val_loss: 1005965312.0000 - val_rmse: 31716.9551\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418430080.0000 - rmse: 20455.5625 - val_loss: 1038308736.0000 - val_rmse: 32222.7988\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405335904.0000 - rmse: 20132.9531 - val_loss: 1047947712.0000 - val_rmse: 32372.0176\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380633760.0000 - rmse: 19509.8359 - val_loss: 627888768.0000 - val_rmse: 25057.7070\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325698944.0000 - rmse: 18047.1289 - val_loss: 1094182656.0000 - val_rmse: 33078.4336\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363481472.0000 - rmse: 19065.1875 - val_loss: 573278400.0000 - val_rmse: 23943.2324\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346748832.0000 - rmse: 18621.1914 - val_loss: 563315904.0000 - val_rmse: 23734.2754\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368580064.0000 - rmse: 19198.4375 - val_loss: 558647296.0000 - val_rmse: 23635.7188\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361404288.0000 - rmse: 19010.6328 - val_loss: 681210880.0000 - val_rmse: 26100.0137\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325457664.0000 - rmse: 18040.4434 - val_loss: 507984032.0000 - val_rmse: 22538.4980\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315408032.0000 - rmse: 17759.7305 - val_loss: 417789952.0000 - val_rmse: 20439.9102\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363437728.0000 - rmse: 19064.0410 - val_loss: 1070453056.0000 - val_rmse: 32717.7793\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342404288.0000 - rmse: 18504.1680 - val_loss: 453856384.0000 - val_rmse: 21303.9023\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344329440.0000 - rmse: 18556.1152 - val_loss: 437233184.0000 - val_rmse: 20910.1191\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360220096.0000 - rmse: 18979.4629 - val_loss: 504077152.0000 - val_rmse: 22451.6621\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357159488.0000 - rmse: 18898.6621 - val_loss: 1275715200.0000 - val_rmse: 35717.1562\n",
      "104/104 [==============================] - 0s 665us/step - loss: 1097408896.0000 - rmse: 33127.1641\n",
      "[1097408896.0, 33127.1640625]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 7000293376.0000 - rmse: 83667.7578 - val_loss: 1364617344.0000 - val_rmse: 36940.7266\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1767912576.0000 - rmse: 42046.5508 - val_loss: 1171985664.0000 - val_rmse: 34234.2773\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1610740352.0000 - rmse: 40134.0312 - val_loss: 990809152.0000 - val_rmse: 31477.1211\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1489205888.0000 - rmse: 38590.2305 - val_loss: 897078784.0000 - val_rmse: 29951.2734\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1409577088.0000 - rmse: 37544.3359 - val_loss: 909023360.0000 - val_rmse: 30150.0137\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1380468224.0000 - rmse: 37154.6523 - val_loss: 874793408.0000 - val_rmse: 29576.9062\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1356459904.0000 - rmse: 36830.1484 - val_loss: 844802240.0000 - val_rmse: 29065.4824\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1333615744.0000 - rmse: 36518.7031 - val_loss: 854429504.0000 - val_rmse: 29230.6270\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1316921088.0000 - rmse: 36289.4062 - val_loss: 932790976.0000 - val_rmse: 30541.6270\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1274201984.0000 - rmse: 35695.9648 - val_loss: 846313024.0000 - val_rmse: 29091.4590\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1296914816.0000 - rmse: 36012.7031 - val_loss: 869588608.0000 - val_rmse: 29488.7871\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1130344832.0000 - rmse: 33620.6016 - val_loss: 892920256.0000 - val_rmse: 29881.7715\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1091750784.0000 - rmse: 33041.6523 - val_loss: 870906240.0000 - val_rmse: 29511.1211\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1118316416.0000 - rmse: 33441.2383 - val_loss: 844034176.0000 - val_rmse: 29052.2656\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1143516032.0000 - rmse: 33815.9141 - val_loss: 856251392.0000 - val_rmse: 29261.7734\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1115605504.0000 - rmse: 33400.6797 - val_loss: 1162574720.0000 - val_rmse: 34096.5508\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1096530432.0000 - rmse: 33113.9023 - val_loss: 1062528704.0000 - val_rmse: 32596.4531\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1001258624.0000 - rmse: 31642.6719 - val_loss: 1221549824.0000 - val_rmse: 34950.6758\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1027689024.0000 - rmse: 32057.5898 - val_loss: 878979968.0000 - val_rmse: 29647.5957\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 938593152.0000 - rmse: 30636.4668 - val_loss: 1020256320.0000 - val_rmse: 31941.4512\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1025934848.0000 - rmse: 32030.2168 - val_loss: 945797120.0000 - val_rmse: 30753.8145\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 966186048.0000 - rmse: 31083.5332 - val_loss: 925168896.0000 - val_rmse: 30416.5898\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 955680896.0000 - rmse: 30914.0898 - val_loss: 1339438976.0000 - val_rmse: 36598.3477\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 920579712.0000 - rmse: 30341.0566 - val_loss: 863083584.0000 - val_rmse: 29378.2852\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 941180032.0000 - rmse: 30678.6582 - val_loss: 2345591552.0000 - val_rmse: 48431.3086\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 812439232.0000 - rmse: 28503.3184 - val_loss: 817299008.0000 - val_rmse: 28588.4414\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 822755072.0000 - rmse: 28683.7070 - val_loss: 763114432.0000 - val_rmse: 27624.5254\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 833637504.0000 - rmse: 28872.7812 - val_loss: 1066016320.0000 - val_rmse: 32649.9062\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 796019392.0000 - rmse: 28213.8164 - val_loss: 987203008.0000 - val_rmse: 31419.7871\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 789166912.0000 - rmse: 28092.1152 - val_loss: 991808832.0000 - val_rmse: 31492.9961\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 762136576.0000 - rmse: 27606.8203 - val_loss: 931929344.0000 - val_rmse: 30527.5176\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 695496896.0000 - rmse: 26372.2734 - val_loss: 760416768.0000 - val_rmse: 27575.6562\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 768420736.0000 - rmse: 27720.4023 - val_loss: 1507406848.0000 - val_rmse: 38825.3359\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 696094464.0000 - rmse: 26383.6016 - val_loss: 1212975104.0000 - val_rmse: 34827.7930\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691179648.0000 - rmse: 26290.2949 - val_loss: 619937856.0000 - val_rmse: 24898.5508\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680989888.0000 - rmse: 26095.7832 - val_loss: 733580928.0000 - val_rmse: 27084.6973\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 708206720.0000 - rmse: 26612.1543 - val_loss: 772437632.0000 - val_rmse: 27792.7617\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 738008576.0000 - rmse: 27166.3125 - val_loss: 624102336.0000 - val_rmse: 24982.0410\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 627409664.0000 - rmse: 25048.1465 - val_loss: 967185280.0000 - val_rmse: 31099.6035\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 595258624.0000 - rmse: 24397.9219 - val_loss: 629636864.0000 - val_rmse: 25092.5664\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677212352.0000 - rmse: 26023.3047 - val_loss: 2182052608.0000 - val_rmse: 46712.4453\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597338944.0000 - rmse: 24440.5176 - val_loss: 1565636480.0000 - val_rmse: 39568.1250\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534616672.0000 - rmse: 23121.7793 - val_loss: 807460736.0000 - val_rmse: 28415.8535\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 708029824.0000 - rmse: 26608.8301 - val_loss: 1327230464.0000 - val_rmse: 36431.1758\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 679594432.0000 - rmse: 26069.0312 - val_loss: 922142592.0000 - val_rmse: 30366.8008\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485528064.0000 - rmse: 22034.6992 - val_loss: 927661056.0000 - val_rmse: 30457.5273\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621165120.0000 - rmse: 24923.1816 - val_loss: 1491491200.0000 - val_rmse: 38619.8281\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531278240.0000 - rmse: 23049.4727 - val_loss: 1080630016.0000 - val_rmse: 32872.9336\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554275712.0000 - rmse: 23543.0586 - val_loss: 1294012544.0000 - val_rmse: 35972.3867\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549084480.0000 - rmse: 23432.5488 - val_loss: 816396608.0000 - val_rmse: 28572.6543\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499756992.0000 - rmse: 22355.2441 - val_loss: 1097901056.0000 - val_rmse: 33134.5898\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517111328.0000 - rmse: 22740.0820 - val_loss: 1080681472.0000 - val_rmse: 32873.7188\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475937152.0000 - rmse: 21815.9844 - val_loss: 1828051712.0000 - val_rmse: 42755.7188\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598746112.0000 - rmse: 24469.2891 - val_loss: 1053330112.0000 - val_rmse: 32455.0469\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390272544.0000 - rmse: 19755.3164 - val_loss: 717322624.0000 - val_rmse: 26782.8789\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 602709888.0000 - rmse: 24550.1504 - val_loss: 939480960.0000 - val_rmse: 30650.9531\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494209248.0000 - rmse: 22230.8184 - val_loss: 1112983680.0000 - val_rmse: 33361.4102\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510552704.0000 - rmse: 22595.4121 - val_loss: 1176207616.0000 - val_rmse: 34295.8828\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544993728.0000 - rmse: 23345.0996 - val_loss: 972761216.0000 - val_rmse: 31189.1211\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402848320.0000 - rmse: 20071.0801 - val_loss: 1401951232.0000 - val_rmse: 37442.6406\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476980288.0000 - rmse: 21839.8789 - val_loss: 1081443968.0000 - val_rmse: 32885.3164\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515324992.0000 - rmse: 22700.7695 - val_loss: 898804416.0000 - val_rmse: 29980.0664\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 600564416.0000 - rmse: 24506.4160 - val_loss: 1281491072.0000 - val_rmse: 35797.9219\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527278400.0000 - rmse: 22962.5430 - val_loss: 841629824.0000 - val_rmse: 29010.8574\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433845440.0000 - rmse: 20828.9570 - val_loss: 1127418112.0000 - val_rmse: 33577.0469\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502513568.0000 - rmse: 22416.8125 - val_loss: 723714304.0000 - val_rmse: 26901.9395\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421012000.0000 - rmse: 20518.5762 - val_loss: 741991552.0000 - val_rmse: 27239.5215\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444905248.0000 - rmse: 21092.7754 - val_loss: 1016888576.0000 - val_rmse: 31888.6875\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502956352.0000 - rmse: 22426.6875 - val_loss: 1156834688.0000 - val_rmse: 34012.2734\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455028608.0000 - rmse: 21331.3965 - val_loss: 707741952.0000 - val_rmse: 26603.4199\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471622432.0000 - rmse: 21716.8691 - val_loss: 722536768.0000 - val_rmse: 26880.0410\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415247104.0000 - rmse: 20377.6113 - val_loss: 912978048.0000 - val_rmse: 30215.5273\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466564128.0000 - rmse: 21600.0957 - val_loss: 992640384.0000 - val_rmse: 31506.1953\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450807392.0000 - rmse: 21232.2246 - val_loss: 2490346752.0000 - val_rmse: 49903.3750\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462667008.0000 - rmse: 21509.6953 - val_loss: 1573094656.0000 - val_rmse: 39662.2578\n",
      "104/104 [==============================] - 0s 672us/step - loss: 455637952.0000 - rmse: 21345.6777\n",
      "[455637952.0, 21345.677734375]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 6070379008.0000 - rmse: 77912.6406 - val_loss: 1305666176.0000 - val_rmse: 36134.0039\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1819377280.0000 - rmse: 42654.1602 - val_loss: 1112445056.0000 - val_rmse: 33353.3359\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1600384896.0000 - rmse: 40004.8125 - val_loss: 1308038144.0000 - val_rmse: 36166.8086\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1527538176.0000 - rmse: 39083.7344 - val_loss: 1038348800.0000 - val_rmse: 32223.4199\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1434454784.0000 - rmse: 37874.1953 - val_loss: 1125256192.0000 - val_rmse: 33544.8398\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1382495616.0000 - rmse: 37181.9258 - val_loss: 1193390208.0000 - val_rmse: 34545.4805\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1387748224.0000 - rmse: 37252.4883 - val_loss: 1072191680.0000 - val_rmse: 32744.3379\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1263660928.0000 - rmse: 35548.0078 - val_loss: 1090768000.0000 - val_rmse: 33026.7773\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1265862784.0000 - rmse: 35578.9648 - val_loss: 870227840.0000 - val_rmse: 29499.6250\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1239001472.0000 - rmse: 35199.4531 - val_loss: 899025152.0000 - val_rmse: 29983.7480\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1161717632.0000 - rmse: 34083.9805 - val_loss: 851900352.0000 - val_rmse: 29187.3301\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1174364800.0000 - rmse: 34269.0078 - val_loss: 910736064.0000 - val_rmse: 30178.4043\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1171184896.0000 - rmse: 34222.5781 - val_loss: 803905536.0000 - val_rmse: 28353.2285\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1023518912.0000 - rmse: 31992.4824 - val_loss: 832547328.0000 - val_rmse: 28853.8965\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1125115776.0000 - rmse: 33542.7461 - val_loss: 973507136.0000 - val_rmse: 31201.0742\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1004437632.0000 - rmse: 31692.8633 - val_loss: 804713088.0000 - val_rmse: 28367.4648\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 995990464.0000 - rmse: 31559.3164 - val_loss: 864632256.0000 - val_rmse: 29404.6289\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 997199104.0000 - rmse: 31578.4590 - val_loss: 888838464.0000 - val_rmse: 29813.3945\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 914618048.0000 - rmse: 30242.6523 - val_loss: 825442432.0000 - val_rmse: 28730.5137\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1010102208.0000 - rmse: 31782.1055 - val_loss: 744083776.0000 - val_rmse: 27277.8984\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 929767808.0000 - rmse: 30492.0938 - val_loss: 887583232.0000 - val_rmse: 29792.3359\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 949913984.0000 - rmse: 30820.6738 - val_loss: 766149952.0000 - val_rmse: 27679.4121\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 851279936.0000 - rmse: 29176.7012 - val_loss: 799835008.0000 - val_rmse: 28281.3535\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 903319552.0000 - rmse: 30055.2754 - val_loss: 1149318784.0000 - val_rmse: 33901.6016\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 889273536.0000 - rmse: 29820.6895 - val_loss: 862572224.0000 - val_rmse: 29369.5781\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 962247872.0000 - rmse: 31020.1191 - val_loss: 923937024.0000 - val_rmse: 30396.3320\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 900714944.0000 - rmse: 30011.9141 - val_loss: 832233152.0000 - val_rmse: 28848.4512\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1041689216.0000 - rmse: 32275.2109 - val_loss: 817381952.0000 - val_rmse: 28589.8906\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 825330624.0000 - rmse: 28728.5684 - val_loss: 724412160.0000 - val_rmse: 26914.9062\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 777946496.0000 - rmse: 27891.6914 - val_loss: 738390784.0000 - val_rmse: 27173.3457\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 839843392.0000 - rmse: 28980.0508 - val_loss: 695196672.0000 - val_rmse: 26366.5820\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 752562368.0000 - rmse: 27432.8711 - val_loss: 866449920.0000 - val_rmse: 29435.5176\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 794968768.0000 - rmse: 28195.1875 - val_loss: 749411712.0000 - val_rmse: 27375.3848\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 711392832.0000 - rmse: 26671.9473 - val_loss: 717551168.0000 - val_rmse: 26787.1465\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 763775232.0000 - rmse: 27636.4805 - val_loss: 687348288.0000 - val_rmse: 26217.3281\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 837119680.0000 - rmse: 28933.0215 - val_loss: 618125824.0000 - val_rmse: 24862.1348\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721597312.0000 - rmse: 26862.5625 - val_loss: 747763520.0000 - val_rmse: 27345.2656\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 678895680.0000 - rmse: 26055.6250 - val_loss: 689787584.0000 - val_rmse: 26263.8066\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 712844032.0000 - rmse: 26699.1367 - val_loss: 615501312.0000 - val_rmse: 24809.2988\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 796151616.0000 - rmse: 28216.1582 - val_loss: 921442880.0000 - val_rmse: 30355.2773\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 748994880.0000 - rmse: 27367.7695 - val_loss: 1986164864.0000 - val_rmse: 44566.4102\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721825792.0000 - rmse: 26866.8164 - val_loss: 764723392.0000 - val_rmse: 27653.6328\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646579904.0000 - rmse: 25427.9336 - val_loss: 698157568.0000 - val_rmse: 26422.6719\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656848512.0000 - rmse: 25629.0547 - val_loss: 816128768.0000 - val_rmse: 28567.9668\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 650084288.0000 - rmse: 25496.7500 - val_loss: 659935040.0000 - val_rmse: 25689.1992\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 678180672.0000 - rmse: 26041.9023 - val_loss: 779744000.0000 - val_rmse: 27923.8945\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597411008.0000 - rmse: 24441.9922 - val_loss: 639668800.0000 - val_rmse: 25291.6738\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652277632.0000 - rmse: 25539.7246 - val_loss: 676154048.0000 - val_rmse: 26002.9609\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668089408.0000 - rmse: 25847.4238 - val_loss: 1017030720.0000 - val_rmse: 31890.9199\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649393152.0000 - rmse: 25483.1934 - val_loss: 1266389632.0000 - val_rmse: 35586.3672\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 657049920.0000 - rmse: 25632.9844 - val_loss: 690845632.0000 - val_rmse: 26283.9434\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594302528.0000 - rmse: 24378.3203 - val_loss: 753325184.0000 - val_rmse: 27446.7695\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580039488.0000 - rmse: 24084.0098 - val_loss: 773562496.0000 - val_rmse: 27812.9922\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562819136.0000 - rmse: 23723.8086 - val_loss: 749870272.0000 - val_rmse: 27383.7598\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584483840.0000 - rmse: 24176.1016 - val_loss: 814938752.0000 - val_rmse: 28547.1289\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 848069824.0000 - rmse: 29121.6367 - val_loss: 734066816.0000 - val_rmse: 27093.6680\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573234496.0000 - rmse: 23942.3145 - val_loss: 690690304.0000 - val_rmse: 26280.9844\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 650782144.0000 - rmse: 25510.4316 - val_loss: 675333376.0000 - val_rmse: 25987.1777\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594521024.0000 - rmse: 24382.8027 - val_loss: 800449920.0000 - val_rmse: 28292.2227\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 635568640.0000 - rmse: 25210.4863 - val_loss: 662712256.0000 - val_rmse: 25743.1992\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501465056.0000 - rmse: 22393.4141 - val_loss: 805821120.0000 - val_rmse: 28386.9883\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 602990080.0000 - rmse: 24555.8555 - val_loss: 939998528.0000 - val_rmse: 30659.3945\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565719168.0000 - rmse: 23784.8516 - val_loss: 791071360.0000 - val_rmse: 28125.9902\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509398048.0000 - rmse: 22569.8477 - val_loss: 706753344.0000 - val_rmse: 26584.8320\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567485888.0000 - rmse: 23821.9629 - val_loss: 763202496.0000 - val_rmse: 27626.1191\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590810816.0000 - rmse: 24306.5977 - val_loss: 737592832.0000 - val_rmse: 27158.6602\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 715556352.0000 - rmse: 26749.8828 - val_loss: 619403840.0000 - val_rmse: 24887.8242\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622420096.0000 - rmse: 24948.3477 - val_loss: 633924096.0000 - val_rmse: 25177.8496\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523747520.0000 - rmse: 22885.5293 - val_loss: 553488448.0000 - val_rmse: 23526.3340\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592746496.0000 - rmse: 24346.3848 - val_loss: 967661056.0000 - val_rmse: 31107.2500\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545115712.0000 - rmse: 23347.7109 - val_loss: 641555904.0000 - val_rmse: 25328.9531\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491877984.0000 - rmse: 22178.3223 - val_loss: 676000448.0000 - val_rmse: 26000.0078\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479434528.0000 - rmse: 21895.9922 - val_loss: 647167168.0000 - val_rmse: 25439.4785\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488009024.0000 - rmse: 22090.9258 - val_loss: 804882240.0000 - val_rmse: 28370.4473\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523556992.0000 - rmse: 22881.3672 - val_loss: 650022912.0000 - val_rmse: 25495.5449\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504595232.0000 - rmse: 22463.1953 - val_loss: 723048128.0000 - val_rmse: 26889.5547\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510806720.0000 - rmse: 22601.0312 - val_loss: 927291776.0000 - val_rmse: 30451.4648\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543693312.0000 - rmse: 23317.2324 - val_loss: 970539648.0000 - val_rmse: 31153.4824\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550221632.0000 - rmse: 23456.8027 - val_loss: 655531392.0000 - val_rmse: 25603.3477\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468576192.0000 - rmse: 21646.6191 - val_loss: 683475392.0000 - val_rmse: 26143.3613\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484939392.0000 - rmse: 22021.3379 - val_loss: 654263168.0000 - val_rmse: 25578.5684\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495044800.0000 - rmse: 22249.6016 - val_loss: 662404992.0000 - val_rmse: 25737.2285\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543432896.0000 - rmse: 23311.6465 - val_loss: 744130880.0000 - val_rmse: 27278.7617\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474487008.0000 - rmse: 21782.7188 - val_loss: 734066752.0000 - val_rmse: 27093.6660\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551261568.0000 - rmse: 23478.9609 - val_loss: 668039744.0000 - val_rmse: 25846.4648\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481109120.0000 - rmse: 21934.1992 - val_loss: 854517184.0000 - val_rmse: 29232.1230\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445571808.0000 - rmse: 21108.5703 - val_loss: 804062208.0000 - val_rmse: 28355.9902\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473380128.0000 - rmse: 21757.2988 - val_loss: 829569344.0000 - val_rmse: 28802.2461\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461534912.0000 - rmse: 21483.3633 - val_loss: 800681792.0000 - val_rmse: 28296.3184\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440210912.0000 - rmse: 20981.2031 - val_loss: 655583936.0000 - val_rmse: 25604.3730\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455247072.0000 - rmse: 21336.5195 - val_loss: 802820096.0000 - val_rmse: 28334.0781\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472413216.0000 - rmse: 21735.0664 - val_loss: 662067776.0000 - val_rmse: 25730.6758\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422388224.0000 - rmse: 20552.0859 - val_loss: 840264448.0000 - val_rmse: 28987.3145\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442827712.0000 - rmse: 21043.4707 - val_loss: 1249445632.0000 - val_rmse: 35347.5000\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464772800.0000 - rmse: 21558.5879 - val_loss: 759555392.0000 - val_rmse: 27560.0312\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464055424.0000 - rmse: 21541.9434 - val_loss: 803437760.0000 - val_rmse: 28344.9785\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452141728.0000 - rmse: 21263.6230 - val_loss: 704798464.0000 - val_rmse: 26548.0410\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450098272.0000 - rmse: 21215.5176 - val_loss: 919747456.0000 - val_rmse: 30327.3379\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477347360.0000 - rmse: 21848.2812 - val_loss: 769827392.0000 - val_rmse: 27745.7637\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483503616.0000 - rmse: 21988.7148 - val_loss: 756902016.0000 - val_rmse: 27511.8516\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384579552.0000 - rmse: 19610.6992 - val_loss: 919719744.0000 - val_rmse: 30326.8809\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423323872.0000 - rmse: 20574.8359 - val_loss: 864214784.0000 - val_rmse: 29397.5293\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507124064.0000 - rmse: 22519.4160 - val_loss: 825298048.0000 - val_rmse: 28728.0000\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435508352.0000 - rmse: 20868.8359 - val_loss: 814893056.0000 - val_rmse: 28546.3320\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412146880.0000 - rmse: 20301.4004 - val_loss: 1131058304.0000 - val_rmse: 33631.2070\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438949344.0000 - rmse: 20951.1172 - val_loss: 642310144.0000 - val_rmse: 25343.8379\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432085952.0000 - rmse: 20786.6758 - val_loss: 669128064.0000 - val_rmse: 25867.5098\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423795808.0000 - rmse: 20586.3008 - val_loss: 723556480.0000 - val_rmse: 26899.0039\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488889344.0000 - rmse: 22110.8418 - val_loss: 854822848.0000 - val_rmse: 29237.3535\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401774752.0000 - rmse: 20044.3184 - val_loss: 1908100480.0000 - val_rmse: 43681.8086\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430768032.0000 - rmse: 20754.9512 - val_loss: 1086677760.0000 - val_rmse: 32964.7969\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416809408.0000 - rmse: 20415.9102 - val_loss: 880467840.0000 - val_rmse: 29672.6758\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484989408.0000 - rmse: 22022.4746 - val_loss: 833733568.0000 - val_rmse: 28874.4434\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491734688.0000 - rmse: 22175.0898 - val_loss: 1131564672.0000 - val_rmse: 33638.7383\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386315744.0000 - rmse: 19654.9141 - val_loss: 1079116032.0000 - val_rmse: 32849.8984\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397443008.0000 - rmse: 19935.9727 - val_loss: 916978112.0000 - val_rmse: 30281.6465\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412158976.0000 - rmse: 20301.6973 - val_loss: 958917888.0000 - val_rmse: 30966.3984\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401036160.0000 - rmse: 20025.8867 - val_loss: 985211584.0000 - val_rmse: 31388.0801\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412539872.0000 - rmse: 20311.0762 - val_loss: 1002184704.0000 - val_rmse: 31657.3008\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421124992.0000 - rmse: 20521.3281 - val_loss: 793310656.0000 - val_rmse: 28165.7715\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425059136.0000 - rmse: 20616.9609 - val_loss: 932614144.0000 - val_rmse: 30538.7285\n",
      "104/104 [==============================] - 0s 694us/step - loss: 437640864.0000 - rmse: 20919.8652\n",
      "[437640864.0, 20919.865234375]\n",
      "[19903.671875, 39376.52734375, 33127.1640625, 21345.677734375, 20919.865234375]\n",
      "26934.58125\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "!python train.py kfold baseline\n",
    "## layer - 1 (16 8) d 0.2"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 19:48:46.196425: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 19:48:46.196461: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 19:48:46.196769: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 19:48:46.402858: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 5865974272.0000 - rmse: 76589.6484 - val_loss: 1190056576.0000 - val_rmse: 34497.1953\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1793152640.0000 - rmse: 42345.6328 - val_loss: 952263488.0000 - val_rmse: 30858.7656\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1625995904.0000 - rmse: 40323.6406 - val_loss: 836418368.0000 - val_rmse: 28920.8984\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1464463232.0000 - rmse: 38268.3047 - val_loss: 747037504.0000 - val_rmse: 27331.9863\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1482328832.0000 - rmse: 38501.0234 - val_loss: 711569280.0000 - val_rmse: 26675.2559\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1355773696.0000 - rmse: 36820.8320 - val_loss: 904777600.0000 - val_rmse: 30079.5215\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1314185344.0000 - rmse: 36251.6953 - val_loss: 789097408.0000 - val_rmse: 28090.8770\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1308099840.0000 - rmse: 36167.6641 - val_loss: 842031616.0000 - val_rmse: 29017.7812\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1233423360.0000 - rmse: 35120.1289 - val_loss: 705884800.0000 - val_rmse: 26568.4922\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1239465472.0000 - rmse: 35206.0430 - val_loss: 671986624.0000 - val_rmse: 25922.7031\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1254473216.0000 - rmse: 35418.5430 - val_loss: 679985856.0000 - val_rmse: 26076.5391\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1161518080.0000 - rmse: 34081.0508 - val_loss: 656737664.0000 - val_rmse: 25626.8926\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1147221888.0000 - rmse: 33870.6641 - val_loss: 706430464.0000 - val_rmse: 26578.7578\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1134958080.0000 - rmse: 33689.1406 - val_loss: 746855808.0000 - val_rmse: 27328.6602\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1063877248.0000 - rmse: 32617.1289 - val_loss: 600044288.0000 - val_rmse: 24495.8008\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1059609088.0000 - rmse: 32551.6367 - val_loss: 681986496.0000 - val_rmse: 26114.8711\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 941251648.0000 - rmse: 30679.8242 - val_loss: 737412992.0000 - val_rmse: 27155.3496\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1044924032.0000 - rmse: 32325.2852 - val_loss: 552543296.0000 - val_rmse: 23506.2402\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 961644032.0000 - rmse: 31010.3867 - val_loss: 608840448.0000 - val_rmse: 24674.6934\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 975716864.0000 - rmse: 31236.4668 - val_loss: 648515136.0000 - val_rmse: 25465.9590\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 863543424.0000 - rmse: 29386.1094 - val_loss: 757964672.0000 - val_rmse: 27531.1582\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 849983168.0000 - rmse: 29154.4707 - val_loss: 597385920.0000 - val_rmse: 24441.4766\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 892274176.0000 - rmse: 29870.9590 - val_loss: 767808256.0000 - val_rmse: 27709.3535\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 898594688.0000 - rmse: 29976.5684 - val_loss: 993355520.0000 - val_rmse: 31517.5430\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 912273984.0000 - rmse: 30203.8730 - val_loss: 824544832.0000 - val_rmse: 28714.8887\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 793690048.0000 - rmse: 28172.5059 - val_loss: 722912128.0000 - val_rmse: 26887.0254\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 819648320.0000 - rmse: 28629.5000 - val_loss: 638565824.0000 - val_rmse: 25269.8594\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 770940416.0000 - rmse: 27765.8145 - val_loss: 544286144.0000 - val_rmse: 23329.9414\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 780294080.0000 - rmse: 27933.7441 - val_loss: 879441216.0000 - val_rmse: 29655.3750\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 827453376.0000 - rmse: 28765.4902 - val_loss: 671041664.0000 - val_rmse: 25904.4707\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751413952.0000 - rmse: 27411.9316 - val_loss: 766481280.0000 - val_rmse: 27685.3984\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 769307456.0000 - rmse: 27736.3906 - val_loss: 688445824.0000 - val_rmse: 26238.2520\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 808848640.0000 - rmse: 28440.2637 - val_loss: 526390272.0000 - val_rmse: 22943.1953\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685565504.0000 - rmse: 26183.3066 - val_loss: 477636032.0000 - val_rmse: 21854.8848\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721997504.0000 - rmse: 26870.0117 - val_loss: 1644121088.0000 - val_rmse: 40547.7617\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 697641856.0000 - rmse: 26412.9102 - val_loss: 481705248.0000 - val_rmse: 21947.7852\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641146176.0000 - rmse: 25320.8652 - val_loss: 630967488.0000 - val_rmse: 25119.0645\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 679109888.0000 - rmse: 26059.7363 - val_loss: 789203840.0000 - val_rmse: 28092.7715\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685385344.0000 - rmse: 26179.8633 - val_loss: 723532544.0000 - val_rmse: 26898.5605\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 635697152.0000 - rmse: 25213.0352 - val_loss: 633158400.0000 - val_rmse: 25162.6387\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 695068672.0000 - rmse: 26364.1543 - val_loss: 708030336.0000 - val_rmse: 26608.8379\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616754048.0000 - rmse: 24834.5312 - val_loss: 1309871744.0000 - val_rmse: 36192.1484\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 670263360.0000 - rmse: 25889.4453 - val_loss: 1804699392.0000 - val_rmse: 42481.7539\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 638381120.0000 - rmse: 25266.2051 - val_loss: 987125184.0000 - val_rmse: 31418.5488\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 650207168.0000 - rmse: 25499.1582 - val_loss: 763334720.0000 - val_rmse: 27628.5137\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 667628992.0000 - rmse: 25838.5176 - val_loss: 957823296.0000 - val_rmse: 30948.7188\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556993664.0000 - rmse: 23600.7129 - val_loss: 970047360.0000 - val_rmse: 31145.5820\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 678214784.0000 - rmse: 26042.5566 - val_loss: 503835872.0000 - val_rmse: 22446.2891\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 625138816.0000 - rmse: 25002.7754 - val_loss: 905978432.0000 - val_rmse: 30099.4766\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609510848.0000 - rmse: 24688.2715 - val_loss: 641794560.0000 - val_rmse: 25333.6641\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612770368.0000 - rmse: 24754.1992 - val_loss: 720355328.0000 - val_rmse: 26839.4355\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572612992.0000 - rmse: 23929.3320 - val_loss: 660582784.0000 - val_rmse: 25701.8047\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578449600.0000 - rmse: 24050.9785 - val_loss: 495514176.0000 - val_rmse: 22260.1465\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577174912.0000 - rmse: 24024.4648 - val_loss: 738994816.0000 - val_rmse: 27184.4590\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618925824.0000 - rmse: 24878.2207 - val_loss: 1298955648.0000 - val_rmse: 36041.0273\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 682073792.0000 - rmse: 26116.5430 - val_loss: 1068441408.0000 - val_rmse: 32687.0215\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512406368.0000 - rmse: 22636.3945 - val_loss: 655336384.0000 - val_rmse: 25599.5391\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593305536.0000 - rmse: 24357.8633 - val_loss: 828891712.0000 - val_rmse: 28790.4785\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570742912.0000 - rmse: 23890.2266 - val_loss: 1699448448.0000 - val_rmse: 41224.3672\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596528192.0000 - rmse: 24423.9238 - val_loss: 549791488.0000 - val_rmse: 23447.6309\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537955520.0000 - rmse: 23193.8672 - val_loss: 815293632.0000 - val_rmse: 28553.3477\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525295136.0000 - rmse: 22919.3164 - val_loss: 585927616.0000 - val_rmse: 24205.9414\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579925056.0000 - rmse: 24081.6328 - val_loss: 1025882432.0000 - val_rmse: 32029.4004\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587310208.0000 - rmse: 24234.4805 - val_loss: 742527552.0000 - val_rmse: 27249.3594\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559049600.0000 - rmse: 23644.2305 - val_loss: 1396348928.0000 - val_rmse: 37367.7539\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610304576.0000 - rmse: 24704.3418 - val_loss: 449268352.0000 - val_rmse: 21195.9492\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516731744.0000 - rmse: 22731.7324 - val_loss: 569926528.0000 - val_rmse: 23873.1348\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519439872.0000 - rmse: 22791.2227 - val_loss: 1225078528.0000 - val_rmse: 35001.1211\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509561696.0000 - rmse: 22573.4727 - val_loss: 600201856.0000 - val_rmse: 24499.0176\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524124736.0000 - rmse: 22893.7695 - val_loss: 445250944.0000 - val_rmse: 21100.9688\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534842304.0000 - rmse: 23126.6562 - val_loss: 460979584.0000 - val_rmse: 21470.4336\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511637728.0000 - rmse: 22619.4082 - val_loss: 946696192.0000 - val_rmse: 30768.4277\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609687744.0000 - rmse: 24691.8555 - val_loss: 957341760.0000 - val_rmse: 30940.9395\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477709472.0000 - rmse: 21856.5664 - val_loss: 1511537536.0000 - val_rmse: 38878.4961\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551628736.0000 - rmse: 23486.7754 - val_loss: 459994656.0000 - val_rmse: 21447.4824\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538418752.0000 - rmse: 23203.8516 - val_loss: 863109376.0000 - val_rmse: 29378.7227\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517917472.0000 - rmse: 22757.8008 - val_loss: 801209792.0000 - val_rmse: 28305.6484\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455004096.0000 - rmse: 21330.8242 - val_loss: 500158176.0000 - val_rmse: 22364.2148\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501381824.0000 - rmse: 22391.5547 - val_loss: 544373312.0000 - val_rmse: 23331.8086\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526604320.0000 - rmse: 22947.8613 - val_loss: 1251945856.0000 - val_rmse: 35382.8477\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499370624.0000 - rmse: 22346.6016 - val_loss: 489642816.0000 - val_rmse: 22127.8711\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446468384.0000 - rmse: 21129.7949 - val_loss: 630831296.0000 - val_rmse: 25116.3555\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484435776.0000 - rmse: 22009.9023 - val_loss: 1048364736.0000 - val_rmse: 32378.4570\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502012736.0000 - rmse: 22405.6387 - val_loss: 701677056.0000 - val_rmse: 26489.1875\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473759648.0000 - rmse: 21766.0176 - val_loss: 884793472.0000 - val_rmse: 29745.4785\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475927808.0000 - rmse: 21815.7695 - val_loss: 773942336.0000 - val_rmse: 27819.8184\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522415104.0000 - rmse: 22856.4004 - val_loss: 1064765120.0000 - val_rmse: 32630.7383\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583882560.0000 - rmse: 24163.6602 - val_loss: 781824512.0000 - val_rmse: 27961.1230\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517914656.0000 - rmse: 22757.7363 - val_loss: 1393470336.0000 - val_rmse: 37329.2148\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501746048.0000 - rmse: 22399.6875 - val_loss: 634297088.0000 - val_rmse: 25185.2539\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435151744.0000 - rmse: 20860.2891 - val_loss: 486490528.0000 - val_rmse: 22056.5273\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449180928.0000 - rmse: 21193.8887 - val_loss: 920438208.0000 - val_rmse: 30338.7207\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482005536.0000 - rmse: 21954.6230 - val_loss: 507764992.0000 - val_rmse: 22533.6387\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431239968.0000 - rmse: 20766.3164 - val_loss: 791839744.0000 - val_rmse: 28139.6465\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461135904.0000 - rmse: 21474.0723 - val_loss: 561901120.0000 - val_rmse: 23704.4512\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483347360.0000 - rmse: 21985.1602 - val_loss: 648253248.0000 - val_rmse: 25460.8184\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477995232.0000 - rmse: 21863.1016 - val_loss: 1279324544.0000 - val_rmse: 35767.6484\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478092960.0000 - rmse: 21865.3340 - val_loss: 708141952.0000 - val_rmse: 26610.9355\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536047232.0000 - rmse: 23152.6934 - val_loss: 858157760.0000 - val_rmse: 29294.3262\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419301792.0000 - rmse: 20476.8594 - val_loss: 1312192000.0000 - val_rmse: 36224.1914\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553769920.0000 - rmse: 23532.3164 - val_loss: 914027392.0000 - val_rmse: 30232.8867\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394897536.0000 - rmse: 19872.0273 - val_loss: 907678016.0000 - val_rmse: 30127.6953\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398546208.0000 - rmse: 19963.6211 - val_loss: 1124818688.0000 - val_rmse: 33538.3164\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411464704.0000 - rmse: 20284.5898 - val_loss: 906665472.0000 - val_rmse: 30110.8867\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455464192.0000 - rmse: 21341.6055 - val_loss: 1267394816.0000 - val_rmse: 35600.4883\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406429984.0000 - rmse: 20160.1055 - val_loss: 931095808.0000 - val_rmse: 30513.8613\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442197120.0000 - rmse: 21028.4824 - val_loss: 4326781440.0000 - val_rmse: 65778.2734\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420319008.0000 - rmse: 20501.6816 - val_loss: 961107776.0000 - val_rmse: 31001.7363\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441564768.0000 - rmse: 21013.4414 - val_loss: 591985984.0000 - val_rmse: 24330.7617\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404683072.0000 - rmse: 20116.7344 - val_loss: 644135168.0000 - val_rmse: 25379.8164\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452927296.0000 - rmse: 21282.0879 - val_loss: 704828352.0000 - val_rmse: 26548.6035\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443982400.0000 - rmse: 21070.8867 - val_loss: 623272832.0000 - val_rmse: 24965.4316\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395508928.0000 - rmse: 19887.4023 - val_loss: 826458944.0000 - val_rmse: 28748.1973\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422281376.0000 - rmse: 20549.4844 - val_loss: 1529901824.0000 - val_rmse: 39113.9570\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473053216.0000 - rmse: 21749.7852 - val_loss: 470044224.0000 - val_rmse: 21680.5020\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392097632.0000 - rmse: 19801.4551 - val_loss: 1570667520.0000 - val_rmse: 39631.6484\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435927008.0000 - rmse: 20878.8633 - val_loss: 586102144.0000 - val_rmse: 24209.5469\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366932896.0000 - rmse: 19155.4922 - val_loss: 783739392.0000 - val_rmse: 27995.3457\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384617216.0000 - rmse: 19611.6602 - val_loss: 889494848.0000 - val_rmse: 29824.3965\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409731552.0000 - rmse: 20241.8262 - val_loss: 741583808.0000 - val_rmse: 27232.0332\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376674656.0000 - rmse: 19408.1055 - val_loss: 938809728.0000 - val_rmse: 30640.0020\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357803872.0000 - rmse: 18915.7031 - val_loss: 1073164608.0000 - val_rmse: 32759.1895\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365986048.0000 - rmse: 19130.7617 - val_loss: 806368768.0000 - val_rmse: 28396.6289\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410738784.0000 - rmse: 20266.6895 - val_loss: 619328640.0000 - val_rmse: 24886.3125\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385730848.0000 - rmse: 19640.0312 - val_loss: 775672448.0000 - val_rmse: 27850.8965\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351938336.0000 - rmse: 18760.0156 - val_loss: 738563520.0000 - val_rmse: 27176.5254\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351063104.0000 - rmse: 18736.6777 - val_loss: 1091011968.0000 - val_rmse: 33030.4688\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356101536.0000 - rmse: 18870.6504 - val_loss: 905300736.0000 - val_rmse: 30088.2168\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375631520.0000 - rmse: 19381.2148 - val_loss: 739510912.0000 - val_rmse: 27193.9492\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376175616.0000 - rmse: 19395.2441 - val_loss: 916963904.0000 - val_rmse: 30281.4121\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328667968.0000 - rmse: 18129.1992 - val_loss: 1436787584.0000 - val_rmse: 37904.9805\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371499296.0000 - rmse: 19274.3145 - val_loss: 515023648.0000 - val_rmse: 22694.1309\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390314080.0000 - rmse: 19756.3672 - val_loss: 557407872.0000 - val_rmse: 23609.4844\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352327680.0000 - rmse: 18770.3926 - val_loss: 1633516160.0000 - val_rmse: 40416.7812\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336439808.0000 - rmse: 18342.2930 - val_loss: 720802688.0000 - val_rmse: 26847.7676\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339899840.0000 - rmse: 18436.3711 - val_loss: 1005182848.0000 - val_rmse: 31704.6191\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457023168.0000 - rmse: 21378.0996 - val_loss: 743519616.0000 - val_rmse: 27267.5547\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322187776.0000 - rmse: 17949.5879 - val_loss: 747542784.0000 - val_rmse: 27341.2285\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379243328.0000 - rmse: 19474.1699 - val_loss: 2185827584.0000 - val_rmse: 46752.8359\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318275200.0000 - rmse: 17840.2676 - val_loss: 1520428032.0000 - val_rmse: 38992.6680\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390550784.0000 - rmse: 19762.3555 - val_loss: 941835200.0000 - val_rmse: 30689.3320\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315187264.0000 - rmse: 17753.5137 - val_loss: 719076480.0000 - val_rmse: 26815.5996\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374231104.0000 - rmse: 19345.0527 - val_loss: 1078319744.0000 - val_rmse: 32837.7773\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308124384.0000 - rmse: 17553.4688 - val_loss: 619647488.0000 - val_rmse: 24892.7188\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307605920.0000 - rmse: 17538.6953 - val_loss: 662337792.0000 - val_rmse: 25735.9238\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384873472.0000 - rmse: 19618.1914 - val_loss: 1092291584.0000 - val_rmse: 33049.8320\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328022272.0000 - rmse: 18111.3828 - val_loss: 709049280.0000 - val_rmse: 26627.9785\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387178304.0000 - rmse: 19676.8477 - val_loss: 1116299008.0000 - val_rmse: 33411.0625\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358334176.0000 - rmse: 18929.7148 - val_loss: 720161280.0000 - val_rmse: 26835.8203\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290821664.0000 - rmse: 17053.4941 - val_loss: 1089133184.0000 - val_rmse: 33002.0195\n",
      "104/104 [==============================] - 0s 688us/step - loss: 526060640.0000 - rmse: 22936.0098\n",
      "[526060640.0, 22936.009765625]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 7169542144.0000 - rmse: 84673.1484 - val_loss: 1299714048.0000 - val_rmse: 36051.5469\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1665231488.0000 - rmse: 40807.2461 - val_loss: 1107188992.0000 - val_rmse: 33274.4492\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1379329664.0000 - rmse: 37139.3281 - val_loss: 1065197632.0000 - val_rmse: 32637.3633\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1239655552.0000 - rmse: 35208.7422 - val_loss: 961304768.0000 - val_rmse: 31004.9141\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1222825984.0000 - rmse: 34968.9297 - val_loss: 891523392.0000 - val_rmse: 29858.3887\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1085279360.0000 - rmse: 32943.5781 - val_loss: 857056768.0000 - val_rmse: 29275.5312\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1098200576.0000 - rmse: 33139.1094 - val_loss: 1462522368.0000 - val_rmse: 38242.9375\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1071175488.0000 - rmse: 32728.8184 - val_loss: 848500480.0000 - val_rmse: 29129.0312\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1018599616.0000 - rmse: 31915.5078 - val_loss: 797408896.0000 - val_rmse: 28238.4297\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 966544512.0000 - rmse: 31089.2988 - val_loss: 796945216.0000 - val_rmse: 28230.2188\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 934584832.0000 - rmse: 30570.9805 - val_loss: 804190656.0000 - val_rmse: 28358.2559\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 912272768.0000 - rmse: 30203.8535 - val_loss: 738200512.0000 - val_rmse: 27169.8438\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 931802176.0000 - rmse: 30525.4355 - val_loss: 759255488.0000 - val_rmse: 27554.5918\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 881559168.0000 - rmse: 29691.0625 - val_loss: 735082944.0000 - val_rmse: 27112.4141\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 813457856.0000 - rmse: 28521.1816 - val_loss: 773866432.0000 - val_rmse: 27818.4551\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782753984.0000 - rmse: 27977.7402 - val_loss: 662886080.0000 - val_rmse: 25746.5742\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 770486208.0000 - rmse: 27757.6328 - val_loss: 645945536.0000 - val_rmse: 25415.4570\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 804239872.0000 - rmse: 28359.1230 - val_loss: 648455616.0000 - val_rmse: 25464.7910\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724826944.0000 - rmse: 26922.6094 - val_loss: 722171392.0000 - val_rmse: 26873.2441\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 688214208.0000 - rmse: 26233.8379 - val_loss: 607295680.0000 - val_rmse: 24643.3691\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677084608.0000 - rmse: 26020.8496 - val_loss: 599675392.0000 - val_rmse: 24488.2695\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 650300352.0000 - rmse: 25500.9863 - val_loss: 738707072.0000 - val_rmse: 27179.1660\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647976640.0000 - rmse: 25455.3848 - val_loss: 574102912.0000 - val_rmse: 23960.4434\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 643666752.0000 - rmse: 25370.5879 - val_loss: 586909312.0000 - val_rmse: 24226.2090\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604224896.0000 - rmse: 24580.9844 - val_loss: 568499200.0000 - val_rmse: 23843.2207\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579870208.0000 - rmse: 24080.4941 - val_loss: 619246208.0000 - val_rmse: 24884.6562\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 603751168.0000 - rmse: 24571.3477 - val_loss: 542030656.0000 - val_rmse: 23281.5527\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568797056.0000 - rmse: 23849.4648 - val_loss: 631274624.0000 - val_rmse: 25125.1797\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522384288.0000 - rmse: 22855.7266 - val_loss: 577984704.0000 - val_rmse: 24041.3105\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575191680.0000 - rmse: 23983.1523 - val_loss: 536496096.0000 - val_rmse: 23162.3848\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530911584.0000 - rmse: 23041.5195 - val_loss: 551235136.0000 - val_rmse: 23478.3965\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548453568.0000 - rmse: 23419.0859 - val_loss: 678103616.0000 - val_rmse: 26040.4219\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539208448.0000 - rmse: 23220.8613 - val_loss: 536377440.0000 - val_rmse: 23159.8223\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538382848.0000 - rmse: 23203.0762 - val_loss: 727374464.0000 - val_rmse: 26969.8809\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467872768.0000 - rmse: 21630.3672 - val_loss: 530116480.0000 - val_rmse: 23024.2578\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489381664.0000 - rmse: 22121.9727 - val_loss: 555839168.0000 - val_rmse: 23576.2402\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472141184.0000 - rmse: 21728.8086 - val_loss: 513335008.0000 - val_rmse: 22656.8984\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454341472.0000 - rmse: 21315.2871 - val_loss: 530863712.0000 - val_rmse: 23040.4785\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492288608.0000 - rmse: 22187.5762 - val_loss: 512124800.0000 - val_rmse: 22630.1738\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436932256.0000 - rmse: 20902.9238 - val_loss: 574064320.0000 - val_rmse: 23959.6387\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431674560.0000 - rmse: 20776.7793 - val_loss: 557155200.0000 - val_rmse: 23604.1348\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423450176.0000 - rmse: 20577.9043 - val_loss: 579749696.0000 - val_rmse: 24077.9922\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495560224.0000 - rmse: 22261.1816 - val_loss: 572102848.0000 - val_rmse: 23918.6719\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438009280.0000 - rmse: 20928.6719 - val_loss: 491395104.0000 - val_rmse: 22167.4336\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508102624.0000 - rmse: 22541.1309 - val_loss: 523474528.0000 - val_rmse: 22879.5645\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432020416.0000 - rmse: 20785.0996 - val_loss: 528134368.0000 - val_rmse: 22981.1719\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424447136.0000 - rmse: 20602.1133 - val_loss: 757900544.0000 - val_rmse: 27529.9941\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426140608.0000 - rmse: 20643.1719 - val_loss: 556076480.0000 - val_rmse: 23581.2734\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420650592.0000 - rmse: 20509.7676 - val_loss: 871427392.0000 - val_rmse: 29519.9492\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421278016.0000 - rmse: 20525.0566 - val_loss: 522165632.0000 - val_rmse: 22850.9434\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417792800.0000 - rmse: 20439.9785 - val_loss: 506773376.0000 - val_rmse: 22511.6270\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403981536.0000 - rmse: 20099.2910 - val_loss: 535316768.0000 - val_rmse: 23136.9121\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432013504.0000 - rmse: 20784.9336 - val_loss: 511587648.0000 - val_rmse: 22618.3027\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401086880.0000 - rmse: 20027.1523 - val_loss: 496773248.0000 - val_rmse: 22288.4082\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384502144.0000 - rmse: 19608.7246 - val_loss: 640687808.0000 - val_rmse: 25311.8105\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387025536.0000 - rmse: 19672.9629 - val_loss: 513620896.0000 - val_rmse: 22663.2051\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395509088.0000 - rmse: 19887.4102 - val_loss: 537762432.0000 - val_rmse: 23189.7051\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363103232.0000 - rmse: 19055.2656 - val_loss: 585464000.0000 - val_rmse: 24196.3613\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376994496.0000 - rmse: 19416.3438 - val_loss: 1171531520.0000 - val_rmse: 34227.6406\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350253216.0000 - rmse: 18715.0488 - val_loss: 793557760.0000 - val_rmse: 28170.1562\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381524352.0000 - rmse: 19532.6484 - val_loss: 548156480.0000 - val_rmse: 23412.7383\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416960576.0000 - rmse: 20419.6094 - val_loss: 532033792.0000 - val_rmse: 23065.8555\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399206048.0000 - rmse: 19980.1387 - val_loss: 605893952.0000 - val_rmse: 24614.9141\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345286496.0000 - rmse: 18581.8828 - val_loss: 791519872.0000 - val_rmse: 28133.9629\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354192544.0000 - rmse: 18820.0039 - val_loss: 549805568.0000 - val_rmse: 23447.9316\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360709984.0000 - rmse: 18992.3652 - val_loss: 473977120.0000 - val_rmse: 21771.0137\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384496512.0000 - rmse: 19608.5801 - val_loss: 527211872.0000 - val_rmse: 22961.0918\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331323136.0000 - rmse: 18202.2812 - val_loss: 504634848.0000 - val_rmse: 22464.0781\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398796960.0000 - rmse: 19969.9004 - val_loss: 540714688.0000 - val_rmse: 23253.2715\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333151744.0000 - rmse: 18252.4434 - val_loss: 521098880.0000 - val_rmse: 22827.5859\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380703680.0000 - rmse: 19511.6270 - val_loss: 585577152.0000 - val_rmse: 24198.7012\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351428896.0000 - rmse: 18746.4355 - val_loss: 599487744.0000 - val_rmse: 24484.4375\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355715936.0000 - rmse: 18860.4316 - val_loss: 579800192.0000 - val_rmse: 24079.0371\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331056032.0000 - rmse: 18194.9453 - val_loss: 525040576.0000 - val_rmse: 22913.7617\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346029184.0000 - rmse: 18601.8574 - val_loss: 515906048.0000 - val_rmse: 22713.5645\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350420160.0000 - rmse: 18719.5098 - val_loss: 531448192.0000 - val_rmse: 23053.1582\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353229952.0000 - rmse: 18794.4102 - val_loss: 590693696.0000 - val_rmse: 24304.1875\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348531872.0000 - rmse: 18669.0078 - val_loss: 479925696.0000 - val_rmse: 21907.2031\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355850208.0000 - rmse: 18863.9902 - val_loss: 470664608.0000 - val_rmse: 21694.8066\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374084416.0000 - rmse: 19341.2578 - val_loss: 547221376.0000 - val_rmse: 23392.7598\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344780192.0000 - rmse: 18568.2559 - val_loss: 526695392.0000 - val_rmse: 22949.8438\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335093472.0000 - rmse: 18305.5566 - val_loss: 558934784.0000 - val_rmse: 23641.7988\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317857760.0000 - rmse: 17828.5625 - val_loss: 526024384.0000 - val_rmse: 22935.2207\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337796544.0000 - rmse: 18379.2422 - val_loss: 782463680.0000 - val_rmse: 27972.5508\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336919072.0000 - rmse: 18355.3535 - val_loss: 495896928.0000 - val_rmse: 22268.7402\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294027584.0000 - rmse: 17147.2305 - val_loss: 520995168.0000 - val_rmse: 22825.3164\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364848448.0000 - rmse: 19101.0039 - val_loss: 474583680.0000 - val_rmse: 21784.9375\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355782208.0000 - rmse: 18862.1875 - val_loss: 541635776.0000 - val_rmse: 23273.0664\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323562112.0000 - rmse: 17987.8281 - val_loss: 588507328.0000 - val_rmse: 24259.1680\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348821184.0000 - rmse: 18676.7520 - val_loss: 600673792.0000 - val_rmse: 24508.6445\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322824224.0000 - rmse: 17967.3086 - val_loss: 538592000.0000 - val_rmse: 23207.5820\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333224000.0000 - rmse: 18254.4219 - val_loss: 590097600.0000 - val_rmse: 24291.9238\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314015488.0000 - rmse: 17720.4805 - val_loss: 488174464.0000 - val_rmse: 22094.6680\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314611648.0000 - rmse: 17737.2910 - val_loss: 583158016.0000 - val_rmse: 24148.6621\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315819968.0000 - rmse: 17771.3223 - val_loss: 504810208.0000 - val_rmse: 22467.9805\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320105792.0000 - rmse: 17891.4980 - val_loss: 461118272.0000 - val_rmse: 21473.6641\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314199680.0000 - rmse: 17725.6758 - val_loss: 522423968.0000 - val_rmse: 22856.5918\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300803552.0000 - rmse: 17343.6875 - val_loss: 462700256.0000 - val_rmse: 21510.4668\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352307840.0000 - rmse: 18769.8633 - val_loss: 479549248.0000 - val_rmse: 21898.6113\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328129824.0000 - rmse: 18114.3535 - val_loss: 521247808.0000 - val_rmse: 22830.8516\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295345376.0000 - rmse: 17185.6152 - val_loss: 538333760.0000 - val_rmse: 23202.0195\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282153056.0000 - rmse: 16797.4102 - val_loss: 606748800.0000 - val_rmse: 24632.2695\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309980480.0000 - rmse: 17606.2598 - val_loss: 472443072.0000 - val_rmse: 21735.7520\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286166112.0000 - rmse: 16916.4434 - val_loss: 473251136.0000 - val_rmse: 21754.3340\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311830880.0000 - rmse: 17658.7305 - val_loss: 536917120.0000 - val_rmse: 23171.4688\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296972064.0000 - rmse: 17232.8750 - val_loss: 442969056.0000 - val_rmse: 21046.8301\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331069472.0000 - rmse: 18195.3125 - val_loss: 482595264.0000 - val_rmse: 21968.0488\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308146432.0000 - rmse: 17554.0977 - val_loss: 467418560.0000 - val_rmse: 21619.8633\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310587424.0000 - rmse: 17623.4883 - val_loss: 487904384.0000 - val_rmse: 22088.5547\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323070592.0000 - rmse: 17974.1621 - val_loss: 431501056.0000 - val_rmse: 20772.6016\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347992384.0000 - rmse: 18654.5527 - val_loss: 494348544.0000 - val_rmse: 22233.9492\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264366672.0000 - rmse: 16259.3535 - val_loss: 449788160.0000 - val_rmse: 21208.2070\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288163072.0000 - rmse: 16975.3613 - val_loss: 525056448.0000 - val_rmse: 22914.1074\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305737856.0000 - rmse: 17485.3594 - val_loss: 488171232.0000 - val_rmse: 22094.5957\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295334912.0000 - rmse: 17185.3086 - val_loss: 499635808.0000 - val_rmse: 22352.5332\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265986496.0000 - rmse: 16309.0898 - val_loss: 489308576.0000 - val_rmse: 22120.3184\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276653280.0000 - rmse: 16632.8945 - val_loss: 484314016.0000 - val_rmse: 22007.1328\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304839296.0000 - rmse: 17459.6465 - val_loss: 474443968.0000 - val_rmse: 21781.7305\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281407008.0000 - rmse: 16775.1895 - val_loss: 566217920.0000 - val_rmse: 23795.3320\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293786048.0000 - rmse: 17140.1855 - val_loss: 479476544.0000 - val_rmse: 21896.9492\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311044672.0000 - rmse: 17636.4551 - val_loss: 557477248.0000 - val_rmse: 23610.9551\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259984896.0000 - rmse: 16124.0430 - val_loss: 447521120.0000 - val_rmse: 21154.6934\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283417120.0000 - rmse: 16834.9922 - val_loss: 448649248.0000 - val_rmse: 21181.3398\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266676480.0000 - rmse: 16330.2295 - val_loss: 533186528.0000 - val_rmse: 23090.8301\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277807808.0000 - rmse: 16667.5625 - val_loss: 458438656.0000 - val_rmse: 21411.1797\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270942432.0000 - rmse: 16460.3242 - val_loss: 497311168.0000 - val_rmse: 22300.4727\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264868336.0000 - rmse: 16274.7734 - val_loss: 471173280.0000 - val_rmse: 21706.5234\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253761312.0000 - rmse: 15929.8857 - val_loss: 444041664.0000 - val_rmse: 21072.2949\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255118864.0000 - rmse: 15972.4365 - val_loss: 477246592.0000 - val_rmse: 21845.9727\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303470496.0000 - rmse: 17420.4023 - val_loss: 492241696.0000 - val_rmse: 22186.5176\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285030944.0000 - rmse: 16882.8574 - val_loss: 564536576.0000 - val_rmse: 23759.9766\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267112384.0000 - rmse: 16343.5703 - val_loss: 433872416.0000 - val_rmse: 20829.6016\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254687248.0000 - rmse: 15958.9209 - val_loss: 454149920.0000 - val_rmse: 21310.7910\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273226912.0000 - rmse: 16529.5742 - val_loss: 466196640.0000 - val_rmse: 21591.5859\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270691200.0000 - rmse: 16452.6914 - val_loss: 637156352.0000 - val_rmse: 25241.9531\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265897536.0000 - rmse: 16306.3623 - val_loss: 477132576.0000 - val_rmse: 21843.3633\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308354752.0000 - rmse: 17560.0312 - val_loss: 446313504.0000 - val_rmse: 21126.1309\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273629376.0000 - rmse: 16541.7441 - val_loss: 427952896.0000 - val_rmse: 20687.0195\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312260736.0000 - rmse: 17670.8984 - val_loss: 459138688.0000 - val_rmse: 21427.5195\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238247536.0000 - rmse: 15435.2666 - val_loss: 477903712.0000 - val_rmse: 21861.0078\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255320496.0000 - rmse: 15978.7480 - val_loss: 475388672.0000 - val_rmse: 21803.4082\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253016944.0000 - rmse: 15906.5039 - val_loss: 548193536.0000 - val_rmse: 23413.5312\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238402912.0000 - rmse: 15440.2988 - val_loss: 437427744.0000 - val_rmse: 20914.7715\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237209200.0000 - rmse: 15401.5938 - val_loss: 480302560.0000 - val_rmse: 21915.8047\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284896928.0000 - rmse: 16878.8867 - val_loss: 493845472.0000 - val_rmse: 22222.6328\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272805568.0000 - rmse: 16516.8223 - val_loss: 569142912.0000 - val_rmse: 23856.7129\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250170816.0000 - rmse: 15816.7852 - val_loss: 510993216.0000 - val_rmse: 22605.1543\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252271776.0000 - rmse: 15883.0625 - val_loss: 432260192.0000 - val_rmse: 20790.8652\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275185408.0000 - rmse: 16588.7090 - val_loss: 588127744.0000 - val_rmse: 24251.3457\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249992176.0000 - rmse: 15811.1377 - val_loss: 519628096.0000 - val_rmse: 22795.3496\n",
      "104/104 [==============================] - 0s 669us/step - loss: 772238208.0000 - rmse: 27789.1719\n",
      "[772238208.0, 27789.171875]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 7409826816.0000 - rmse: 86080.3516 - val_loss: 1657304192.0000 - val_rmse: 40710.0000\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1582125824.0000 - rmse: 39775.9453 - val_loss: 1243607936.0000 - val_rmse: 35264.8242\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1408972928.0000 - rmse: 37536.2891 - val_loss: 1104232704.0000 - val_rmse: 33229.9961\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1249568384.0000 - rmse: 35349.2344 - val_loss: 1020518912.0000 - val_rmse: 31945.5625\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1212911488.0000 - rmse: 34826.8789 - val_loss: 1129955584.0000 - val_rmse: 33614.8086\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1123461120.0000 - rmse: 33518.0703 - val_loss: 954557824.0000 - val_rmse: 30895.9199\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1057242048.0000 - rmse: 32515.2578 - val_loss: 951767936.0000 - val_rmse: 30850.7363\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1087888768.0000 - rmse: 32983.1602 - val_loss: 898344192.0000 - val_rmse: 29972.3906\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1015280768.0000 - rmse: 31863.4707 - val_loss: 876362816.0000 - val_rmse: 29603.4258\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1010443840.0000 - rmse: 31787.4785 - val_loss: 1019696512.0000 - val_rmse: 31932.6875\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 979635968.0000 - rmse: 31299.1367 - val_loss: 904368960.0000 - val_rmse: 30072.7285\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1024772928.0000 - rmse: 32012.0742 - val_loss: 861461952.0000 - val_rmse: 29350.6719\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 958889664.0000 - rmse: 30965.9434 - val_loss: 888926464.0000 - val_rmse: 29814.8691\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 954685696.0000 - rmse: 30897.9863 - val_loss: 1263545088.0000 - val_rmse: 35546.3789\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 933702720.0000 - rmse: 30556.5488 - val_loss: 868565888.0000 - val_rmse: 29471.4414\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 928905792.0000 - rmse: 30477.9551 - val_loss: 842429248.0000 - val_rmse: 29024.6309\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 877977856.0000 - rmse: 29630.6914 - val_loss: 858219776.0000 - val_rmse: 29295.3887\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 832042688.0000 - rmse: 28845.1484 - val_loss: 977519872.0000 - val_rmse: 31265.3145\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 846263680.0000 - rmse: 29090.6113 - val_loss: 830202432.0000 - val_rmse: 28813.2344\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 819822400.0000 - rmse: 28632.5410 - val_loss: 926743232.0000 - val_rmse: 30442.4570\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 806465984.0000 - rmse: 28398.3457 - val_loss: 857187648.0000 - val_rmse: 29277.7656\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 746934464.0000 - rmse: 27330.1016 - val_loss: 1003511744.0000 - val_rmse: 31678.2539\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 813369664.0000 - rmse: 28519.6367 - val_loss: 1104492160.0000 - val_rmse: 33233.9023\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 797028032.0000 - rmse: 28231.6855 - val_loss: 1442409728.0000 - val_rmse: 37979.0703\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 771668608.0000 - rmse: 27778.9238 - val_loss: 924516992.0000 - val_rmse: 30405.8711\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 802022016.0000 - rmse: 28319.9941 - val_loss: 1002516096.0000 - val_rmse: 31662.5352\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 755121024.0000 - rmse: 27479.4648 - val_loss: 1079474432.0000 - val_rmse: 32855.3555\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 741196224.0000 - rmse: 27224.9199 - val_loss: 899386624.0000 - val_rmse: 29989.7754\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 707585856.0000 - rmse: 26600.4863 - val_loss: 785130624.0000 - val_rmse: 28020.1816\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730526464.0000 - rmse: 27028.2520 - val_loss: 815617088.0000 - val_rmse: 28559.0098\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 662099712.0000 - rmse: 25731.2969 - val_loss: 1169307904.0000 - val_rmse: 34195.1445\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706047808.0000 - rmse: 26571.5586 - val_loss: 842345664.0000 - val_rmse: 29023.1914\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 753985344.0000 - rmse: 27458.7930 - val_loss: 818950272.0000 - val_rmse: 28617.3066\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 711828032.0000 - rmse: 26680.1055 - val_loss: 803380544.0000 - val_rmse: 28343.9668\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 635936768.0000 - rmse: 25217.7852 - val_loss: 871906432.0000 - val_rmse: 29528.0625\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 695746048.0000 - rmse: 26376.9980 - val_loss: 934040000.0000 - val_rmse: 30562.0684\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685527360.0000 - rmse: 26182.5781 - val_loss: 1380293248.0000 - val_rmse: 37152.2969\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 745510720.0000 - rmse: 27304.0391 - val_loss: 1539051520.0000 - val_rmse: 39230.7461\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676475904.0000 - rmse: 26009.1504 - val_loss: 1145851136.0000 - val_rmse: 33850.4219\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 703002432.0000 - rmse: 26514.1934 - val_loss: 1211866624.0000 - val_rmse: 34811.8750\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642067136.0000 - rmse: 25339.0430 - val_loss: 1030461824.0000 - val_rmse: 32100.8047\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 608491904.0000 - rmse: 24667.6250 - val_loss: 700225408.0000 - val_rmse: 26461.7734\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580458752.0000 - rmse: 24092.7109 - val_loss: 1021400192.0000 - val_rmse: 31959.3516\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580801856.0000 - rmse: 24099.8301 - val_loss: 818106816.0000 - val_rmse: 28602.5664\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634396288.0000 - rmse: 25187.2227 - val_loss: 772735168.0000 - val_rmse: 27798.1152\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523701984.0000 - rmse: 22884.5332 - val_loss: 1246999936.0000 - val_rmse: 35312.8867\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606829120.0000 - rmse: 24633.8984 - val_loss: 872112576.0000 - val_rmse: 29531.5527\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540477760.0000 - rmse: 23248.1758 - val_loss: 938376896.0000 - val_rmse: 30632.9375\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585646272.0000 - rmse: 24200.1270 - val_loss: 718673408.0000 - val_rmse: 26808.0820\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550231040.0000 - rmse: 23457.0039 - val_loss: 584780608.0000 - val_rmse: 24182.2363\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610327616.0000 - rmse: 24704.8105 - val_loss: 1117460736.0000 - val_rmse: 33428.4414\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547731392.0000 - rmse: 23403.6602 - val_loss: 924955136.0000 - val_rmse: 30413.0742\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473590560.0000 - rmse: 21762.1367 - val_loss: 859333120.0000 - val_rmse: 29314.3848\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509700032.0000 - rmse: 22576.5371 - val_loss: 1383833728.0000 - val_rmse: 37199.9141\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471048864.0000 - rmse: 21703.6602 - val_loss: 844348544.0000 - val_rmse: 29057.6758\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521727712.0000 - rmse: 22841.3574 - val_loss: 1153562624.0000 - val_rmse: 33964.1367\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572645248.0000 - rmse: 23930.0059 - val_loss: 858356032.0000 - val_rmse: 29297.7109\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493587904.0000 - rmse: 22216.8379 - val_loss: 501714432.0000 - val_rmse: 22398.9824\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520020096.0000 - rmse: 22803.9492 - val_loss: 1128464512.0000 - val_rmse: 33592.6250\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552525056.0000 - rmse: 23505.8516 - val_loss: 662736640.0000 - val_rmse: 25743.6719\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568970496.0000 - rmse: 23853.1016 - val_loss: 550617408.0000 - val_rmse: 23465.2363\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493134688.0000 - rmse: 22206.6348 - val_loss: 778264832.0000 - val_rmse: 27897.3984\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522705792.0000 - rmse: 22862.7578 - val_loss: 999485632.0000 - val_rmse: 31614.6426\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490545408.0000 - rmse: 22148.2598 - val_loss: 571644672.0000 - val_rmse: 23909.0898\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511342304.0000 - rmse: 22612.8789 - val_loss: 610374336.0000 - val_rmse: 24705.7539\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478355104.0000 - rmse: 21871.3281 - val_loss: 526853056.0000 - val_rmse: 22953.2793\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573468032.0000 - rmse: 23947.1914 - val_loss: 529068512.0000 - val_rmse: 23001.4883\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476626720.0000 - rmse: 21831.7832 - val_loss: 679361024.0000 - val_rmse: 26064.5527\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503762688.0000 - rmse: 22444.6562 - val_loss: 695055040.0000 - val_rmse: 26363.8945\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447961824.0000 - rmse: 21165.1074 - val_loss: 844328896.0000 - val_rmse: 29057.3379\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493472896.0000 - rmse: 22214.2480 - val_loss: 803216384.0000 - val_rmse: 28341.0703\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455783424.0000 - rmse: 21349.0840 - val_loss: 679625152.0000 - val_rmse: 26069.6191\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450160128.0000 - rmse: 21216.9746 - val_loss: 1222259712.0000 - val_rmse: 34960.8281\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459100416.0000 - rmse: 21426.6270 - val_loss: 801615936.0000 - val_rmse: 28312.8223\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465558560.0000 - rmse: 21576.8047 - val_loss: 686613504.0000 - val_rmse: 26203.3086\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476849504.0000 - rmse: 21836.8809 - val_loss: 841317184.0000 - val_rmse: 29005.4688\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460604480.0000 - rmse: 21461.6953 - val_loss: 595198080.0000 - val_rmse: 24396.6816\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491181856.0000 - rmse: 22162.6211 - val_loss: 628533952.0000 - val_rmse: 25070.5762\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456141440.0000 - rmse: 21357.4688 - val_loss: 596074944.0000 - val_rmse: 24414.6465\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477967744.0000 - rmse: 21862.4727 - val_loss: 538540416.0000 - val_rmse: 23206.4707\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443690976.0000 - rmse: 21063.9707 - val_loss: 640128960.0000 - val_rmse: 25300.7695\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474495104.0000 - rmse: 21782.9062 - val_loss: 544642304.0000 - val_rmse: 23337.5723\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398519424.0000 - rmse: 19962.9492 - val_loss: 522407616.0000 - val_rmse: 22856.2383\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403135008.0000 - rmse: 20078.2227 - val_loss: 704037824.0000 - val_rmse: 26533.7090\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442476544.0000 - rmse: 21035.1250 - val_loss: 717152256.0000 - val_rmse: 26779.6973\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375709408.0000 - rmse: 19383.2207 - val_loss: 1202666368.0000 - val_rmse: 34679.4805\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420082560.0000 - rmse: 20495.9121 - val_loss: 542981120.0000 - val_rmse: 23301.9551\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467515104.0000 - rmse: 21622.0977 - val_loss: 516379744.0000 - val_rmse: 22723.9863\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458807808.0000 - rmse: 21419.7969 - val_loss: 671239040.0000 - val_rmse: 25908.2793\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423432544.0000 - rmse: 20577.4746 - val_loss: 480270912.0000 - val_rmse: 21915.0820\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359861504.0000 - rmse: 18970.0117 - val_loss: 669097536.0000 - val_rmse: 25866.9180\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374682720.0000 - rmse: 19356.7207 - val_loss: 485674944.0000 - val_rmse: 22038.0293\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422075584.0000 - rmse: 20544.4785 - val_loss: 593635264.0000 - val_rmse: 24364.6309\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425648416.0000 - rmse: 20631.2461 - val_loss: 397771392.0000 - val_rmse: 19944.2051\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398962688.0000 - rmse: 19974.0469 - val_loss: 489320896.0000 - val_rmse: 22120.5977\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409472864.0000 - rmse: 20235.4336 - val_loss: 417640224.0000 - val_rmse: 20436.2461\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444022240.0000 - rmse: 21071.8359 - val_loss: 916387584.0000 - val_rmse: 30271.8945\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418176832.0000 - rmse: 20449.3711 - val_loss: 458079200.0000 - val_rmse: 21402.7852\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393847520.0000 - rmse: 19845.5898 - val_loss: 429583136.0000 - val_rmse: 20726.3848\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428663904.0000 - rmse: 20704.1992 - val_loss: 738328192.0000 - val_rmse: 27172.1934\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381424000.0000 - rmse: 19530.0762 - val_loss: 850107136.0000 - val_rmse: 29156.5938\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366510560.0000 - rmse: 19144.4629 - val_loss: 514171104.0000 - val_rmse: 22675.3418\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393891744.0000 - rmse: 19846.7031 - val_loss: 544917568.0000 - val_rmse: 23343.4668\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378588704.0000 - rmse: 19457.3535 - val_loss: 526295296.0000 - val_rmse: 22941.1250\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386100192.0000 - rmse: 19649.4316 - val_loss: 507234624.0000 - val_rmse: 22521.8672\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470894432.0000 - rmse: 21700.1016 - val_loss: 537214912.0000 - val_rmse: 23177.8965\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354665440.0000 - rmse: 18832.5625 - val_loss: 566783296.0000 - val_rmse: 23807.2090\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437105216.0000 - rmse: 20907.0605 - val_loss: 750466752.0000 - val_rmse: 27394.6465\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392982784.0000 - rmse: 19823.7910 - val_loss: 525702624.0000 - val_rmse: 22928.2012\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395257888.0000 - rmse: 19881.0918 - val_loss: 556030336.0000 - val_rmse: 23580.2930\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397786720.0000 - rmse: 19944.5898 - val_loss: 607064512.0000 - val_rmse: 24638.6758\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373317792.0000 - rmse: 19321.4316 - val_loss: 477398816.0000 - val_rmse: 21849.4570\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409169248.0000 - rmse: 20227.9316 - val_loss: 454878976.0000 - val_rmse: 21327.8906\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380175296.0000 - rmse: 19498.0820 - val_loss: 919433984.0000 - val_rmse: 30322.1660\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361085664.0000 - rmse: 19002.2520 - val_loss: 728473728.0000 - val_rmse: 26990.2520\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363935584.0000 - rmse: 19077.0918 - val_loss: 498942336.0000 - val_rmse: 22337.0156\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410224160.0000 - rmse: 20253.9922 - val_loss: 577474368.0000 - val_rmse: 24030.6914\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340290912.0000 - rmse: 18446.9727 - val_loss: 422494912.0000 - val_rmse: 20554.6797\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326172064.0000 - rmse: 18060.2324 - val_loss: 415055808.0000 - val_rmse: 20372.9160\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359993824.0000 - rmse: 18973.5000 - val_loss: 443282880.0000 - val_rmse: 21054.2812\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349927584.0000 - rmse: 18706.3477 - val_loss: 517548768.0000 - val_rmse: 22749.6953\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384364992.0000 - rmse: 19605.2266 - val_loss: 429466112.0000 - val_rmse: 20723.5645\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341092064.0000 - rmse: 18468.6758 - val_loss: 450231488.0000 - val_rmse: 21218.6562\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398419552.0000 - rmse: 19960.4473 - val_loss: 668787136.0000 - val_rmse: 25860.9199\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297145696.0000 - rmse: 17237.9121 - val_loss: 567694144.0000 - val_rmse: 23826.3320\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340347872.0000 - rmse: 18448.5176 - val_loss: 463998688.0000 - val_rmse: 21540.6270\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317917280.0000 - rmse: 17830.2324 - val_loss: 764525504.0000 - val_rmse: 27650.0527\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314563200.0000 - rmse: 17735.9277 - val_loss: 624429248.0000 - val_rmse: 24988.5781\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402531360.0000 - rmse: 20063.1816 - val_loss: 540522688.0000 - val_rmse: 23249.1445\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315426688.0000 - rmse: 17760.2539 - val_loss: 449008640.0000 - val_rmse: 21189.8223\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323439232.0000 - rmse: 17984.4141 - val_loss: 374000992.0000 - val_rmse: 19339.1035\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367465440.0000 - rmse: 19169.3848 - val_loss: 612975168.0000 - val_rmse: 24758.3320\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404002720.0000 - rmse: 20099.8184 - val_loss: 682892992.0000 - val_rmse: 26132.2207\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340655712.0000 - rmse: 18456.8594 - val_loss: 600400512.0000 - val_rmse: 24503.0684\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359824192.0000 - rmse: 18969.0293 - val_loss: 553330880.0000 - val_rmse: 23522.9844\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344420480.0000 - rmse: 18558.5664 - val_loss: 648070848.0000 - val_rmse: 25457.2363\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341777696.0000 - rmse: 18487.2285 - val_loss: 577212288.0000 - val_rmse: 24025.2422\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342282720.0000 - rmse: 18500.8809 - val_loss: 386112896.0000 - val_rmse: 19649.7520\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335449600.0000 - rmse: 18315.2812 - val_loss: 425413984.0000 - val_rmse: 20625.5645\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305368928.0000 - rmse: 17474.8066 - val_loss: 421617888.0000 - val_rmse: 20533.3359\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278942176.0000 - rmse: 16701.5586 - val_loss: 510270144.0000 - val_rmse: 22589.1582\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344496928.0000 - rmse: 18560.6250 - val_loss: 755809216.0000 - val_rmse: 27491.9844\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297644672.0000 - rmse: 17252.3789 - val_loss: 418174592.0000 - val_rmse: 20449.3145\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296560992.0000 - rmse: 17220.9453 - val_loss: 581442880.0000 - val_rmse: 24113.1250\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260979808.0000 - rmse: 16154.8662 - val_loss: 380945280.0000 - val_rmse: 19517.8184\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296129728.0000 - rmse: 17208.4180 - val_loss: 452122112.0000 - val_rmse: 21263.1621\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339149824.0000 - rmse: 18416.0195 - val_loss: 571017280.0000 - val_rmse: 23895.9629\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288541312.0000 - rmse: 16986.5020 - val_loss: 384959904.0000 - val_rmse: 19620.3945\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297112096.0000 - rmse: 17236.9355 - val_loss: 461472832.0000 - val_rmse: 21481.9160\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283628448.0000 - rmse: 16841.2695 - val_loss: 325089728.0000 - val_rmse: 18030.2422\n",
      "104/104 [==============================] - 0s 695us/step - loss: 700671744.0000 - rmse: 26470.2031\n",
      "[700671744.0, 26470.203125]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 7114935296.0000 - rmse: 84350.0781 - val_loss: 1327835520.0000 - val_rmse: 36439.4766\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1768670208.0000 - rmse: 42055.5625 - val_loss: 1041968512.0000 - val_rmse: 32279.5371\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1557555200.0000 - rmse: 39465.8750 - val_loss: 933345408.0000 - val_rmse: 30550.7031\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1446833664.0000 - rmse: 38037.2656 - val_loss: 883608320.0000 - val_rmse: 29725.5508\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1349365120.0000 - rmse: 36733.7070 - val_loss: 964725568.0000 - val_rmse: 31060.0312\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1357175808.0000 - rmse: 36839.8672 - val_loss: 917940416.0000 - val_rmse: 30297.5312\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1286811392.0000 - rmse: 35872.1523 - val_loss: 887809344.0000 - val_rmse: 29796.1289\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1254376832.0000 - rmse: 35417.1836 - val_loss: 850646912.0000 - val_rmse: 29165.8516\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1200177280.0000 - rmse: 34643.5742 - val_loss: 831290240.0000 - val_rmse: 28832.1035\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1195816320.0000 - rmse: 34580.5742 - val_loss: 939526464.0000 - val_rmse: 30651.6953\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1095854336.0000 - rmse: 33103.6914 - val_loss: 865422720.0000 - val_rmse: 29418.0684\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1088359040.0000 - rmse: 32990.2852 - val_loss: 861923136.0000 - val_rmse: 29358.5273\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1080651776.0000 - rmse: 32873.2695 - val_loss: 820963840.0000 - val_rmse: 28652.4668\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1054118976.0000 - rmse: 32467.1992 - val_loss: 866397184.0000 - val_rmse: 29434.6250\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 967530752.0000 - rmse: 31105.1562 - val_loss: 860280256.0000 - val_rmse: 29330.5352\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 953845632.0000 - rmse: 30884.3906 - val_loss: 926089856.0000 - val_rmse: 30431.7246\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 914501120.0000 - rmse: 30240.7188 - val_loss: 848925632.0000 - val_rmse: 29136.3281\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 931965888.0000 - rmse: 30528.1172 - val_loss: 911549440.0000 - val_rmse: 30191.8770\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 860970496.0000 - rmse: 29342.2988 - val_loss: 774645184.0000 - val_rmse: 27832.4473\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 903124032.0000 - rmse: 30052.0215 - val_loss: 908691520.0000 - val_rmse: 30144.5098\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 830677952.0000 - rmse: 28821.4844 - val_loss: 937808960.0000 - val_rmse: 30623.6660\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 831117760.0000 - rmse: 28829.1133 - val_loss: 803313152.0000 - val_rmse: 28342.7793\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 803447232.0000 - rmse: 28345.1445 - val_loss: 1437706240.0000 - val_rmse: 37917.0977\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 761710272.0000 - rmse: 27599.0996 - val_loss: 1270685312.0000 - val_rmse: 35646.6719\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 795143744.0000 - rmse: 28198.2930 - val_loss: 986817472.0000 - val_rmse: 31413.6504\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 758999360.0000 - rmse: 27549.9434 - val_loss: 770184576.0000 - val_rmse: 27752.1992\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 758916544.0000 - rmse: 27548.4375 - val_loss: 997970368.0000 - val_rmse: 31590.6699\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687667456.0000 - rmse: 26223.4121 - val_loss: 822803200.0000 - val_rmse: 28684.5469\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 840069376.0000 - rmse: 28983.9512 - val_loss: 789646144.0000 - val_rmse: 28100.6426\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666083328.0000 - rmse: 25808.5898 - val_loss: 1050362112.0000 - val_rmse: 32409.2910\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 690130688.0000 - rmse: 26270.3379 - val_loss: 1361251712.0000 - val_rmse: 36895.1445\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 732180032.0000 - rmse: 27058.8262 - val_loss: 1139248896.0000 - val_rmse: 33752.7617\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618001984.0000 - rmse: 24859.6465 - val_loss: 1126026112.0000 - val_rmse: 33556.3125\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630698432.0000 - rmse: 25113.7070 - val_loss: 748667392.0000 - val_rmse: 27361.7852\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698154496.0000 - rmse: 26422.6133 - val_loss: 1386274560.0000 - val_rmse: 37232.7070\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661099776.0000 - rmse: 25711.8613 - val_loss: 993966144.0000 - val_rmse: 31527.2285\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618168384.0000 - rmse: 24862.9883 - val_loss: 1307001088.0000 - val_rmse: 36152.4688\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590423424.0000 - rmse: 24298.6309 - val_loss: 959044416.0000 - val_rmse: 30968.4414\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615804736.0000 - rmse: 24815.4121 - val_loss: 1122584960.0000 - val_rmse: 33505.0000\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 635590528.0000 - rmse: 25210.9199 - val_loss: 779889664.0000 - val_rmse: 27926.5020\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782304064.0000 - rmse: 27969.6973 - val_loss: 895959296.0000 - val_rmse: 29932.5801\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566474304.0000 - rmse: 23800.7188 - val_loss: 1139308288.0000 - val_rmse: 33753.6406\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582957504.0000 - rmse: 24144.5098 - val_loss: 1353568768.0000 - val_rmse: 36790.8789\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524047840.0000 - rmse: 22892.0898 - val_loss: 3147265024.0000 - val_rmse: 56100.4922\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562632320.0000 - rmse: 23719.8711 - val_loss: 1643358720.0000 - val_rmse: 40538.3594\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549799872.0000 - rmse: 23447.8105 - val_loss: 982614720.0000 - val_rmse: 31346.6855\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489057248.0000 - rmse: 22114.6367 - val_loss: 930893760.0000 - val_rmse: 30510.5508\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 602869888.0000 - rmse: 24553.4043 - val_loss: 1082502016.0000 - val_rmse: 32901.3984\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524513152.0000 - rmse: 22902.2520 - val_loss: 1110156032.0000 - val_rmse: 33319.0039\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593091392.0000 - rmse: 24353.4668 - val_loss: 1807109376.0000 - val_rmse: 42510.1055\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587188032.0000 - rmse: 24231.9609 - val_loss: 1107072640.0000 - val_rmse: 33272.6992\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448645376.0000 - rmse: 21181.2500 - val_loss: 1119320192.0000 - val_rmse: 33456.2422\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559903168.0000 - rmse: 23662.2734 - val_loss: 1365511296.0000 - val_rmse: 36952.8242\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525379104.0000 - rmse: 22921.1484 - val_loss: 1235381760.0000 - val_rmse: 35147.9961\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598224512.0000 - rmse: 24458.6270 - val_loss: 968626112.0000 - val_rmse: 31122.7578\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531007040.0000 - rmse: 23043.5898 - val_loss: 1722315520.0000 - val_rmse: 41500.7891\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495685120.0000 - rmse: 22263.9863 - val_loss: 1674978560.0000 - val_rmse: 40926.5000\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507220576.0000 - rmse: 22521.5586 - val_loss: 1297809024.0000 - val_rmse: 36025.1172\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431136320.0000 - rmse: 20763.8203 - val_loss: 1037549760.0000 - val_rmse: 32211.0176\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480365088.0000 - rmse: 21917.2324 - val_loss: 791692608.0000 - val_rmse: 28137.0332\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477487456.0000 - rmse: 21851.4844 - val_loss: 1036620352.0000 - val_rmse: 32196.5879\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499644992.0000 - rmse: 22352.7383 - val_loss: 1228912128.0000 - val_rmse: 35055.8438\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479172320.0000 - rmse: 21890.0059 - val_loss: 1199110144.0000 - val_rmse: 34628.1680\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456447328.0000 - rmse: 21364.6270 - val_loss: 1005266624.0000 - val_rmse: 31705.9395\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477112096.0000 - rmse: 21842.8945 - val_loss: 931455424.0000 - val_rmse: 30519.7520\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469071872.0000 - rmse: 21658.0664 - val_loss: 1028284864.0000 - val_rmse: 32066.8809\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518520896.0000 - rmse: 22771.0547 - val_loss: 942922880.0000 - val_rmse: 30707.0488\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374706656.0000 - rmse: 19357.3398 - val_loss: 1107395840.0000 - val_rmse: 33277.5586\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452414112.0000 - rmse: 21270.0293 - val_loss: 1056454656.0000 - val_rmse: 32503.1465\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441928704.0000 - rmse: 21022.0996 - val_loss: 996953280.0000 - val_rmse: 31574.5645\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404889216.0000 - rmse: 20121.8594 - val_loss: 779348352.0000 - val_rmse: 27916.8086\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424753696.0000 - rmse: 20609.5527 - val_loss: 1331526784.0000 - val_rmse: 36490.0898\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423491008.0000 - rmse: 20578.8945 - val_loss: 1529536640.0000 - val_rmse: 39109.2891\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394489664.0000 - rmse: 19861.7617 - val_loss: 2036085376.0000 - val_rmse: 45123.0039\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467603808.0000 - rmse: 21624.1465 - val_loss: 1002377408.0000 - val_rmse: 31660.3438\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408618336.0000 - rmse: 20214.3086 - val_loss: 859975296.0000 - val_rmse: 29325.3340\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421532832.0000 - rmse: 20531.2637 - val_loss: 882953024.0000 - val_rmse: 29714.5234\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389499072.0000 - rmse: 19735.7285 - val_loss: 840922112.0000 - val_rmse: 28998.6562\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451024000.0000 - rmse: 21237.3262 - val_loss: 762057280.0000 - val_rmse: 27605.3848\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363941600.0000 - rmse: 19077.2520 - val_loss: 1013205888.0000 - val_rmse: 31830.8945\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345159040.0000 - rmse: 18578.4531 - val_loss: 1292789760.0000 - val_rmse: 35955.3867\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436299136.0000 - rmse: 20887.7734 - val_loss: 1696739584.0000 - val_rmse: 41191.5000\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365729664.0000 - rmse: 19124.0586 - val_loss: 1099685632.0000 - val_rmse: 33161.5078\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313604160.0000 - rmse: 17708.8711 - val_loss: 885279168.0000 - val_rmse: 29753.6387\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356653568.0000 - rmse: 18885.2734 - val_loss: 1931644416.0000 - val_rmse: 43950.4766\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410354304.0000 - rmse: 20257.2012 - val_loss: 1451947136.0000 - val_rmse: 38104.4258\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378828512.0000 - rmse: 19463.5156 - val_loss: 772479744.0000 - val_rmse: 27793.5195\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359527136.0000 - rmse: 18961.1992 - val_loss: 764798528.0000 - val_rmse: 27654.9902\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344019552.0000 - rmse: 18547.7637 - val_loss: 679625536.0000 - val_rmse: 26069.6270\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354277056.0000 - rmse: 18822.2480 - val_loss: 1685515776.0000 - val_rmse: 41055.0352\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330492352.0000 - rmse: 18179.4473 - val_loss: 793467072.0000 - val_rmse: 28168.5469\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444530336.0000 - rmse: 21083.8867 - val_loss: 2120405120.0000 - val_rmse: 46047.8555\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343167008.0000 - rmse: 18524.7637 - val_loss: 1330317312.0000 - val_rmse: 36473.5156\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409629760.0000 - rmse: 20239.3105 - val_loss: 929519360.0000 - val_rmse: 30488.0195\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355565408.0000 - rmse: 18856.4395 - val_loss: 1025814720.0000 - val_rmse: 32028.3418\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314085888.0000 - rmse: 17722.4648 - val_loss: 961044096.0000 - val_rmse: 31000.7109\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385949984.0000 - rmse: 19645.6094 - val_loss: 996526912.0000 - val_rmse: 31567.8145\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356713248.0000 - rmse: 18886.8516 - val_loss: 1236984832.0000 - val_rmse: 35170.7930\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322321312.0000 - rmse: 17953.3066 - val_loss: 643171136.0000 - val_rmse: 25360.8184\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296566592.0000 - rmse: 17221.1074 - val_loss: 956255296.0000 - val_rmse: 30923.3770\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350047520.0000 - rmse: 18709.5547 - val_loss: 2620423680.0000 - val_rmse: 51190.0742\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322639936.0000 - rmse: 17962.1797 - val_loss: 673073408.0000 - val_rmse: 25943.6582\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308909600.0000 - rmse: 17575.8223 - val_loss: 1677426048.0000 - val_rmse: 40956.3906\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452160064.0000 - rmse: 21264.0527 - val_loss: 857291840.0000 - val_rmse: 29279.5469\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344819232.0000 - rmse: 18569.3086 - val_loss: 1240397312.0000 - val_rmse: 35219.2734\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281387488.0000 - rmse: 16774.6074 - val_loss: 1186928640.0000 - val_rmse: 34451.8320\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371434336.0000 - rmse: 19272.6289 - val_loss: 1488843136.0000 - val_rmse: 38585.5312\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289233088.0000 - rmse: 17006.8516 - val_loss: 829170944.0000 - val_rmse: 28795.3281\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320493664.0000 - rmse: 17902.3359 - val_loss: 985085312.0000 - val_rmse: 31386.0684\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338452864.0000 - rmse: 18397.0879 - val_loss: 764996928.0000 - val_rmse: 27658.5742\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330573984.0000 - rmse: 18181.6914 - val_loss: 842720704.0000 - val_rmse: 29029.6523\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375288064.0000 - rmse: 19372.3516 - val_loss: 1204397056.0000 - val_rmse: 34704.4258\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301434016.0000 - rmse: 17361.8535 - val_loss: 1011859520.0000 - val_rmse: 31809.7402\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384376800.0000 - rmse: 19605.5273 - val_loss: 582685952.0000 - val_rmse: 24138.8887\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298847744.0000 - rmse: 17287.2129 - val_loss: 1138945280.0000 - val_rmse: 33748.2617\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291621088.0000 - rmse: 17076.9160 - val_loss: 543065536.0000 - val_rmse: 23303.7656\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311557760.0000 - rmse: 17650.9980 - val_loss: 715121920.0000 - val_rmse: 26741.7617\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325217728.0000 - rmse: 18033.7930 - val_loss: 739966272.0000 - val_rmse: 27202.3203\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342422432.0000 - rmse: 18504.6562 - val_loss: 1364322432.0000 - val_rmse: 36936.7344\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316027136.0000 - rmse: 17777.1504 - val_loss: 966890560.0000 - val_rmse: 31094.8594\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291587904.0000 - rmse: 17075.9434 - val_loss: 522638080.0000 - val_rmse: 22861.2773\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296059808.0000 - rmse: 17206.3867 - val_loss: 881554624.0000 - val_rmse: 29690.9824\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281909024.0000 - rmse: 16790.1445 - val_loss: 815759168.0000 - val_rmse: 28561.4980\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307765536.0000 - rmse: 17543.2441 - val_loss: 600691648.0000 - val_rmse: 24509.0117\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328630976.0000 - rmse: 18128.1816 - val_loss: 1241185408.0000 - val_rmse: 35230.4609\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281635712.0000 - rmse: 16782.0020 - val_loss: 624263808.0000 - val_rmse: 24985.2715\n",
      "104/104 [==============================] - 0s 723us/step - loss: 392207456.0000 - rmse: 19804.2266\n",
      "[392207456.0, 19804.2265625]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 6524370432.0000 - rmse: 80773.5781 - val_loss: 1281977728.0000 - val_rmse: 35804.7148\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1796862080.0000 - rmse: 42389.4102 - val_loss: 1103970176.0000 - val_rmse: 33226.0469\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1621592960.0000 - rmse: 40269.0078 - val_loss: 1033299200.0000 - val_rmse: 32144.9707\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1461404416.0000 - rmse: 38228.3203 - val_loss: 930602688.0000 - val_rmse: 30505.7812\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1386223360.0000 - rmse: 37232.0195 - val_loss: 1079436800.0000 - val_rmse: 32854.7852\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1298356224.0000 - rmse: 36032.7109 - val_loss: 986185920.0000 - val_rmse: 31403.5957\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1272814464.0000 - rmse: 35676.5273 - val_loss: 1076813568.0000 - val_rmse: 32814.8359\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1205452672.0000 - rmse: 34719.6289 - val_loss: 943971584.0000 - val_rmse: 30724.1211\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1191668224.0000 - rmse: 34520.5469 - val_loss: 938687680.0000 - val_rmse: 30638.0098\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1164089088.0000 - rmse: 34118.7461 - val_loss: 896255424.0000 - val_rmse: 29937.5254\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1050508736.0000 - rmse: 32411.5527 - val_loss: 911693312.0000 - val_rmse: 30194.2598\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1055604352.0000 - rmse: 32490.0664 - val_loss: 877922624.0000 - val_rmse: 29629.7598\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1063237696.0000 - rmse: 32607.3262 - val_loss: 1491319680.0000 - val_rmse: 38617.6094\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1025824384.0000 - rmse: 32028.4941 - val_loss: 864402624.0000 - val_rmse: 29400.7246\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 883963520.0000 - rmse: 29731.5234 - val_loss: 796399360.0000 - val_rmse: 28220.5488\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1016716160.0000 - rmse: 31885.9863 - val_loss: 948042432.0000 - val_rmse: 30790.2969\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 906325824.0000 - rmse: 30105.2461 - val_loss: 1101696896.0000 - val_rmse: 33191.8203\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 853817088.0000 - rmse: 29220.1484 - val_loss: 714713600.0000 - val_rmse: 26734.1289\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 841098688.0000 - rmse: 29001.7012 - val_loss: 754487616.0000 - val_rmse: 27467.9375\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 794133824.0000 - rmse: 28180.3809 - val_loss: 731037632.0000 - val_rmse: 27037.7070\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 832638016.0000 - rmse: 28855.4668 - val_loss: 650477248.0000 - val_rmse: 25504.4551\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 843153600.0000 - rmse: 29037.1074 - val_loss: 734676608.0000 - val_rmse: 27104.9180\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 771446976.0000 - rmse: 27774.9336 - val_loss: 785513792.0000 - val_rmse: 28027.0195\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 846698816.0000 - rmse: 29098.0898 - val_loss: 658292096.0000 - val_rmse: 25657.2031\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 785465280.0000 - rmse: 28026.1543 - val_loss: 625495616.0000 - val_rmse: 25009.9102\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 710992128.0000 - rmse: 26664.4355 - val_loss: 776318528.0000 - val_rmse: 27862.4922\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 688634112.0000 - rmse: 26241.8398 - val_loss: 911627456.0000 - val_rmse: 30193.1660\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687532608.0000 - rmse: 26220.8438 - val_loss: 789745600.0000 - val_rmse: 28102.4121\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630243072.0000 - rmse: 25104.6426 - val_loss: 653864064.0000 - val_rmse: 25570.7656\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691748928.0000 - rmse: 26301.1191 - val_loss: 683564608.0000 - val_rmse: 26145.0664\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611235712.0000 - rmse: 24723.1797 - val_loss: 759630976.0000 - val_rmse: 27561.4043\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 635571456.0000 - rmse: 25210.5410 - val_loss: 822807232.0000 - val_rmse: 28684.6152\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606713600.0000 - rmse: 24631.5566 - val_loss: 698881408.0000 - val_rmse: 26436.3652\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573865728.0000 - rmse: 23955.4941 - val_loss: 3716372992.0000 - val_rmse: 60962.0625\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621823616.0000 - rmse: 24936.3906 - val_loss: 695966720.0000 - val_rmse: 26381.1816\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634278976.0000 - rmse: 25184.8965 - val_loss: 1662701696.0000 - val_rmse: 40776.2383\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588333760.0000 - rmse: 24255.5918 - val_loss: 703099264.0000 - val_rmse: 26516.0195\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532385344.0000 - rmse: 23073.4766 - val_loss: 716772992.0000 - val_rmse: 26772.6133\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642794240.0000 - rmse: 25353.3867 - val_loss: 677074432.0000 - val_rmse: 26020.6523\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579408384.0000 - rmse: 24070.9023 - val_loss: 1062703040.0000 - val_rmse: 32599.1270\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556304832.0000 - rmse: 23586.1152 - val_loss: 828975680.0000 - val_rmse: 28791.9375\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529808608.0000 - rmse: 23017.5703 - val_loss: 675305536.0000 - val_rmse: 25986.6406\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531056128.0000 - rmse: 23044.6543 - val_loss: 696500736.0000 - val_rmse: 26391.3008\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607466432.0000 - rmse: 24646.8340 - val_loss: 687957760.0000 - val_rmse: 26228.9492\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549464704.0000 - rmse: 23440.6602 - val_loss: 1206804736.0000 - val_rmse: 34739.0938\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481800288.0000 - rmse: 21949.9492 - val_loss: 850294592.0000 - val_rmse: 29159.8105\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590121792.0000 - rmse: 24292.4219 - val_loss: 647498624.0000 - val_rmse: 25445.9941\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543829120.0000 - rmse: 23320.1445 - val_loss: 780290624.0000 - val_rmse: 27933.6797\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548601408.0000 - rmse: 23422.2383 - val_loss: 648876864.0000 - val_rmse: 25473.0625\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514018272.0000 - rmse: 22671.9707 - val_loss: 721429760.0000 - val_rmse: 26859.4453\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552070848.0000 - rmse: 23496.1875 - val_loss: 989449664.0000 - val_rmse: 31455.5195\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457459072.0000 - rmse: 21388.2930 - val_loss: 939426368.0000 - val_rmse: 30650.0625\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517769184.0000 - rmse: 22754.5430 - val_loss: 875381824.0000 - val_rmse: 29586.8516\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476159328.0000 - rmse: 21821.0742 - val_loss: 1051481088.0000 - val_rmse: 32426.5488\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477700832.0000 - rmse: 21856.3672 - val_loss: 725631168.0000 - val_rmse: 26937.5410\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465221376.0000 - rmse: 21568.9902 - val_loss: 690401664.0000 - val_rmse: 26275.4961\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508314528.0000 - rmse: 22545.8301 - val_loss: 675955072.0000 - val_rmse: 25999.1348\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457179680.0000 - rmse: 21381.7598 - val_loss: 773280192.0000 - val_rmse: 27807.9141\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511619744.0000 - rmse: 22619.0137 - val_loss: 663371392.0000 - val_rmse: 25755.9980\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448334336.0000 - rmse: 21173.9062 - val_loss: 916602752.0000 - val_rmse: 30275.4473\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439973472.0000 - rmse: 20975.5449 - val_loss: 612987968.0000 - val_rmse: 24758.5938\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444805248.0000 - rmse: 21090.4062 - val_loss: 1000996992.0000 - val_rmse: 31638.5371\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424303968.0000 - rmse: 20598.6406 - val_loss: 652865856.0000 - val_rmse: 25551.2383\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484657216.0000 - rmse: 22014.9316 - val_loss: 738853632.0000 - val_rmse: 27181.8613\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439690816.0000 - rmse: 20968.8066 - val_loss: 637344960.0000 - val_rmse: 25245.6914\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466840320.0000 - rmse: 21606.4883 - val_loss: 1074918144.0000 - val_rmse: 32785.9414\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433158976.0000 - rmse: 20812.4688 - val_loss: 712513792.0000 - val_rmse: 26692.9531\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383821152.0000 - rmse: 19591.3516 - val_loss: 881935936.0000 - val_rmse: 29697.4062\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435759424.0000 - rmse: 20874.8496 - val_loss: 792966912.0000 - val_rmse: 28159.6680\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381375840.0000 - rmse: 19528.8438 - val_loss: 1420224768.0000 - val_rmse: 37685.8711\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414956544.0000 - rmse: 20370.4805 - val_loss: 701523712.0000 - val_rmse: 26486.2910\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402810976.0000 - rmse: 20070.1484 - val_loss: 1110615936.0000 - val_rmse: 33325.9023\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432930624.0000 - rmse: 20806.9844 - val_loss: 1580574464.0000 - val_rmse: 39756.4375\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400231616.0000 - rmse: 20005.7891 - val_loss: 1215503232.0000 - val_rmse: 34864.0664\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435305312.0000 - rmse: 20863.9688 - val_loss: 1112019456.0000 - val_rmse: 33346.9531\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410685632.0000 - rmse: 20265.3789 - val_loss: 1295196288.0000 - val_rmse: 35988.8320\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368874624.0000 - rmse: 19206.1074 - val_loss: 873204160.0000 - val_rmse: 29550.0254\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377873984.0000 - rmse: 19438.9805 - val_loss: 834279552.0000 - val_rmse: 28883.8984\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344947552.0000 - rmse: 18572.7637 - val_loss: 802197440.0000 - val_rmse: 28323.0898\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354095424.0000 - rmse: 18817.4219 - val_loss: 793176384.0000 - val_rmse: 28163.3867\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394854560.0000 - rmse: 19870.9473 - val_loss: 1174065408.0000 - val_rmse: 34264.6367\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386272800.0000 - rmse: 19653.8223 - val_loss: 755691328.0000 - val_rmse: 27489.8359\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371080288.0000 - rmse: 19263.4434 - val_loss: 670250048.0000 - val_rmse: 25889.1875\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406757184.0000 - rmse: 20168.2207 - val_loss: 799511552.0000 - val_rmse: 28275.6328\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319824608.0000 - rmse: 17883.6406 - val_loss: 1333199744.0000 - val_rmse: 36513.0078\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393693248.0000 - rmse: 19841.7031 - val_loss: 766356480.0000 - val_rmse: 27683.1445\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406035488.0000 - rmse: 20150.3223 - val_loss: 812839360.0000 - val_rmse: 28510.3379\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328783104.0000 - rmse: 18132.3750 - val_loss: 843425152.0000 - val_rmse: 29041.7832\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344057728.0000 - rmse: 18548.7930 - val_loss: 875354496.0000 - val_rmse: 29586.3906\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289552640.0000 - rmse: 17016.2461 - val_loss: 991157184.0000 - val_rmse: 31482.6484\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331059936.0000 - rmse: 18195.0488 - val_loss: 789502912.0000 - val_rmse: 28098.0938\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326594880.0000 - rmse: 18071.9355 - val_loss: 1019216256.0000 - val_rmse: 31925.1660\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334469216.0000 - rmse: 18288.4980 - val_loss: 910714048.0000 - val_rmse: 30178.0371\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309131168.0000 - rmse: 17582.1250 - val_loss: 814628800.0000 - val_rmse: 28541.7031\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377637632.0000 - rmse: 19432.9004 - val_loss: 1083467904.0000 - val_rmse: 32916.0703\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333064544.0000 - rmse: 18250.0547 - val_loss: 957292736.0000 - val_rmse: 30940.1465\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312081888.0000 - rmse: 17665.8398 - val_loss: 836352128.0000 - val_rmse: 28919.7539\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335917536.0000 - rmse: 18328.0508 - val_loss: 973993408.0000 - val_rmse: 31208.8672\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321230560.0000 - rmse: 17922.9043 - val_loss: 874161088.0000 - val_rmse: 29566.2148\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347016320.0000 - rmse: 18628.3750 - val_loss: 1047986368.0000 - val_rmse: 32372.6172\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331886784.0000 - rmse: 18217.7578 - val_loss: 897721280.0000 - val_rmse: 29961.9980\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321526528.0000 - rmse: 17931.1582 - val_loss: 824797440.0000 - val_rmse: 28719.2852\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311454144.0000 - rmse: 17648.0625 - val_loss: 992365120.0000 - val_rmse: 31501.8281\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282288896.0000 - rmse: 16801.4531 - val_loss: 1231695616.0000 - val_rmse: 35095.5195\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305888992.0000 - rmse: 17489.6797 - val_loss: 771570432.0000 - val_rmse: 27777.1562\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339236768.0000 - rmse: 18418.3809 - val_loss: 794963712.0000 - val_rmse: 28195.0996\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304983936.0000 - rmse: 17463.7891 - val_loss: 1216311680.0000 - val_rmse: 34875.6562\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281372896.0000 - rmse: 16774.1719 - val_loss: 951614592.0000 - val_rmse: 30848.2520\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292290016.0000 - rmse: 17096.4902 - val_loss: 973944320.0000 - val_rmse: 31208.0801\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359708224.0000 - rmse: 18965.9746 - val_loss: 862725760.0000 - val_rmse: 29372.1934\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294754688.0000 - rmse: 17168.4199 - val_loss: 756537152.0000 - val_rmse: 27505.2188\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312359360.0000 - rmse: 17673.6895 - val_loss: 897868480.0000 - val_rmse: 29964.4531\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341727840.0000 - rmse: 18485.8828 - val_loss: 1182372736.0000 - val_rmse: 34385.6484\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333279456.0000 - rmse: 18255.9414 - val_loss: 1215573120.0000 - val_rmse: 34865.0703\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292974912.0000 - rmse: 17116.5078 - val_loss: 936712192.0000 - val_rmse: 30605.7539\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287896128.0000 - rmse: 16967.5020 - val_loss: 677844864.0000 - val_rmse: 26035.4531\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310403232.0000 - rmse: 17618.2598 - val_loss: 839458368.0000 - val_rmse: 28973.4062\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312642848.0000 - rmse: 17681.7090 - val_loss: 788088000.0000 - val_rmse: 28072.9023\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314456800.0000 - rmse: 17732.9297 - val_loss: 1148377472.0000 - val_rmse: 33887.7148\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310778688.0000 - rmse: 17628.9141 - val_loss: 1168434688.0000 - val_rmse: 34182.3750\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289150368.0000 - rmse: 17004.4199 - val_loss: 637855104.0000 - val_rmse: 25255.7930\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305586368.0000 - rmse: 17481.0273 - val_loss: 855062464.0000 - val_rmse: 29241.4473\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279270272.0000 - rmse: 16711.3809 - val_loss: 902397760.0000 - val_rmse: 30039.9355\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284390944.0000 - rmse: 16863.8945 - val_loss: 1105231616.0000 - val_rmse: 33245.0234\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256812512.0000 - rmse: 16025.3691 - val_loss: 911161024.0000 - val_rmse: 30185.4414\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292640448.0000 - rmse: 17106.7344 - val_loss: 832100864.0000 - val_rmse: 28846.1562\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282075328.0000 - rmse: 16795.0957 - val_loss: 822309440.0000 - val_rmse: 28675.9375\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347956640.0000 - rmse: 18653.5938 - val_loss: 776679616.0000 - val_rmse: 27868.9727\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268703008.0000 - rmse: 16392.1602 - val_loss: 1146492416.0000 - val_rmse: 33859.8945\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291515200.0000 - rmse: 17073.8125 - val_loss: 1393996032.0000 - val_rmse: 37336.2578\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267933456.0000 - rmse: 16368.6709 - val_loss: 891321088.0000 - val_rmse: 29855.0020\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284212672.0000 - rmse: 16858.6074 - val_loss: 819542720.0000 - val_rmse: 28627.6562\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258635408.0000 - rmse: 16082.1445 - val_loss: 853190464.0000 - val_rmse: 29209.4238\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261547440.0000 - rmse: 16172.4258 - val_loss: 743763712.0000 - val_rmse: 27272.0312\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303462272.0000 - rmse: 17420.1680 - val_loss: 775525440.0000 - val_rmse: 27848.2559\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297168928.0000 - rmse: 17238.5879 - val_loss: 747776896.0000 - val_rmse: 27345.5078\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240286400.0000 - rmse: 15501.1729 - val_loss: 922954432.0000 - val_rmse: 30380.1621\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268759168.0000 - rmse: 16393.8730 - val_loss: 864935168.0000 - val_rmse: 29409.7793\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264910112.0000 - rmse: 16276.0576 - val_loss: 950865472.0000 - val_rmse: 30836.1055\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286545344.0000 - rmse: 16927.6484 - val_loss: 822168512.0000 - val_rmse: 28673.4805\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231432752.0000 - rmse: 15212.9111 - val_loss: 791522176.0000 - val_rmse: 28134.0039\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280585376.0000 - rmse: 16750.6797 - val_loss: 702328512.0000 - val_rmse: 26501.4785\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264812720.0000 - rmse: 16273.0654 - val_loss: 728408256.0000 - val_rmse: 26989.0391\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264895024.0000 - rmse: 16275.5947 - val_loss: 825427392.0000 - val_rmse: 28730.2500\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309386336.0000 - rmse: 17589.3809 - val_loss: 842651072.0000 - val_rmse: 29028.4512\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258694528.0000 - rmse: 16083.9805 - val_loss: 835506560.0000 - val_rmse: 28905.1289\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261171888.0000 - rmse: 16160.8105 - val_loss: 812246784.0000 - val_rmse: 28499.9434\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279395520.0000 - rmse: 16715.1270 - val_loss: 747006528.0000 - val_rmse: 27331.4199\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257494272.0000 - rmse: 16046.6270 - val_loss: 741693696.0000 - val_rmse: 27234.0527\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264701808.0000 - rmse: 16269.6572 - val_loss: 821629952.0000 - val_rmse: 28664.0879\n",
      "104/104 [==============================] - 0s 675us/step - loss: 405817984.0000 - rmse: 20144.9238\n",
      "[405817984.0, 20144.923828125]\n",
      "[22936.009765625, 27789.171875, 26470.203125, 19804.2265625, 20144.923828125]\n",
      "23428.90703125\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "!python train.py kfold light\n",
    "# (32 16 16)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 19:59:39.232514: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 19:59:39.232552: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 19:59:39.232955: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 19:59:39.439476: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 6272768000.0000 - rmse: 79200.8047 - val_loss: 1102178304.0000 - val_rmse: 33199.0703\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1878180224.0000 - rmse: 43337.9766 - val_loss: 1262465280.0000 - val_rmse: 35531.1875\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1811106688.0000 - rmse: 42557.0977 - val_loss: 945838784.0000 - val_rmse: 30754.4922\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1706534272.0000 - rmse: 41310.2188 - val_loss: 845790272.0000 - val_rmse: 29082.4727\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1556619264.0000 - rmse: 39454.0156 - val_loss: 867941504.0000 - val_rmse: 29460.8477\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1424132480.0000 - rmse: 37737.6797 - val_loss: 734453696.0000 - val_rmse: 27100.8066\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1491774080.0000 - rmse: 38623.4922 - val_loss: 657881344.0000 - val_rmse: 25649.1973\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1397924224.0000 - rmse: 37388.8242 - val_loss: 977564480.0000 - val_rmse: 31266.0273\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1222991744.0000 - rmse: 34971.3008 - val_loss: 561455360.0000 - val_rmse: 23695.0488\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1235988608.0000 - rmse: 35156.6289 - val_loss: 1200230528.0000 - val_rmse: 34644.3438\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1254587392.0000 - rmse: 35420.1562 - val_loss: 901105024.0000 - val_rmse: 30018.4121\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1148595072.0000 - rmse: 33890.9297 - val_loss: 580718144.0000 - val_rmse: 24098.0938\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1165498496.0000 - rmse: 34139.3984 - val_loss: 923524608.0000 - val_rmse: 30389.5469\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1182651904.0000 - rmse: 34389.7070 - val_loss: 490819072.0000 - val_rmse: 22154.4375\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1023704192.0000 - rmse: 31995.3770 - val_loss: 551708160.0000 - val_rmse: 23488.4688\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1060452608.0000 - rmse: 32564.5918 - val_loss: 489185344.0000 - val_rmse: 22117.5352\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1145379072.0000 - rmse: 33843.4492 - val_loss: 510909728.0000 - val_rmse: 22603.3125\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1032627776.0000 - rmse: 32134.5254 - val_loss: 603985024.0000 - val_rmse: 24576.1074\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 993993920.0000 - rmse: 31527.6699 - val_loss: 618528704.0000 - val_rmse: 24870.2383\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1010997184.0000 - rmse: 31796.1816 - val_loss: 505990752.0000 - val_rmse: 22494.2383\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 986148608.0000 - rmse: 31403.0039 - val_loss: 416627776.0000 - val_rmse: 20411.4609\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 977949056.0000 - rmse: 31272.1777 - val_loss: 413563712.0000 - val_rmse: 20336.2656\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 897925568.0000 - rmse: 29965.4062 - val_loss: 378813216.0000 - val_rmse: 19463.1250\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 902655232.0000 - rmse: 30044.2207 - val_loss: 440286240.0000 - val_rmse: 20982.9980\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 948021952.0000 - rmse: 30789.9648 - val_loss: 394001344.0000 - val_rmse: 19849.4668\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 912380352.0000 - rmse: 30205.6348 - val_loss: 661572928.0000 - val_rmse: 25721.0605\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 875756032.0000 - rmse: 29593.1758 - val_loss: 408186976.0000 - val_rmse: 20203.6367\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 823684544.0000 - rmse: 28699.9043 - val_loss: 807558016.0000 - val_rmse: 28417.5645\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 901410688.0000 - rmse: 30023.5020 - val_loss: 601704000.0000 - val_rmse: 24529.6562\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 840164800.0000 - rmse: 28985.5957 - val_loss: 397749088.0000 - val_rmse: 19943.6484\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 886337216.0000 - rmse: 29771.4160 - val_loss: 355814208.0000 - val_rmse: 18863.0391\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 910952000.0000 - rmse: 30181.9805 - val_loss: 378915488.0000 - val_rmse: 19465.7520\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 829699520.0000 - rmse: 28804.5059 - val_loss: 451101088.0000 - val_rmse: 21239.1406\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 968943616.0000 - rmse: 31127.8594 - val_loss: 767576832.0000 - val_rmse: 27705.1777\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 779376960.0000 - rmse: 27917.3242 - val_loss: 574958144.0000 - val_rmse: 23978.2852\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 826601792.0000 - rmse: 28750.6836 - val_loss: 385761504.0000 - val_rmse: 19640.8125\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 773321408.0000 - rmse: 27808.6562 - val_loss: 1115012608.0000 - val_rmse: 33391.8047\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 824645952.0000 - rmse: 28716.6484 - val_loss: 1236075648.0000 - val_rmse: 35157.8672\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 792225024.0000 - rmse: 28146.4922 - val_loss: 400669248.0000 - val_rmse: 20016.7246\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 778220992.0000 - rmse: 27896.6133 - val_loss: 384091872.0000 - val_rmse: 19598.2617\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 695586624.0000 - rmse: 26373.9766 - val_loss: 474053312.0000 - val_rmse: 21772.7656\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 785951360.0000 - rmse: 28034.8242 - val_loss: 537321920.0000 - val_rmse: 23180.2051\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 796640384.0000 - rmse: 28224.8184 - val_loss: 386003200.0000 - val_rmse: 19646.9648\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 762807168.0000 - rmse: 27618.9648 - val_loss: 646659072.0000 - val_rmse: 25429.4922\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 762768192.0000 - rmse: 27618.2578 - val_loss: 389312768.0000 - val_rmse: 19731.0098\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 717810816.0000 - rmse: 26791.9922 - val_loss: 408154176.0000 - val_rmse: 20202.8242\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 695617664.0000 - rmse: 26374.5645 - val_loss: 818794304.0000 - val_rmse: 28614.5820\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684474112.0000 - rmse: 26162.4570 - val_loss: 675241280.0000 - val_rmse: 25985.4043\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800103104.0000 - rmse: 28286.0938 - val_loss: 455703040.0000 - val_rmse: 21347.2031\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 767927488.0000 - rmse: 27711.5039 - val_loss: 373306848.0000 - val_rmse: 19321.1504\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 740695040.0000 - rmse: 27215.7129 - val_loss: 330474048.0000 - val_rmse: 18178.9453\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742205056.0000 - rmse: 27243.4414 - val_loss: 396253312.0000 - val_rmse: 19906.1133\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 786226560.0000 - rmse: 28039.7324 - val_loss: 321282336.0000 - val_rmse: 17924.3496\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687465152.0000 - rmse: 26219.5566 - val_loss: 438677632.0000 - val_rmse: 20944.6328\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641211328.0000 - rmse: 25322.1504 - val_loss: 355220256.0000 - val_rmse: 18847.2871\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 658020864.0000 - rmse: 25651.9180 - val_loss: 298908992.0000 - val_rmse: 17288.9844\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 667080256.0000 - rmse: 25827.8965 - val_loss: 363181216.0000 - val_rmse: 19057.3145\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 659617600.0000 - rmse: 25683.0215 - val_loss: 328250144.0000 - val_rmse: 18117.6758\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621479936.0000 - rmse: 24929.5000 - val_loss: 294391200.0000 - val_rmse: 17157.8320\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 643258688.0000 - rmse: 25362.5449 - val_loss: 485489600.0000 - val_rmse: 22033.8281\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676650176.0000 - rmse: 26012.5000 - val_loss: 349907424.0000 - val_rmse: 18705.8125\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677986240.0000 - rmse: 26038.1680 - val_loss: 350129088.0000 - val_rmse: 18711.7363\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651123136.0000 - rmse: 25517.1152 - val_loss: 314943040.0000 - val_rmse: 17746.6348\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623102272.0000 - rmse: 24962.0156 - val_loss: 299230720.0000 - val_rmse: 17298.2871\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 627580992.0000 - rmse: 25051.5664 - val_loss: 296399264.0000 - val_rmse: 17216.2500\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640212160.0000 - rmse: 25302.4141 - val_loss: 500719040.0000 - val_rmse: 22376.7520\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618172032.0000 - rmse: 24863.0664 - val_loss: 439491776.0000 - val_rmse: 20964.0586\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 672513280.0000 - rmse: 25932.8613 - val_loss: 287416000.0000 - val_rmse: 16953.3477\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 644405504.0000 - rmse: 25385.1426 - val_loss: 397195424.0000 - val_rmse: 19929.7617\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584501184.0000 - rmse: 24176.4590 - val_loss: 302515328.0000 - val_rmse: 17392.9688\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 617931200.0000 - rmse: 24858.2227 - val_loss: 511619776.0000 - val_rmse: 22619.0137\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629605120.0000 - rmse: 25091.9336 - val_loss: 338759776.0000 - val_rmse: 18405.4277\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580575296.0000 - rmse: 24095.1309 - val_loss: 392261440.0000 - val_rmse: 19805.5918\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 659577984.0000 - rmse: 25682.2500 - val_loss: 288217408.0000 - val_rmse: 16976.9668\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605045760.0000 - rmse: 24597.6777 - val_loss: 326721600.0000 - val_rmse: 18075.4414\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571996160.0000 - rmse: 23916.4414 - val_loss: 1166134784.0000 - val_rmse: 34148.7148\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532756064.0000 - rmse: 23081.5098 - val_loss: 315383072.0000 - val_rmse: 17759.0273\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575743488.0000 - rmse: 23994.6562 - val_loss: 453544032.0000 - val_rmse: 21296.5742\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614309696.0000 - rmse: 24785.2715 - val_loss: 437525984.0000 - val_rmse: 20917.1211\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525664320.0000 - rmse: 22927.3711 - val_loss: 269801856.0000 - val_rmse: 16425.6465\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551710080.0000 - rmse: 23488.5098 - val_loss: 332636448.0000 - val_rmse: 18238.3242\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573720832.0000 - rmse: 23952.4707 - val_loss: 575825152.0000 - val_rmse: 23996.3574\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 617499456.0000 - rmse: 24849.5371 - val_loss: 414556096.0000 - val_rmse: 20360.6504\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534630080.0000 - rmse: 23122.0684 - val_loss: 326789536.0000 - val_rmse: 18077.3203\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575928512.0000 - rmse: 23998.5098 - val_loss: 439850272.0000 - val_rmse: 20972.6074\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555250752.0000 - rmse: 23563.7598 - val_loss: 365660288.0000 - val_rmse: 19122.2461\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495707104.0000 - rmse: 22264.4805 - val_loss: 278345344.0000 - val_rmse: 16683.6855\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520132128.0000 - rmse: 22806.4062 - val_loss: 266178944.0000 - val_rmse: 16314.9912\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542989568.0000 - rmse: 23302.1367 - val_loss: 473236064.0000 - val_rmse: 21753.9902\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495006784.0000 - rmse: 22248.7480 - val_loss: 382127264.0000 - val_rmse: 19548.0762\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485977888.0000 - rmse: 22044.9062 - val_loss: 323296160.0000 - val_rmse: 17980.4375\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500859680.0000 - rmse: 22379.8945 - val_loss: 268659424.0000 - val_rmse: 16390.8340\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 595501312.0000 - rmse: 24402.8965 - val_loss: 407465632.0000 - val_rmse: 20185.7773\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491353856.0000 - rmse: 22166.5039 - val_loss: 279073504.0000 - val_rmse: 16705.4941\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465849728.0000 - rmse: 21583.5527 - val_loss: 245274544.0000 - val_rmse: 15661.2432\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491949312.0000 - rmse: 22179.9297 - val_loss: 242595616.0000 - val_rmse: 15575.4814\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514933536.0000 - rmse: 22692.1465 - val_loss: 240800224.0000 - val_rmse: 15517.7393\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469359104.0000 - rmse: 21664.6973 - val_loss: 838601536.0000 - val_rmse: 28958.6172\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505062976.0000 - rmse: 22473.6055 - val_loss: 250437248.0000 - val_rmse: 15825.2090\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494971840.0000 - rmse: 22247.9629 - val_loss: 231438784.0000 - val_rmse: 15213.1123\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473544064.0000 - rmse: 21761.0684 - val_loss: 346120064.0000 - val_rmse: 18604.3027\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424424032.0000 - rmse: 20601.5547 - val_loss: 359084832.0000 - val_rmse: 18949.5332\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486987840.0000 - rmse: 22067.8008 - val_loss: 250764656.0000 - val_rmse: 15835.5508\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416713952.0000 - rmse: 20413.5723 - val_loss: 304299712.0000 - val_rmse: 17444.1895\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426920800.0000 - rmse: 20662.0625 - val_loss: 283096896.0000 - val_rmse: 16825.4844\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566496000.0000 - rmse: 23801.1758 - val_loss: 251978688.0000 - val_rmse: 15873.8369\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487876896.0000 - rmse: 22087.9355 - val_loss: 318883296.0000 - val_rmse: 17857.3047\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474012640.0000 - rmse: 21771.8320 - val_loss: 291237824.0000 - val_rmse: 17065.6914\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449996512.0000 - rmse: 21213.1211 - val_loss: 274766240.0000 - val_rmse: 16576.0742\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427858848.0000 - rmse: 20684.7500 - val_loss: 384121472.0000 - val_rmse: 19599.0176\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389903264.0000 - rmse: 19745.9688 - val_loss: 303224352.0000 - val_rmse: 17413.3379\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417035744.0000 - rmse: 20421.4531 - val_loss: 212118704.0000 - val_rmse: 14564.2959\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407271904.0000 - rmse: 20180.9785 - val_loss: 509246080.0000 - val_rmse: 22566.4805\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366188896.0000 - rmse: 19136.0625 - val_loss: 211936704.0000 - val_rmse: 14558.0459\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495518592.0000 - rmse: 22260.2461 - val_loss: 222755760.0000 - val_rmse: 14925.0049\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459579680.0000 - rmse: 21437.8105 - val_loss: 241404832.0000 - val_rmse: 15537.2080\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438281152.0000 - rmse: 20935.1660 - val_loss: 258667184.0000 - val_rmse: 16083.1338\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374084704.0000 - rmse: 19341.2695 - val_loss: 327615136.0000 - val_rmse: 18100.1426\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384948192.0000 - rmse: 19620.0957 - val_loss: 324920608.0000 - val_rmse: 18025.5547\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401895008.0000 - rmse: 20047.3184 - val_loss: 309415232.0000 - val_rmse: 17590.2031\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406699808.0000 - rmse: 20166.7988 - val_loss: 275656864.0000 - val_rmse: 16602.9180\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436216704.0000 - rmse: 20885.8008 - val_loss: 388021632.0000 - val_rmse: 19698.2656\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391039104.0000 - rmse: 19774.7090 - val_loss: 307761760.0000 - val_rmse: 17543.1406\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391166688.0000 - rmse: 19777.9336 - val_loss: 241257248.0000 - val_rmse: 15532.4580\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431251264.0000 - rmse: 20766.5898 - val_loss: 302906912.0000 - val_rmse: 17404.2207\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348650592.0000 - rmse: 18672.1875 - val_loss: 333370208.0000 - val_rmse: 18258.4277\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369620736.0000 - rmse: 19225.5234 - val_loss: 316289152.0000 - val_rmse: 17784.5195\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349457280.0000 - rmse: 18693.7773 - val_loss: 343284192.0000 - val_rmse: 18527.9297\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406658272.0000 - rmse: 20165.7695 - val_loss: 242579936.0000 - val_rmse: 15574.9775\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364317440.0000 - rmse: 19087.1016 - val_loss: 351963232.0000 - val_rmse: 18760.6836\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374975840.0000 - rmse: 19364.2930 - val_loss: 214413280.0000 - val_rmse: 14642.8574\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394825728.0000 - rmse: 19870.2227 - val_loss: 267580672.0000 - val_rmse: 16357.8936\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463606752.0000 - rmse: 21531.5293 - val_loss: 268211744.0000 - val_rmse: 16377.1719\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385951136.0000 - rmse: 19645.6387 - val_loss: 394312416.0000 - val_rmse: 19857.3008\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372873248.0000 - rmse: 19309.9258 - val_loss: 301796064.0000 - val_rmse: 17372.2793\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418494784.0000 - rmse: 20457.1445 - val_loss: 395413600.0000 - val_rmse: 19885.0098\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348902656.0000 - rmse: 18678.9355 - val_loss: 257230432.0000 - val_rmse: 16038.4053\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338406976.0000 - rmse: 18395.8418 - val_loss: 343365280.0000 - val_rmse: 18530.1191\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332908480.0000 - rmse: 18245.7793 - val_loss: 222406672.0000 - val_rmse: 14913.3057\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334935424.0000 - rmse: 18301.2402 - val_loss: 415056160.0000 - val_rmse: 20372.9277\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359697216.0000 - rmse: 18965.6855 - val_loss: 506182304.0000 - val_rmse: 22498.4961\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363094304.0000 - rmse: 19055.0332 - val_loss: 302210400.0000 - val_rmse: 17384.1992\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377202784.0000 - rmse: 19421.7090 - val_loss: 257309344.0000 - val_rmse: 16040.8652\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370825696.0000 - rmse: 19256.8359 - val_loss: 263856976.0000 - val_rmse: 16243.6748\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340136608.0000 - rmse: 18442.7930 - val_loss: 285596832.0000 - val_rmse: 16899.6113\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336641024.0000 - rmse: 18347.7793 - val_loss: 729993408.0000 - val_rmse: 27018.3906\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369280320.0000 - rmse: 19216.6680 - val_loss: 271848672.0000 - val_rmse: 16487.8340\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332541120.0000 - rmse: 18235.7109 - val_loss: 213387632.0000 - val_rmse: 14607.7939\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314038208.0000 - rmse: 17721.1230 - val_loss: 234847744.0000 - val_rmse: 15324.7432\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354288064.0000 - rmse: 18822.5410 - val_loss: 273351680.0000 - val_rmse: 16533.3496\n",
      "104/104 [==============================] - 0s 751us/step - loss: 600134848.0000 - rmse: 24497.6504\n",
      "[600134848.0, 24497.650390625]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 6882881536.0000 - rmse: 82963.1328 - val_loss: 1274999040.0000 - val_rmse: 35707.1289\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1572354432.0000 - rmse: 39652.9258 - val_loss: 1135291776.0000 - val_rmse: 33694.0898\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1489167744.0000 - rmse: 38589.7344 - val_loss: 1060272768.0000 - val_rmse: 32561.8301\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1448729472.0000 - rmse: 38062.1797 - val_loss: 1053488384.0000 - val_rmse: 32457.4863\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1319925632.0000 - rmse: 36330.7812 - val_loss: 931269952.0000 - val_rmse: 30516.7168\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1267218560.0000 - rmse: 35598.0117 - val_loss: 917132032.0000 - val_rmse: 30284.1875\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1197062784.0000 - rmse: 34598.5938 - val_loss: 909378432.0000 - val_rmse: 30155.9023\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1082562816.0000 - rmse: 32902.3242 - val_loss: 1038360064.0000 - val_rmse: 32223.5957\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1151497856.0000 - rmse: 33933.7266 - val_loss: 768443776.0000 - val_rmse: 27720.8184\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1124356608.0000 - rmse: 33531.4258 - val_loss: 741080960.0000 - val_rmse: 27222.8027\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1013602368.0000 - rmse: 31837.1230 - val_loss: 857988544.0000 - val_rmse: 29291.4414\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1068353472.0000 - rmse: 32685.6777 - val_loss: 770047040.0000 - val_rmse: 27749.7207\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1029248448.0000 - rmse: 32081.9023 - val_loss: 669061248.0000 - val_rmse: 25866.2188\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1031090496.0000 - rmse: 32110.5977 - val_loss: 662318336.0000 - val_rmse: 25735.5469\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 996790144.0000 - rmse: 31571.9844 - val_loss: 682019712.0000 - val_rmse: 26115.5078\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 972118720.0000 - rmse: 31178.8184 - val_loss: 875037568.0000 - val_rmse: 29581.0332\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 907097664.0000 - rmse: 30118.0625 - val_loss: 620635520.0000 - val_rmse: 24912.5566\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 936129344.0000 - rmse: 30596.2305 - val_loss: 616749056.0000 - val_rmse: 24834.4336\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 907574336.0000 - rmse: 30125.9746 - val_loss: 719085312.0000 - val_rmse: 26815.7656\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 827371520.0000 - rmse: 28764.0664 - val_loss: 591265536.0000 - val_rmse: 24315.9531\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 900225536.0000 - rmse: 30003.7578 - val_loss: 977178624.0000 - val_rmse: 31259.8555\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 789197120.0000 - rmse: 28092.6523 - val_loss: 587898752.0000 - val_rmse: 24246.6230\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 840460352.0000 - rmse: 28990.6934 - val_loss: 600572736.0000 - val_rmse: 24506.5859\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 830295616.0000 - rmse: 28814.8516 - val_loss: 655577792.0000 - val_rmse: 25604.2539\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 756466816.0000 - rmse: 27503.9414 - val_loss: 593902208.0000 - val_rmse: 24370.1094\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 795252928.0000 - rmse: 28200.2285 - val_loss: 545032448.0000 - val_rmse: 23345.9297\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 781853824.0000 - rmse: 27961.6484 - val_loss: 637197632.0000 - val_rmse: 25242.7734\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 714993536.0000 - rmse: 26739.3633 - val_loss: 567819200.0000 - val_rmse: 23828.9570\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 793237440.0000 - rmse: 28164.4707 - val_loss: 684644608.0000 - val_rmse: 26165.7148\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698324992.0000 - rmse: 26425.8398 - val_loss: 553162048.0000 - val_rmse: 23519.3965\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654517056.0000 - rmse: 25583.5312 - val_loss: 541804928.0000 - val_rmse: 23276.7031\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 725848640.0000 - rmse: 26941.5781 - val_loss: 624647040.0000 - val_rmse: 24992.9395\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652345024.0000 - rmse: 25541.0449 - val_loss: 876224000.0000 - val_rmse: 29601.0820\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648121792.0000 - rmse: 25458.2363 - val_loss: 598122240.0000 - val_rmse: 24456.5371\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646182272.0000 - rmse: 25420.1152 - val_loss: 953555584.0000 - val_rmse: 30879.6953\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 663799744.0000 - rmse: 25764.3105 - val_loss: 516250784.0000 - val_rmse: 22721.1523\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656183360.0000 - rmse: 25616.0762 - val_loss: 490020512.0000 - val_rmse: 22136.4062\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668599104.0000 - rmse: 25857.2832 - val_loss: 494993440.0000 - val_rmse: 22248.4473\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666280384.0000 - rmse: 25812.4082 - val_loss: 489971360.0000 - val_rmse: 22135.2969\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652451968.0000 - rmse: 25543.1387 - val_loss: 483825984.0000 - val_rmse: 21996.0449\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 670638976.0000 - rmse: 25896.6973 - val_loss: 448025728.0000 - val_rmse: 21166.6191\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 617887360.0000 - rmse: 24857.3398 - val_loss: 686205632.0000 - val_rmse: 26195.5273\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 602788352.0000 - rmse: 24551.7480 - val_loss: 452086528.0000 - val_rmse: 21262.3262\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589266368.0000 - rmse: 24274.8086 - val_loss: 444958336.0000 - val_rmse: 21094.0352\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 631270976.0000 - rmse: 25125.1074 - val_loss: 537293824.0000 - val_rmse: 23179.5996\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 602573568.0000 - rmse: 24547.3730 - val_loss: 419130048.0000 - val_rmse: 20472.6641\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637255808.0000 - rmse: 25243.9258 - val_loss: 482685312.0000 - val_rmse: 21970.0996\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610352640.0000 - rmse: 24705.3164 - val_loss: 421998656.0000 - val_rmse: 20542.6055\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620827392.0000 - rmse: 24916.4082 - val_loss: 439353728.0000 - val_rmse: 20960.7656\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 662151040.0000 - rmse: 25732.2949 - val_loss: 493225056.0000 - val_rmse: 22208.6699\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610672128.0000 - rmse: 24711.7812 - val_loss: 490898016.0000 - val_rmse: 22156.2188\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547392064.0000 - rmse: 23396.4121 - val_loss: 500220864.0000 - val_rmse: 22365.6172\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553551360.0000 - rmse: 23527.6719 - val_loss: 426647104.0000 - val_rmse: 20655.4375\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558607232.0000 - rmse: 23634.8730 - val_loss: 404980800.0000 - val_rmse: 20124.1348\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508982304.0000 - rmse: 22560.6367 - val_loss: 407014688.0000 - val_rmse: 20174.6055\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521980352.0000 - rmse: 22846.8887 - val_loss: 427797184.0000 - val_rmse: 20683.2578\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551943232.0000 - rmse: 23493.4727 - val_loss: 512730400.0000 - val_rmse: 22643.5508\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551582080.0000 - rmse: 23485.7852 - val_loss: 720034304.0000 - val_rmse: 26833.4551\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534936192.0000 - rmse: 23128.6875 - val_loss: 386644736.0000 - val_rmse: 19663.2832\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565676608.0000 - rmse: 23783.9570 - val_loss: 417498176.0000 - val_rmse: 20432.7715\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509491840.0000 - rmse: 22571.9258 - val_loss: 515512608.0000 - val_rmse: 22704.9023\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496008544.0000 - rmse: 22271.2500 - val_loss: 376170816.0000 - val_rmse: 19395.1230\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517004160.0000 - rmse: 22737.7246 - val_loss: 503843040.0000 - val_rmse: 22446.4492\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491790784.0000 - rmse: 22176.3555 - val_loss: 389471776.0000 - val_rmse: 19735.0391\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472863168.0000 - rmse: 21745.4180 - val_loss: 390807296.0000 - val_rmse: 19768.8457\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493222112.0000 - rmse: 22208.6035 - val_loss: 377411680.0000 - val_rmse: 19427.0859\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456094912.0000 - rmse: 21356.3789 - val_loss: 522391040.0000 - val_rmse: 22855.8750\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506305312.0000 - rmse: 22501.2285 - val_loss: 415106848.0000 - val_rmse: 20374.1719\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466873216.0000 - rmse: 21607.2500 - val_loss: 408932864.0000 - val_rmse: 20222.0879\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482325696.0000 - rmse: 21961.9141 - val_loss: 524855104.0000 - val_rmse: 22909.7168\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474793504.0000 - rmse: 21789.7578 - val_loss: 421644160.0000 - val_rmse: 20533.9766\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447808736.0000 - rmse: 21161.4922 - val_loss: 393392512.0000 - val_rmse: 19834.1250\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524551936.0000 - rmse: 22903.0996 - val_loss: 443417760.0000 - val_rmse: 21057.4863\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488118656.0000 - rmse: 22093.4082 - val_loss: 494161024.0000 - val_rmse: 22229.7324\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419611552.0000 - rmse: 20484.4219 - val_loss: 369020192.0000 - val_rmse: 19209.8984\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438524832.0000 - rmse: 20940.9844 - val_loss: 477174368.0000 - val_rmse: 21844.3203\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459700416.0000 - rmse: 21440.6250 - val_loss: 500524224.0000 - val_rmse: 22372.3984\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423165024.0000 - rmse: 20570.9746 - val_loss: 440105856.0000 - val_rmse: 20978.6992\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394260832.0000 - rmse: 19856.0020 - val_loss: 397812160.0000 - val_rmse: 19945.2285\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414526656.0000 - rmse: 20359.9277 - val_loss: 431809760.0000 - val_rmse: 20780.0332\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423513056.0000 - rmse: 20579.4336 - val_loss: 392296928.0000 - val_rmse: 19806.4863\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411205472.0000 - rmse: 20278.2012 - val_loss: 345938560.0000 - val_rmse: 18599.4238\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399436768.0000 - rmse: 19985.9141 - val_loss: 404281504.0000 - val_rmse: 20106.7520\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377834240.0000 - rmse: 19437.9590 - val_loss: 419792480.0000 - val_rmse: 20488.8379\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421488128.0000 - rmse: 20530.1758 - val_loss: 364960320.0000 - val_rmse: 19103.9355\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398774112.0000 - rmse: 19969.3301 - val_loss: 452948896.0000 - val_rmse: 21282.5957\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377644672.0000 - rmse: 19433.0820 - val_loss: 421300608.0000 - val_rmse: 20525.6094\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365400064.0000 - rmse: 19115.4414 - val_loss: 402560896.0000 - val_rmse: 20063.9199\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387447360.0000 - rmse: 19683.6816 - val_loss: 381467072.0000 - val_rmse: 19531.1816\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376370720.0000 - rmse: 19400.2754 - val_loss: 494268896.0000 - val_rmse: 22232.1582\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380857536.0000 - rmse: 19515.5723 - val_loss: 457584608.0000 - val_rmse: 21391.2266\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377622464.0000 - rmse: 19432.5098 - val_loss: 450262240.0000 - val_rmse: 21219.3828\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398246848.0000 - rmse: 19956.1230 - val_loss: 474910432.0000 - val_rmse: 21792.4395\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401688672.0000 - rmse: 20042.1719 - val_loss: 385239328.0000 - val_rmse: 19627.5137\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388679904.0000 - rmse: 19714.9668 - val_loss: 372492768.0000 - val_rmse: 19300.0723\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347244832.0000 - rmse: 18634.5059 - val_loss: 391439328.0000 - val_rmse: 19784.8262\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340956256.0000 - rmse: 18465.0000 - val_loss: 383494592.0000 - val_rmse: 19583.0176\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335012864.0000 - rmse: 18303.3574 - val_loss: 442804768.0000 - val_rmse: 21042.9277\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358098784.0000 - rmse: 18923.4980 - val_loss: 369170752.0000 - val_rmse: 19213.8164\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346840608.0000 - rmse: 18623.6562 - val_loss: 633393600.0000 - val_rmse: 25167.3125\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335146688.0000 - rmse: 18307.0117 - val_loss: 404206016.0000 - val_rmse: 20104.8750\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355395936.0000 - rmse: 18851.9473 - val_loss: 473576800.0000 - val_rmse: 21761.8203\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337381856.0000 - rmse: 18367.9570 - val_loss: 388717056.0000 - val_rmse: 19715.9082\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375194944.0000 - rmse: 19369.9492 - val_loss: 377086976.0000 - val_rmse: 19418.7266\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371120256.0000 - rmse: 19264.4824 - val_loss: 377484704.0000 - val_rmse: 19428.9648\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356971616.0000 - rmse: 18893.6934 - val_loss: 419082112.0000 - val_rmse: 20471.4941\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353154752.0000 - rmse: 18792.4121 - val_loss: 429468032.0000 - val_rmse: 20723.6113\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361177920.0000 - rmse: 19004.6816 - val_loss: 435190720.0000 - val_rmse: 20861.2246\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346348672.0000 - rmse: 18610.4453 - val_loss: 439265632.0000 - val_rmse: 20958.6641\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325780480.0000 - rmse: 18049.3906 - val_loss: 385022016.0000 - val_rmse: 19621.9785\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324007328.0000 - rmse: 18000.2031 - val_loss: 399131424.0000 - val_rmse: 19978.2734\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350772736.0000 - rmse: 18728.9277 - val_loss: 361298496.0000 - val_rmse: 19007.8535\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324661024.0000 - rmse: 18018.3516 - val_loss: 382524736.0000 - val_rmse: 19558.2402\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341640384.0000 - rmse: 18483.5156 - val_loss: 449164928.0000 - val_rmse: 21193.5117\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310609600.0000 - rmse: 17624.1191 - val_loss: 369156576.0000 - val_rmse: 19213.4473\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300557440.0000 - rmse: 17336.5918 - val_loss: 410052992.0000 - val_rmse: 20249.7656\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287366432.0000 - rmse: 16951.8867 - val_loss: 423250592.0000 - val_rmse: 20573.0547\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301756448.0000 - rmse: 17371.1387 - val_loss: 350075200.0000 - val_rmse: 18710.2969\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286877568.0000 - rmse: 16937.4609 - val_loss: 390740480.0000 - val_rmse: 19767.1562\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331068224.0000 - rmse: 18195.2793 - val_loss: 419201536.0000 - val_rmse: 20474.4121\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281822208.0000 - rmse: 16787.5605 - val_loss: 492882112.0000 - val_rmse: 22200.9492\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328851008.0000 - rmse: 18134.2500 - val_loss: 392961920.0000 - val_rmse: 19823.2676\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330055008.0000 - rmse: 18167.4160 - val_loss: 387857632.0000 - val_rmse: 19694.1016\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318542880.0000 - rmse: 17847.7695 - val_loss: 346255872.0000 - val_rmse: 18607.9512\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320085760.0000 - rmse: 17890.9414 - val_loss: 389511936.0000 - val_rmse: 19736.0566\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278508544.0000 - rmse: 16688.5762 - val_loss: 401665504.0000 - val_rmse: 20041.5938\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310634112.0000 - rmse: 17624.8145 - val_loss: 372658848.0000 - val_rmse: 19304.3730\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271220896.0000 - rmse: 16468.7852 - val_loss: 416079040.0000 - val_rmse: 20398.0156\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318869792.0000 - rmse: 17856.9258 - val_loss: 531235456.0000 - val_rmse: 23048.5449\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284228160.0000 - rmse: 16859.0684 - val_loss: 407926336.0000 - val_rmse: 20197.1855\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318095808.0000 - rmse: 17835.2402 - val_loss: 423212096.0000 - val_rmse: 20572.1191\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365924224.0000 - rmse: 19129.1465 - val_loss: 459289504.0000 - val_rmse: 21431.0410\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328165920.0000 - rmse: 18115.3496 - val_loss: 436197248.0000 - val_rmse: 20885.3359\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290878784.0000 - rmse: 17055.1680 - val_loss: 414477568.0000 - val_rmse: 20358.7227\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340076960.0000 - rmse: 18441.1758 - val_loss: 465261664.0000 - val_rmse: 21569.9258\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325984704.0000 - rmse: 18055.0469 - val_loss: 398466560.0000 - val_rmse: 19961.6270\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296168288.0000 - rmse: 17209.5410 - val_loss: 348589568.0000 - val_rmse: 18670.5527\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283744224.0000 - rmse: 16844.7090 - val_loss: 426494528.0000 - val_rmse: 20651.7441\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284323328.0000 - rmse: 16861.8906 - val_loss: 543379712.0000 - val_rmse: 23310.5059\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303564192.0000 - rmse: 17423.0938 - val_loss: 395120032.0000 - val_rmse: 19877.6270\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313747584.0000 - rmse: 17712.9219 - val_loss: 352045408.0000 - val_rmse: 18762.8730\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264701904.0000 - rmse: 16269.6621 - val_loss: 377163776.0000 - val_rmse: 19420.7051\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275718432.0000 - rmse: 16604.7715 - val_loss: 488048480.0000 - val_rmse: 22091.8184\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300016736.0000 - rmse: 17320.9902 - val_loss: 393032672.0000 - val_rmse: 19825.0508\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284130144.0000 - rmse: 16856.1602 - val_loss: 382791296.0000 - val_rmse: 19565.0527\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308770976.0000 - rmse: 17571.8809 - val_loss: 484948288.0000 - val_rmse: 22021.5410\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314623648.0000 - rmse: 17737.6328 - val_loss: 333752768.0000 - val_rmse: 18268.9023\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271222112.0000 - rmse: 16468.8223 - val_loss: 402558880.0000 - val_rmse: 20063.8691\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277615872.0000 - rmse: 16661.8086 - val_loss: 402777664.0000 - val_rmse: 20069.3223\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294229440.0000 - rmse: 17153.1172 - val_loss: 439415936.0000 - val_rmse: 20962.2500\n",
      "104/104 [==============================] - 0s 726us/step - loss: 902144192.0000 - rmse: 30035.7148\n",
      "[902144192.0, 30035.71484375]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 6124067840.0000 - rmse: 78256.4219 - val_loss: 1344245248.0000 - val_rmse: 36663.9492\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1660571776.0000 - rmse: 40750.1133 - val_loss: 1438880640.0000 - val_rmse: 37932.5820\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1490639616.0000 - rmse: 38608.8008 - val_loss: 1597213568.0000 - val_rmse: 39965.1562\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1416773120.0000 - rmse: 37640.0469 - val_loss: 1254216064.0000 - val_rmse: 35414.9141\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1416999680.0000 - rmse: 37643.0547 - val_loss: 1112040832.0000 - val_rmse: 33347.2773\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1283874560.0000 - rmse: 35831.1953 - val_loss: 1022104704.0000 - val_rmse: 31970.3730\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1209712768.0000 - rmse: 34780.9258 - val_loss: 1041675712.0000 - val_rmse: 32275.0020\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1121944064.0000 - rmse: 33495.4336 - val_loss: 1038938688.0000 - val_rmse: 32232.5723\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1145914624.0000 - rmse: 33851.3594 - val_loss: 862440448.0000 - val_rmse: 29367.3359\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1182303360.0000 - rmse: 34384.6367 - val_loss: 959338624.0000 - val_rmse: 30973.1914\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1057567488.0000 - rmse: 32520.2637 - val_loss: 1657475584.0000 - val_rmse: 40712.1055\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1021049408.0000 - rmse: 31953.8633 - val_loss: 832494400.0000 - val_rmse: 28852.9785\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 966155712.0000 - rmse: 31083.0449 - val_loss: 1044370816.0000 - val_rmse: 32316.7266\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1056950272.0000 - rmse: 32510.7715 - val_loss: 909747520.0000 - val_rmse: 30162.0215\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 920800576.0000 - rmse: 30344.6953 - val_loss: 973740544.0000 - val_rmse: 31204.8164\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 991335488.0000 - rmse: 31485.4805 - val_loss: 1116140288.0000 - val_rmse: 33408.6875\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 952704704.0000 - rmse: 30865.9141 - val_loss: 683897600.0000 - val_rmse: 26151.4355\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 867364736.0000 - rmse: 29451.0566 - val_loss: 685339904.0000 - val_rmse: 26178.9980\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 882740864.0000 - rmse: 29710.9551 - val_loss: 811443584.0000 - val_rmse: 28485.8496\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 862986240.0000 - rmse: 29376.6270 - val_loss: 752945408.0000 - val_rmse: 27439.8516\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 813683648.0000 - rmse: 28525.1406 - val_loss: 687405376.0000 - val_rmse: 26218.4160\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 802527552.0000 - rmse: 28328.9180 - val_loss: 701680128.0000 - val_rmse: 26489.2461\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 838256000.0000 - rmse: 28952.6504 - val_loss: 631687168.0000 - val_rmse: 25133.3867\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 840110656.0000 - rmse: 28984.6621 - val_loss: 637399616.0000 - val_rmse: 25246.7734\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 776002816.0000 - rmse: 27856.8262 - val_loss: 1376019072.0000 - val_rmse: 37094.7305\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 783377856.0000 - rmse: 27988.8887 - val_loss: 571762688.0000 - val_rmse: 23911.5605\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 812324864.0000 - rmse: 28501.3125 - val_loss: 604336064.0000 - val_rmse: 24583.2480\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 859913216.0000 - rmse: 29324.2773 - val_loss: 628960192.0000 - val_rmse: 25079.0781\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 813446272.0000 - rmse: 28520.9805 - val_loss: 534117152.0000 - val_rmse: 23110.9746\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 770932928.0000 - rmse: 27765.6797 - val_loss: 539085632.0000 - val_rmse: 23218.2168\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 760818112.0000 - rmse: 27582.9316 - val_loss: 590603840.0000 - val_rmse: 24302.3418\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 746496256.0000 - rmse: 27322.0840 - val_loss: 556177152.0000 - val_rmse: 23583.4082\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 747093504.0000 - rmse: 27333.0117 - val_loss: 551595072.0000 - val_rmse: 23486.0605\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 752462592.0000 - rmse: 27431.0508 - val_loss: 629620736.0000 - val_rmse: 25092.2441\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733539456.0000 - rmse: 27083.9336 - val_loss: 598125504.0000 - val_rmse: 24456.6055\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 679625920.0000 - rmse: 26069.6367 - val_loss: 797434560.0000 - val_rmse: 28238.8848\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666935232.0000 - rmse: 25825.0898 - val_loss: 459932480.0000 - val_rmse: 21446.0371\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652700672.0000 - rmse: 25548.0078 - val_loss: 448721600.0000 - val_rmse: 21183.0508\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680298688.0000 - rmse: 26082.5352 - val_loss: 519983808.0000 - val_rmse: 22803.1543\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721513600.0000 - rmse: 26861.0059 - val_loss: 458149568.0000 - val_rmse: 21404.4277\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661580608.0000 - rmse: 25721.2090 - val_loss: 478155232.0000 - val_rmse: 21866.7617\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 731761664.0000 - rmse: 27051.0938 - val_loss: 509672224.0000 - val_rmse: 22575.9219\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 679396672.0000 - rmse: 26065.2383 - val_loss: 629717056.0000 - val_rmse: 25094.1641\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733654848.0000 - rmse: 27086.0645 - val_loss: 473261152.0000 - val_rmse: 21754.5664\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634240192.0000 - rmse: 25184.1250 - val_loss: 457690752.0000 - val_rmse: 21393.7090\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 655264000.0000 - rmse: 25598.1250 - val_loss: 423236288.0000 - val_rmse: 20572.7070\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 639008064.0000 - rmse: 25278.6094 - val_loss: 571390080.0000 - val_rmse: 23903.7676\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 663099840.0000 - rmse: 25750.7246 - val_loss: 622494720.0000 - val_rmse: 24949.8438\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609224896.0000 - rmse: 24682.4824 - val_loss: 441786848.0000 - val_rmse: 21018.7266\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677381376.0000 - rmse: 26026.5508 - val_loss: 417678464.0000 - val_rmse: 20437.1836\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 619906176.0000 - rmse: 24897.9160 - val_loss: 534714272.0000 - val_rmse: 23123.8887\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 608046464.0000 - rmse: 24658.5977 - val_loss: 450410208.0000 - val_rmse: 21222.8691\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604700480.0000 - rmse: 24590.6582 - val_loss: 399062912.0000 - val_rmse: 19976.5586\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 627286080.0000 - rmse: 25045.6797 - val_loss: 395860064.0000 - val_rmse: 19896.2324\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 645314304.0000 - rmse: 25403.0371 - val_loss: 440965952.0000 - val_rmse: 20999.1895\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614790720.0000 - rmse: 24794.9746 - val_loss: 433782656.0000 - val_rmse: 20827.4492\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585887616.0000 - rmse: 24205.1152 - val_loss: 598715392.0000 - val_rmse: 24468.6621\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586867264.0000 - rmse: 24225.3438 - val_loss: 397551872.0000 - val_rmse: 19938.7031\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606596288.0000 - rmse: 24629.1758 - val_loss: 403847360.0000 - val_rmse: 20095.9531\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569593536.0000 - rmse: 23866.1582 - val_loss: 1550591360.0000 - val_rmse: 39377.5508\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553207808.0000 - rmse: 23520.3691 - val_loss: 425130048.0000 - val_rmse: 20618.6816\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566620800.0000 - rmse: 23803.7988 - val_loss: 424489728.0000 - val_rmse: 20603.1484\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548602176.0000 - rmse: 23422.2578 - val_loss: 407718496.0000 - val_rmse: 20192.0410\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 836319360.0000 - rmse: 28919.1875 - val_loss: 457694144.0000 - val_rmse: 21393.7871\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588017600.0000 - rmse: 24249.0742 - val_loss: 370165952.0000 - val_rmse: 19239.6973\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514021120.0000 - rmse: 22672.0332 - val_loss: 380489856.0000 - val_rmse: 19506.1484\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533520736.0000 - rmse: 23098.0684 - val_loss: 408476704.0000 - val_rmse: 20210.8066\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563938624.0000 - rmse: 23747.3926 - val_loss: 439680704.0000 - val_rmse: 20968.5645\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563292544.0000 - rmse: 23733.7852 - val_loss: 448030656.0000 - val_rmse: 21166.7344\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551750912.0000 - rmse: 23489.3789 - val_loss: 604846848.0000 - val_rmse: 24593.6348\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585121984.0000 - rmse: 24189.2949 - val_loss: 433657856.0000 - val_rmse: 20824.4531\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587369408.0000 - rmse: 24235.7051 - val_loss: 649499520.0000 - val_rmse: 25485.2812\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563807232.0000 - rmse: 23744.6250 - val_loss: 366198592.0000 - val_rmse: 19136.3164\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550095296.0000 - rmse: 23454.1113 - val_loss: 361418400.0000 - val_rmse: 19011.0078\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523153504.0000 - rmse: 22872.5488 - val_loss: 362659680.0000 - val_rmse: 19043.6250\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594611136.0000 - rmse: 24384.6504 - val_loss: 378818752.0000 - val_rmse: 19463.2676\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518943616.0000 - rmse: 22780.3340 - val_loss: 554705280.0000 - val_rmse: 23552.1816\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532391744.0000 - rmse: 23073.6152 - val_loss: 341275232.0000 - val_rmse: 18473.6367\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495649600.0000 - rmse: 22263.1895 - val_loss: 463031232.0000 - val_rmse: 21518.1602\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486966912.0000 - rmse: 22067.3262 - val_loss: 584157184.0000 - val_rmse: 24169.3438\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499230240.0000 - rmse: 22343.4609 - val_loss: 376248608.0000 - val_rmse: 19397.1289\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514480160.0000 - rmse: 22682.1543 - val_loss: 338811264.0000 - val_rmse: 18406.8262\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504783712.0000 - rmse: 22467.3926 - val_loss: 456386592.0000 - val_rmse: 21363.2070\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509561376.0000 - rmse: 22573.4668 - val_loss: 365708992.0000 - val_rmse: 19123.5195\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454256544.0000 - rmse: 21313.2949 - val_loss: 353907680.0000 - val_rmse: 18812.4336\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487388704.0000 - rmse: 22076.8809 - val_loss: 330080448.0000 - val_rmse: 18168.1172\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508663904.0000 - rmse: 22553.5781 - val_loss: 341226688.0000 - val_rmse: 18472.3223\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496511744.0000 - rmse: 22282.5430 - val_loss: 321516096.0000 - val_rmse: 17930.8691\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526673920.0000 - rmse: 22949.3770 - val_loss: 395173408.0000 - val_rmse: 19878.9688\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476647936.0000 - rmse: 21832.2676 - val_loss: 405314112.0000 - val_rmse: 20132.4141\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500990432.0000 - rmse: 22382.8164 - val_loss: 435633184.0000 - val_rmse: 20871.8281\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475769920.0000 - rmse: 21812.1504 - val_loss: 340128320.0000 - val_rmse: 18442.5684\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433529024.0000 - rmse: 20821.3594 - val_loss: 566746560.0000 - val_rmse: 23806.4395\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465106368.0000 - rmse: 21566.3242 - val_loss: 336199904.0000 - val_rmse: 18335.7539\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466701600.0000 - rmse: 21603.2773 - val_loss: 321758208.0000 - val_rmse: 17937.6191\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465288896.0000 - rmse: 21570.5566 - val_loss: 387236000.0000 - val_rmse: 19678.3125\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467828064.0000 - rmse: 21629.3340 - val_loss: 393634592.0000 - val_rmse: 19840.2266\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447591328.0000 - rmse: 21156.3535 - val_loss: 432819232.0000 - val_rmse: 20804.3086\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495318912.0000 - rmse: 22255.7617 - val_loss: 392889408.0000 - val_rmse: 19821.4375\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441538464.0000 - rmse: 21012.8164 - val_loss: 584034304.0000 - val_rmse: 24166.8008\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439828704.0000 - rmse: 20972.0938 - val_loss: 397257952.0000 - val_rmse: 19931.3301\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487188832.0000 - rmse: 22072.3535 - val_loss: 331262752.0000 - val_rmse: 18200.6250\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441226048.0000 - rmse: 21005.3809 - val_loss: 343677984.0000 - val_rmse: 18538.5547\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435256576.0000 - rmse: 20862.8027 - val_loss: 413118304.0000 - val_rmse: 20325.3125\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442089408.0000 - rmse: 21025.9219 - val_loss: 307833632.0000 - val_rmse: 17545.1875\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487707008.0000 - rmse: 22084.0898 - val_loss: 330559968.0000 - val_rmse: 18181.3086\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418335552.0000 - rmse: 20453.2520 - val_loss: 420657792.0000 - val_rmse: 20509.9434\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443275936.0000 - rmse: 21054.1191 - val_loss: 550896896.0000 - val_rmse: 23471.1934\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399677472.0000 - rmse: 19991.9355 - val_loss: 417041248.0000 - val_rmse: 20421.5879\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430944096.0000 - rmse: 20759.1934 - val_loss: 407810336.0000 - val_rmse: 20194.3145\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556969152.0000 - rmse: 23600.1934 - val_loss: 411135680.0000 - val_rmse: 20276.4805\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403771616.0000 - rmse: 20094.0684 - val_loss: 467962432.0000 - val_rmse: 21632.4395\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411131136.0000 - rmse: 20276.3691 - val_loss: 312754528.0000 - val_rmse: 17684.8672\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428463904.0000 - rmse: 20699.3691 - val_loss: 384667648.0000 - val_rmse: 19612.9453\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422259904.0000 - rmse: 20548.9629 - val_loss: 396387360.0000 - val_rmse: 19909.4785\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331884640.0000 - rmse: 18217.7012 - val_loss: 312015008.0000 - val_rmse: 17663.9473\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384197600.0000 - rmse: 19600.9590 - val_loss: 491275520.0000 - val_rmse: 22164.7363\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417959712.0000 - rmse: 20444.0625 - val_loss: 427405888.0000 - val_rmse: 20673.7969\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458941504.0000 - rmse: 21422.9199 - val_loss: 491676704.0000 - val_rmse: 22173.7832\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361595872.0000 - rmse: 19015.6738 - val_loss: 392786176.0000 - val_rmse: 19818.8340\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458276896.0000 - rmse: 21407.4023 - val_loss: 363336064.0000 - val_rmse: 19061.3770\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352194624.0000 - rmse: 18766.8496 - val_loss: 516061952.0000 - val_rmse: 22716.9961\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400167296.0000 - rmse: 20004.1816 - val_loss: 477269792.0000 - val_rmse: 21846.5059\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384594048.0000 - rmse: 19611.0703 - val_loss: 476151040.0000 - val_rmse: 21820.8848\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410238336.0000 - rmse: 20254.3418 - val_loss: 695750144.0000 - val_rmse: 26377.0762\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403508544.0000 - rmse: 20087.5215 - val_loss: 469739872.0000 - val_rmse: 21673.4824\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421088448.0000 - rmse: 20520.4395 - val_loss: 730702528.0000 - val_rmse: 27031.5098\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385334880.0000 - rmse: 19629.9492 - val_loss: 579776448.0000 - val_rmse: 24078.5469\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313287872.0000 - rmse: 17699.9395 - val_loss: 485587296.0000 - val_rmse: 22036.0449\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412275872.0000 - rmse: 20304.5781 - val_loss: 661422080.0000 - val_rmse: 25718.1270\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354599072.0000 - rmse: 18830.8008 - val_loss: 309497088.0000 - val_rmse: 17592.5293\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382131712.0000 - rmse: 19548.1895 - val_loss: 370152736.0000 - val_rmse: 19239.3535\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416718720.0000 - rmse: 20413.6895 - val_loss: 297725024.0000 - val_rmse: 17254.7109\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367916736.0000 - rmse: 19181.1562 - val_loss: 655199616.0000 - val_rmse: 25596.8672\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338638944.0000 - rmse: 18402.1445 - val_loss: 501701344.0000 - val_rmse: 22398.6914\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429941952.0000 - rmse: 20735.0410 - val_loss: 494014944.0000 - val_rmse: 22226.4473\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404195872.0000 - rmse: 20104.6230 - val_loss: 561618688.0000 - val_rmse: 23698.4961\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360166560.0000 - rmse: 18978.0547 - val_loss: 408026528.0000 - val_rmse: 20199.6660\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349131392.0000 - rmse: 18685.0586 - val_loss: 332565600.0000 - val_rmse: 18236.3809\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366616736.0000 - rmse: 19147.2383 - val_loss: 410625728.0000 - val_rmse: 20263.9023\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393861600.0000 - rmse: 19845.9473 - val_loss: 346328480.0000 - val_rmse: 18609.9023\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380120704.0000 - rmse: 19496.6836 - val_loss: 523628704.0000 - val_rmse: 22882.9355\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317686848.0000 - rmse: 17823.7715 - val_loss: 468954528.0000 - val_rmse: 21655.3574\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328379328.0000 - rmse: 18121.2402 - val_loss: 565880832.0000 - val_rmse: 23788.2500\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378021216.0000 - rmse: 19442.7676 - val_loss: 306275776.0000 - val_rmse: 17500.7363\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348842752.0000 - rmse: 18677.3320 - val_loss: 529881280.0000 - val_rmse: 23019.1504\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347543296.0000 - rmse: 18642.5137 - val_loss: 511707104.0000 - val_rmse: 22620.9434\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344108320.0000 - rmse: 18550.1562 - val_loss: 494187008.0000 - val_rmse: 22230.3164\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424547744.0000 - rmse: 20604.5566 - val_loss: 446437792.0000 - val_rmse: 21129.0742\n",
      "104/104 [==============================] - 0s 730us/step - loss: 779468992.0000 - rmse: 27918.9727\n",
      "[779468992.0, 27918.97265625]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 6022445056.0000 - rmse: 77604.4141 - val_loss: 1234832896.0000 - val_rmse: 35140.1875\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1832611200.0000 - rmse: 42809.0078 - val_loss: 1410864384.0000 - val_rmse: 37561.4766\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1718544000.0000 - rmse: 41455.3242 - val_loss: 1064600064.0000 - val_rmse: 32628.2090\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1532746240.0000 - rmse: 39150.3047 - val_loss: 1319168896.0000 - val_rmse: 36320.3633\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1469767936.0000 - rmse: 38337.5508 - val_loss: 1007960576.0000 - val_rmse: 31748.3945\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1392165504.0000 - rmse: 37311.7344 - val_loss: 1249416704.0000 - val_rmse: 35347.0898\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1353493760.0000 - rmse: 36789.8594 - val_loss: 826697728.0000 - val_rmse: 28752.3516\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1253467136.0000 - rmse: 35404.3359 - val_loss: 780496512.0000 - val_rmse: 27937.3672\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1215693824.0000 - rmse: 34866.8008 - val_loss: 834827072.0000 - val_rmse: 28893.3750\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1193001344.0000 - rmse: 34539.8516 - val_loss: 764285696.0000 - val_rmse: 27645.7168\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1204456320.0000 - rmse: 34705.2773 - val_loss: 710996608.0000 - val_rmse: 26664.5195\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1045904960.0000 - rmse: 32340.4531 - val_loss: 686953984.0000 - val_rmse: 26209.8066\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1064264256.0000 - rmse: 32623.0625 - val_loss: 693662912.0000 - val_rmse: 26337.4805\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1127282176.0000 - rmse: 33575.0234 - val_loss: 708119936.0000 - val_rmse: 26610.5234\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 961729024.0000 - rmse: 31011.7559 - val_loss: 748168640.0000 - val_rmse: 27352.6719\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 968507712.0000 - rmse: 31120.8574 - val_loss: 700983232.0000 - val_rmse: 26476.0879\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1002573888.0000 - rmse: 31663.4473 - val_loss: 779207360.0000 - val_rmse: 27914.2852\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 968127616.0000 - rmse: 31114.7500 - val_loss: 595803520.0000 - val_rmse: 24409.0859\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 977022848.0000 - rmse: 31257.3652 - val_loss: 710676160.0000 - val_rmse: 26658.5098\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 950891392.0000 - rmse: 30836.5273 - val_loss: 701123264.0000 - val_rmse: 26478.7324\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 930742080.0000 - rmse: 30508.0664 - val_loss: 918515264.0000 - val_rmse: 30307.0176\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 931933760.0000 - rmse: 30527.5898 - val_loss: 593413376.0000 - val_rmse: 24360.0781\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 798024192.0000 - rmse: 28249.3223 - val_loss: 587229888.0000 - val_rmse: 24232.8262\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 851479872.0000 - rmse: 29180.1289 - val_loss: 797892096.0000 - val_rmse: 28246.9844\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 794356096.0000 - rmse: 28184.3242 - val_loss: 899943936.0000 - val_rmse: 29999.0664\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 814157952.0000 - rmse: 28533.4531 - val_loss: 530400288.0000 - val_rmse: 23030.4219\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 740789184.0000 - rmse: 27217.4434 - val_loss: 685859392.0000 - val_rmse: 26188.9180\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 865037184.0000 - rmse: 29411.5137 - val_loss: 490670144.0000 - val_rmse: 22151.0762\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721495488.0000 - rmse: 26860.6680 - val_loss: 971172928.0000 - val_rmse: 31163.6484\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 700020416.0000 - rmse: 26457.8984 - val_loss: 604005056.0000 - val_rmse: 24576.5137\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 667905152.0000 - rmse: 25843.8613 - val_loss: 573881664.0000 - val_rmse: 23955.8281\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661176896.0000 - rmse: 25713.3594 - val_loss: 630574784.0000 - val_rmse: 25111.2480\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 710840704.0000 - rmse: 26661.5957 - val_loss: 487938368.0000 - val_rmse: 22089.3262\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 744280000.0000 - rmse: 27281.4961 - val_loss: 656906176.0000 - val_rmse: 25630.1816\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 657034816.0000 - rmse: 25632.6895 - val_loss: 462879296.0000 - val_rmse: 21514.6289\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698430656.0000 - rmse: 26427.8379 - val_loss: 572476416.0000 - val_rmse: 23926.4785\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 659457536.0000 - rmse: 25679.9062 - val_loss: 457980960.0000 - val_rmse: 21400.4902\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599648448.0000 - rmse: 24487.7207 - val_loss: 575191808.0000 - val_rmse: 23983.1562\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590791232.0000 - rmse: 24306.1973 - val_loss: 600296576.0000 - val_rmse: 24500.9512\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597692416.0000 - rmse: 24447.7480 - val_loss: 605480640.0000 - val_rmse: 24606.5156\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 658833856.0000 - rmse: 25667.7598 - val_loss: 649995456.0000 - val_rmse: 25495.0078\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593334720.0000 - rmse: 24358.4629 - val_loss: 509426592.0000 - val_rmse: 22570.4805\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677244672.0000 - rmse: 26023.9258 - val_loss: 517882816.0000 - val_rmse: 22757.0391\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632149120.0000 - rmse: 25142.5762 - val_loss: 530680320.0000 - val_rmse: 23036.5000\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561712960.0000 - rmse: 23700.4844 - val_loss: 653856832.0000 - val_rmse: 25570.6250\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 528125824.0000 - rmse: 22980.9883 - val_loss: 603351872.0000 - val_rmse: 24563.2227\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 595856576.0000 - rmse: 24410.1738 - val_loss: 572095104.0000 - val_rmse: 23918.5098\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567571840.0000 - rmse: 23823.7656 - val_loss: 670622976.0000 - val_rmse: 25896.3887\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491703360.0000 - rmse: 22174.3848 - val_loss: 634367104.0000 - val_rmse: 25186.6445\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524949984.0000 - rmse: 22911.7871 - val_loss: 563495680.0000 - val_rmse: 23738.0645\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460616928.0000 - rmse: 21461.9883 - val_loss: 468224448.0000 - val_rmse: 21638.4941\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491115232.0000 - rmse: 22161.1191 - val_loss: 463013408.0000 - val_rmse: 21517.7461\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509048032.0000 - rmse: 22562.0938 - val_loss: 613342656.0000 - val_rmse: 24765.7559\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456275648.0000 - rmse: 21360.6094 - val_loss: 433621312.0000 - val_rmse: 20823.5762\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442706240.0000 - rmse: 21040.5859 - val_loss: 473143360.0000 - val_rmse: 21751.8594\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467648160.0000 - rmse: 21625.1738 - val_loss: 465647968.0000 - val_rmse: 21578.8770\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480533376.0000 - rmse: 21921.0723 - val_loss: 490754048.0000 - val_rmse: 22152.9688\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525036000.0000 - rmse: 22913.6641 - val_loss: 535312992.0000 - val_rmse: 23136.8320\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458311232.0000 - rmse: 21408.2051 - val_loss: 518753984.0000 - val_rmse: 22776.1719\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451392800.0000 - rmse: 21246.0059 - val_loss: 554714752.0000 - val_rmse: 23552.3828\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441642304.0000 - rmse: 21015.2871 - val_loss: 476906592.0000 - val_rmse: 21838.1914\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508744032.0000 - rmse: 22555.3555 - val_loss: 561744832.0000 - val_rmse: 23701.1562\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456554144.0000 - rmse: 21367.1270 - val_loss: 521876608.0000 - val_rmse: 22844.6191\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417856800.0000 - rmse: 20441.5469 - val_loss: 669776448.0000 - val_rmse: 25880.0391\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458037920.0000 - rmse: 21401.8203 - val_loss: 513488896.0000 - val_rmse: 22660.2930\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458414976.0000 - rmse: 21410.6270 - val_loss: 551750336.0000 - val_rmse: 23489.3672\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442110656.0000 - rmse: 21026.4277 - val_loss: 575906752.0000 - val_rmse: 23998.0566\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479690624.0000 - rmse: 21901.8398 - val_loss: 519230144.0000 - val_rmse: 22786.6211\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400662336.0000 - rmse: 20016.5508 - val_loss: 553836608.0000 - val_rmse: 23533.7344\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412498304.0000 - rmse: 20310.0547 - val_loss: 548598080.0000 - val_rmse: 23422.1699\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406869632.0000 - rmse: 20171.0098 - val_loss: 540094272.0000 - val_rmse: 23239.9277\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429238080.0000 - rmse: 20718.0625 - val_loss: 522138272.0000 - val_rmse: 22850.3457\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400922176.0000 - rmse: 20023.0410 - val_loss: 495105216.0000 - val_rmse: 22250.9590\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453203264.0000 - rmse: 21288.5703 - val_loss: 495462528.0000 - val_rmse: 22258.9883\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378605248.0000 - rmse: 19457.7812 - val_loss: 511585376.0000 - val_rmse: 22618.2539\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409934112.0000 - rmse: 20246.8301 - val_loss: 511767776.0000 - val_rmse: 22622.2852\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394366304.0000 - rmse: 19858.6582 - val_loss: 664996352.0000 - val_rmse: 25787.5234\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427885664.0000 - rmse: 20685.3965 - val_loss: 625692032.0000 - val_rmse: 25013.8359\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366140576.0000 - rmse: 19134.8008 - val_loss: 793434176.0000 - val_rmse: 28167.9629\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453830336.0000 - rmse: 21303.2949 - val_loss: 543624640.0000 - val_rmse: 23315.7598\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441070720.0000 - rmse: 21001.6836 - val_loss: 533353152.0000 - val_rmse: 23094.4395\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417377152.0000 - rmse: 20429.8105 - val_loss: 474663712.0000 - val_rmse: 21786.7793\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382545376.0000 - rmse: 19558.7676 - val_loss: 577777408.0000 - val_rmse: 24037.0000\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322877536.0000 - rmse: 17968.7930 - val_loss: 501903360.0000 - val_rmse: 22403.1992\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345313152.0000 - rmse: 18582.6035 - val_loss: 575899264.0000 - val_rmse: 23997.9004\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441472608.0000 - rmse: 21011.2500 - val_loss: 705580096.0000 - val_rmse: 26562.7578\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420258208.0000 - rmse: 20500.1992 - val_loss: 505075648.0000 - val_rmse: 22473.8887\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378767968.0000 - rmse: 19461.9629 - val_loss: 580217408.0000 - val_rmse: 24087.7031\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342562208.0000 - rmse: 18508.4355 - val_loss: 543587456.0000 - val_rmse: 23314.9629\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361452384.0000 - rmse: 19011.9004 - val_loss: 575196544.0000 - val_rmse: 23983.2559\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395923008.0000 - rmse: 19897.8145 - val_loss: 478433280.0000 - val_rmse: 21873.1172\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319645536.0000 - rmse: 17878.6328 - val_loss: 428611648.0000 - val_rmse: 20702.9375\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357944064.0000 - rmse: 18919.4102 - val_loss: 476092224.0000 - val_rmse: 21819.5371\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375426368.0000 - rmse: 19375.9219 - val_loss: 416328256.0000 - val_rmse: 20404.1230\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347220800.0000 - rmse: 18633.8613 - val_loss: 550161664.0000 - val_rmse: 23455.5254\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356079328.0000 - rmse: 18870.0645 - val_loss: 610563072.0000 - val_rmse: 24709.5742\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334357792.0000 - rmse: 18285.4531 - val_loss: 617794240.0000 - val_rmse: 24855.4668\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346180032.0000 - rmse: 18605.9141 - val_loss: 379699264.0000 - val_rmse: 19485.8730\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320930688.0000 - rmse: 17914.5391 - val_loss: 417358720.0000 - val_rmse: 20429.3594\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345732192.0000 - rmse: 18593.8750 - val_loss: 488950400.0000 - val_rmse: 22112.2227\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307428128.0000 - rmse: 17533.6289 - val_loss: 399477376.0000 - val_rmse: 19986.9297\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330254912.0000 - rmse: 18172.9160 - val_loss: 400533856.0000 - val_rmse: 20013.3418\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370580256.0000 - rmse: 19250.4609 - val_loss: 414177696.0000 - val_rmse: 20351.3555\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356057216.0000 - rmse: 18869.4785 - val_loss: 582938368.0000 - val_rmse: 24144.1172\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324729568.0000 - rmse: 18020.2539 - val_loss: 472846624.0000 - val_rmse: 21745.0371\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355630048.0000 - rmse: 18858.1562 - val_loss: 574606208.0000 - val_rmse: 23970.9453\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361535136.0000 - rmse: 19014.0781 - val_loss: 611337472.0000 - val_rmse: 24725.2402\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336985728.0000 - rmse: 18357.1719 - val_loss: 493993248.0000 - val_rmse: 22225.9590\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358231008.0000 - rmse: 18926.9922 - val_loss: 475371520.0000 - val_rmse: 21803.0156\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318348288.0000 - rmse: 17842.3164 - val_loss: 480931648.0000 - val_rmse: 21930.1543\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364829632.0000 - rmse: 19100.5137 - val_loss: 427310016.0000 - val_rmse: 20671.4785\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322468544.0000 - rmse: 17957.4082 - val_loss: 382010336.0000 - val_rmse: 19545.0840\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307948640.0000 - rmse: 17548.4648 - val_loss: 445838336.0000 - val_rmse: 21114.8848\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283162112.0000 - rmse: 16827.4219 - val_loss: 478796192.0000 - val_rmse: 21881.4121\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282472224.0000 - rmse: 16806.9102 - val_loss: 544966784.0000 - val_rmse: 23344.5234\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343043488.0000 - rmse: 18521.4336 - val_loss: 490611584.0000 - val_rmse: 22149.7539\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316011872.0000 - rmse: 17776.7227 - val_loss: 423040992.0000 - val_rmse: 20567.9609\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314059328.0000 - rmse: 17721.7188 - val_loss: 457388000.0000 - val_rmse: 21386.6309\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362994400.0000 - rmse: 19052.4121 - val_loss: 347795712.0000 - val_rmse: 18649.2812\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302505152.0000 - rmse: 17392.6758 - val_loss: 545617280.0000 - val_rmse: 23358.4512\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350996128.0000 - rmse: 18734.8906 - val_loss: 584047680.0000 - val_rmse: 24167.0781\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350561408.0000 - rmse: 18723.2852 - val_loss: 460333216.0000 - val_rmse: 21455.3770\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313880064.0000 - rmse: 17716.6602 - val_loss: 486313760.0000 - val_rmse: 22052.5234\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319623744.0000 - rmse: 17878.0234 - val_loss: 446011296.0000 - val_rmse: 21118.9805\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335566944.0000 - rmse: 18318.4863 - val_loss: 371943296.0000 - val_rmse: 19285.8320\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273718656.0000 - rmse: 16544.4453 - val_loss: 476913248.0000 - val_rmse: 21838.3438\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311873408.0000 - rmse: 17659.9375 - val_loss: 405148928.0000 - val_rmse: 20128.3125\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333470464.0000 - rmse: 18261.1738 - val_loss: 439803040.0000 - val_rmse: 20971.4824\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288986816.0000 - rmse: 16999.6113 - val_loss: 392186496.0000 - val_rmse: 19803.6992\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273942240.0000 - rmse: 16551.2012 - val_loss: 365912544.0000 - val_rmse: 19128.8398\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298086976.0000 - rmse: 17265.1953 - val_loss: 492223552.0000 - val_rmse: 22186.1113\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284503040.0000 - rmse: 16867.2188 - val_loss: 439764096.0000 - val_rmse: 20970.5527\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326070304.0000 - rmse: 18057.4160 - val_loss: 462865056.0000 - val_rmse: 21514.2988\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301778848.0000 - rmse: 17371.7832 - val_loss: 402020064.0000 - val_rmse: 20050.4375\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361934784.0000 - rmse: 19024.5840 - val_loss: 515897120.0000 - val_rmse: 22713.3691\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291370304.0000 - rmse: 17069.5723 - val_loss: 364949120.0000 - val_rmse: 19103.6406\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320606144.0000 - rmse: 17905.4785 - val_loss: 439237632.0000 - val_rmse: 20957.9961\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328807424.0000 - rmse: 18133.0469 - val_loss: 652015424.0000 - val_rmse: 25534.5918\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301549152.0000 - rmse: 17365.1699 - val_loss: 439093952.0000 - val_rmse: 20954.5684\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289233792.0000 - rmse: 17006.8750 - val_loss: 659889024.0000 - val_rmse: 25688.3047\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313072544.0000 - rmse: 17693.8555 - val_loss: 399028448.0000 - val_rmse: 19975.6973\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269465024.0000 - rmse: 16415.3906 - val_loss: 413180864.0000 - val_rmse: 20326.8516\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257256032.0000 - rmse: 16039.2031 - val_loss: 477127904.0000 - val_rmse: 21843.2578\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313887104.0000 - rmse: 17716.8594 - val_loss: 419745120.0000 - val_rmse: 20487.6816\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315786560.0000 - rmse: 17770.3848 - val_loss: 484885440.0000 - val_rmse: 22020.1152\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283239200.0000 - rmse: 16829.7109 - val_loss: 349216352.0000 - val_rmse: 18687.3320\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279388128.0000 - rmse: 16714.9082 - val_loss: 474709760.0000 - val_rmse: 21787.8359\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377532224.0000 - rmse: 19430.1875 - val_loss: 587884992.0000 - val_rmse: 24246.3398\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259361296.0000 - rmse: 16104.6982 - val_loss: 437648320.0000 - val_rmse: 20920.0449\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269594976.0000 - rmse: 16419.3477 - val_loss: 408949568.0000 - val_rmse: 20222.5020\n",
      "104/104 [==============================] - 0s 748us/step - loss: 528006656.0000 - rmse: 22978.3945\n",
      "[528006656.0, 22978.39453125]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,689\n",
      "Trainable params: 20,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 6971929600.0000 - rmse: 83498.0781 - val_loss: 1357401984.0000 - val_rmse: 36842.9375\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1948021120.0000 - rmse: 44136.3906 - val_loss: 1038511232.0000 - val_rmse: 32225.9395\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1736696832.0000 - rmse: 41673.6953 - val_loss: 965760128.0000 - val_rmse: 31076.6816\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1650035328.0000 - rmse: 40620.6250 - val_loss: 967998080.0000 - val_rmse: 31112.6680\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1543363712.0000 - rmse: 39285.6680 - val_loss: 801135872.0000 - val_rmse: 28304.3438\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1420317824.0000 - rmse: 37687.1055 - val_loss: 936851008.0000 - val_rmse: 30608.0215\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1383002368.0000 - rmse: 37188.7383 - val_loss: 745016576.0000 - val_rmse: 27294.9922\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1287350528.0000 - rmse: 35879.6680 - val_loss: 714863360.0000 - val_rmse: 26736.9297\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1196879488.0000 - rmse: 34595.9453 - val_loss: 687772800.0000 - val_rmse: 26225.4219\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1182471296.0000 - rmse: 34387.0820 - val_loss: 670450432.0000 - val_rmse: 25893.0566\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1179005696.0000 - rmse: 34336.6523 - val_loss: 656666944.0000 - val_rmse: 25625.5137\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1178135296.0000 - rmse: 34323.9766 - val_loss: 804483072.0000 - val_rmse: 28363.4102\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1146758784.0000 - rmse: 33863.8281 - val_loss: 617131584.0000 - val_rmse: 24842.1328\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1015342976.0000 - rmse: 31864.4473 - val_loss: 658520512.0000 - val_rmse: 25661.6543\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1055835072.0000 - rmse: 32493.6152 - val_loss: 614410496.0000 - val_rmse: 24787.3047\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 983132608.0000 - rmse: 31354.9453 - val_loss: 555335040.0000 - val_rmse: 23565.5469\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 980514368.0000 - rmse: 31313.1660 - val_loss: 561272192.0000 - val_rmse: 23691.1836\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 954805312.0000 - rmse: 30899.9238 - val_loss: 526716128.0000 - val_rmse: 22950.2969\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 958459136.0000 - rmse: 30958.9902 - val_loss: 511407168.0000 - val_rmse: 22614.3125\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 944788416.0000 - rmse: 30737.4102 - val_loss: 593580480.0000 - val_rmse: 24363.5078\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 900625088.0000 - rmse: 30010.4160 - val_loss: 675864192.0000 - val_rmse: 25997.3887\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 870231040.0000 - rmse: 29499.6777 - val_loss: 545331328.0000 - val_rmse: 23352.3301\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 885512384.0000 - rmse: 29757.5605 - val_loss: 519161792.0000 - val_rmse: 22785.1230\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 870436416.0000 - rmse: 29503.1602 - val_loss: 828392704.0000 - val_rmse: 28781.8125\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 875456640.0000 - rmse: 29588.1172 - val_loss: 524377664.0000 - val_rmse: 22899.2949\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 823688448.0000 - rmse: 28699.9727 - val_loss: 476668320.0000 - val_rmse: 21832.7344\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 877872832.0000 - rmse: 29628.9180 - val_loss: 664514560.0000 - val_rmse: 25778.1797\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 861331968.0000 - rmse: 29348.4570 - val_loss: 536636800.0000 - val_rmse: 23165.4219\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 816722176.0000 - rmse: 28578.3516 - val_loss: 1157247488.0000 - val_rmse: 34018.3398\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 738429504.0000 - rmse: 27174.0586 - val_loss: 460758208.0000 - val_rmse: 21465.2793\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 767687872.0000 - rmse: 27707.1816 - val_loss: 1535113600.0000 - val_rmse: 39180.5273\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 828601088.0000 - rmse: 28785.4316 - val_loss: 572983808.0000 - val_rmse: 23937.0801\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 822968512.0000 - rmse: 28687.4277 - val_loss: 531885632.0000 - val_rmse: 23062.6465\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 803767360.0000 - rmse: 28350.7910 - val_loss: 459660704.0000 - val_rmse: 21439.6992\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 808952000.0000 - rmse: 28442.0820 - val_loss: 473004576.0000 - val_rmse: 21748.6680\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742053760.0000 - rmse: 27240.6641 - val_loss: 523378656.0000 - val_rmse: 22877.4707\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 862494912.0000 - rmse: 29368.2637 - val_loss: 488597216.0000 - val_rmse: 22104.2344\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 741126336.0000 - rmse: 27223.6348 - val_loss: 535012608.0000 - val_rmse: 23130.3398\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 728665088.0000 - rmse: 26993.7969 - val_loss: 536571840.0000 - val_rmse: 23164.0195\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 735147648.0000 - rmse: 27113.6055 - val_loss: 443014176.0000 - val_rmse: 21047.9023\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706082240.0000 - rmse: 26572.2090 - val_loss: 421815456.0000 - val_rmse: 20538.1465\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729621312.0000 - rmse: 27011.5039 - val_loss: 482647424.0000 - val_rmse: 21969.2383\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 717241472.0000 - rmse: 26781.3652 - val_loss: 469317440.0000 - val_rmse: 21663.7363\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 714667392.0000 - rmse: 26733.2637 - val_loss: 501882944.0000 - val_rmse: 22402.7441\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 709474688.0000 - rmse: 26635.9668 - val_loss: 562169664.0000 - val_rmse: 23710.1172\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 805054144.0000 - rmse: 28373.4766 - val_loss: 514218016.0000 - val_rmse: 22676.3750\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 693641088.0000 - rmse: 26337.0664 - val_loss: 425883328.0000 - val_rmse: 20636.9414\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676503104.0000 - rmse: 26009.6738 - val_loss: 636685760.0000 - val_rmse: 25232.6328\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 750508288.0000 - rmse: 27395.4062 - val_loss: 579653824.0000 - val_rmse: 24076.0020\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691148544.0000 - rmse: 26289.7051 - val_loss: 462600416.0000 - val_rmse: 21508.1484\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583847360.0000 - rmse: 24162.9336 - val_loss: 440950336.0000 - val_rmse: 20998.8184\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 696491072.0000 - rmse: 26391.1172 - val_loss: 483746112.0000 - val_rmse: 21994.2285\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 681422080.0000 - rmse: 26104.0625 - val_loss: 464254400.0000 - val_rmse: 21546.5645\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677664192.0000 - rmse: 26031.9844 - val_loss: 471050560.0000 - val_rmse: 21703.6992\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601324864.0000 - rmse: 24521.9258 - val_loss: 493191104.0000 - val_rmse: 22207.9062\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 644182912.0000 - rmse: 25380.7578 - val_loss: 490883488.0000 - val_rmse: 22155.8906\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622032896.0000 - rmse: 24940.5879 - val_loss: 460293280.0000 - val_rmse: 21454.4473\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586903552.0000 - rmse: 24226.0918 - val_loss: 523360192.0000 - val_rmse: 22877.0664\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637469184.0000 - rmse: 25248.1523 - val_loss: 444214816.0000 - val_rmse: 21076.4043\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 643984256.0000 - rmse: 25376.8457 - val_loss: 497799072.0000 - val_rmse: 22311.4121\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607954752.0000 - rmse: 24656.7383 - val_loss: 444988000.0000 - val_rmse: 21094.7383\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566629696.0000 - rmse: 23803.9844 - val_loss: 518201760.0000 - val_rmse: 22764.0449\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565181184.0000 - rmse: 23773.5391 - val_loss: 436507680.0000 - val_rmse: 20892.7656\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575865472.0000 - rmse: 23997.1973 - val_loss: 422521984.0000 - val_rmse: 20555.3398\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618846208.0000 - rmse: 24876.6191 - val_loss: 531534080.0000 - val_rmse: 23055.0234\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558931520.0000 - rmse: 23641.7324 - val_loss: 495906400.0000 - val_rmse: 22268.9551\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520617056.0000 - rmse: 22817.0352 - val_loss: 450467648.0000 - val_rmse: 21224.2227\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487878848.0000 - rmse: 22087.9805 - val_loss: 786962432.0000 - val_rmse: 28052.8516\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546509184.0000 - rmse: 23377.5352 - val_loss: 441146080.0000 - val_rmse: 21003.4785\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505755904.0000 - rmse: 22489.0176 - val_loss: 562568448.0000 - val_rmse: 23718.5254\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520074720.0000 - rmse: 22805.1465 - val_loss: 529732800.0000 - val_rmse: 23015.9258\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527019616.0000 - rmse: 22956.9082 - val_loss: 555085312.0000 - val_rmse: 23560.2480\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541677632.0000 - rmse: 23273.9688 - val_loss: 437128640.0000 - val_rmse: 20907.6211\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497917248.0000 - rmse: 22314.0586 - val_loss: 465091296.0000 - val_rmse: 21565.9746\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537279872.0000 - rmse: 23179.2988 - val_loss: 528010304.0000 - val_rmse: 22978.4746\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529867808.0000 - rmse: 23018.8574 - val_loss: 539450624.0000 - val_rmse: 23226.0762\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494557344.0000 - rmse: 22238.6445 - val_loss: 453089632.0000 - val_rmse: 21285.9023\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484343808.0000 - rmse: 22007.8125 - val_loss: 465040352.0000 - val_rmse: 21564.7949\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469132928.0000 - rmse: 21659.4766 - val_loss: 428610976.0000 - val_rmse: 20702.9219\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540507520.0000 - rmse: 23248.8184 - val_loss: 456014880.0000 - val_rmse: 21354.5059\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512653504.0000 - rmse: 22641.8535 - val_loss: 524094080.0000 - val_rmse: 22893.1016\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501937536.0000 - rmse: 22403.9629 - val_loss: 475764448.0000 - val_rmse: 21812.0254\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506560224.0000 - rmse: 22506.8926 - val_loss: 963443904.0000 - val_rmse: 31039.3926\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475383872.0000 - rmse: 21803.2988 - val_loss: 501217696.0000 - val_rmse: 22387.8926\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436198272.0000 - rmse: 20885.3594 - val_loss: 417370976.0000 - val_rmse: 20429.6602\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511218240.0000 - rmse: 22610.1367 - val_loss: 542529152.0000 - val_rmse: 23292.2559\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411743232.0000 - rmse: 20291.4570 - val_loss: 435199680.0000 - val_rmse: 20861.4395\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494318112.0000 - rmse: 22233.2656 - val_loss: 512655584.0000 - val_rmse: 22641.8984\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465264640.0000 - rmse: 21569.9941 - val_loss: 439752768.0000 - val_rmse: 20970.2832\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434659296.0000 - rmse: 20848.4844 - val_loss: 461017632.0000 - val_rmse: 21471.3203\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445613536.0000 - rmse: 21109.5605 - val_loss: 577785280.0000 - val_rmse: 24037.1641\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414889472.0000 - rmse: 20368.8359 - val_loss: 725187776.0000 - val_rmse: 26929.3105\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445859072.0000 - rmse: 21115.3750 - val_loss: 442330368.0000 - val_rmse: 21031.6523\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444594048.0000 - rmse: 21085.3984 - val_loss: 394912320.0000 - val_rmse: 19872.4004\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442534432.0000 - rmse: 21036.5020 - val_loss: 416831584.0000 - val_rmse: 20416.4531\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491152224.0000 - rmse: 22161.9551 - val_loss: 520745440.0000 - val_rmse: 22819.8477\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395409664.0000 - rmse: 19884.9102 - val_loss: 557047424.0000 - val_rmse: 23601.8516\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497678304.0000 - rmse: 22308.7051 - val_loss: 471166848.0000 - val_rmse: 21706.3789\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411210912.0000 - rmse: 20278.3359 - val_loss: 445687360.0000 - val_rmse: 21111.3086\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379231424.0000 - rmse: 19473.8652 - val_loss: 469503744.0000 - val_rmse: 21668.0352\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432884064.0000 - rmse: 20805.8652 - val_loss: 486106464.0000 - val_rmse: 22047.8223\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392628288.0000 - rmse: 19814.8496 - val_loss: 455893152.0000 - val_rmse: 21351.6543\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381061024.0000 - rmse: 19520.7852 - val_loss: 485377152.0000 - val_rmse: 22031.2773\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424433248.0000 - rmse: 20601.7773 - val_loss: 820848576.0000 - val_rmse: 28650.4551\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384573152.0000 - rmse: 19610.5371 - val_loss: 608352832.0000 - val_rmse: 24664.8105\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348495488.0000 - rmse: 18668.0332 - val_loss: 466567936.0000 - val_rmse: 21600.1836\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416613504.0000 - rmse: 20411.1113 - val_loss: 480837664.0000 - val_rmse: 21928.0117\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376950944.0000 - rmse: 19415.2246 - val_loss: 459265760.0000 - val_rmse: 21430.4863\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394820224.0000 - rmse: 19870.0840 - val_loss: 503696576.0000 - val_rmse: 22443.1855\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337019200.0000 - rmse: 18358.0820 - val_loss: 620555904.0000 - val_rmse: 24910.9590\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380664512.0000 - rmse: 19510.6250 - val_loss: 549953600.0000 - val_rmse: 23451.0898\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406659552.0000 - rmse: 20165.8008 - val_loss: 836830400.0000 - val_rmse: 28928.0215\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393777856.0000 - rmse: 19843.8359 - val_loss: 508733248.0000 - val_rmse: 22555.1152\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352608480.0000 - rmse: 18777.8730 - val_loss: 479786368.0000 - val_rmse: 21904.0254\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346923744.0000 - rmse: 18625.8887 - val_loss: 550026112.0000 - val_rmse: 23452.6348\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372528192.0000 - rmse: 19300.9902 - val_loss: 490749408.0000 - val_rmse: 22152.8652\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457572320.0000 - rmse: 21390.9395 - val_loss: 551095808.0000 - val_rmse: 23475.4297\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341847840.0000 - rmse: 18489.1270 - val_loss: 627045184.0000 - val_rmse: 25040.8711\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323391936.0000 - rmse: 17983.1016 - val_loss: 588087104.0000 - val_rmse: 24250.5078\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350205632.0000 - rmse: 18713.7812 - val_loss: 556531200.0000 - val_rmse: 23590.9141\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319751264.0000 - rmse: 17881.5898 - val_loss: 442897216.0000 - val_rmse: 21045.1230\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371678016.0000 - rmse: 19278.9531 - val_loss: 552117888.0000 - val_rmse: 23497.1895\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328151968.0000 - rmse: 18114.9648 - val_loss: 478313472.0000 - val_rmse: 21870.3789\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357987360.0000 - rmse: 18920.5547 - val_loss: 431530688.0000 - val_rmse: 20773.3164\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363621440.0000 - rmse: 19068.8613 - val_loss: 622814208.0000 - val_rmse: 24956.2461\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362053472.0000 - rmse: 19027.7031 - val_loss: 649055936.0000 - val_rmse: 25476.5762\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342952704.0000 - rmse: 18518.9824 - val_loss: 504129120.0000 - val_rmse: 22452.8203\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397000384.0000 - rmse: 19924.8691 - val_loss: 585937088.0000 - val_rmse: 24206.1367\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420094048.0000 - rmse: 20496.1953 - val_loss: 614952704.0000 - val_rmse: 24798.2402\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385953632.0000 - rmse: 19645.7031 - val_loss: 578676544.0000 - val_rmse: 24055.6973\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380804960.0000 - rmse: 19514.2246 - val_loss: 683261440.0000 - val_rmse: 26139.2695\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366131008.0000 - rmse: 19134.5508 - val_loss: 611209152.0000 - val_rmse: 24722.6445\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347924160.0000 - rmse: 18652.7246 - val_loss: 667966528.0000 - val_rmse: 25845.0488\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324117984.0000 - rmse: 18003.2773 - val_loss: 624236864.0000 - val_rmse: 24984.7324\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309201504.0000 - rmse: 17584.1270 - val_loss: 440037536.0000 - val_rmse: 20977.0723\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354576320.0000 - rmse: 18830.1973 - val_loss: 607346752.0000 - val_rmse: 24644.4062\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343118432.0000 - rmse: 18523.4570 - val_loss: 701279232.0000 - val_rmse: 26481.6777\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357302624.0000 - rmse: 18902.4512 - val_loss: 712130368.0000 - val_rmse: 26685.7715\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295709696.0000 - rmse: 17196.2109 - val_loss: 633986176.0000 - val_rmse: 25179.0820\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357884992.0000 - rmse: 18917.8477 - val_loss: 462233824.0000 - val_rmse: 21499.6230\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334408096.0000 - rmse: 18286.8281 - val_loss: 551425920.0000 - val_rmse: 23482.4590\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337382240.0000 - rmse: 18367.9668 - val_loss: 509045152.0000 - val_rmse: 22562.0293\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330723136.0000 - rmse: 18185.7949 - val_loss: 569459392.0000 - val_rmse: 23863.3477\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311188448.0000 - rmse: 17640.5352 - val_loss: 644503360.0000 - val_rmse: 25387.0703\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316973600.0000 - rmse: 17803.7520 - val_loss: 698260928.0000 - val_rmse: 26424.6270\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304482048.0000 - rmse: 17449.4141 - val_loss: 623482880.0000 - val_rmse: 24969.6387\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333301408.0000 - rmse: 18256.5449 - val_loss: 576372160.0000 - val_rmse: 24007.7520\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322726624.0000 - rmse: 17964.5938 - val_loss: 487899264.0000 - val_rmse: 22088.4414\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289276832.0000 - rmse: 17008.1406 - val_loss: 616573504.0000 - val_rmse: 24830.8984\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307456384.0000 - rmse: 17534.4336 - val_loss: 597323264.0000 - val_rmse: 24440.1973\n",
      "104/104 [==============================] - 0s 710us/step - loss: 317360288.0000 - rmse: 17814.6094\n",
      "[317360288.0, 17814.609375]\n",
      "[24497.650390625, 30035.71484375, 27918.97265625, 22978.39453125, 17814.609375]\n",
      "24649.068359375\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "!python train.py kfold light\n",
    "# (32 16) with  d_2 l2"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 20:10:00.168307: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 20:10:00.168344: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 20:10:00.168661: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,417\n",
      "Trainable params: 20,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,417\n",
      "Trainable params: 20,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 20:10:00.342115: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 10481358848.0000 - rmse: 102378.5078 - val_loss: 1497092480.0000 - val_rmse: 38692.2812\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1949013120.0000 - rmse: 44147.6289 - val_loss: 1085296384.0000 - val_rmse: 32943.8359\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1755228288.0000 - rmse: 41895.4414 - val_loss: 1055072896.0000 - val_rmse: 32481.8848\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1731641728.0000 - rmse: 41613.0000 - val_loss: 936555520.0000 - val_rmse: 30603.1953\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1637051392.0000 - rmse: 40460.4922 - val_loss: 860171072.0000 - val_rmse: 29328.6738\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1538343936.0000 - rmse: 39221.7266 - val_loss: 791814144.0000 - val_rmse: 28139.1914\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1493353600.0000 - rmse: 38643.9297 - val_loss: 1043109696.0000 - val_rmse: 32297.2070\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1462311040.0000 - rmse: 38240.1758 - val_loss: 746181760.0000 - val_rmse: 27316.3262\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1444173568.0000 - rmse: 38002.2812 - val_loss: 854593088.0000 - val_rmse: 29233.4238\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1325975168.0000 - rmse: 36413.9414 - val_loss: 696145152.0000 - val_rmse: 26384.5625\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1238628736.0000 - rmse: 35194.1562 - val_loss: 670015104.0000 - val_rmse: 25884.6484\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1220806784.0000 - rmse: 34940.0430 - val_loss: 744308288.0000 - val_rmse: 27282.0137\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1254378240.0000 - rmse: 35417.1992 - val_loss: 1267605632.0000 - val_rmse: 35603.4492\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1206434048.0000 - rmse: 34733.7578 - val_loss: 586596736.0000 - val_rmse: 24219.7578\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1163579648.0000 - rmse: 34111.2812 - val_loss: 580076800.0000 - val_rmse: 24084.7812\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1091046656.0000 - rmse: 33030.9922 - val_loss: 594061824.0000 - val_rmse: 24373.3828\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1210896768.0000 - rmse: 34797.9414 - val_loss: 690157440.0000 - val_rmse: 26270.8477\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1071907136.0000 - rmse: 32739.9902 - val_loss: 563879424.0000 - val_rmse: 23746.1445\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1048538688.0000 - rmse: 32381.1465 - val_loss: 686979520.0000 - val_rmse: 26210.2930\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1027806464.0000 - rmse: 32059.4199 - val_loss: 529576704.0000 - val_rmse: 23012.5332\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1087159680.0000 - rmse: 32972.1055 - val_loss: 497613984.0000 - val_rmse: 22307.2578\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1035638080.0000 - rmse: 32181.3281 - val_loss: 635831872.0000 - val_rmse: 25215.7051\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1010496192.0000 - rmse: 31788.2988 - val_loss: 511725024.0000 - val_rmse: 22621.3359\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1003865216.0000 - rmse: 31683.8320 - val_loss: 532629312.0000 - val_rmse: 23078.7617\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1030425216.0000 - rmse: 32100.2363 - val_loss: 610967680.0000 - val_rmse: 24717.7598\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 967205376.0000 - rmse: 31099.9238 - val_loss: 601010112.0000 - val_rmse: 24515.5039\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 959313728.0000 - rmse: 30972.7910 - val_loss: 432919616.0000 - val_rmse: 20806.7168\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1021660096.0000 - rmse: 31963.4180 - val_loss: 505938208.0000 - val_rmse: 22493.0664\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 922616704.0000 - rmse: 30374.6035 - val_loss: 464392928.0000 - val_rmse: 21549.7773\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 960386560.0000 - rmse: 30990.1035 - val_loss: 458814656.0000 - val_rmse: 21419.9551\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 924198720.0000 - rmse: 30400.6367 - val_loss: 610411392.0000 - val_rmse: 24706.5020\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 944248128.0000 - rmse: 30728.6172 - val_loss: 562878784.0000 - val_rmse: 23725.0664\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 951643840.0000 - rmse: 30848.7246 - val_loss: 415397568.0000 - val_rmse: 20381.3008\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 949286016.0000 - rmse: 30810.4824 - val_loss: 420437024.0000 - val_rmse: 20504.5586\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 897651264.0000 - rmse: 29960.8281 - val_loss: 436600352.0000 - val_rmse: 20894.9805\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 888717440.0000 - rmse: 29811.3633 - val_loss: 637243904.0000 - val_rmse: 25243.6875\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 839857344.0000 - rmse: 28980.2910 - val_loss: 394069696.0000 - val_rmse: 19851.1855\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 916486528.0000 - rmse: 30273.5273 - val_loss: 418049120.0000 - val_rmse: 20446.2480\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 828849536.0000 - rmse: 28789.7461 - val_loss: 591793024.0000 - val_rmse: 24326.7930\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 887757056.0000 - rmse: 29795.2480 - val_loss: 407760512.0000 - val_rmse: 20193.0781\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 864100032.0000 - rmse: 29395.5781 - val_loss: 429106464.0000 - val_rmse: 20714.8828\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 909093184.0000 - rmse: 30151.1660 - val_loss: 399456736.0000 - val_rmse: 19986.4102\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 837942208.0000 - rmse: 28947.2305 - val_loss: 381997408.0000 - val_rmse: 19544.7520\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 825768512.0000 - rmse: 28736.1855 - val_loss: 436473504.0000 - val_rmse: 20891.9453\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 818681280.0000 - rmse: 28612.6055 - val_loss: 423898240.0000 - val_rmse: 20588.7891\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 886311808.0000 - rmse: 29770.9883 - val_loss: 393563040.0000 - val_rmse: 19838.4219\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 811873728.0000 - rmse: 28493.3965 - val_loss: 414707232.0000 - val_rmse: 20364.3555\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 770109504.0000 - rmse: 27750.8438 - val_loss: 347914368.0000 - val_rmse: 18652.4609\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 765576960.0000 - rmse: 27669.0586 - val_loss: 369820736.0000 - val_rmse: 19230.7207\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 766534464.0000 - rmse: 27686.3574 - val_loss: 361866304.0000 - val_rmse: 19022.7812\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 825639296.0000 - rmse: 28733.9375 - val_loss: 362482080.0000 - val_rmse: 19038.9590\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 821338368.0000 - rmse: 28659.0000 - val_loss: 361633248.0000 - val_rmse: 19016.6562\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 757483200.0000 - rmse: 27522.4121 - val_loss: 471850944.0000 - val_rmse: 21722.1289\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 844757184.0000 - rmse: 29064.7070 - val_loss: 539392704.0000 - val_rmse: 23224.8262\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 808815552.0000 - rmse: 28439.6797 - val_loss: 356599136.0000 - val_rmse: 18883.8301\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 779470912.0000 - rmse: 27919.0039 - val_loss: 355647392.0000 - val_rmse: 18858.6133\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687238080.0000 - rmse: 26215.2246 - val_loss: 360072512.0000 - val_rmse: 18975.5742\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 741834112.0000 - rmse: 27236.6289 - val_loss: 347555936.0000 - val_rmse: 18642.8457\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 809440128.0000 - rmse: 28450.6582 - val_loss: 376837024.0000 - val_rmse: 19412.2871\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691138624.0000 - rmse: 26289.5117 - val_loss: 408053952.0000 - val_rmse: 20200.3438\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733672640.0000 - rmse: 27086.3887 - val_loss: 350520480.0000 - val_rmse: 18722.1895\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 753380224.0000 - rmse: 27447.7695 - val_loss: 341870048.0000 - val_rmse: 18489.7246\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 705254848.0000 - rmse: 26556.6328 - val_loss: 555248320.0000 - val_rmse: 23563.7070\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 804265216.0000 - rmse: 28359.5684 - val_loss: 307986304.0000 - val_rmse: 17549.5332\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 710112512.0000 - rmse: 26647.9355 - val_loss: 368048960.0000 - val_rmse: 19184.5996\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698517824.0000 - rmse: 26429.4863 - val_loss: 444864000.0000 - val_rmse: 21091.7949\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718116800.0000 - rmse: 26797.6992 - val_loss: 350988736.0000 - val_rmse: 18734.6914\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661942784.0000 - rmse: 25728.2441 - val_loss: 330173632.0000 - val_rmse: 18170.6777\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 660466496.0000 - rmse: 25699.5391 - val_loss: 341279968.0000 - val_rmse: 18473.7598\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698860672.0000 - rmse: 26435.9707 - val_loss: 312466752.0000 - val_rmse: 17676.7246\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 763581056.0000 - rmse: 27632.9648 - val_loss: 307391168.0000 - val_rmse: 17532.5703\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692321344.0000 - rmse: 26311.9980 - val_loss: 313751936.0000 - val_rmse: 17713.0391\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656782336.0000 - rmse: 25627.7617 - val_loss: 322316608.0000 - val_rmse: 17953.1738\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 650487488.0000 - rmse: 25504.6543 - val_loss: 349070816.0000 - val_rmse: 18683.4336\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721131008.0000 - rmse: 26853.8828 - val_loss: 318810464.0000 - val_rmse: 17855.2598\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646819776.0000 - rmse: 25432.6484 - val_loss: 434526048.0000 - val_rmse: 20845.2852\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607599936.0000 - rmse: 24649.5391 - val_loss: 337223520.0000 - val_rmse: 18363.6426\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 653827712.0000 - rmse: 25570.0508 - val_loss: 333156480.0000 - val_rmse: 18252.5703\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 664761792.0000 - rmse: 25782.9707 - val_loss: 290803296.0000 - val_rmse: 17052.9531\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 631574848.0000 - rmse: 25131.1504 - val_loss: 293599328.0000 - val_rmse: 17134.7363\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691084288.0000 - rmse: 26288.4785 - val_loss: 344286560.0000 - val_rmse: 18554.9570\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 613531264.0000 - rmse: 24769.5605 - val_loss: 310956288.0000 - val_rmse: 17633.9492\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591618560.0000 - rmse: 24323.2070 - val_loss: 303049376.0000 - val_rmse: 17408.3105\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 697626304.0000 - rmse: 26412.6152 - val_loss: 465388480.0000 - val_rmse: 21572.8594\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685873472.0000 - rmse: 26189.1855 - val_loss: 304315872.0000 - val_rmse: 17444.6484\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647135680.0000 - rmse: 25438.8574 - val_loss: 294133376.0000 - val_rmse: 17150.3125\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 707629952.0000 - rmse: 26601.3145 - val_loss: 328861440.0000 - val_rmse: 18134.5352\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632884480.0000 - rmse: 25157.1914 - val_loss: 322136832.0000 - val_rmse: 17948.1641\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 734724864.0000 - rmse: 27105.8027 - val_loss: 487990880.0000 - val_rmse: 22090.5098\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597963456.0000 - rmse: 24453.2891 - val_loss: 342716640.0000 - val_rmse: 18512.6016\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632145600.0000 - rmse: 25142.5000 - val_loss: 282487808.0000 - val_rmse: 16807.3691\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630856256.0000 - rmse: 25116.8438 - val_loss: 325191328.0000 - val_rmse: 18033.0586\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652205824.0000 - rmse: 25538.3164 - val_loss: 456380320.0000 - val_rmse: 21363.0566\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622770688.0000 - rmse: 24955.3711 - val_loss: 392737088.0000 - val_rmse: 19817.5918\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 655398784.0000 - rmse: 25600.7559 - val_loss: 287025216.0000 - val_rmse: 16941.8145\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574607872.0000 - rmse: 23970.9766 - val_loss: 433443712.0000 - val_rmse: 20819.3066\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637234240.0000 - rmse: 25243.4961 - val_loss: 626748736.0000 - val_rmse: 25034.9473\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575854016.0000 - rmse: 23996.9551 - val_loss: 368923904.0000 - val_rmse: 19207.3867\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569847232.0000 - rmse: 23871.4688 - val_loss: 278647456.0000 - val_rmse: 16692.7305\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 635469632.0000 - rmse: 25208.5195 - val_loss: 313210400.0000 - val_rmse: 17697.7461\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583859584.0000 - rmse: 24163.1797 - val_loss: 297420896.0000 - val_rmse: 17245.8887\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648487040.0000 - rmse: 25465.4062 - val_loss: 261139072.0000 - val_rmse: 16159.7910\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524303264.0000 - rmse: 22897.6641 - val_loss: 331306080.0000 - val_rmse: 18201.8086\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554536192.0000 - rmse: 23548.5859 - val_loss: 302948736.0000 - val_rmse: 17405.4180\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594839040.0000 - rmse: 24389.3203 - val_loss: 335606464.0000 - val_rmse: 18319.5605\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 674484352.0000 - rmse: 25970.8320 - val_loss: 267859648.0000 - val_rmse: 16366.4131\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543139776.0000 - rmse: 23305.3535 - val_loss: 249115264.0000 - val_rmse: 15783.3779\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614254272.0000 - rmse: 24784.1504 - val_loss: 267425344.0000 - val_rmse: 16353.1396\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562023040.0000 - rmse: 23707.0215 - val_loss: 304458720.0000 - val_rmse: 17448.7363\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570495552.0000 - rmse: 23885.0469 - val_loss: 266728512.0000 - val_rmse: 16331.8223\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594314752.0000 - rmse: 24378.5664 - val_loss: 275914176.0000 - val_rmse: 16610.6582\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580208128.0000 - rmse: 24087.5078 - val_loss: 247333280.0000 - val_rmse: 15726.8271\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565341760.0000 - rmse: 23776.9121 - val_loss: 280001088.0000 - val_rmse: 16733.2246\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546057920.0000 - rmse: 23367.8789 - val_loss: 245742640.0000 - val_rmse: 15676.1748\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559856576.0000 - rmse: 23661.2793 - val_loss: 241542960.0000 - val_rmse: 15541.6465\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565294016.0000 - rmse: 23775.9062 - val_loss: 291825312.0000 - val_rmse: 17082.8887\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519276352.0000 - rmse: 22787.6309 - val_loss: 404659008.0000 - val_rmse: 20116.1348\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507907808.0000 - rmse: 22536.8066 - val_loss: 317944192.0000 - val_rmse: 17830.9844\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596132160.0000 - rmse: 24415.8105 - val_loss: 263397600.0000 - val_rmse: 16229.5234\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611391872.0000 - rmse: 24726.3379 - val_loss: 311135616.0000 - val_rmse: 17639.0332\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496555808.0000 - rmse: 22283.5293 - val_loss: 296783488.0000 - val_rmse: 17227.4004\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588642560.0000 - rmse: 24261.9512 - val_loss: 259948352.0000 - val_rmse: 16122.9053\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516918400.0000 - rmse: 22735.8359 - val_loss: 291117440.0000 - val_rmse: 17062.1582\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582203712.0000 - rmse: 24128.8945 - val_loss: 387211040.0000 - val_rmse: 19677.6758\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526589568.0000 - rmse: 22947.5352 - val_loss: 311247456.0000 - val_rmse: 17642.1992\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508489568.0000 - rmse: 22549.7109 - val_loss: 280047456.0000 - val_rmse: 16734.6133\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538603712.0000 - rmse: 23207.8340 - val_loss: 310098304.0000 - val_rmse: 17609.6035\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532690880.0000 - rmse: 23080.0938 - val_loss: 334236800.0000 - val_rmse: 18282.1387\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573001344.0000 - rmse: 23937.4414 - val_loss: 559977024.0000 - val_rmse: 23663.8301\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544139008.0000 - rmse: 23326.7812 - val_loss: 357026592.0000 - val_rmse: 18895.1406\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468377600.0000 - rmse: 21642.0312 - val_loss: 327530176.0000 - val_rmse: 18097.7891\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540531648.0000 - rmse: 23249.3301 - val_loss: 371083296.0000 - val_rmse: 19263.5176\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498673312.0000 - rmse: 22330.9902 - val_loss: 368881472.0000 - val_rmse: 19206.2812\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473295744.0000 - rmse: 21755.3574 - val_loss: 261389808.0000 - val_rmse: 16167.5459\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463751648.0000 - rmse: 21534.8867 - val_loss: 249996240.0000 - val_rmse: 15811.2607\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577944960.0000 - rmse: 24040.4805 - val_loss: 378430400.0000 - val_rmse: 19453.2793\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512960928.0000 - rmse: 22648.6387 - val_loss: 292679744.0000 - val_rmse: 17107.8789\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505405728.0000 - rmse: 22481.2227 - val_loss: 306245600.0000 - val_rmse: 17499.8672\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524861344.0000 - rmse: 22909.8457 - val_loss: 304432640.0000 - val_rmse: 17447.9922\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628716864.0000 - rmse: 25074.2227 - val_loss: 285739232.0000 - val_rmse: 16903.8164\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426031840.0000 - rmse: 20640.5332 - val_loss: 242200192.0000 - val_rmse: 15562.7764\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480111072.0000 - rmse: 21911.4336 - val_loss: 370290720.0000 - val_rmse: 19242.9316\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521572736.0000 - rmse: 22837.9609 - val_loss: 382349952.0000 - val_rmse: 19553.7637\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487743712.0000 - rmse: 22084.9160 - val_loss: 296914176.0000 - val_rmse: 17231.1914\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416792320.0000 - rmse: 20415.4863 - val_loss: 465211456.0000 - val_rmse: 21568.7559\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454988000.0000 - rmse: 21330.4434 - val_loss: 249583824.0000 - val_rmse: 15798.2158\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467723072.0000 - rmse: 21626.9004 - val_loss: 227333232.0000 - val_rmse: 15077.5645\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484013440.0000 - rmse: 22000.3008 - val_loss: 272650048.0000 - val_rmse: 16512.1113\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498031424.0000 - rmse: 22316.6133 - val_loss: 309651264.0000 - val_rmse: 17596.9023\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488002688.0000 - rmse: 22090.7793 - val_loss: 320313600.0000 - val_rmse: 17897.3008\n",
      "104/104 [==============================] - 0s 665us/step - loss: 434629920.0000 - rmse: 20847.7754\n",
      "[434629920.0, 20847.775390625]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,417\n",
      "Trainable params: 20,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,417\n",
      "Trainable params: 20,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 11161976832.0000 - rmse: 105650.2578 - val_loss: 1754373248.0000 - val_rmse: 41885.2383\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1848313088.0000 - rmse: 42992.0117 - val_loss: 1233197440.0000 - val_rmse: 35116.9102\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1589637248.0000 - rmse: 39870.2539 - val_loss: 1200761472.0000 - val_rmse: 34652.0039\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1503551104.0000 - rmse: 38775.6523 - val_loss: 1111312896.0000 - val_rmse: 33336.3594\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1365632768.0000 - rmse: 36954.4688 - val_loss: 1025493376.0000 - val_rmse: 32023.3242\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1274511872.0000 - rmse: 35700.3047 - val_loss: 976041216.0000 - val_rmse: 31241.6562\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1263317632.0000 - rmse: 35543.1797 - val_loss: 1022229120.0000 - val_rmse: 31972.3164\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1143006080.0000 - rmse: 33808.3711 - val_loss: 918687360.0000 - val_rmse: 30309.8555\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1204599680.0000 - rmse: 34707.3398 - val_loss: 862052800.0000 - val_rmse: 29360.7344\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1157829376.0000 - rmse: 34026.8906 - val_loss: 801779392.0000 - val_rmse: 28315.7090\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1079697536.0000 - rmse: 32858.7500 - val_loss: 781071232.0000 - val_rmse: 27947.6523\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1058974336.0000 - rmse: 32541.8867 - val_loss: 842397504.0000 - val_rmse: 29024.0840\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1038214272.0000 - rmse: 32221.3320 - val_loss: 740888768.0000 - val_rmse: 27219.2695\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1057596736.0000 - rmse: 32520.7129 - val_loss: 706579584.0000 - val_rmse: 26581.5645\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 950297216.0000 - rmse: 30826.8906 - val_loss: 697703296.0000 - val_rmse: 26414.0723\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 960451200.0000 - rmse: 30991.1465 - val_loss: 661279232.0000 - val_rmse: 25715.3477\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 918248128.0000 - rmse: 30302.6074 - val_loss: 681872448.0000 - val_rmse: 26112.6855\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 913564224.0000 - rmse: 30225.2246 - val_loss: 639339072.0000 - val_rmse: 25285.1543\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 922418176.0000 - rmse: 30371.3359 - val_loss: 645267584.0000 - val_rmse: 25402.1172\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 904123008.0000 - rmse: 30068.6367 - val_loss: 619078208.0000 - val_rmse: 24881.2793\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 873623232.0000 - rmse: 29557.1152 - val_loss: 715632512.0000 - val_rmse: 26751.3086\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 894436416.0000 - rmse: 29907.1289 - val_loss: 599856320.0000 - val_rmse: 24491.9609\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 804167488.0000 - rmse: 28357.8457 - val_loss: 637071488.0000 - val_rmse: 25240.2734\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 837590016.0000 - rmse: 28941.1445 - val_loss: 585086528.0000 - val_rmse: 24188.5625\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 833443968.0000 - rmse: 28869.4277 - val_loss: 587805440.0000 - val_rmse: 24244.6973\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782543616.0000 - rmse: 27973.9785 - val_loss: 548887040.0000 - val_rmse: 23428.3379\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 747033792.0000 - rmse: 27331.9180 - val_loss: 582743040.0000 - val_rmse: 24140.0703\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 756422912.0000 - rmse: 27503.1387 - val_loss: 635970304.0000 - val_rmse: 25218.4492\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751791744.0000 - rmse: 27418.8203 - val_loss: 569770496.0000 - val_rmse: 23869.8652\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 693806400.0000 - rmse: 26340.1992 - val_loss: 533438816.0000 - val_rmse: 23096.2910\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 735963520.0000 - rmse: 27128.6465 - val_loss: 562228608.0000 - val_rmse: 23711.3594\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 748956096.0000 - rmse: 27367.0605 - val_loss: 524476576.0000 - val_rmse: 22901.4512\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640556608.0000 - rmse: 25309.2168 - val_loss: 486808416.0000 - val_rmse: 22063.7324\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 682321664.0000 - rmse: 26121.2832 - val_loss: 535485856.0000 - val_rmse: 23140.5645\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 731111424.0000 - rmse: 27039.0684 - val_loss: 582928448.0000 - val_rmse: 24143.9082\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718351680.0000 - rmse: 26802.0781 - val_loss: 498903488.0000 - val_rmse: 22336.1426\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612004928.0000 - rmse: 24738.7305 - val_loss: 480858592.0000 - val_rmse: 21928.4863\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641567168.0000 - rmse: 25329.1738 - val_loss: 467295904.0000 - val_rmse: 21617.0254\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618510976.0000 - rmse: 24869.8789 - val_loss: 481298368.0000 - val_rmse: 21938.5117\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646030144.0000 - rmse: 25417.1211 - val_loss: 467231744.0000 - val_rmse: 21615.5430\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615393152.0000 - rmse: 24807.1172 - val_loss: 594101376.0000 - val_rmse: 24374.1914\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651650240.0000 - rmse: 25527.4355 - val_loss: 441937152.0000 - val_rmse: 21022.2988\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676094976.0000 - rmse: 26001.8242 - val_loss: 466695456.0000 - val_rmse: 21603.1348\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562760000.0000 - rmse: 23722.5625 - val_loss: 465358272.0000 - val_rmse: 21572.1621\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579187584.0000 - rmse: 24066.3145 - val_loss: 457960768.0000 - val_rmse: 21400.0156\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 595622464.0000 - rmse: 24405.3770 - val_loss: 464718112.0000 - val_rmse: 21557.3184\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597103488.0000 - rmse: 24435.6953 - val_loss: 436448288.0000 - val_rmse: 20891.3418\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593137216.0000 - rmse: 24354.4043 - val_loss: 439739488.0000 - val_rmse: 20969.9648\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558398080.0000 - rmse: 23630.4453 - val_loss: 476352960.0000 - val_rmse: 21825.5078\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 595238592.0000 - rmse: 24397.5098 - val_loss: 502711904.0000 - val_rmse: 22421.2363\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606927744.0000 - rmse: 24635.9004 - val_loss: 441001824.0000 - val_rmse: 21000.0410\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514514400.0000 - rmse: 22682.9062 - val_loss: 448382208.0000 - val_rmse: 21175.0352\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535138880.0000 - rmse: 23133.0664 - val_loss: 440356992.0000 - val_rmse: 20984.6836\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573298176.0000 - rmse: 23943.6426 - val_loss: 461341952.0000 - val_rmse: 21478.8691\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545200960.0000 - rmse: 23349.5352 - val_loss: 443220896.0000 - val_rmse: 21052.8086\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551566464.0000 - rmse: 23485.4512 - val_loss: 398479776.0000 - val_rmse: 19961.9551\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540697984.0000 - rmse: 23252.9102 - val_loss: 407733088.0000 - val_rmse: 20192.3984\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493993184.0000 - rmse: 22225.9551 - val_loss: 436135136.0000 - val_rmse: 20883.8477\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500799680.0000 - rmse: 22378.5488 - val_loss: 392128480.0000 - val_rmse: 19802.2305\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530705696.0000 - rmse: 23037.0469 - val_loss: 453266048.0000 - val_rmse: 21290.0410\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504781344.0000 - rmse: 22467.3359 - val_loss: 426485792.0000 - val_rmse: 20651.5273\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566622592.0000 - rmse: 23803.8340 - val_loss: 457999744.0000 - val_rmse: 21400.9258\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526872256.0000 - rmse: 22953.6953 - val_loss: 415170880.0000 - val_rmse: 20375.7383\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500806496.0000 - rmse: 22378.7051 - val_loss: 427292064.0000 - val_rmse: 20671.0410\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501318112.0000 - rmse: 22390.1328 - val_loss: 531181344.0000 - val_rmse: 23047.3691\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468975904.0000 - rmse: 21655.8477 - val_loss: 466666432.0000 - val_rmse: 21602.4590\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482018208.0000 - rmse: 21954.9102 - val_loss: 372879296.0000 - val_rmse: 19310.0801\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501533216.0000 - rmse: 22394.9336 - val_loss: 429444992.0000 - val_rmse: 20723.0508\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483382080.0000 - rmse: 21985.9473 - val_loss: 403082528.0000 - val_rmse: 20076.9121\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486843648.0000 - rmse: 22064.5293 - val_loss: 408922560.0000 - val_rmse: 20221.8281\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450775872.0000 - rmse: 21231.4785 - val_loss: 413575712.0000 - val_rmse: 20336.5586\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467274144.0000 - rmse: 21616.5215 - val_loss: 419919872.0000 - val_rmse: 20491.9434\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496779872.0000 - rmse: 22288.5566 - val_loss: 415575552.0000 - val_rmse: 20385.6660\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491008512.0000 - rmse: 22158.7090 - val_loss: 378408064.0000 - val_rmse: 19452.7070\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451196288.0000 - rmse: 21241.3770 - val_loss: 395287904.0000 - val_rmse: 19881.8438\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484162528.0000 - rmse: 22003.6934 - val_loss: 379601856.0000 - val_rmse: 19483.3672\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490536352.0000 - rmse: 22148.0527 - val_loss: 417917568.0000 - val_rmse: 20443.0273\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455914656.0000 - rmse: 21352.1523 - val_loss: 362015040.0000 - val_rmse: 19026.6895\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462048256.0000 - rmse: 21495.3047 - val_loss: 489941600.0000 - val_rmse: 22134.6211\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471811104.0000 - rmse: 21721.2090 - val_loss: 470502976.0000 - val_rmse: 21691.0762\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466195296.0000 - rmse: 21591.5508 - val_loss: 425660480.0000 - val_rmse: 20631.5391\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477692896.0000 - rmse: 21856.1836 - val_loss: 386513408.0000 - val_rmse: 19659.9375\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468243008.0000 - rmse: 21638.9199 - val_loss: 394794656.0000 - val_rmse: 19869.4375\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433445280.0000 - rmse: 20819.3457 - val_loss: 344922720.0000 - val_rmse: 18572.0918\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435918912.0000 - rmse: 20878.6660 - val_loss: 382679136.0000 - val_rmse: 19562.1836\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475403488.0000 - rmse: 21803.7480 - val_loss: 390669312.0000 - val_rmse: 19765.3516\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459262304.0000 - rmse: 21430.4023 - val_loss: 370807232.0000 - val_rmse: 19256.3516\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446430944.0000 - rmse: 21128.9082 - val_loss: 376031008.0000 - val_rmse: 19391.5137\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419148000.0000 - rmse: 20473.1016 - val_loss: 432757568.0000 - val_rmse: 20802.8223\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424995680.0000 - rmse: 20615.4199 - val_loss: 385752512.0000 - val_rmse: 19640.5801\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465946592.0000 - rmse: 21585.7930 - val_loss: 341973376.0000 - val_rmse: 18492.5195\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434105728.0000 - rmse: 20835.1992 - val_loss: 364103488.0000 - val_rmse: 19081.4922\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427373152.0000 - rmse: 20673.0000 - val_loss: 377864224.0000 - val_rmse: 19438.7246\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426851776.0000 - rmse: 20660.3867 - val_loss: 386845984.0000 - val_rmse: 19668.3984\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429507744.0000 - rmse: 20724.5625 - val_loss: 459726560.0000 - val_rmse: 21441.2324\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509365888.0000 - rmse: 22569.1309 - val_loss: 348855744.0000 - val_rmse: 18677.6777\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396948896.0000 - rmse: 19923.5723 - val_loss: 400442912.0000 - val_rmse: 20011.0664\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410031168.0000 - rmse: 20249.2227 - val_loss: 394509312.0000 - val_rmse: 19862.2559\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396745696.0000 - rmse: 19918.4746 - val_loss: 351832128.0000 - val_rmse: 18757.1836\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434602112.0000 - rmse: 20847.1074 - val_loss: 390031360.0000 - val_rmse: 19749.2090\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406489600.0000 - rmse: 20161.5840 - val_loss: 380049984.0000 - val_rmse: 19494.8691\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450518368.0000 - rmse: 21225.4141 - val_loss: 348462368.0000 - val_rmse: 18667.1445\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405007968.0000 - rmse: 20124.8066 - val_loss: 354880480.0000 - val_rmse: 18838.2695\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429883136.0000 - rmse: 20733.6211 - val_loss: 363742656.0000 - val_rmse: 19072.0352\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410831392.0000 - rmse: 20268.9727 - val_loss: 429387424.0000 - val_rmse: 20721.6641\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437675328.0000 - rmse: 20920.6875 - val_loss: 376046176.0000 - val_rmse: 19391.9062\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406962720.0000 - rmse: 20173.3105 - val_loss: 339368032.0000 - val_rmse: 18421.9414\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445986496.0000 - rmse: 21118.3887 - val_loss: 355058464.0000 - val_rmse: 18842.9922\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388072544.0000 - rmse: 19699.5527 - val_loss: 405142112.0000 - val_rmse: 20128.1367\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422085408.0000 - rmse: 20544.7129 - val_loss: 381247424.0000 - val_rmse: 19525.5547\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393433920.0000 - rmse: 19835.1641 - val_loss: 353224320.0000 - val_rmse: 18794.2598\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402103328.0000 - rmse: 20052.5098 - val_loss: 320115424.0000 - val_rmse: 17891.7656\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381816192.0000 - rmse: 19540.1133 - val_loss: 368839904.0000 - val_rmse: 19205.1992\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384863200.0000 - rmse: 19617.9277 - val_loss: 358119744.0000 - val_rmse: 18924.0488\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397487136.0000 - rmse: 19937.0762 - val_loss: 428122464.0000 - val_rmse: 20691.1172\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405664320.0000 - rmse: 20141.1055 - val_loss: 328754656.0000 - val_rmse: 18131.5859\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421408640.0000 - rmse: 20528.2383 - val_loss: 350480096.0000 - val_rmse: 18721.1094\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412221824.0000 - rmse: 20303.2441 - val_loss: 409610944.0000 - val_rmse: 20238.8438\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398553920.0000 - rmse: 19963.8105 - val_loss: 405758080.0000 - val_rmse: 20143.4336\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399791392.0000 - rmse: 19994.7812 - val_loss: 329147072.0000 - val_rmse: 18142.4082\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399332576.0000 - rmse: 19983.3047 - val_loss: 338363296.0000 - val_rmse: 18394.6504\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412610176.0000 - rmse: 20312.8066 - val_loss: 328037696.0000 - val_rmse: 18111.8047\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378367520.0000 - rmse: 19451.6699 - val_loss: 360914592.0000 - val_rmse: 18997.7480\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347419264.0000 - rmse: 18639.1836 - val_loss: 321985728.0000 - val_rmse: 17943.9551\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366482464.0000 - rmse: 19143.7305 - val_loss: 454158656.0000 - val_rmse: 21310.9941\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413558208.0000 - rmse: 20336.1270 - val_loss: 332674944.0000 - val_rmse: 18239.3730\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349666752.0000 - rmse: 18699.3750 - val_loss: 340455264.0000 - val_rmse: 18451.4238\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423286304.0000 - rmse: 20573.9199 - val_loss: 367899744.0000 - val_rmse: 19180.7090\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414920608.0000 - rmse: 20369.5977 - val_loss: 334885568.0000 - val_rmse: 18299.8750\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382223776.0000 - rmse: 19550.5391 - val_loss: 502065312.0000 - val_rmse: 22406.8105\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383045664.0000 - rmse: 19571.5469 - val_loss: 354088768.0000 - val_rmse: 18817.2422\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351667040.0000 - rmse: 18752.7832 - val_loss: 342814016.0000 - val_rmse: 18515.2305\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389183040.0000 - rmse: 19727.7188 - val_loss: 355192672.0000 - val_rmse: 18846.5508\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447341728.0000 - rmse: 21150.4492 - val_loss: 428137824.0000 - val_rmse: 20691.4883\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406164224.0000 - rmse: 20153.5117 - val_loss: 390575264.0000 - val_rmse: 19762.9727\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388392128.0000 - rmse: 19707.6641 - val_loss: 343625472.0000 - val_rmse: 18537.1328\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391866624.0000 - rmse: 19795.6172 - val_loss: 325924160.0000 - val_rmse: 18053.3652\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345613888.0000 - rmse: 18590.6875 - val_loss: 410676992.0000 - val_rmse: 20265.1641\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396445824.0000 - rmse: 19910.9434 - val_loss: 326914976.0000 - val_rmse: 18080.7852\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370152672.0000 - rmse: 19239.3477 - val_loss: 391444032.0000 - val_rmse: 19784.9414\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349111552.0000 - rmse: 18684.5254 - val_loss: 342521728.0000 - val_rmse: 18507.3379\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392025280.0000 - rmse: 19799.6230 - val_loss: 356481728.0000 - val_rmse: 18880.7188\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338802176.0000 - rmse: 18406.5742 - val_loss: 490846880.0000 - val_rmse: 22155.0605\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385316800.0000 - rmse: 19629.4824 - val_loss: 368873792.0000 - val_rmse: 19206.0840\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370390336.0000 - rmse: 19245.5234 - val_loss: 319063968.0000 - val_rmse: 17862.3555\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362817216.0000 - rmse: 19047.7559 - val_loss: 310360256.0000 - val_rmse: 17617.0410\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336656544.0000 - rmse: 18348.1992 - val_loss: 372084768.0000 - val_rmse: 19289.4961\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350330848.0000 - rmse: 18717.1250 - val_loss: 353103584.0000 - val_rmse: 18791.0449\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364771040.0000 - rmse: 19098.9746 - val_loss: 313017408.0000 - val_rmse: 17692.2910\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349821600.0000 - rmse: 18703.5117 - val_loss: 583124864.0000 - val_rmse: 24147.9727\n",
      "104/104 [==============================] - 0s 660us/step - loss: 885270464.0000 - rmse: 29753.4941\n",
      "[885270464.0, 29753.494140625]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,417\n",
      "Trainable params: 20,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,417\n",
      "Trainable params: 20,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 9504575488.0000 - rmse: 97491.4141 - val_loss: 1599445376.0000 - val_rmse: 39993.0664\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1666107648.0000 - rmse: 40817.9805 - val_loss: 1231893888.0000 - val_rmse: 35098.3477\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1423536512.0000 - rmse: 37729.7812 - val_loss: 1217693184.0000 - val_rmse: 34895.4609\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1349811840.0000 - rmse: 36739.7852 - val_loss: 1057364480.0000 - val_rmse: 32517.1406\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1220109312.0000 - rmse: 34930.0625 - val_loss: 1006800000.0000 - val_rmse: 31730.1113\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1157719808.0000 - rmse: 34025.2812 - val_loss: 856686016.0000 - val_rmse: 29269.1992\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1133494912.0000 - rmse: 33667.4180 - val_loss: 834059840.0000 - val_rmse: 28880.0938\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1060700672.0000 - rmse: 32568.3965 - val_loss: 819241856.0000 - val_rmse: 28622.4004\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1010816704.0000 - rmse: 31793.3418 - val_loss: 750664704.0000 - val_rmse: 27398.2598\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 975936640.0000 - rmse: 31239.9844 - val_loss: 811520192.0000 - val_rmse: 28487.1895\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 934752640.0000 - rmse: 30573.7246 - val_loss: 702187584.0000 - val_rmse: 26498.8223\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 995079168.0000 - rmse: 31544.8750 - val_loss: 651815552.0000 - val_rmse: 25530.6777\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 963801472.0000 - rmse: 31045.1523 - val_loss: 654606016.0000 - val_rmse: 25585.2676\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 893682368.0000 - rmse: 29894.5195 - val_loss: 647869760.0000 - val_rmse: 25453.2852\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 867370112.0000 - rmse: 29451.1484 - val_loss: 841746048.0000 - val_rmse: 29012.8594\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 928545536.0000 - rmse: 30472.0449 - val_loss: 689713728.0000 - val_rmse: 26262.3984\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 898344192.0000 - rmse: 29972.3906 - val_loss: 623637888.0000 - val_rmse: 24972.7422\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 843350144.0000 - rmse: 29040.4883 - val_loss: 593406848.0000 - val_rmse: 24359.9414\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 862098688.0000 - rmse: 29361.5156 - val_loss: 585169024.0000 - val_rmse: 24190.2617\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 796167168.0000 - rmse: 28216.4336 - val_loss: 1139521664.0000 - val_rmse: 33756.8008\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 785484416.0000 - rmse: 28026.4941 - val_loss: 617092992.0000 - val_rmse: 24841.3555\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 789152256.0000 - rmse: 28091.8535 - val_loss: 554751232.0000 - val_rmse: 23553.1562\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 803797568.0000 - rmse: 28351.3223 - val_loss: 793553344.0000 - val_rmse: 28170.0762\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 744615872.0000 - rmse: 27287.6484 - val_loss: 579640064.0000 - val_rmse: 24075.7148\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 826846144.0000 - rmse: 28754.9297 - val_loss: 759407104.0000 - val_rmse: 27557.3398\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 757792576.0000 - rmse: 27528.0312 - val_loss: 507862592.0000 - val_rmse: 22535.8047\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 712475968.0000 - rmse: 26692.2441 - val_loss: 570909312.0000 - val_rmse: 23893.7051\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 728770112.0000 - rmse: 26995.7383 - val_loss: 757844224.0000 - val_rmse: 27528.9688\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724225472.0000 - rmse: 26911.4336 - val_loss: 488964608.0000 - val_rmse: 22112.5410\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 696555968.0000 - rmse: 26392.3457 - val_loss: 531097088.0000 - val_rmse: 23045.5410\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692174528.0000 - rmse: 26309.2090 - val_loss: 476920160.0000 - val_rmse: 21838.5000\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666784768.0000 - rmse: 25822.1738 - val_loss: 484264736.0000 - val_rmse: 22006.0137\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 714448512.0000 - rmse: 26729.1680 - val_loss: 546775808.0000 - val_rmse: 23383.2363\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 693976640.0000 - rmse: 26343.4336 - val_loss: 514694848.0000 - val_rmse: 22686.8848\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 625767616.0000 - rmse: 25015.3477 - val_loss: 431600608.0000 - val_rmse: 20774.9941\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 664800960.0000 - rmse: 25783.7285 - val_loss: 430344128.0000 - val_rmse: 20744.7344\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 645786560.0000 - rmse: 25412.3301 - val_loss: 459630592.0000 - val_rmse: 21438.9941\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684346048.0000 - rmse: 26160.0059 - val_loss: 493428064.0000 - val_rmse: 22213.2383\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 653977344.0000 - rmse: 25572.9785 - val_loss: 442929952.0000 - val_rmse: 21045.8965\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677939584.0000 - rmse: 26037.2715 - val_loss: 475076160.0000 - val_rmse: 21796.2402\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637354688.0000 - rmse: 25245.8828 - val_loss: 701563520.0000 - val_rmse: 26487.0430\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 657879168.0000 - rmse: 25649.1543 - val_loss: 413070048.0000 - val_rmse: 20324.1230\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626479616.0000 - rmse: 25029.5703 - val_loss: 411339968.0000 - val_rmse: 20281.5156\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611912960.0000 - rmse: 24736.8711 - val_loss: 425513952.0000 - val_rmse: 20627.9863\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654874688.0000 - rmse: 25590.5176 - val_loss: 411703872.0000 - val_rmse: 20290.4844\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 631968064.0000 - rmse: 25138.9727 - val_loss: 425144256.0000 - val_rmse: 20619.0234\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661811584.0000 - rmse: 25725.6934 - val_loss: 420019104.0000 - val_rmse: 20494.3652\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610053312.0000 - rmse: 24699.2578 - val_loss: 797219392.0000 - val_rmse: 28235.0742\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626752000.0000 - rmse: 25035.0117 - val_loss: 387345696.0000 - val_rmse: 19681.0977\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612489408.0000 - rmse: 24748.5215 - val_loss: 377508992.0000 - val_rmse: 19429.5859\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 639884096.0000 - rmse: 25295.9277 - val_loss: 451656768.0000 - val_rmse: 21252.2148\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604040640.0000 - rmse: 24577.2363 - val_loss: 382161952.0000 - val_rmse: 19548.9609\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568761216.0000 - rmse: 23848.7148 - val_loss: 445327200.0000 - val_rmse: 21102.7734\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547131712.0000 - rmse: 23390.8457 - val_loss: 384173152.0000 - val_rmse: 19600.3340\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 613957376.0000 - rmse: 24778.1602 - val_loss: 447779712.0000 - val_rmse: 21160.8047\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601805760.0000 - rmse: 24531.7285 - val_loss: 366559936.0000 - val_rmse: 19145.7500\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572288128.0000 - rmse: 23922.5430 - val_loss: 363017280.0000 - val_rmse: 19053.0098\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585581376.0000 - rmse: 24198.7871 - val_loss: 387622496.0000 - val_rmse: 19688.1289\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570976768.0000 - rmse: 23895.1211 - val_loss: 400365568.0000 - val_rmse: 20009.1328\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588223296.0000 - rmse: 24253.3105 - val_loss: 406922112.0000 - val_rmse: 20172.3066\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577933376.0000 - rmse: 24040.2441 - val_loss: 366793280.0000 - val_rmse: 19151.8438\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571743552.0000 - rmse: 23911.1562 - val_loss: 356166528.0000 - val_rmse: 18872.3730\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560960768.0000 - rmse: 23684.6094 - val_loss: 385528608.0000 - val_rmse: 19634.8770\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563641728.0000 - rmse: 23741.1367 - val_loss: 363604896.0000 - val_rmse: 19068.4238\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550958144.0000 - rmse: 23472.4941 - val_loss: 372502016.0000 - val_rmse: 19300.3086\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572547264.0000 - rmse: 23927.9551 - val_loss: 403720576.0000 - val_rmse: 20092.7969\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615071424.0000 - rmse: 24800.6309 - val_loss: 442543360.0000 - val_rmse: 21036.7109\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570078016.0000 - rmse: 23876.3047 - val_loss: 374309312.0000 - val_rmse: 19347.0742\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549889088.0000 - rmse: 23449.7129 - val_loss: 372917632.0000 - val_rmse: 19311.0742\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541199488.0000 - rmse: 23263.6934 - val_loss: 348366976.0000 - val_rmse: 18664.5859\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527228288.0000 - rmse: 22961.4512 - val_loss: 392054112.0000 - val_rmse: 19800.3516\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604264832.0000 - rmse: 24581.7969 - val_loss: 327522752.0000 - val_rmse: 18097.5859\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541139712.0000 - rmse: 23262.4062 - val_loss: 323294080.0000 - val_rmse: 17980.3750\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541279232.0000 - rmse: 23265.4062 - val_loss: 328862368.0000 - val_rmse: 18134.5605\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509963968.0000 - rmse: 22582.3789 - val_loss: 339535488.0000 - val_rmse: 18426.4863\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569827136.0000 - rmse: 23871.0488 - val_loss: 416116320.0000 - val_rmse: 20398.9277\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461330368.0000 - rmse: 21478.5977 - val_loss: 344091424.0000 - val_rmse: 18549.6973\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524314752.0000 - rmse: 22897.9199 - val_loss: 341726656.0000 - val_rmse: 18485.8477\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502932896.0000 - rmse: 22426.1641 - val_loss: 367132032.0000 - val_rmse: 19160.6855\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557439680.0000 - rmse: 23610.1562 - val_loss: 436646144.0000 - val_rmse: 20896.0781\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501420640.0000 - rmse: 22392.4199 - val_loss: 306655424.0000 - val_rmse: 17511.5762\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563211520.0000 - rmse: 23732.0762 - val_loss: 356705248.0000 - val_rmse: 18886.6387\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504301728.0000 - rmse: 22456.6602 - val_loss: 405759328.0000 - val_rmse: 20143.4668\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574371584.0000 - rmse: 23966.0488 - val_loss: 352469824.0000 - val_rmse: 18774.1777\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496406496.0000 - rmse: 22280.1797 - val_loss: 357551040.0000 - val_rmse: 18909.0176\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609697280.0000 - rmse: 24692.0469 - val_loss: 448257952.0000 - val_rmse: 21172.0977\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540066688.0000 - rmse: 23239.3301 - val_loss: 323606496.0000 - val_rmse: 17989.0625\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506876416.0000 - rmse: 22513.9141 - val_loss: 315142592.0000 - val_rmse: 17752.2520\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515263456.0000 - rmse: 22699.4102 - val_loss: 321223904.0000 - val_rmse: 17922.7148\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515904832.0000 - rmse: 22713.5371 - val_loss: 314157504.0000 - val_rmse: 17724.4844\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489606976.0000 - rmse: 22127.0586 - val_loss: 329268032.0000 - val_rmse: 18145.7402\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499221824.0000 - rmse: 22343.2676 - val_loss: 326256896.0000 - val_rmse: 18062.5801\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487263264.0000 - rmse: 22074.0371 - val_loss: 306113408.0000 - val_rmse: 17496.0938\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518983200.0000 - rmse: 22781.1992 - val_loss: 297310976.0000 - val_rmse: 17242.7051\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500151584.0000 - rmse: 22364.0645 - val_loss: 322468288.0000 - val_rmse: 17957.3984\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471715104.0000 - rmse: 21719.0000 - val_loss: 554179072.0000 - val_rmse: 23541.0059\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500758688.0000 - rmse: 22377.6328 - val_loss: 403948160.0000 - val_rmse: 20098.4570\n",
      "104/104 [==============================] - 0s 675us/step - loss: 983032256.0000 - rmse: 31353.3438\n",
      "[983032256.0, 31353.34375]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,417\n",
      "Trainable params: 20,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,417\n",
      "Trainable params: 20,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 9907422208.0000 - rmse: 99536.0312 - val_loss: 1472504832.0000 - val_rmse: 38373.2305\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1878301312.0000 - rmse: 43339.3750 - val_loss: 1252734592.0000 - val_rmse: 35393.9922\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1729739520.0000 - rmse: 41590.1367 - val_loss: 1173306240.0000 - val_rmse: 34253.5586\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1590386816.0000 - rmse: 39879.6523 - val_loss: 1035241984.0000 - val_rmse: 32175.1758\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1587284864.0000 - rmse: 39840.7422 - val_loss: 1015631168.0000 - val_rmse: 31868.9688\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1521727232.0000 - rmse: 39009.3203 - val_loss: 1133727616.0000 - val_rmse: 33670.8711\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1425531520.0000 - rmse: 37756.2109 - val_loss: 1056872704.0000 - val_rmse: 32509.5781\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1433529088.0000 - rmse: 37861.9727 - val_loss: 986292736.0000 - val_rmse: 31405.2988\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1370161152.0000 - rmse: 37015.6875 - val_loss: 985045120.0000 - val_rmse: 31385.4277\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1344589824.0000 - rmse: 36668.6484 - val_loss: 982861568.0000 - val_rmse: 31350.6211\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1374045312.0000 - rmse: 37068.1172 - val_loss: 913350592.0000 - val_rmse: 30221.6895\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1278229376.0000 - rmse: 35752.3320 - val_loss: 910917632.0000 - val_rmse: 30181.4121\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1179877376.0000 - rmse: 34349.3438 - val_loss: 844617600.0000 - val_rmse: 29062.3027\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1141977472.0000 - rmse: 33793.1562 - val_loss: 907225216.0000 - val_rmse: 30120.1797\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1172280448.0000 - rmse: 34238.5820 - val_loss: 815209280.0000 - val_rmse: 28551.8691\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1117896192.0000 - rmse: 33434.9531 - val_loss: 770634816.0000 - val_rmse: 27760.3105\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1073464768.0000 - rmse: 32763.7695 - val_loss: 953671872.0000 - val_rmse: 30881.5762\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1026516864.0000 - rmse: 32039.3008 - val_loss: 789829184.0000 - val_rmse: 28103.8984\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1046529216.0000 - rmse: 32350.1035 - val_loss: 729987520.0000 - val_rmse: 27018.2793\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1073986944.0000 - rmse: 32771.7383 - val_loss: 730183424.0000 - val_rmse: 27021.9062\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 985443264.0000 - rmse: 31391.7715 - val_loss: 849616128.0000 - val_rmse: 29148.1758\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1053130240.0000 - rmse: 32451.9688 - val_loss: 907866176.0000 - val_rmse: 30130.8184\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1016336960.0000 - rmse: 31880.0371 - val_loss: 763103104.0000 - val_rmse: 27624.3203\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 988995136.0000 - rmse: 31448.2891 - val_loss: 794630272.0000 - val_rmse: 28189.1855\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 975784512.0000 - rmse: 31237.5488 - val_loss: 664733632.0000 - val_rmse: 25782.4258\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 929444288.0000 - rmse: 30486.7852 - val_loss: 686171392.0000 - val_rmse: 26194.8711\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 920682432.0000 - rmse: 30342.7480 - val_loss: 677533888.0000 - val_rmse: 26029.4805\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 916422720.0000 - rmse: 30272.4707 - val_loss: 662157120.0000 - val_rmse: 25732.4121\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 916638464.0000 - rmse: 30276.0352 - val_loss: 722615744.0000 - val_rmse: 26881.5098\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 845331008.0000 - rmse: 29074.5762 - val_loss: 627682048.0000 - val_rmse: 25053.5840\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 825177216.0000 - rmse: 28725.8945 - val_loss: 648952448.0000 - val_rmse: 25474.5430\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 923927744.0000 - rmse: 30396.1758 - val_loss: 680379520.0000 - val_rmse: 26084.0859\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 838908608.0000 - rmse: 28963.9160 - val_loss: 570752896.0000 - val_rmse: 23890.4316\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 909087936.0000 - rmse: 30151.0820 - val_loss: 623537280.0000 - val_rmse: 24970.7285\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 861248704.0000 - rmse: 29347.0391 - val_loss: 647710400.0000 - val_rmse: 25450.1543\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782223168.0000 - rmse: 27968.2500 - val_loss: 634953024.0000 - val_rmse: 25198.2734\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 862340160.0000 - rmse: 29365.6270 - val_loss: 667356096.0000 - val_rmse: 25833.2363\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782461248.0000 - rmse: 27972.5098 - val_loss: 558838272.0000 - val_rmse: 23639.7578\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 830732352.0000 - rmse: 28822.4277 - val_loss: 605706240.0000 - val_rmse: 24611.0996\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 789613248.0000 - rmse: 28100.0586 - val_loss: 655043968.0000 - val_rmse: 25593.8262\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 834722496.0000 - rmse: 28891.5625 - val_loss: 562234432.0000 - val_rmse: 23711.4824\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 812166016.0000 - rmse: 28498.5234 - val_loss: 594470400.0000 - val_rmse: 24381.7617\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 772875712.0000 - rmse: 27800.6406 - val_loss: 516636704.0000 - val_rmse: 22729.6426\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733223104.0000 - rmse: 27078.0918 - val_loss: 586097728.0000 - val_rmse: 24209.4531\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 753243840.0000 - rmse: 27445.2852 - val_loss: 745532608.0000 - val_rmse: 27304.4414\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 723533888.0000 - rmse: 26898.5820 - val_loss: 536207520.0000 - val_rmse: 23156.1543\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 693801600.0000 - rmse: 26340.1113 - val_loss: 498881600.0000 - val_rmse: 22335.6562\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 745645248.0000 - rmse: 27306.5039 - val_loss: 533095488.0000 - val_rmse: 23088.8594\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 775020928.0000 - rmse: 27839.1973 - val_loss: 596422784.0000 - val_rmse: 24421.7656\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 755590976.0000 - rmse: 27488.0156 - val_loss: 496489792.0000 - val_rmse: 22282.0488\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 712718336.0000 - rmse: 26696.7832 - val_loss: 498692512.0000 - val_rmse: 22331.4219\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729921792.0000 - rmse: 27017.0645 - val_loss: 480448544.0000 - val_rmse: 21919.1348\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 663920192.0000 - rmse: 25766.6484 - val_loss: 480472192.0000 - val_rmse: 21919.6738\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 679260672.0000 - rmse: 26062.6289 - val_loss: 690512192.0000 - val_rmse: 26277.5957\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652711680.0000 - rmse: 25548.2227 - val_loss: 533487072.0000 - val_rmse: 23097.3359\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654147648.0000 - rmse: 25576.3086 - val_loss: 522513728.0000 - val_rmse: 22858.5586\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 694375744.0000 - rmse: 26351.0098 - val_loss: 474131328.0000 - val_rmse: 21774.5527\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684541696.0000 - rmse: 26163.7461 - val_loss: 556207424.0000 - val_rmse: 23584.0488\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 595906112.0000 - rmse: 24411.1855 - val_loss: 618320896.0000 - val_rmse: 24866.0566\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620962048.0000 - rmse: 24919.1094 - val_loss: 626917504.0000 - val_rmse: 25038.3203\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 681529600.0000 - rmse: 26106.1191 - val_loss: 459468640.0000 - val_rmse: 21435.2168\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 671462080.0000 - rmse: 25912.5820 - val_loss: 439833696.0000 - val_rmse: 20972.2109\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624802176.0000 - rmse: 24996.0391 - val_loss: 463681184.0000 - val_rmse: 21533.2559\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604089344.0000 - rmse: 24578.2285 - val_loss: 430413184.0000 - val_rmse: 20746.3984\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 627139904.0000 - rmse: 25042.7578 - val_loss: 430966464.0000 - val_rmse: 20759.7305\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 600893184.0000 - rmse: 24513.1191 - val_loss: 446652608.0000 - val_rmse: 21134.1543\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 643602752.0000 - rmse: 25369.3223 - val_loss: 452412512.0000 - val_rmse: 21269.9863\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626779904.0000 - rmse: 25035.5723 - val_loss: 452448064.0000 - val_rmse: 21270.8242\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588697792.0000 - rmse: 24263.0918 - val_loss: 509784864.0000 - val_rmse: 22578.4141\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615265920.0000 - rmse: 24804.5547 - val_loss: 428655392.0000 - val_rmse: 20703.9922\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 581048896.0000 - rmse: 24104.9531 - val_loss: 616625600.0000 - val_rmse: 24831.9453\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618877312.0000 - rmse: 24877.2402 - val_loss: 467647040.0000 - val_rmse: 21625.1445\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684159808.0000 - rmse: 26156.4492 - val_loss: 462734112.0000 - val_rmse: 21511.2539\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569110208.0000 - rmse: 23856.0293 - val_loss: 480982784.0000 - val_rmse: 21931.3184\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526836256.0000 - rmse: 22952.9121 - val_loss: 400663744.0000 - val_rmse: 20016.5820\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532794208.0000 - rmse: 23082.3340 - val_loss: 553697280.0000 - val_rmse: 23530.7715\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 633211904.0000 - rmse: 25163.6992 - val_loss: 395407776.0000 - val_rmse: 19884.8574\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552117696.0000 - rmse: 23497.1816 - val_loss: 494323744.0000 - val_rmse: 22233.3906\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547224192.0000 - rmse: 23392.8223 - val_loss: 514178016.0000 - val_rmse: 22675.4902\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555667008.0000 - rmse: 23572.5879 - val_loss: 435122560.0000 - val_rmse: 20859.5898\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499173888.0000 - rmse: 22342.1992 - val_loss: 419627264.0000 - val_rmse: 20484.8027\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582797376.0000 - rmse: 24141.1934 - val_loss: 508964736.0000 - val_rmse: 22560.2441\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611308608.0000 - rmse: 24724.6484 - val_loss: 414320064.0000 - val_rmse: 20354.8516\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478494240.0000 - rmse: 21874.5078 - val_loss: 460624224.0000 - val_rmse: 21462.1543\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579625088.0000 - rmse: 24075.4023 - val_loss: 377457440.0000 - val_rmse: 19428.2598\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563609344.0000 - rmse: 23740.4570 - val_loss: 379282144.0000 - val_rmse: 19475.1660\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591645952.0000 - rmse: 24323.7715 - val_loss: 409508032.0000 - val_rmse: 20236.3027\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564930432.0000 - rmse: 23768.2617 - val_loss: 358304928.0000 - val_rmse: 18928.9414\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590740608.0000 - rmse: 24305.1543 - val_loss: 520236768.0000 - val_rmse: 22808.6953\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540532608.0000 - rmse: 23249.3555 - val_loss: 580268224.0000 - val_rmse: 24088.7559\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526720288.0000 - rmse: 22950.3848 - val_loss: 394415648.0000 - val_rmse: 19859.8984\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588526016.0000 - rmse: 24259.5527 - val_loss: 383023872.0000 - val_rmse: 19570.9941\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539880576.0000 - rmse: 23235.3242 - val_loss: 417828960.0000 - val_rmse: 20440.8613\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534398112.0000 - rmse: 23117.0508 - val_loss: 452867680.0000 - val_rmse: 21280.6855\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542357632.0000 - rmse: 23288.5684 - val_loss: 392429408.0000 - val_rmse: 19809.8281\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525138944.0000 - rmse: 22915.9062 - val_loss: 608708480.0000 - val_rmse: 24672.0137\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467553184.0000 - rmse: 21622.9746 - val_loss: 385378528.0000 - val_rmse: 19631.0586\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614979008.0000 - rmse: 24798.7656 - val_loss: 382348736.0000 - val_rmse: 19553.7344\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490261376.0000 - rmse: 22141.8438 - val_loss: 437963744.0000 - val_rmse: 20927.5801\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544351424.0000 - rmse: 23331.3340 - val_loss: 424951616.0000 - val_rmse: 20614.3477\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501338016.0000 - rmse: 22390.5742 - val_loss: 415952320.0000 - val_rmse: 20394.9062\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473152736.0000 - rmse: 21752.0703 - val_loss: 366409664.0000 - val_rmse: 19141.8262\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487471968.0000 - rmse: 22078.7656 - val_loss: 485529792.0000 - val_rmse: 22034.7363\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485872352.0000 - rmse: 22042.5059 - val_loss: 372101280.0000 - val_rmse: 19289.9258\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523778336.0000 - rmse: 22886.2012 - val_loss: 468361184.0000 - val_rmse: 21641.6523\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597196288.0000 - rmse: 24437.5957 - val_loss: 400034560.0000 - val_rmse: 20000.8594\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530366240.0000 - rmse: 23029.6777 - val_loss: 428167808.0000 - val_rmse: 20692.2129\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503872416.0000 - rmse: 22447.0996 - val_loss: 377886784.0000 - val_rmse: 19439.3066\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472817152.0000 - rmse: 21744.3555 - val_loss: 413940864.0000 - val_rmse: 20345.5312\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448089984.0000 - rmse: 21168.1328 - val_loss: 381364160.0000 - val_rmse: 19528.5449\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471166688.0000 - rmse: 21706.3691 - val_loss: 430672736.0000 - val_rmse: 20752.6523\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504100032.0000 - rmse: 22452.1680 - val_loss: 494626880.0000 - val_rmse: 22240.2051\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448291328.0000 - rmse: 21172.8867 - val_loss: 529110912.0000 - val_rmse: 23002.4082\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505322752.0000 - rmse: 22479.3848 - val_loss: 451519840.0000 - val_rmse: 21248.9922\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468724832.0000 - rmse: 21650.0508 - val_loss: 452792288.0000 - val_rmse: 21278.9121\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458604736.0000 - rmse: 21415.0547 - val_loss: 502035136.0000 - val_rmse: 22406.1348\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411889568.0000 - rmse: 20295.0586 - val_loss: 380312480.0000 - val_rmse: 19501.5996\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450127808.0000 - rmse: 21216.2129 - val_loss: 386235008.0000 - val_rmse: 19652.8574\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382373792.0000 - rmse: 19554.3750 - val_loss: 980564288.0000 - val_rmse: 31313.9629\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415782752.0000 - rmse: 20390.7441 - val_loss: 455438624.0000 - val_rmse: 21341.0039\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433148512.0000 - rmse: 20812.2168 - val_loss: 614313728.0000 - val_rmse: 24785.3496\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 595302272.0000 - rmse: 24398.8145 - val_loss: 423146912.0000 - val_rmse: 20570.5312\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467080608.0000 - rmse: 21612.0449 - val_loss: 402649792.0000 - val_rmse: 20066.1309\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442747392.0000 - rmse: 21041.5605 - val_loss: 478529216.0000 - val_rmse: 21875.3086\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368823808.0000 - rmse: 19204.7812 - val_loss: 452090176.0000 - val_rmse: 21262.4082\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410732352.0000 - rmse: 20266.5293 - val_loss: 378127008.0000 - val_rmse: 19445.4824\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388934464.0000 - rmse: 19721.4141 - val_loss: 386872576.0000 - val_rmse: 19669.0703\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431633888.0000 - rmse: 20775.7949 - val_loss: 428361824.0000 - val_rmse: 20696.8984\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430858048.0000 - rmse: 20757.1152 - val_loss: 498027648.0000 - val_rmse: 22316.5293\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387478496.0000 - rmse: 19684.4648 - val_loss: 431956640.0000 - val_rmse: 20783.5625\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323495040.0000 - rmse: 17985.9648 - val_loss: 449288800.0000 - val_rmse: 21196.4297\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395342304.0000 - rmse: 19883.2129 - val_loss: 508051808.0000 - val_rmse: 22540.0020\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429909952.0000 - rmse: 20734.2637 - val_loss: 493457440.0000 - val_rmse: 22213.9004\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493584128.0000 - rmse: 22216.7480 - val_loss: 430271104.0000 - val_rmse: 20742.9707\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372355968.0000 - rmse: 19296.5234 - val_loss: 410309728.0000 - val_rmse: 20256.0996\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403913344.0000 - rmse: 20097.5898 - val_loss: 483639296.0000 - val_rmse: 21991.7949\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397161632.0000 - rmse: 19928.9102 - val_loss: 439724640.0000 - val_rmse: 20969.6094\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423122592.0000 - rmse: 20569.9395 - val_loss: 411742112.0000 - val_rmse: 20291.4258\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454227552.0000 - rmse: 21312.6094 - val_loss: 575011456.0000 - val_rmse: 23979.3945\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390429248.0000 - rmse: 19759.2793 - val_loss: 460000128.0000 - val_rmse: 21447.6074\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371295008.0000 - rmse: 19269.0117 - val_loss: 482384736.0000 - val_rmse: 21963.2539\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386565760.0000 - rmse: 19661.2715 - val_loss: 374238432.0000 - val_rmse: 19345.2383\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406447296.0000 - rmse: 20160.5352 - val_loss: 453545760.0000 - val_rmse: 21296.6094\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374277632.0000 - rmse: 19346.2520 - val_loss: 500454336.0000 - val_rmse: 22370.8320\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321995936.0000 - rmse: 17944.2422 - val_loss: 464742944.0000 - val_rmse: 21557.8945\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356839904.0000 - rmse: 18890.2012 - val_loss: 556468096.0000 - val_rmse: 23589.5742\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334516000.0000 - rmse: 18289.7715 - val_loss: 577412416.0000 - val_rmse: 24029.4043\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452048448.0000 - rmse: 21261.4277 - val_loss: 436211808.0000 - val_rmse: 20885.6816\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325178976.0000 - rmse: 18032.7109 - val_loss: 494956896.0000 - val_rmse: 22247.6230\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360010848.0000 - rmse: 18973.9473 - val_loss: 484665760.0000 - val_rmse: 22015.1191\n",
      "104/104 [==============================] - 0s 708us/step - loss: 603514304.0000 - rmse: 24566.5234\n",
      "[603514304.0, 24566.5234375]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,417\n",
      "Trainable params: 20,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,417\n",
      "Trainable params: 20,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 11484848128.0000 - rmse: 107167.3828 - val_loss: 1590626560.0000 - val_rmse: 39882.6602\n",
      "Epoch 2/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2028436224.0000 - rmse: 45038.1641 - val_loss: 1317858304.0000 - val_rmse: 36302.3164\n",
      "Epoch 3/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1739958784.0000 - rmse: 41712.8125 - val_loss: 1936868736.0000 - val_rmse: 44009.8711\n",
      "Epoch 4/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1760196096.0000 - rmse: 41954.6914 - val_loss: 1070721664.0000 - val_rmse: 32721.8828\n",
      "Epoch 5/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1629438208.0000 - rmse: 40366.3008 - val_loss: 994423168.0000 - val_rmse: 31534.4746\n",
      "Epoch 6/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1513694464.0000 - rmse: 38906.2266 - val_loss: 936703808.0000 - val_rmse: 30605.6152\n",
      "Epoch 7/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1458429312.0000 - rmse: 38189.3867 - val_loss: 1229288064.0000 - val_rmse: 35061.2031\n",
      "Epoch 8/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1434383744.0000 - rmse: 37873.2578 - val_loss: 882067072.0000 - val_rmse: 29699.6113\n",
      "Epoch 9/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1486426112.0000 - rmse: 38554.1953 - val_loss: 895592128.0000 - val_rmse: 29926.4434\n",
      "Epoch 10/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1377846272.0000 - rmse: 37119.3516 - val_loss: 818311168.0000 - val_rmse: 28606.1367\n",
      "Epoch 11/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1233527936.0000 - rmse: 35121.6133 - val_loss: 769812544.0000 - val_rmse: 27745.4961\n",
      "Epoch 12/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1240402176.0000 - rmse: 35219.3438 - val_loss: 777629056.0000 - val_rmse: 27886.0000\n",
      "Epoch 13/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1285773568.0000 - rmse: 35857.6836 - val_loss: 873124736.0000 - val_rmse: 29548.6836\n",
      "Epoch 14/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1176830720.0000 - rmse: 34304.9648 - val_loss: 809039040.0000 - val_rmse: 28443.6113\n",
      "Epoch 15/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1200105984.0000 - rmse: 34642.5469 - val_loss: 731630848.0000 - val_rmse: 27048.6758\n",
      "Epoch 16/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1140665472.0000 - rmse: 33773.7344 - val_loss: 759684416.0000 - val_rmse: 27562.3730\n",
      "Epoch 17/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1078936832.0000 - rmse: 32847.1719 - val_loss: 659197312.0000 - val_rmse: 25674.8359\n",
      "Epoch 18/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1062202624.0000 - rmse: 32591.4492 - val_loss: 701418176.0000 - val_rmse: 26484.2988\n",
      "Epoch 19/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1048752384.0000 - rmse: 32384.4473 - val_loss: 713465600.0000 - val_rmse: 26710.7773\n",
      "Epoch 20/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1035811264.0000 - rmse: 32184.0215 - val_loss: 636570048.0000 - val_rmse: 25230.3359\n",
      "Epoch 21/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 982719040.0000 - rmse: 31348.3496 - val_loss: 852441152.0000 - val_rmse: 29196.5957\n",
      "Epoch 22/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 988518080.0000 - rmse: 31440.7070 - val_loss: 642277120.0000 - val_rmse: 25343.1836\n",
      "Epoch 23/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 912426432.0000 - rmse: 30206.3965 - val_loss: 635890368.0000 - val_rmse: 25216.8633\n",
      "Epoch 24/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 926267840.0000 - rmse: 30434.6465 - val_loss: 544869696.0000 - val_rmse: 23342.4434\n",
      "Epoch 25/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 875773376.0000 - rmse: 29593.4648 - val_loss: 766744320.0000 - val_rmse: 27690.1484\n",
      "Epoch 26/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 897034176.0000 - rmse: 29950.5293 - val_loss: 755321408.0000 - val_rmse: 27483.1094\n",
      "Epoch 27/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 924645440.0000 - rmse: 30407.9805 - val_loss: 550112896.0000 - val_rmse: 23454.4824\n",
      "Epoch 28/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 861019136.0000 - rmse: 29343.1270 - val_loss: 504643200.0000 - val_rmse: 22464.2598\n",
      "Epoch 29/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 795200000.0000 - rmse: 28199.2891 - val_loss: 873756096.0000 - val_rmse: 29559.3633\n",
      "Epoch 30/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 792200320.0000 - rmse: 28146.0508 - val_loss: 567494848.0000 - val_rmse: 23822.1484\n",
      "Epoch 31/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 887107712.0000 - rmse: 29784.3535 - val_loss: 495898656.0000 - val_rmse: 22268.7793\n",
      "Epoch 32/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 835562112.0000 - rmse: 28906.0879 - val_loss: 597312512.0000 - val_rmse: 24439.9746\n",
      "Epoch 33/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782371584.0000 - rmse: 27970.9043 - val_loss: 555549696.0000 - val_rmse: 23570.0977\n",
      "Epoch 34/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 823835264.0000 - rmse: 28702.5293 - val_loss: 467026272.0000 - val_rmse: 21610.7852\n",
      "Epoch 35/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 761716096.0000 - rmse: 27599.2012 - val_loss: 566099712.0000 - val_rmse: 23792.8496\n",
      "Epoch 36/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 796503424.0000 - rmse: 28222.3926 - val_loss: 456410336.0000 - val_rmse: 21363.7598\n",
      "Epoch 37/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 744745280.0000 - rmse: 27290.0156 - val_loss: 508299040.0000 - val_rmse: 22545.4844\n",
      "Epoch 38/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 775038400.0000 - rmse: 27839.5098 - val_loss: 419374208.0000 - val_rmse: 20478.6250\n",
      "Epoch 39/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 714824448.0000 - rmse: 26736.1992 - val_loss: 530781568.0000 - val_rmse: 23038.6953\n",
      "Epoch 40/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 769779520.0000 - rmse: 27744.9004 - val_loss: 438978976.0000 - val_rmse: 20951.8242\n",
      "Epoch 41/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 744949632.0000 - rmse: 27293.7617 - val_loss: 458552064.0000 - val_rmse: 21413.8242\n",
      "Epoch 42/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 747877376.0000 - rmse: 27347.3457 - val_loss: 433284672.0000 - val_rmse: 20815.4883\n",
      "Epoch 43/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 762831296.0000 - rmse: 27619.4004 - val_loss: 424320000.0000 - val_rmse: 20599.0293\n",
      "Epoch 44/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 715774848.0000 - rmse: 26753.9648 - val_loss: 460272544.0000 - val_rmse: 21453.9609\n",
      "Epoch 45/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 759255360.0000 - rmse: 27554.5879 - val_loss: 410880096.0000 - val_rmse: 20270.1758\n",
      "Epoch 46/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 671955712.0000 - rmse: 25922.1094 - val_loss: 437897664.0000 - val_rmse: 20926.0020\n",
      "Epoch 47/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 695386752.0000 - rmse: 26370.1836 - val_loss: 454796736.0000 - val_rmse: 21325.9590\n",
      "Epoch 48/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621068800.0000 - rmse: 24921.2500 - val_loss: 419403296.0000 - val_rmse: 20479.3359\n",
      "Epoch 49/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800289920.0000 - rmse: 28289.3945 - val_loss: 436307744.0000 - val_rmse: 20887.9766\n",
      "Epoch 50/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 650727808.0000 - rmse: 25509.3652 - val_loss: 442797600.0000 - val_rmse: 21042.7539\n",
      "Epoch 51/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 736098048.0000 - rmse: 27131.1230 - val_loss: 555310656.0000 - val_rmse: 23565.0254\n",
      "Epoch 52/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 645860096.0000 - rmse: 25413.7773 - val_loss: 388078432.0000 - val_rmse: 19699.7031\n",
      "Epoch 53/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 673792320.0000 - rmse: 25957.5059 - val_loss: 373623520.0000 - val_rmse: 19329.3379\n",
      "Epoch 54/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 643056768.0000 - rmse: 25358.5605 - val_loss: 402155808.0000 - val_rmse: 20053.8203\n",
      "Epoch 55/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 655550208.0000 - rmse: 25603.7129 - val_loss: 396415936.0000 - val_rmse: 19910.1934\n",
      "Epoch 56/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 658255232.0000 - rmse: 25656.4844 - val_loss: 410131776.0000 - val_rmse: 20251.7070\n",
      "Epoch 57/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 657561536.0000 - rmse: 25642.9609 - val_loss: 557590976.0000 - val_rmse: 23613.3613\n",
      "Epoch 58/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 613205504.0000 - rmse: 24762.9844 - val_loss: 443124800.0000 - val_rmse: 21050.5273\n",
      "Epoch 59/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605246848.0000 - rmse: 24601.7617 - val_loss: 443521568.0000 - val_rmse: 21059.9492\n",
      "Epoch 60/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692358528.0000 - rmse: 26312.7051 - val_loss: 384272448.0000 - val_rmse: 19602.8633\n",
      "Epoch 61/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583283136.0000 - rmse: 24151.2520 - val_loss: 419246752.0000 - val_rmse: 20475.5117\n",
      "Epoch 62/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586199168.0000 - rmse: 24211.5469 - val_loss: 387737088.0000 - val_rmse: 19691.0391\n",
      "Epoch 63/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 603822720.0000 - rmse: 24572.8027 - val_loss: 363250720.0000 - val_rmse: 19059.1328\n",
      "Epoch 64/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626563072.0000 - rmse: 25031.2402 - val_loss: 514715584.0000 - val_rmse: 22687.3418\n",
      "Epoch 65/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649027520.0000 - rmse: 25476.0156 - val_loss: 594860224.0000 - val_rmse: 24389.7520\n",
      "Epoch 66/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609312512.0000 - rmse: 24684.2559 - val_loss: 475622336.0000 - val_rmse: 21808.7637\n",
      "Epoch 67/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612731392.0000 - rmse: 24753.4082 - val_loss: 386568192.0000 - val_rmse: 19661.3340\n",
      "Epoch 68/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 603764288.0000 - rmse: 24571.6133 - val_loss: 382836416.0000 - val_rmse: 19566.2012\n",
      "Epoch 69/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626261312.0000 - rmse: 25025.2109 - val_loss: 399717696.0000 - val_rmse: 19992.9375\n",
      "Epoch 70/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578956672.0000 - rmse: 24061.5176 - val_loss: 372453024.0000 - val_rmse: 19299.0391\n",
      "Epoch 71/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568724736.0000 - rmse: 23847.9473 - val_loss: 391647968.0000 - val_rmse: 19790.0938\n",
      "Epoch 72/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629267648.0000 - rmse: 25085.2070 - val_loss: 397738432.0000 - val_rmse: 19943.3770\n",
      "Epoch 73/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584095872.0000 - rmse: 24168.0723 - val_loss: 378216192.0000 - val_rmse: 19447.7773\n",
      "Epoch 74/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570139264.0000 - rmse: 23877.5879 - val_loss: 480649472.0000 - val_rmse: 21923.7148\n",
      "Epoch 75/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568715072.0000 - rmse: 23847.7441 - val_loss: 399470432.0000 - val_rmse: 19986.7520\n",
      "Epoch 76/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572201856.0000 - rmse: 23920.7363 - val_loss: 395303968.0000 - val_rmse: 19882.2480\n",
      "Epoch 77/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611192320.0000 - rmse: 24722.3008 - val_loss: 416422112.0000 - val_rmse: 20406.4199\n",
      "Epoch 78/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 638636224.0000 - rmse: 25271.2500 - val_loss: 419553856.0000 - val_rmse: 20483.0098\n",
      "Epoch 79/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575226432.0000 - rmse: 23983.8750 - val_loss: 358730368.0000 - val_rmse: 18940.1738\n",
      "Epoch 80/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609231488.0000 - rmse: 24682.6133 - val_loss: 483630688.0000 - val_rmse: 21991.5996\n",
      "Epoch 81/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536915136.0000 - rmse: 23171.4238 - val_loss: 401751680.0000 - val_rmse: 20043.7402\n",
      "Epoch 82/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501155232.0000 - rmse: 22386.4941 - val_loss: 395223648.0000 - val_rmse: 19880.2285\n",
      "Epoch 83/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550344896.0000 - rmse: 23459.4277 - val_loss: 369036928.0000 - val_rmse: 19210.3281\n",
      "Epoch 84/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565768768.0000 - rmse: 23785.8926 - val_loss: 386510752.0000 - val_rmse: 19659.8750\n",
      "Epoch 85/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529158784.0000 - rmse: 23003.4492 - val_loss: 506728256.0000 - val_rmse: 22510.6211\n",
      "Epoch 86/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533618976.0000 - rmse: 23100.1895 - val_loss: 457834656.0000 - val_rmse: 21397.0684\n",
      "Epoch 87/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525516416.0000 - rmse: 22924.1406 - val_loss: 444711648.0000 - val_rmse: 21088.1836\n",
      "Epoch 88/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483562240.0000 - rmse: 21990.0449 - val_loss: 381193376.0000 - val_rmse: 19524.1699\n",
      "Epoch 89/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 639657856.0000 - rmse: 25291.4531 - val_loss: 375273664.0000 - val_rmse: 19371.9766\n",
      "Epoch 90/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491227296.0000 - rmse: 22163.6465 - val_loss: 361698976.0000 - val_rmse: 19018.3809\n",
      "Epoch 91/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587663360.0000 - rmse: 24241.7617 - val_loss: 355044640.0000 - val_rmse: 18842.6230\n",
      "Epoch 92/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530289984.0000 - rmse: 23028.0234 - val_loss: 360687552.0000 - val_rmse: 18991.7715\n",
      "Epoch 93/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604813632.0000 - rmse: 24592.9551 - val_loss: 358164000.0000 - val_rmse: 18925.2168\n",
      "Epoch 94/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534198400.0000 - rmse: 23112.7305 - val_loss: 349732416.0000 - val_rmse: 18701.1289\n",
      "Epoch 95/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523186720.0000 - rmse: 22873.2695 - val_loss: 372154560.0000 - val_rmse: 19291.3027\n",
      "Epoch 96/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577813376.0000 - rmse: 24037.7441 - val_loss: 406899136.0000 - val_rmse: 20171.7383\n",
      "Epoch 97/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550984064.0000 - rmse: 23473.0469 - val_loss: 552574208.0000 - val_rmse: 23506.8926\n",
      "Epoch 98/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538774784.0000 - rmse: 23211.5195 - val_loss: 389220480.0000 - val_rmse: 19728.6641\n",
      "Epoch 99/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505329536.0000 - rmse: 22479.5332 - val_loss: 414327040.0000 - val_rmse: 20355.0215\n",
      "Epoch 100/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484430912.0000 - rmse: 22009.7891 - val_loss: 392184640.0000 - val_rmse: 19803.6465\n",
      "Epoch 101/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448308160.0000 - rmse: 21173.2852 - val_loss: 388837216.0000 - val_rmse: 19718.9492\n",
      "Epoch 102/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520899360.0000 - rmse: 22823.2168 - val_loss: 387764928.0000 - val_rmse: 19691.7422\n",
      "Epoch 103/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540015104.0000 - rmse: 23238.2188 - val_loss: 392266656.0000 - val_rmse: 19805.7188\n",
      "Epoch 104/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582471552.0000 - rmse: 24134.4414 - val_loss: 441079840.0000 - val_rmse: 21001.8945\n",
      "Epoch 105/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475883392.0000 - rmse: 21814.7461 - val_loss: 361343296.0000 - val_rmse: 19009.0273\n",
      "Epoch 106/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430895072.0000 - rmse: 20758.0078 - val_loss: 367176768.0000 - val_rmse: 19161.8516\n",
      "Epoch 107/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450742304.0000 - rmse: 21230.6875 - val_loss: 546082048.0000 - val_rmse: 23368.3945\n",
      "Epoch 108/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508564192.0000 - rmse: 22551.3613 - val_loss: 377015744.0000 - val_rmse: 19416.8867\n",
      "Epoch 109/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483183904.0000 - rmse: 21981.4414 - val_loss: 386952128.0000 - val_rmse: 19671.0938\n",
      "Epoch 110/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516275680.0000 - rmse: 22721.6973 - val_loss: 349631936.0000 - val_rmse: 18698.4414\n",
      "Epoch 111/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539538752.0000 - rmse: 23227.9688 - val_loss: 389198592.0000 - val_rmse: 19728.1094\n",
      "Epoch 112/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491717248.0000 - rmse: 22174.6953 - val_loss: 489393632.0000 - val_rmse: 22122.2383\n",
      "Epoch 113/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511380096.0000 - rmse: 22613.7129 - val_loss: 453639168.0000 - val_rmse: 21298.8008\n",
      "Epoch 114/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428246848.0000 - rmse: 20694.1191 - val_loss: 460768768.0000 - val_rmse: 21465.5215\n",
      "Epoch 115/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496281472.0000 - rmse: 22277.3730 - val_loss: 372324896.0000 - val_rmse: 19295.7148\n",
      "Epoch 116/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517867200.0000 - rmse: 22756.6914 - val_loss: 528146560.0000 - val_rmse: 22981.4355\n",
      "Epoch 117/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522977696.0000 - rmse: 22868.6992 - val_loss: 454526336.0000 - val_rmse: 21319.6172\n",
      "Epoch 118/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448634528.0000 - rmse: 21180.9883 - val_loss: 420547648.0000 - val_rmse: 20507.2539\n",
      "Epoch 119/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437355648.0000 - rmse: 20913.0430 - val_loss: 389798304.0000 - val_rmse: 19743.3047\n",
      "Epoch 120/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407836576.0000 - rmse: 20194.9570 - val_loss: 351925344.0000 - val_rmse: 18759.6699\n",
      "Epoch 121/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484716288.0000 - rmse: 22016.2695 - val_loss: 517484896.0000 - val_rmse: 22748.2891\n",
      "Epoch 122/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510884320.0000 - rmse: 22602.7441 - val_loss: 411647424.0000 - val_rmse: 20289.0898\n",
      "Epoch 123/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465374560.0000 - rmse: 21572.5352 - val_loss: 407711616.0000 - val_rmse: 20191.8652\n",
      "Epoch 124/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426038304.0000 - rmse: 20640.6875 - val_loss: 387343456.0000 - val_rmse: 19681.0371\n",
      "Epoch 125/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470544064.0000 - rmse: 21692.0234 - val_loss: 431443648.0000 - val_rmse: 20771.2148\n",
      "Epoch 126/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415065472.0000 - rmse: 20373.1484 - val_loss: 386454848.0000 - val_rmse: 19658.4473\n",
      "Epoch 127/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522566752.0000 - rmse: 22859.7168 - val_loss: 360782912.0000 - val_rmse: 18994.2793\n",
      "Epoch 128/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462497408.0000 - rmse: 21505.7500 - val_loss: 381087424.0000 - val_rmse: 19521.4570\n",
      "Epoch 129/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429373888.0000 - rmse: 20721.3340 - val_loss: 369240512.0000 - val_rmse: 19215.6270\n",
      "Epoch 130/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448524416.0000 - rmse: 21178.3887 - val_loss: 384232800.0000 - val_rmse: 19601.8496\n",
      "Epoch 131/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460824320.0000 - rmse: 21466.8164 - val_loss: 356760352.0000 - val_rmse: 18888.0938\n",
      "Epoch 132/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416428832.0000 - rmse: 20406.5840 - val_loss: 377723712.0000 - val_rmse: 19435.1094\n",
      "Epoch 133/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496293376.0000 - rmse: 22277.6367 - val_loss: 350189920.0000 - val_rmse: 18713.3555\n",
      "Epoch 134/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431445664.0000 - rmse: 20771.2637 - val_loss: 357402240.0000 - val_rmse: 18905.0801\n",
      "Epoch 135/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414703200.0000 - rmse: 20364.2598 - val_loss: 356268224.0000 - val_rmse: 18875.0605\n",
      "Epoch 136/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457375680.0000 - rmse: 21386.3379 - val_loss: 354382048.0000 - val_rmse: 18825.0312\n",
      "Epoch 137/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405758144.0000 - rmse: 20143.4336 - val_loss: 367876256.0000 - val_rmse: 19180.0938\n",
      "Epoch 138/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404906016.0000 - rmse: 20122.2676 - val_loss: 382493856.0000 - val_rmse: 19557.4434\n",
      "Epoch 139/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473855072.0000 - rmse: 21768.2090 - val_loss: 376973376.0000 - val_rmse: 19415.7969\n",
      "Epoch 140/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442406976.0000 - rmse: 21033.4688 - val_loss: 332956736.0000 - val_rmse: 18247.0957\n",
      "Epoch 141/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482402528.0000 - rmse: 21963.6582 - val_loss: 430728768.0000 - val_rmse: 20754.0000\n",
      "Epoch 142/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428177568.0000 - rmse: 20692.4453 - val_loss: 390872960.0000 - val_rmse: 19770.5000\n",
      "Epoch 143/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367688064.0000 - rmse: 19175.1895 - val_loss: 517203840.0000 - val_rmse: 22742.1113\n",
      "Epoch 144/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376027488.0000 - rmse: 19391.4238 - val_loss: 504667104.0000 - val_rmse: 22464.7930\n",
      "Epoch 145/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475002752.0000 - rmse: 21794.5527 - val_loss: 356904480.0000 - val_rmse: 18891.9102\n",
      "Epoch 146/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358238912.0000 - rmse: 18927.1953 - val_loss: 345061984.0000 - val_rmse: 18575.8398\n",
      "Epoch 147/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448382688.0000 - rmse: 21175.0449 - val_loss: 353219328.0000 - val_rmse: 18794.1250\n",
      "Epoch 148/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399845312.0000 - rmse: 19996.1250 - val_loss: 394163136.0000 - val_rmse: 19853.5352\n",
      "Epoch 149/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374642688.0000 - rmse: 19355.6836 - val_loss: 543685760.0000 - val_rmse: 23317.0645\n",
      "Epoch 150/150\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411545568.0000 - rmse: 20286.5781 - val_loss: 412718272.0000 - val_rmse: 20315.4629\n",
      "104/104 [==============================] - 0s 669us/step - loss: 374083328.0000 - rmse: 19341.2266\n",
      "[374083328.0, 19341.2265625]\n",
      "[20847.775390625, 29753.494140625, 31353.34375, 24566.5234375, 19341.2265625]\n",
      "25172.47265625\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "!python train.py kfold baseline\n",
    "# epoch 300 p 30 lr 5e-3"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 20:19:44.746464: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 20:19:44.746504: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 20:19:44.746865: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 20:19:44.956913: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 13453645824.0000 - rmse: 115989.8516 - val_loss: 1999677312.0000 - val_rmse: 44717.7500\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2289764864.0000 - rmse: 47851.4883 - val_loss: 1256247296.0000 - val_rmse: 35443.5781\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1851953920.0000 - rmse: 43034.3359 - val_loss: 1044425024.0000 - val_rmse: 32317.5645\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1734677376.0000 - rmse: 41649.4570 - val_loss: 909344064.0000 - val_rmse: 30155.3320\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1554452096.0000 - rmse: 39426.5391 - val_loss: 841904128.0000 - val_rmse: 29015.5840\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1494133248.0000 - rmse: 38654.0195 - val_loss: 789207680.0000 - val_rmse: 28092.8398\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1412338816.0000 - rmse: 37581.0977 - val_loss: 773684800.0000 - val_rmse: 27815.1895\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1395547136.0000 - rmse: 37357.0234 - val_loss: 724694272.0000 - val_rmse: 26920.1465\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1347187456.0000 - rmse: 36704.0508 - val_loss: 718209984.0000 - val_rmse: 26799.4395\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1343587072.0000 - rmse: 36654.9727 - val_loss: 700280384.0000 - val_rmse: 26462.8105\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1299506432.0000 - rmse: 36048.6680 - val_loss: 803976576.0000 - val_rmse: 28354.4805\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1310928512.0000 - rmse: 36206.7461 - val_loss: 685164544.0000 - val_rmse: 26175.6484\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1272439168.0000 - rmse: 35671.2656 - val_loss: 703574272.0000 - val_rmse: 26524.9746\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1251108736.0000 - rmse: 35371.0156 - val_loss: 690580928.0000 - val_rmse: 26278.9062\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1218049408.0000 - rmse: 34900.5664 - val_loss: 735934528.0000 - val_rmse: 27128.1113\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1159538816.0000 - rmse: 34052.0000 - val_loss: 683790208.0000 - val_rmse: 26149.3828\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1192446080.0000 - rmse: 34531.8125 - val_loss: 678122944.0000 - val_rmse: 26040.7930\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1177691904.0000 - rmse: 34317.5156 - val_loss: 686099648.0000 - val_rmse: 26193.5039\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1176748288.0000 - rmse: 34303.7656 - val_loss: 666894272.0000 - val_rmse: 25824.2969\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1159778816.0000 - rmse: 34055.5273 - val_loss: 672466496.0000 - val_rmse: 25931.9590\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1165186944.0000 - rmse: 34134.8359 - val_loss: 674607232.0000 - val_rmse: 25973.2031\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1160871168.0000 - rmse: 34071.5586 - val_loss: 682445952.0000 - val_rmse: 26123.6660\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1119810304.0000 - rmse: 33463.5664 - val_loss: 686369408.0000 - val_rmse: 26198.6523\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1103790080.0000 - rmse: 33223.3359 - val_loss: 693343040.0000 - val_rmse: 26331.4082\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1085156096.0000 - rmse: 32941.7070 - val_loss: 667952512.0000 - val_rmse: 25844.7773\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1070177920.0000 - rmse: 32713.5742 - val_loss: 696167936.0000 - val_rmse: 26384.9941\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1055076736.0000 - rmse: 32481.9434 - val_loss: 769971520.0000 - val_rmse: 27748.3594\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 981158976.0000 - rmse: 31323.4570 - val_loss: 720896768.0000 - val_rmse: 26849.5215\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1025461888.0000 - rmse: 32022.8340 - val_loss: 711199552.0000 - val_rmse: 26668.3242\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 966602944.0000 - rmse: 31090.2383 - val_loss: 747370432.0000 - val_rmse: 27338.0762\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 956606144.0000 - rmse: 30929.0488 - val_loss: 694385344.0000 - val_rmse: 26351.1934\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 929256256.0000 - rmse: 30483.7051 - val_loss: 647328000.0000 - val_rmse: 25442.6406\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 902809408.0000 - rmse: 30046.7871 - val_loss: 704053440.0000 - val_rmse: 26534.0059\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 921608896.0000 - rmse: 30358.0117 - val_loss: 656128256.0000 - val_rmse: 25615.0000\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 935107328.0000 - rmse: 30579.5234 - val_loss: 647069376.0000 - val_rmse: 25437.5586\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 883366912.0000 - rmse: 29721.4883 - val_loss: 669558848.0000 - val_rmse: 25875.8359\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 849319616.0000 - rmse: 29143.0859 - val_loss: 693180992.0000 - val_rmse: 26328.3301\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 786266688.0000 - rmse: 28040.4473 - val_loss: 630118464.0000 - val_rmse: 25102.1602\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 812270400.0000 - rmse: 28500.3574 - val_loss: 781427648.0000 - val_rmse: 27954.0254\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 780202688.0000 - rmse: 27932.1074 - val_loss: 628021888.0000 - val_rmse: 25060.3652\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 737271744.0000 - rmse: 27152.7480 - val_loss: 690271232.0000 - val_rmse: 26273.0137\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 741828224.0000 - rmse: 27236.5234 - val_loss: 702214976.0000 - val_rmse: 26499.3398\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 758455232.0000 - rmse: 27540.0664 - val_loss: 649596928.0000 - val_rmse: 25487.1895\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 784396736.0000 - rmse: 28007.0820 - val_loss: 592267136.0000 - val_rmse: 24336.5391\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 712908544.0000 - rmse: 26700.3477 - val_loss: 672077376.0000 - val_rmse: 25924.4551\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721762368.0000 - rmse: 26865.6348 - val_loss: 571088832.0000 - val_rmse: 23897.4648\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 731317120.0000 - rmse: 27042.8750 - val_loss: 566390528.0000 - val_rmse: 23798.9590\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721529664.0000 - rmse: 26861.3027 - val_loss: 615791680.0000 - val_rmse: 24815.1504\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 728165824.0000 - rmse: 26984.5488 - val_loss: 595445952.0000 - val_rmse: 24401.7617\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 662855552.0000 - rmse: 25745.9805 - val_loss: 632775360.0000 - val_rmse: 25155.0273\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 710247552.0000 - rmse: 26650.4688 - val_loss: 687553536.0000 - val_rmse: 26221.2422\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733271936.0000 - rmse: 27078.9941 - val_loss: 675993344.0000 - val_rmse: 25999.8711\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 673504000.0000 - rmse: 25951.9551 - val_loss: 896503872.0000 - val_rmse: 29941.6738\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 658929408.0000 - rmse: 25669.6211 - val_loss: 791792896.0000 - val_rmse: 28138.8145\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592762432.0000 - rmse: 24346.7129 - val_loss: 621387200.0000 - val_rmse: 24927.6387\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588918720.0000 - rmse: 24267.6484 - val_loss: 597338048.0000 - val_rmse: 24440.5000\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626763968.0000 - rmse: 25035.2539 - val_loss: 625406016.0000 - val_rmse: 25008.1191\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577744576.0000 - rmse: 24036.3184 - val_loss: 690663168.0000 - val_rmse: 26280.4707\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533185888.0000 - rmse: 23090.8184 - val_loss: 615364160.0000 - val_rmse: 24806.5352\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 639689408.0000 - rmse: 25292.0801 - val_loss: 605135872.0000 - val_rmse: 24599.5098\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545270336.0000 - rmse: 23351.0234 - val_loss: 870473856.0000 - val_rmse: 29503.7930\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588773632.0000 - rmse: 24264.6562 - val_loss: 617193984.0000 - val_rmse: 24843.3887\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525670048.0000 - rmse: 22927.4961 - val_loss: 607630656.0000 - val_rmse: 24650.1660\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 600785920.0000 - rmse: 24510.9355 - val_loss: 1124162432.0000 - val_rmse: 33528.5312\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523327168.0000 - rmse: 22876.3438 - val_loss: 1183323264.0000 - val_rmse: 34399.4648\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535903520.0000 - rmse: 23149.5898 - val_loss: 725002752.0000 - val_rmse: 26925.8750\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547038912.0000 - rmse: 23388.8633 - val_loss: 907277056.0000 - val_rmse: 30121.0410\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481771584.0000 - rmse: 21949.2949 - val_loss: 498954336.0000 - val_rmse: 22337.2852\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473546944.0000 - rmse: 21761.1348 - val_loss: 621951168.0000 - val_rmse: 24938.9492\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526564320.0000 - rmse: 22946.9883 - val_loss: 577877248.0000 - val_rmse: 24039.0762\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439334720.0000 - rmse: 20960.3125 - val_loss: 790787456.0000 - val_rmse: 28120.9434\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442315936.0000 - rmse: 21031.3086 - val_loss: 967459392.0000 - val_rmse: 31104.0098\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579103936.0000 - rmse: 24064.5762 - val_loss: 629984384.0000 - val_rmse: 25099.4902\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493547424.0000 - rmse: 22215.9277 - val_loss: 671872640.0000 - val_rmse: 25920.5059\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466136000.0000 - rmse: 21590.1816 - val_loss: 1148562304.0000 - val_rmse: 33890.4453\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480461248.0000 - rmse: 21919.4258 - val_loss: 823575424.0000 - val_rmse: 28698.0039\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504270624.0000 - rmse: 22455.9707 - val_loss: 477547168.0000 - val_rmse: 21852.8516\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464704768.0000 - rmse: 21557.0098 - val_loss: 705158656.0000 - val_rmse: 26554.8242\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496958016.0000 - rmse: 22292.5547 - val_loss: 562517056.0000 - val_rmse: 23717.4395\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462100672.0000 - rmse: 21496.5273 - val_loss: 695659968.0000 - val_rmse: 26375.3652\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471278560.0000 - rmse: 21708.9512 - val_loss: 924820544.0000 - val_rmse: 30410.8613\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417309248.0000 - rmse: 20428.1465 - val_loss: 838836224.0000 - val_rmse: 28962.6680\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510936448.0000 - rmse: 22603.9043 - val_loss: 789484352.0000 - val_rmse: 28097.7617\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479950656.0000 - rmse: 21907.7754 - val_loss: 680320768.0000 - val_rmse: 26082.9590\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436884064.0000 - rmse: 20901.7695 - val_loss: 975834048.0000 - val_rmse: 31238.3418\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450161408.0000 - rmse: 21217.0059 - val_loss: 638624832.0000 - val_rmse: 25271.0273\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455705248.0000 - rmse: 21347.2539 - val_loss: 734532224.0000 - val_rmse: 27102.2539\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402727456.0000 - rmse: 20068.0703 - val_loss: 680452992.0000 - val_rmse: 26085.4941\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452494560.0000 - rmse: 21271.9199 - val_loss: 1114954880.0000 - val_rmse: 33390.9414\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443723360.0000 - rmse: 21064.7422 - val_loss: 896980928.0000 - val_rmse: 29949.6406\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392051872.0000 - rmse: 19800.2988 - val_loss: 702970240.0000 - val_rmse: 26513.5859\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407463136.0000 - rmse: 20185.7148 - val_loss: 691519808.0000 - val_rmse: 26296.7617\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440508512.0000 - rmse: 20988.2949 - val_loss: 593514176.0000 - val_rmse: 24362.1445\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436405600.0000 - rmse: 20890.3223 - val_loss: 980190656.0000 - val_rmse: 31307.9961\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438225440.0000 - rmse: 20933.8340 - val_loss: 747910528.0000 - val_rmse: 27347.9531\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416573440.0000 - rmse: 20410.1289 - val_loss: 940987776.0000 - val_rmse: 30675.5215\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444277792.0000 - rmse: 21077.8984 - val_loss: 804117248.0000 - val_rmse: 28356.9609\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433848320.0000 - rmse: 20829.0254 - val_loss: 1018284608.0000 - val_rmse: 31910.5723\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402685152.0000 - rmse: 20067.0137 - val_loss: 809934912.0000 - val_rmse: 28459.3555\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379112416.0000 - rmse: 19470.8086 - val_loss: 809642816.0000 - val_rmse: 28454.2227\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387824000.0000 - rmse: 19693.2480 - val_loss: 1232085504.0000 - val_rmse: 35101.0742\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396603232.0000 - rmse: 19914.8984 - val_loss: 1104111488.0000 - val_rmse: 33228.1719\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412869504.0000 - rmse: 20319.1895 - val_loss: 981762752.0000 - val_rmse: 31333.0918\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387939008.0000 - rmse: 19696.1680 - val_loss: 543080768.0000 - val_rmse: 23304.0938\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414635712.0000 - rmse: 20362.6035 - val_loss: 1037620352.0000 - val_rmse: 32212.1152\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376400928.0000 - rmse: 19401.0527 - val_loss: 1117973248.0000 - val_rmse: 33436.1055\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382714624.0000 - rmse: 19563.0918 - val_loss: 678176576.0000 - val_rmse: 26041.8242\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367266720.0000 - rmse: 19164.2051 - val_loss: 729511424.0000 - val_rmse: 27009.4688\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386457184.0000 - rmse: 19658.5137 - val_loss: 707471680.0000 - val_rmse: 26598.3398\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361145504.0000 - rmse: 19003.8281 - val_loss: 1629330432.0000 - val_rmse: 40364.9648\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423529280.0000 - rmse: 20579.8262 - val_loss: 843458176.0000 - val_rmse: 29042.3516\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357028832.0000 - rmse: 18895.2051 - val_loss: 547676352.0000 - val_rmse: 23402.4863\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358113536.0000 - rmse: 18923.8867 - val_loss: 870641152.0000 - val_rmse: 29506.6289\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346430496.0000 - rmse: 18612.6426 - val_loss: 965398720.0000 - val_rmse: 31070.8652\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365438944.0000 - rmse: 19116.4551 - val_loss: 844980736.0000 - val_rmse: 29068.5527\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348126048.0000 - rmse: 18658.1367 - val_loss: 560197056.0000 - val_rmse: 23668.4824\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354897280.0000 - rmse: 18838.7168 - val_loss: 1032297088.0000 - val_rmse: 32129.3770\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359163840.0000 - rmse: 18951.6172 - val_loss: 1024703040.0000 - val_rmse: 32010.9824\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324053824.0000 - rmse: 18001.4941 - val_loss: 1217818624.0000 - val_rmse: 34897.2578\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344317504.0000 - rmse: 18555.7930 - val_loss: 1347380864.0000 - val_rmse: 36706.6875\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358941472.0000 - rmse: 18945.7500 - val_loss: 991936896.0000 - val_rmse: 31495.0293\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358141376.0000 - rmse: 18924.6230 - val_loss: 1180980224.0000 - val_rmse: 34365.3906\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326770848.0000 - rmse: 18076.8027 - val_loss: 931999552.0000 - val_rmse: 30528.6680\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347730240.0000 - rmse: 18647.5254 - val_loss: 615996288.0000 - val_rmse: 24819.2734\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327840608.0000 - rmse: 18106.3672 - val_loss: 723921664.0000 - val_rmse: 26905.7910\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364146720.0000 - rmse: 19082.6270 - val_loss: 1193612032.0000 - val_rmse: 34548.6914\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310218400.0000 - rmse: 17613.0156 - val_loss: 871368256.0000 - val_rmse: 29518.9473\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317683328.0000 - rmse: 17823.6699 - val_loss: 594008832.0000 - val_rmse: 24372.2930\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317488288.0000 - rmse: 17818.1992 - val_loss: 1059295488.0000 - val_rmse: 32546.8184\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357127360.0000 - rmse: 18897.8125 - val_loss: 1108693888.0000 - val_rmse: 33297.0547\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347038944.0000 - rmse: 18628.9805 - val_loss: 591845824.0000 - val_rmse: 24327.8809\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306313888.0000 - rmse: 17501.8223 - val_loss: 936815104.0000 - val_rmse: 30607.4336\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300644672.0000 - rmse: 17339.1055 - val_loss: 957267712.0000 - val_rmse: 30939.7441\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308984128.0000 - rmse: 17577.9453 - val_loss: 840534784.0000 - val_rmse: 28991.9785\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319527104.0000 - rmse: 17875.3203 - val_loss: 2056502656.0000 - val_rmse: 45348.6797\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353547936.0000 - rmse: 18802.8691 - val_loss: 1082875520.0000 - val_rmse: 32907.0742\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342990496.0000 - rmse: 18520.0000 - val_loss: 635565504.0000 - val_rmse: 25210.4238\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331003072.0000 - rmse: 18193.4883 - val_loss: 929378368.0000 - val_rmse: 30485.7070\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301610592.0000 - rmse: 17366.9375 - val_loss: 805685632.0000 - val_rmse: 28384.6016\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313675360.0000 - rmse: 17710.8828 - val_loss: 678108288.0000 - val_rmse: 26040.5117\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288529376.0000 - rmse: 16986.1504 - val_loss: 1681595776.0000 - val_rmse: 41007.2656\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326608640.0000 - rmse: 18072.3164 - val_loss: 959255872.0000 - val_rmse: 30971.8555\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300926400.0000 - rmse: 17347.2305 - val_loss: 796222336.0000 - val_rmse: 28217.4121\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343820608.0000 - rmse: 18542.3984 - val_loss: 1383773056.0000 - val_rmse: 37199.0977\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339852448.0000 - rmse: 18435.0879 - val_loss: 877522752.0000 - val_rmse: 29623.0078\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274502432.0000 - rmse: 16568.1152 - val_loss: 1295575424.0000 - val_rmse: 35994.1016\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323004960.0000 - rmse: 17972.3379 - val_loss: 612665088.0000 - val_rmse: 24752.0703\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296178560.0000 - rmse: 17209.8398 - val_loss: 847349056.0000 - val_rmse: 29109.2598\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311273120.0000 - rmse: 17642.9336 - val_loss: 1045525824.0000 - val_rmse: 32334.5898\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348498624.0000 - rmse: 18668.1152 - val_loss: 1402811520.0000 - val_rmse: 37454.1250\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358243552.0000 - rmse: 18927.3223 - val_loss: 909951296.0000 - val_rmse: 30165.3984\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301043232.0000 - rmse: 17350.5938 - val_loss: 1510798592.0000 - val_rmse: 38868.9922\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328075360.0000 - rmse: 18112.8496 - val_loss: 986296832.0000 - val_rmse: 31405.3633\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310681376.0000 - rmse: 17626.1543 - val_loss: 1090754304.0000 - val_rmse: 33026.5703\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341745760.0000 - rmse: 18486.3672 - val_loss: 1222857344.0000 - val_rmse: 34969.3789\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314314784.0000 - rmse: 17728.9238 - val_loss: 1168282368.0000 - val_rmse: 34180.1445\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313015904.0000 - rmse: 17692.2559 - val_loss: 1006250176.0000 - val_rmse: 31721.4473\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300559104.0000 - rmse: 17336.6387 - val_loss: 1760026752.0000 - val_rmse: 41952.6719\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308015680.0000 - rmse: 17550.3750 - val_loss: 984623360.0000 - val_rmse: 31378.7070\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305909024.0000 - rmse: 17490.2539 - val_loss: 1194502272.0000 - val_rmse: 34561.5703\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282476544.0000 - rmse: 16807.0352 - val_loss: 706722624.0000 - val_rmse: 26584.2559\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316474336.0000 - rmse: 17789.7246 - val_loss: 965677056.0000 - val_rmse: 31075.3457\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288402304.0000 - rmse: 16982.4102 - val_loss: 1154461440.0000 - val_rmse: 33977.3672\n",
      "Epoch 164/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277867136.0000 - rmse: 16669.3457 - val_loss: 1103854208.0000 - val_rmse: 33224.3008\n",
      "Epoch 165/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269991232.0000 - rmse: 16431.4102 - val_loss: 1674129792.0000 - val_rmse: 40916.1328\n",
      "Epoch 166/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355763744.0000 - rmse: 18861.6973 - val_loss: 1313990272.0000 - val_rmse: 36249.0039\n",
      "Epoch 167/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283246208.0000 - rmse: 16829.9180 - val_loss: 717517504.0000 - val_rmse: 26786.5156\n",
      "Epoch 168/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255626000.0000 - rmse: 15988.3066 - val_loss: 1244566272.0000 - val_rmse: 35278.4102\n",
      "Epoch 169/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372294976.0000 - rmse: 19294.9453 - val_loss: 1014404480.0000 - val_rmse: 31849.7168\n",
      "Epoch 170/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275831776.0000 - rmse: 16608.1836 - val_loss: 678302080.0000 - val_rmse: 26044.2324\n",
      "Epoch 171/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258980448.0000 - rmse: 16092.8672 - val_loss: 563762624.0000 - val_rmse: 23743.6855\n",
      "Epoch 172/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260155968.0000 - rmse: 16129.3506 - val_loss: 644007808.0000 - val_rmse: 25377.3086\n",
      "Epoch 173/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303915008.0000 - rmse: 17433.1582 - val_loss: 583363392.0000 - val_rmse: 24152.9160\n",
      "Epoch 174/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313189312.0000 - rmse: 17697.1543 - val_loss: 1064217088.0000 - val_rmse: 32622.3379\n",
      "Epoch 175/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281678016.0000 - rmse: 16783.2656 - val_loss: 943185344.0000 - val_rmse: 30711.3203\n",
      "Epoch 176/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231885952.0000 - rmse: 15227.8008 - val_loss: 1441465216.0000 - val_rmse: 37966.6328\n",
      "Epoch 177/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291099680.0000 - rmse: 17061.6426 - val_loss: 943531072.0000 - val_rmse: 30716.9512\n",
      "Epoch 178/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240914064.0000 - rmse: 15521.4053 - val_loss: 491088032.0000 - val_rmse: 22160.5059\n",
      "Epoch 179/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246749392.0000 - rmse: 15708.2568 - val_loss: 655822080.0000 - val_rmse: 25609.0234\n",
      "Epoch 180/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248983536.0000 - rmse: 15779.2109 - val_loss: 866723968.0000 - val_rmse: 29440.1758\n",
      "Epoch 181/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248961408.0000 - rmse: 15778.5098 - val_loss: 1691352832.0000 - val_rmse: 41126.0586\n",
      "Epoch 182/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278150848.0000 - rmse: 16677.8516 - val_loss: 759705472.0000 - val_rmse: 27562.7520\n",
      "Epoch 183/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251278672.0000 - rmse: 15851.7695 - val_loss: 830857088.0000 - val_rmse: 28824.5918\n",
      "Epoch 184/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253373712.0000 - rmse: 15917.7158 - val_loss: 965182016.0000 - val_rmse: 31067.3770\n",
      "Epoch 185/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340152384.0000 - rmse: 18443.2188 - val_loss: 888986560.0000 - val_rmse: 29815.8770\n",
      "Epoch 186/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235749584.0000 - rmse: 15354.1377 - val_loss: 892336320.0000 - val_rmse: 29871.9961\n",
      "Epoch 187/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303254592.0000 - rmse: 17414.2070 - val_loss: 753177408.0000 - val_rmse: 27444.0781\n",
      "Epoch 188/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306098688.0000 - rmse: 17495.6758 - val_loss: 1638570112.0000 - val_rmse: 40479.2539\n",
      "Epoch 189/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252749408.0000 - rmse: 15898.0928 - val_loss: 1029508032.0000 - val_rmse: 32085.9473\n",
      "Epoch 190/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265849280.0000 - rmse: 16304.8838 - val_loss: 934657216.0000 - val_rmse: 30572.1621\n",
      "Epoch 191/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236426704.0000 - rmse: 15376.1719 - val_loss: 1336968576.0000 - val_rmse: 36564.5820\n",
      "Epoch 192/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272680416.0000 - rmse: 16513.0371 - val_loss: 1053042432.0000 - val_rmse: 32450.6152\n",
      "Epoch 193/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256606128.0000 - rmse: 16018.9287 - val_loss: 982918848.0000 - val_rmse: 31351.5371\n",
      "Epoch 194/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278450528.0000 - rmse: 16686.8379 - val_loss: 922272832.0000 - val_rmse: 30368.9453\n",
      "Epoch 195/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264227824.0000 - rmse: 16255.0840 - val_loss: 1033823680.0000 - val_rmse: 32153.1289\n",
      "Epoch 196/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253393680.0000 - rmse: 15918.3438 - val_loss: 528723616.0000 - val_rmse: 22993.9902\n",
      "Epoch 197/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264926736.0000 - rmse: 16276.5684 - val_loss: 1224041344.0000 - val_rmse: 34986.3008\n",
      "Epoch 198/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 223638880.0000 - rmse: 14954.5586 - val_loss: 910831744.0000 - val_rmse: 30179.9883\n",
      "Epoch 199/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251222112.0000 - rmse: 15849.9854 - val_loss: 1100812288.0000 - val_rmse: 33178.4883\n",
      "Epoch 200/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242850496.0000 - rmse: 15583.6602 - val_loss: 1061400192.0000 - val_rmse: 32579.1367\n",
      "Epoch 201/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227376288.0000 - rmse: 15079.0000 - val_loss: 1245991808.0000 - val_rmse: 35298.6094\n",
      "Epoch 202/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258075680.0000 - rmse: 16064.7334 - val_loss: 1546111872.0000 - val_rmse: 39320.6289\n",
      "Epoch 203/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249955200.0000 - rmse: 15809.9697 - val_loss: 1266542848.0000 - val_rmse: 35588.5195\n",
      "Epoch 204/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247607792.0000 - rmse: 15735.5566 - val_loss: 1085914112.0000 - val_rmse: 32953.2109\n",
      "Epoch 205/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263826928.0000 - rmse: 16242.7490 - val_loss: 797468736.0000 - val_rmse: 28239.4883\n",
      "Epoch 206/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218120448.0000 - rmse: 14768.8994 - val_loss: 490658528.0000 - val_rmse: 22150.8125\n",
      "Epoch 207/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251190304.0000 - rmse: 15848.9824 - val_loss: 946956672.0000 - val_rmse: 30772.6602\n",
      "Epoch 208/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234020912.0000 - rmse: 15297.7402 - val_loss: 691646784.0000 - val_rmse: 26299.1777\n",
      "Epoch 209/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251358288.0000 - rmse: 15854.2822 - val_loss: 1222724480.0000 - val_rmse: 34967.4766\n",
      "Epoch 210/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251622688.0000 - rmse: 15862.6162 - val_loss: 1664393984.0000 - val_rmse: 40796.9805\n",
      "Epoch 211/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263911824.0000 - rmse: 16245.3633 - val_loss: 1234773248.0000 - val_rmse: 35139.3398\n",
      "Epoch 212/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 205140960.0000 - rmse: 14322.7412 - val_loss: 1188171520.0000 - val_rmse: 34469.8633\n",
      "Epoch 213/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239805136.0000 - rmse: 15485.6406 - val_loss: 1893189120.0000 - val_rmse: 43510.7930\n",
      "Epoch 214/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221583312.0000 - rmse: 14885.6738 - val_loss: 984197184.0000 - val_rmse: 31371.9180\n",
      "Epoch 215/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228899440.0000 - rmse: 15129.4209 - val_loss: 910020032.0000 - val_rmse: 30166.5371\n",
      "Epoch 216/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241080304.0000 - rmse: 15526.7588 - val_loss: 540059904.0000 - val_rmse: 23239.1895\n",
      "Epoch 217/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259503936.0000 - rmse: 16109.1250 - val_loss: 613371456.0000 - val_rmse: 24766.3379\n",
      "Epoch 218/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228053904.0000 - rmse: 15101.4512 - val_loss: 1074874368.0000 - val_rmse: 32785.2773\n",
      "Epoch 219/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224463792.0000 - rmse: 14982.1143 - val_loss: 1665751296.0000 - val_rmse: 40813.6172\n",
      "Epoch 220/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237712752.0000 - rmse: 15417.9326 - val_loss: 1688982272.0000 - val_rmse: 41097.2305\n",
      "Epoch 221/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 225662800.0000 - rmse: 15022.0762 - val_loss: 970391936.0000 - val_rmse: 31151.1152\n",
      "Epoch 222/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278799328.0000 - rmse: 16697.2832 - val_loss: 957179648.0000 - val_rmse: 30938.3203\n",
      "Epoch 223/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 208059168.0000 - rmse: 14424.2549 - val_loss: 1302990592.0000 - val_rmse: 36096.9570\n",
      "Epoch 224/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236904048.0000 - rmse: 15391.6846 - val_loss: 1144986880.0000 - val_rmse: 33837.6562\n",
      "Epoch 225/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 205898272.0000 - rmse: 14349.1543 - val_loss: 1353741184.0000 - val_rmse: 36793.2188\n",
      "Epoch 226/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255334992.0000 - rmse: 15979.2031 - val_loss: 883181312.0000 - val_rmse: 29718.3652\n",
      "Epoch 227/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231838976.0000 - rmse: 15226.2578 - val_loss: 554789760.0000 - val_rmse: 23553.9746\n",
      "Epoch 228/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238423344.0000 - rmse: 15440.9609 - val_loss: 743393792.0000 - val_rmse: 27265.2480\n",
      "Epoch 229/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221013648.0000 - rmse: 14866.5254 - val_loss: 1097272064.0000 - val_rmse: 33125.0977\n",
      "Epoch 230/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244134016.0000 - rmse: 15624.7871 - val_loss: 948242112.0000 - val_rmse: 30793.5410\n",
      "Epoch 231/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226598528.0000 - rmse: 15053.1875 - val_loss: 488991584.0000 - val_rmse: 22113.1543\n",
      "Epoch 232/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241747712.0000 - rmse: 15548.2363 - val_loss: 1096083712.0000 - val_rmse: 33107.1523\n",
      "Epoch 233/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219261376.0000 - rmse: 14807.4756 - val_loss: 798492864.0000 - val_rmse: 28257.6152\n",
      "Epoch 234/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202923648.0000 - rmse: 14245.1250 - val_loss: 999976704.0000 - val_rmse: 31622.4082\n",
      "Epoch 235/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 195663696.0000 - rmse: 13987.9834 - val_loss: 1595298432.0000 - val_rmse: 39941.1875\n",
      "Epoch 236/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226186240.0000 - rmse: 15039.4873 - val_loss: 860586176.0000 - val_rmse: 29335.7500\n",
      "Epoch 237/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261557008.0000 - rmse: 16172.7236 - val_loss: 1576492032.0000 - val_rmse: 39705.0625\n",
      "Epoch 238/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 187797392.0000 - rmse: 13703.9180 - val_loss: 1437839104.0000 - val_rmse: 37918.8477\n",
      "Epoch 239/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 205867472.0000 - rmse: 14348.0811 - val_loss: 1966012800.0000 - val_rmse: 44339.7422\n",
      "Epoch 240/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222693728.0000 - rmse: 14922.9248 - val_loss: 1269422720.0000 - val_rmse: 35628.9609\n",
      "Epoch 241/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236551920.0000 - rmse: 15380.2432 - val_loss: 1142133632.0000 - val_rmse: 33795.4688\n",
      "Epoch 242/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291050496.0000 - rmse: 17060.2012 - val_loss: 938593088.0000 - val_rmse: 30636.4668\n",
      "Epoch 243/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 203895856.0000 - rmse: 14279.2100 - val_loss: 1063753024.0000 - val_rmse: 32615.2246\n",
      "Epoch 244/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214594640.0000 - rmse: 14649.0488 - val_loss: 1381903488.0000 - val_rmse: 37173.9609\n",
      "Epoch 245/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290190656.0000 - rmse: 17034.9805 - val_loss: 951409920.0000 - val_rmse: 30844.9336\n",
      "Epoch 246/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222991728.0000 - rmse: 14932.9062 - val_loss: 1416831488.0000 - val_rmse: 37640.8203\n",
      "Epoch 247/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 199635008.0000 - rmse: 14129.2236 - val_loss: 950910912.0000 - val_rmse: 30836.8438\n",
      "Epoch 248/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217934864.0000 - rmse: 14762.6152 - val_loss: 1426547712.0000 - val_rmse: 37769.6680\n",
      "Epoch 249/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217670336.0000 - rmse: 14753.6533 - val_loss: 512911200.0000 - val_rmse: 22647.5391\n",
      "Epoch 250/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249872784.0000 - rmse: 15807.3643 - val_loss: 1296243072.0000 - val_rmse: 36003.3750\n",
      "Epoch 251/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221947664.0000 - rmse: 14897.9053 - val_loss: 1448813952.0000 - val_rmse: 38063.2852\n",
      "Epoch 252/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281088608.0000 - rmse: 16765.6973 - val_loss: 1147446016.0000 - val_rmse: 33873.9727\n",
      "Epoch 253/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228812208.0000 - rmse: 15126.5361 - val_loss: 1320804608.0000 - val_rmse: 36342.8750\n",
      "Epoch 254/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240395232.0000 - rmse: 15504.6826 - val_loss: 796369792.0000 - val_rmse: 28220.0215\n",
      "Epoch 255/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224717616.0000 - rmse: 14990.5830 - val_loss: 2060378752.0000 - val_rmse: 45391.3945\n",
      "Epoch 256/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229624064.0000 - rmse: 15153.3496 - val_loss: 1187537792.0000 - val_rmse: 34460.6719\n",
      "Epoch 257/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222641440.0000 - rmse: 14921.1738 - val_loss: 942084032.0000 - val_rmse: 30693.3848\n",
      "Epoch 258/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 233870304.0000 - rmse: 15292.8174 - val_loss: 1589745408.0000 - val_rmse: 39871.6133\n",
      "Epoch 259/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246854288.0000 - rmse: 15711.5957 - val_loss: 1471990400.0000 - val_rmse: 38366.5273\n",
      "Epoch 260/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220793824.0000 - rmse: 14859.1299 - val_loss: 1001827648.0000 - val_rmse: 31651.6602\n",
      "Epoch 261/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230231280.0000 - rmse: 15173.3721 - val_loss: 716140352.0000 - val_rmse: 26760.7969\n",
      "Epoch 262/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236578288.0000 - rmse: 15381.0996 - val_loss: 690173120.0000 - val_rmse: 26271.1445\n",
      "Epoch 263/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246996128.0000 - rmse: 15716.1084 - val_loss: 1282971392.0000 - val_rmse: 35818.5898\n",
      "Epoch 264/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 194777504.0000 - rmse: 13956.2695 - val_loss: 1018391552.0000 - val_rmse: 31912.2480\n",
      "Epoch 265/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218600976.0000 - rmse: 14785.1592 - val_loss: 1716485376.0000 - val_rmse: 41430.4844\n",
      "Epoch 266/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248658224.0000 - rmse: 15768.8984 - val_loss: 759899776.0000 - val_rmse: 27566.2793\n",
      "Epoch 267/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214551264.0000 - rmse: 14647.5645 - val_loss: 780091968.0000 - val_rmse: 27930.1270\n",
      "Epoch 268/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207407168.0000 - rmse: 14401.6357 - val_loss: 2053095552.0000 - val_rmse: 45311.0977\n",
      "104/104 [==============================] - 0s 681us/step - loss: 448117184.0000 - rmse: 21168.7773\n",
      "[448117184.0, 21168.77734375]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 13095890944.0000 - rmse: 114437.2812 - val_loss: 1985340032.0000 - val_rmse: 44557.1562\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2065319936.0000 - rmse: 45445.7930 - val_loss: 1290237952.0000 - val_rmse: 35919.8828\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1625919872.0000 - rmse: 40322.6953 - val_loss: 1115034624.0000 - val_rmse: 33392.1328\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1462545280.0000 - rmse: 38243.2383 - val_loss: 1051580928.0000 - val_rmse: 32428.0879\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1316642560.0000 - rmse: 36285.5703 - val_loss: 1044257024.0000 - val_rmse: 32314.9668\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1292516352.0000 - rmse: 35951.5820 - val_loss: 967732352.0000 - val_rmse: 31108.3965\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1173334656.0000 - rmse: 34253.9727 - val_loss: 925542016.0000 - val_rmse: 30422.7227\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1170355072.0000 - rmse: 34210.4531 - val_loss: 1018543168.0000 - val_rmse: 31914.6230\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1084654336.0000 - rmse: 32934.0898 - val_loss: 856729664.0000 - val_rmse: 29269.9453\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1065942400.0000 - rmse: 32648.7734 - val_loss: 896848512.0000 - val_rmse: 29947.4297\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1068904128.0000 - rmse: 32694.0996 - val_loss: 822185856.0000 - val_rmse: 28673.7832\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1047185792.0000 - rmse: 32360.2500 - val_loss: 830564224.0000 - val_rmse: 28819.5117\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 994553024.0000 - rmse: 31536.5352 - val_loss: 796159360.0000 - val_rmse: 28216.2949\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 978317824.0000 - rmse: 31278.0723 - val_loss: 803834112.0000 - val_rmse: 28351.9688\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 934527936.0000 - rmse: 30570.0488 - val_loss: 924454784.0000 - val_rmse: 30404.8477\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 964696960.0000 - rmse: 31059.5703 - val_loss: 777625728.0000 - val_rmse: 27885.9414\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 920033792.0000 - rmse: 30332.0586 - val_loss: 777419200.0000 - val_rmse: 27882.2383\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 881812736.0000 - rmse: 29695.3320 - val_loss: 752063040.0000 - val_rmse: 27423.7676\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 855422400.0000 - rmse: 29247.6055 - val_loss: 735694080.0000 - val_rmse: 27123.6797\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 838775168.0000 - rmse: 28961.6152 - val_loss: 711012992.0000 - val_rmse: 26664.8262\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 841418816.0000 - rmse: 29007.2188 - val_loss: 931798400.0000 - val_rmse: 30525.3730\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 825805504.0000 - rmse: 28736.8301 - val_loss: 682414784.0000 - val_rmse: 26123.0703\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 804101824.0000 - rmse: 28356.6895 - val_loss: 703201088.0000 - val_rmse: 26517.9395\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 765818752.0000 - rmse: 27673.4297 - val_loss: 698348544.0000 - val_rmse: 26426.2832\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 750574848.0000 - rmse: 27396.6211 - val_loss: 717695296.0000 - val_rmse: 26789.8359\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733615616.0000 - rmse: 27085.3398 - val_loss: 740657216.0000 - val_rmse: 27215.0176\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 701927616.0000 - rmse: 26493.9160 - val_loss: 668713088.0000 - val_rmse: 25859.4883\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 695082816.0000 - rmse: 26364.4219 - val_loss: 655890240.0000 - val_rmse: 25610.3535\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 681718144.0000 - rmse: 26109.7324 - val_loss: 627089408.0000 - val_rmse: 25041.7520\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 657573120.0000 - rmse: 25643.1895 - val_loss: 631620288.0000 - val_rmse: 25132.0566\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665299328.0000 - rmse: 25793.3965 - val_loss: 631835392.0000 - val_rmse: 25136.3359\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614757696.0000 - rmse: 24794.3066 - val_loss: 675724416.0000 - val_rmse: 25994.6973\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634653568.0000 - rmse: 25192.3320 - val_loss: 630020416.0000 - val_rmse: 25100.2070\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585463232.0000 - rmse: 24196.3477 - val_loss: 595388928.0000 - val_rmse: 24400.5898\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586424256.0000 - rmse: 24216.1973 - val_loss: 596062464.0000 - val_rmse: 24414.3906\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614718656.0000 - rmse: 24793.5195 - val_loss: 592398336.0000 - val_rmse: 24339.2344\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 602422528.0000 - rmse: 24544.2969 - val_loss: 579863616.0000 - val_rmse: 24080.3574\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 617654848.0000 - rmse: 24852.6621 - val_loss: 569572544.0000 - val_rmse: 23865.7188\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548903104.0000 - rmse: 23428.6816 - val_loss: 585940096.0000 - val_rmse: 24206.1992\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536842016.0000 - rmse: 23169.8516 - val_loss: 572688448.0000 - val_rmse: 23930.9102\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541123968.0000 - rmse: 23262.0723 - val_loss: 685774912.0000 - val_rmse: 26187.3047\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539725952.0000 - rmse: 23232.0020 - val_loss: 533302784.0000 - val_rmse: 23093.3496\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520915776.0000 - rmse: 22823.5801 - val_loss: 558895232.0000 - val_rmse: 23640.9648\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472602304.0000 - rmse: 21739.4180 - val_loss: 623692160.0000 - val_rmse: 24973.8281\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483837184.0000 - rmse: 21996.2988 - val_loss: 537281728.0000 - val_rmse: 23179.3379\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503130432.0000 - rmse: 22430.5684 - val_loss: 548652352.0000 - val_rmse: 23423.3281\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499450240.0000 - rmse: 22348.3828 - val_loss: 549729088.0000 - val_rmse: 23446.3027\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460396416.0000 - rmse: 21456.8496 - val_loss: 546178176.0000 - val_rmse: 23370.4531\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446634112.0000 - rmse: 21133.7188 - val_loss: 528201984.0000 - val_rmse: 22982.6445\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472863392.0000 - rmse: 21745.4199 - val_loss: 508837120.0000 - val_rmse: 22557.4160\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435171648.0000 - rmse: 20860.7676 - val_loss: 540143360.0000 - val_rmse: 23240.9844\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489625920.0000 - rmse: 22127.4922 - val_loss: 545288896.0000 - val_rmse: 23351.4219\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449080512.0000 - rmse: 21191.5176 - val_loss: 530906816.0000 - val_rmse: 23041.4141\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438215328.0000 - rmse: 20933.5938 - val_loss: 509893024.0000 - val_rmse: 22580.8105\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428886944.0000 - rmse: 20709.5840 - val_loss: 513852064.0000 - val_rmse: 22668.3027\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406882176.0000 - rmse: 20171.3203 - val_loss: 579390400.0000 - val_rmse: 24070.5293\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459431424.0000 - rmse: 21434.3496 - val_loss: 520318624.0000 - val_rmse: 22810.4941\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427607328.0000 - rmse: 20678.6680 - val_loss: 645971072.0000 - val_rmse: 25415.9609\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440955776.0000 - rmse: 20998.9434 - val_loss: 517396928.0000 - val_rmse: 22746.3613\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433676704.0000 - rmse: 20824.9062 - val_loss: 548468288.0000 - val_rmse: 23419.4004\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397822688.0000 - rmse: 19945.4902 - val_loss: 457892064.0000 - val_rmse: 21398.4121\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400646304.0000 - rmse: 20016.1484 - val_loss: 464768672.0000 - val_rmse: 21558.4922\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397686560.0000 - rmse: 19942.0801 - val_loss: 508599904.0000 - val_rmse: 22552.1582\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386824352.0000 - rmse: 19667.8496 - val_loss: 526639488.0000 - val_rmse: 22948.6270\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393284832.0000 - rmse: 19831.4102 - val_loss: 511941504.0000 - val_rmse: 22626.1250\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409459904.0000 - rmse: 20235.1152 - val_loss: 507757120.0000 - val_rmse: 22533.4668\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365591552.0000 - rmse: 19120.4473 - val_loss: 512213088.0000 - val_rmse: 22632.1250\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377494720.0000 - rmse: 19429.2227 - val_loss: 512652672.0000 - val_rmse: 22641.8340\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389606176.0000 - rmse: 19738.4434 - val_loss: 492463424.0000 - val_rmse: 22191.5156\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349039200.0000 - rmse: 18682.5898 - val_loss: 866819200.0000 - val_rmse: 29441.7930\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344372192.0000 - rmse: 18557.2676 - val_loss: 556339712.0000 - val_rmse: 23586.8535\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362024704.0000 - rmse: 19026.9473 - val_loss: 494024736.0000 - val_rmse: 22226.6680\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356962560.0000 - rmse: 18893.4512 - val_loss: 490459232.0000 - val_rmse: 22146.3145\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335531936.0000 - rmse: 18317.5312 - val_loss: 529776576.0000 - val_rmse: 23016.8750\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425245824.0000 - rmse: 20621.4863 - val_loss: 516277856.0000 - val_rmse: 22721.7480\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343233472.0000 - rmse: 18526.5586 - val_loss: 567593280.0000 - val_rmse: 23824.2168\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372826912.0000 - rmse: 19308.7246 - val_loss: 487770016.0000 - val_rmse: 22085.5137\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336273760.0000 - rmse: 18337.7676 - val_loss: 503279840.0000 - val_rmse: 22433.8984\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342185056.0000 - rmse: 18498.2441 - val_loss: 628530944.0000 - val_rmse: 25070.5195\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342360352.0000 - rmse: 18502.9805 - val_loss: 502909536.0000 - val_rmse: 22425.6445\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363853856.0000 - rmse: 19074.9531 - val_loss: 589195712.0000 - val_rmse: 24273.3535\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348160352.0000 - rmse: 18659.0547 - val_loss: 543246336.0000 - val_rmse: 23307.6445\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328175808.0000 - rmse: 18115.6230 - val_loss: 488805344.0000 - val_rmse: 22108.9414\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341016800.0000 - rmse: 18466.6387 - val_loss: 524035488.0000 - val_rmse: 22891.8223\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329191712.0000 - rmse: 18143.6406 - val_loss: 513985984.0000 - val_rmse: 22671.2578\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331810912.0000 - rmse: 18215.6777 - val_loss: 641797632.0000 - val_rmse: 25333.7246\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375150144.0000 - rmse: 19368.7930 - val_loss: 600424000.0000 - val_rmse: 24503.5488\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314802752.0000 - rmse: 17742.6797 - val_loss: 525410624.0000 - val_rmse: 22921.8359\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312089664.0000 - rmse: 17666.0586 - val_loss: 521549152.0000 - val_rmse: 22837.4492\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354564736.0000 - rmse: 18829.8887 - val_loss: 482158208.0000 - val_rmse: 21958.0996\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337906816.0000 - rmse: 18382.2402 - val_loss: 576714752.0000 - val_rmse: 24014.8867\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300882656.0000 - rmse: 17345.9668 - val_loss: 552090944.0000 - val_rmse: 23496.6152\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297359104.0000 - rmse: 17244.1016 - val_loss: 515814688.0000 - val_rmse: 22711.5547\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291510720.0000 - rmse: 17073.6836 - val_loss: 502533696.0000 - val_rmse: 22417.2637\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280974624.0000 - rmse: 16762.2969 - val_loss: 553618240.0000 - val_rmse: 23529.0898\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314643584.0000 - rmse: 17738.1953 - val_loss: 535371200.0000 - val_rmse: 23138.0898\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316087200.0000 - rmse: 17778.8379 - val_loss: 494303872.0000 - val_rmse: 22232.9453\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255111568.0000 - rmse: 15972.2109 - val_loss: 532722720.0000 - val_rmse: 23080.7852\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293451296.0000 - rmse: 17130.4180 - val_loss: 487136480.0000 - val_rmse: 22071.1680\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286438016.0000 - rmse: 16924.4785 - val_loss: 510109312.0000 - val_rmse: 22585.5996\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297558080.0000 - rmse: 17249.8711 - val_loss: 542005952.0000 - val_rmse: 23281.0215\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287993280.0000 - rmse: 16970.3633 - val_loss: 551470784.0000 - val_rmse: 23483.4141\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300418240.0000 - rmse: 17332.5762 - val_loss: 517921440.0000 - val_rmse: 22757.8867\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317462176.0000 - rmse: 17817.4688 - val_loss: 531353792.0000 - val_rmse: 23051.1133\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280801408.0000 - rmse: 16757.1270 - val_loss: 516890464.0000 - val_rmse: 22735.2246\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320946624.0000 - rmse: 17914.9805 - val_loss: 567588288.0000 - val_rmse: 23824.1094\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290343520.0000 - rmse: 17039.4668 - val_loss: 609250240.0000 - val_rmse: 24682.9941\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300381600.0000 - rmse: 17331.5195 - val_loss: 505652288.0000 - val_rmse: 22486.7129\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330728032.0000 - rmse: 18185.9277 - val_loss: 492864128.0000 - val_rmse: 22200.5430\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261290000.0000 - rmse: 16164.4658 - val_loss: 539800640.0000 - val_rmse: 23233.6094\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297418592.0000 - rmse: 17245.8281 - val_loss: 503142208.0000 - val_rmse: 22430.8301\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292746496.0000 - rmse: 17109.8359 - val_loss: 503096896.0000 - val_rmse: 22429.8223\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285089600.0000 - rmse: 16884.5938 - val_loss: 502851424.0000 - val_rmse: 22424.3496\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303435712.0000 - rmse: 17419.4043 - val_loss: 484774208.0000 - val_rmse: 22017.5879\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256371776.0000 - rmse: 16011.6113 - val_loss: 644706752.0000 - val_rmse: 25391.0762\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298981856.0000 - rmse: 17291.0898 - val_loss: 555594944.0000 - val_rmse: 23571.0625\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292788384.0000 - rmse: 17111.0586 - val_loss: 576799424.0000 - val_rmse: 24016.6484\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264012400.0000 - rmse: 16248.4570 - val_loss: 512389728.0000 - val_rmse: 22636.0273\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293620512.0000 - rmse: 17135.3574 - val_loss: 510026592.0000 - val_rmse: 22583.7676\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259146928.0000 - rmse: 16098.0391 - val_loss: 510287488.0000 - val_rmse: 22589.5430\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249215264.0000 - rmse: 15786.5518 - val_loss: 533153152.0000 - val_rmse: 23090.1094\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253690048.0000 - rmse: 15927.6504 - val_loss: 637441088.0000 - val_rmse: 25247.5977\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288880544.0000 - rmse: 16996.4863 - val_loss: 493843456.0000 - val_rmse: 22222.5879\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253894432.0000 - rmse: 15934.0645 - val_loss: 504229824.0000 - val_rmse: 22455.0625\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290081088.0000 - rmse: 17031.7676 - val_loss: 536118848.0000 - val_rmse: 23154.2402\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304481920.0000 - rmse: 17449.4102 - val_loss: 515580736.0000 - val_rmse: 22706.4023\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262233072.0000 - rmse: 16193.6104 - val_loss: 574036736.0000 - val_rmse: 23959.0625\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292788448.0000 - rmse: 17111.0605 - val_loss: 477343648.0000 - val_rmse: 21848.1934\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263970496.0000 - rmse: 16247.1650 - val_loss: 525855936.0000 - val_rmse: 22931.5469\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292297408.0000 - rmse: 17096.7051 - val_loss: 676244224.0000 - val_rmse: 26004.6953\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274867712.0000 - rmse: 16579.1328 - val_loss: 555475904.0000 - val_rmse: 23568.5371\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279191840.0000 - rmse: 16709.0332 - val_loss: 555287616.0000 - val_rmse: 23564.5410\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246234464.0000 - rmse: 15691.8574 - val_loss: 468771168.0000 - val_rmse: 21651.1230\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293449984.0000 - rmse: 17130.3809 - val_loss: 500602912.0000 - val_rmse: 22374.1543\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257428192.0000 - rmse: 16044.5664 - val_loss: 479845120.0000 - val_rmse: 21905.3652\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264593872.0000 - rmse: 16266.3398 - val_loss: 488236896.0000 - val_rmse: 22096.0801\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270395296.0000 - rmse: 16443.6992 - val_loss: 475731520.0000 - val_rmse: 21811.2695\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238256368.0000 - rmse: 15435.5537 - val_loss: 699330688.0000 - val_rmse: 26444.8594\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251034160.0000 - rmse: 15844.0566 - val_loss: 491263680.0000 - val_rmse: 22164.4688\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284409440.0000 - rmse: 16864.4395 - val_loss: 633493184.0000 - val_rmse: 25169.2910\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232559808.0000 - rmse: 15249.9111 - val_loss: 632506496.0000 - val_rmse: 25149.6797\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258528976.0000 - rmse: 16078.8350 - val_loss: 490628896.0000 - val_rmse: 22150.1426\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231928400.0000 - rmse: 15229.1924 - val_loss: 485641408.0000 - val_rmse: 22037.2715\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240446576.0000 - rmse: 15506.3369 - val_loss: 471536224.0000 - val_rmse: 21714.8828\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234870992.0000 - rmse: 15325.5000 - val_loss: 473142112.0000 - val_rmse: 21751.8301\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227450912.0000 - rmse: 15081.4746 - val_loss: 516875744.0000 - val_rmse: 22734.9004\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288782816.0000 - rmse: 16993.6094 - val_loss: 475861056.0000 - val_rmse: 21814.2402\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279930144.0000 - rmse: 16731.1113 - val_loss: 476678848.0000 - val_rmse: 21832.9746\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253539280.0000 - rmse: 15922.9150 - val_loss: 539451008.0000 - val_rmse: 23226.0840\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241616816.0000 - rmse: 15544.0264 - val_loss: 499220608.0000 - val_rmse: 22343.2461\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266289536.0000 - rmse: 16318.3789 - val_loss: 523088544.0000 - val_rmse: 22871.1289\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224517376.0000 - rmse: 14983.9023 - val_loss: 483450816.0000 - val_rmse: 21987.5137\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227462688.0000 - rmse: 15081.8643 - val_loss: 509958944.0000 - val_rmse: 22582.2695\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216935792.0000 - rmse: 14728.7393 - val_loss: 581330752.0000 - val_rmse: 24110.8008\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224754720.0000 - rmse: 14991.8203 - val_loss: 483066368.0000 - val_rmse: 21978.7695\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229664560.0000 - rmse: 15154.6855 - val_loss: 456724096.0000 - val_rmse: 21371.1035\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220732832.0000 - rmse: 14857.0791 - val_loss: 445323744.0000 - val_rmse: 21102.6953\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249422288.0000 - rmse: 15793.1074 - val_loss: 479512704.0000 - val_rmse: 21897.7773\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230271920.0000 - rmse: 15174.7129 - val_loss: 476523104.0000 - val_rmse: 21829.4062\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 198408944.0000 - rmse: 14085.7695 - val_loss: 486036288.0000 - val_rmse: 22046.2305\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241663584.0000 - rmse: 15545.5303 - val_loss: 509441120.0000 - val_rmse: 22570.8008\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 213403632.0000 - rmse: 14608.3398 - val_loss: 466699072.0000 - val_rmse: 21603.2168\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257577152.0000 - rmse: 16049.2080 - val_loss: 495697376.0000 - val_rmse: 22264.2617\n",
      "Epoch 164/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237855968.0000 - rmse: 15422.5781 - val_loss: 501005856.0000 - val_rmse: 22383.1602\n",
      "Epoch 165/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245116240.0000 - rmse: 15656.1885 - val_loss: 517091392.0000 - val_rmse: 22739.6426\n",
      "Epoch 166/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235228784.0000 - rmse: 15337.1680 - val_loss: 539241856.0000 - val_rmse: 23221.5820\n",
      "Epoch 167/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221371936.0000 - rmse: 14878.5713 - val_loss: 506030720.0000 - val_rmse: 22495.1250\n",
      "Epoch 168/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224227184.0000 - rmse: 14974.2148 - val_loss: 455745600.0000 - val_rmse: 21348.1973\n",
      "Epoch 169/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246627184.0000 - rmse: 15704.3672 - val_loss: 479928704.0000 - val_rmse: 21907.2754\n",
      "Epoch 170/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221105840.0000 - rmse: 14869.6270 - val_loss: 478731424.0000 - val_rmse: 21879.9297\n",
      "Epoch 171/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230115760.0000 - rmse: 15169.5654 - val_loss: 466600128.0000 - val_rmse: 21600.9258\n",
      "Epoch 172/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 189285616.0000 - rmse: 13758.1094 - val_loss: 444844800.0000 - val_rmse: 21091.3418\n",
      "Epoch 173/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206910016.0000 - rmse: 14384.3652 - val_loss: 708171904.0000 - val_rmse: 26611.4980\n",
      "Epoch 174/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222012016.0000 - rmse: 14900.0645 - val_loss: 497405760.0000 - val_rmse: 22302.5938\n",
      "Epoch 175/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235617424.0000 - rmse: 15349.8330 - val_loss: 488969120.0000 - val_rmse: 22112.6445\n",
      "Epoch 176/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214743520.0000 - rmse: 14654.1279 - val_loss: 531554368.0000 - val_rmse: 23055.4629\n",
      "Epoch 177/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241949408.0000 - rmse: 15554.7217 - val_loss: 575834816.0000 - val_rmse: 23996.5566\n",
      "Epoch 178/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218307200.0000 - rmse: 14775.2217 - val_loss: 454678848.0000 - val_rmse: 21323.1992\n",
      "Epoch 179/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207729952.0000 - rmse: 14412.8389 - val_loss: 477165568.0000 - val_rmse: 21844.1191\n",
      "Epoch 180/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235446528.0000 - rmse: 15344.2646 - val_loss: 503468288.0000 - val_rmse: 22438.0977\n",
      "Epoch 181/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226348224.0000 - rmse: 15044.8721 - val_loss: 503701600.0000 - val_rmse: 22443.2969\n",
      "Epoch 182/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250277936.0000 - rmse: 15820.1738 - val_loss: 497344288.0000 - val_rmse: 22301.2148\n",
      "Epoch 183/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206912640.0000 - rmse: 14384.4570 - val_loss: 476643712.0000 - val_rmse: 21832.1699\n",
      "Epoch 184/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220397920.0000 - rmse: 14845.8018 - val_loss: 471511808.0000 - val_rmse: 21714.3203\n",
      "Epoch 185/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 188997664.0000 - rmse: 13747.6406 - val_loss: 502046528.0000 - val_rmse: 22406.3945\n",
      "Epoch 186/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244939392.0000 - rmse: 15650.5381 - val_loss: 692824640.0000 - val_rmse: 26321.5605\n",
      "Epoch 187/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256206048.0000 - rmse: 16006.4365 - val_loss: 479151136.0000 - val_rmse: 21889.5195\n",
      "Epoch 188/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224947456.0000 - rmse: 14998.2471 - val_loss: 454481184.0000 - val_rmse: 21318.5645\n",
      "Epoch 189/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 197766496.0000 - rmse: 14062.9443 - val_loss: 565353408.0000 - val_rmse: 23777.1602\n",
      "Epoch 190/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217431504.0000 - rmse: 14745.5586 - val_loss: 488849792.0000 - val_rmse: 22109.9473\n",
      "Epoch 191/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231265376.0000 - rmse: 15207.4102 - val_loss: 473796160.0000 - val_rmse: 21766.8574\n",
      "Epoch 192/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236417216.0000 - rmse: 15375.8613 - val_loss: 476253888.0000 - val_rmse: 21823.2402\n",
      "Epoch 193/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 192066528.0000 - rmse: 13858.8047 - val_loss: 511249568.0000 - val_rmse: 22610.8281\n",
      "Epoch 194/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214662448.0000 - rmse: 14651.3613 - val_loss: 463172544.0000 - val_rmse: 21521.4434\n",
      "Epoch 195/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214406256.0000 - rmse: 14642.6152 - val_loss: 501355872.0000 - val_rmse: 22390.9766\n",
      "Epoch 196/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234864976.0000 - rmse: 15325.3037 - val_loss: 451663360.0000 - val_rmse: 21252.3711\n",
      "Epoch 197/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220597440.0000 - rmse: 14852.5215 - val_loss: 512054304.0000 - val_rmse: 22628.6172\n",
      "Epoch 198/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 204195424.0000 - rmse: 14289.6934 - val_loss: 450917792.0000 - val_rmse: 21234.8242\n",
      "Epoch 199/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218515984.0000 - rmse: 14782.2842 - val_loss: 470801600.0000 - val_rmse: 21697.9629\n",
      "Epoch 200/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224380224.0000 - rmse: 14979.3252 - val_loss: 462842688.0000 - val_rmse: 21513.7773\n",
      "Epoch 201/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217811936.0000 - rmse: 14758.4512 - val_loss: 559282176.0000 - val_rmse: 23649.1465\n",
      "Epoch 202/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240648320.0000 - rmse: 15512.8408 - val_loss: 463437024.0000 - val_rmse: 21527.5879\n",
      "Epoch 203/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226824496.0000 - rmse: 15060.6904 - val_loss: 426382016.0000 - val_rmse: 20649.0176\n",
      "Epoch 204/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236669280.0000 - rmse: 15384.0576 - val_loss: 440387200.0000 - val_rmse: 20985.4043\n",
      "Epoch 205/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 203871600.0000 - rmse: 14278.3584 - val_loss: 460991136.0000 - val_rmse: 21470.7031\n",
      "Epoch 206/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207730000.0000 - rmse: 14412.8398 - val_loss: 465421024.0000 - val_rmse: 21573.6172\n",
      "Epoch 207/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220617264.0000 - rmse: 14853.1875 - val_loss: 439388640.0000 - val_rmse: 20961.5977\n",
      "Epoch 208/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 196926976.0000 - rmse: 14033.0654 - val_loss: 492273568.0000 - val_rmse: 22187.2363\n",
      "Epoch 209/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206005232.0000 - rmse: 14352.8799 - val_loss: 464244288.0000 - val_rmse: 21546.3281\n",
      "Epoch 210/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229264288.0000 - rmse: 15141.4727 - val_loss: 469943872.0000 - val_rmse: 21678.1875\n",
      "Epoch 211/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215738288.0000 - rmse: 14688.0303 - val_loss: 483433920.0000 - val_rmse: 21987.1289\n",
      "Epoch 212/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202134512.0000 - rmse: 14217.3984 - val_loss: 500852288.0000 - val_rmse: 22379.7266\n",
      "Epoch 213/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 200079136.0000 - rmse: 14144.9307 - val_loss: 471226112.0000 - val_rmse: 21707.7422\n",
      "Epoch 214/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 203915552.0000 - rmse: 14279.8955 - val_loss: 480275072.0000 - val_rmse: 21915.1777\n",
      "Epoch 215/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202150176.0000 - rmse: 14217.9512 - val_loss: 544889920.0000 - val_rmse: 23342.8750\n",
      "104/104 [==============================] - 0s 672us/step - loss: 1125280384.0000 - rmse: 33545.1953\n",
      "[1125280384.0, 33545.1953125]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 11089736704.0000 - rmse: 105307.8203 - val_loss: 2053898752.0000 - val_rmse: 45319.9609\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1932961792.0000 - rmse: 43965.4609 - val_loss: 1409046272.0000 - val_rmse: 37537.2656\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1498748032.0000 - rmse: 38713.6680 - val_loss: 1379851136.0000 - val_rmse: 37146.3477\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1334815872.0000 - rmse: 36535.1328 - val_loss: 1078387328.0000 - val_rmse: 32838.8086\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1214878208.0000 - rmse: 34855.1016 - val_loss: 1030603392.0000 - val_rmse: 32103.0117\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1196708992.0000 - rmse: 34593.4805 - val_loss: 1079677440.0000 - val_rmse: 32858.4453\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1127472000.0000 - rmse: 33577.8516 - val_loss: 955882496.0000 - val_rmse: 30917.3496\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1082641792.0000 - rmse: 32903.5234 - val_loss: 928464192.0000 - val_rmse: 30470.7109\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1048375296.0000 - rmse: 32378.6250 - val_loss: 962306816.0000 - val_rmse: 31021.0703\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1089688320.0000 - rmse: 33010.4258 - val_loss: 904165056.0000 - val_rmse: 30069.3379\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1035725696.0000 - rmse: 32182.6934 - val_loss: 945304128.0000 - val_rmse: 30745.7988\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1007935872.0000 - rmse: 31748.0059 - val_loss: 917935104.0000 - val_rmse: 30297.4434\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 975361280.0000 - rmse: 31230.7754 - val_loss: 944316864.0000 - val_rmse: 30729.7363\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 970223872.0000 - rmse: 31148.4160 - val_loss: 895485952.0000 - val_rmse: 29924.6699\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 980997056.0000 - rmse: 31320.8730 - val_loss: 962910080.0000 - val_rmse: 31030.7930\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 973619328.0000 - rmse: 31202.8730 - val_loss: 878483712.0000 - val_rmse: 29639.2266\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 909977216.0000 - rmse: 30165.8281 - val_loss: 875767424.0000 - val_rmse: 29593.3672\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 941389504.0000 - rmse: 30682.0723 - val_loss: 965113216.0000 - val_rmse: 31066.2715\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 907235648.0000 - rmse: 30120.3516 - val_loss: 892714368.0000 - val_rmse: 29878.3262\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 909636032.0000 - rmse: 30160.1738 - val_loss: 861674624.0000 - val_rmse: 29354.2949\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 875960512.0000 - rmse: 29596.6309 - val_loss: 843314688.0000 - val_rmse: 29039.8809\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 836362432.0000 - rmse: 28919.9316 - val_loss: 943001856.0000 - val_rmse: 30708.3359\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 860496256.0000 - rmse: 29334.2168 - val_loss: 883904320.0000 - val_rmse: 29730.5293\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 823041216.0000 - rmse: 28688.6953 - val_loss: 1071475840.0000 - val_rmse: 32733.4062\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 839550656.0000 - rmse: 28974.9980 - val_loss: 951136640.0000 - val_rmse: 30840.5039\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 814821696.0000 - rmse: 28545.0820 - val_loss: 982131456.0000 - val_rmse: 31338.9766\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 842095232.0000 - rmse: 29018.8770 - val_loss: 862831552.0000 - val_rmse: 29373.9941\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 769517568.0000 - rmse: 27740.1797 - val_loss: 905245312.0000 - val_rmse: 30087.2949\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 776142720.0000 - rmse: 27859.3379 - val_loss: 890720704.0000 - val_rmse: 29844.9453\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 777188928.0000 - rmse: 27878.1094 - val_loss: 882655168.0000 - val_rmse: 29709.5137\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 826343744.0000 - rmse: 28746.1953 - val_loss: 845649408.0000 - val_rmse: 29080.0488\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 723548480.0000 - rmse: 26898.8574 - val_loss: 810528832.0000 - val_rmse: 28469.7871\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 723031296.0000 - rmse: 26889.2422 - val_loss: 938151872.0000 - val_rmse: 30629.2656\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 738421568.0000 - rmse: 27173.9141 - val_loss: 970857536.0000 - val_rmse: 31158.5859\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 770910976.0000 - rmse: 27765.2832 - val_loss: 844011840.0000 - val_rmse: 29051.8828\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 690167360.0000 - rmse: 26271.0352 - val_loss: 796989952.0000 - val_rmse: 28231.0098\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 702905344.0000 - rmse: 26512.3613 - val_loss: 918862720.0000 - val_rmse: 30312.7461\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730648256.0000 - rmse: 27030.5059 - val_loss: 876658496.0000 - val_rmse: 29608.4199\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661545472.0000 - rmse: 25720.5234 - val_loss: 867604608.0000 - val_rmse: 29455.1289\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 659242816.0000 - rmse: 25675.7246 - val_loss: 805971008.0000 - val_rmse: 28389.6289\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 645304384.0000 - rmse: 25402.8398 - val_loss: 815705408.0000 - val_rmse: 28560.5566\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 664225344.0000 - rmse: 25772.5664 - val_loss: 941226816.0000 - val_rmse: 30679.4199\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 699606336.0000 - rmse: 26450.0723 - val_loss: 913616960.0000 - val_rmse: 30226.0957\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680219328.0000 - rmse: 26081.0156 - val_loss: 1020160960.0000 - val_rmse: 31939.9590\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654224128.0000 - rmse: 25577.8047 - val_loss: 894277952.0000 - val_rmse: 29904.4805\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634855232.0000 - rmse: 25196.3340 - val_loss: 994268544.0000 - val_rmse: 31532.0234\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623718784.0000 - rmse: 24974.3633 - val_loss: 797990976.0000 - val_rmse: 28248.7324\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 683141760.0000 - rmse: 26136.9805 - val_loss: 1061341888.0000 - val_rmse: 32578.2422\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666767936.0000 - rmse: 25821.8496 - val_loss: 1038065088.0000 - val_rmse: 32219.0176\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641334016.0000 - rmse: 25324.5742 - val_loss: 900164608.0000 - val_rmse: 30002.7441\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611981632.0000 - rmse: 24738.2617 - val_loss: 867875840.0000 - val_rmse: 29459.7324\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 631354240.0000 - rmse: 25126.7637 - val_loss: 1019611520.0000 - val_rmse: 31931.3555\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 619449408.0000 - rmse: 24888.7402 - val_loss: 983845952.0000 - val_rmse: 31366.3184\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584283008.0000 - rmse: 24171.9453 - val_loss: 989666944.0000 - val_rmse: 31458.9727\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562101440.0000 - rmse: 23708.6777 - val_loss: 1096202752.0000 - val_rmse: 33108.9531\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588788352.0000 - rmse: 24264.9609 - val_loss: 879522752.0000 - val_rmse: 29656.7480\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543812672.0000 - rmse: 23319.7910 - val_loss: 812550080.0000 - val_rmse: 28505.2637\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564056512.0000 - rmse: 23749.8711 - val_loss: 1101879808.0000 - val_rmse: 33194.5742\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552495232.0000 - rmse: 23505.2168 - val_loss: 710835904.0000 - val_rmse: 26661.5059\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542735744.0000 - rmse: 23296.6895 - val_loss: 832239872.0000 - val_rmse: 28848.5684\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592959424.0000 - rmse: 24350.7578 - val_loss: 1141582336.0000 - val_rmse: 33787.3086\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560076224.0000 - rmse: 23665.9297 - val_loss: 855731392.0000 - val_rmse: 29252.8867\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550426304.0000 - rmse: 23461.1660 - val_loss: 873554624.0000 - val_rmse: 29555.9570\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590645632.0000 - rmse: 24303.2012 - val_loss: 860207744.0000 - val_rmse: 29329.2988\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540079936.0000 - rmse: 23239.6191 - val_loss: 1014969408.0000 - val_rmse: 31858.5840\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546874688.0000 - rmse: 23385.3516 - val_loss: 844102784.0000 - val_rmse: 29053.4453\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546398784.0000 - rmse: 23375.1738 - val_loss: 858088448.0000 - val_rmse: 29293.1445\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540626688.0000 - rmse: 23251.3809 - val_loss: 816779584.0000 - val_rmse: 28579.3555\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464025984.0000 - rmse: 21541.2617 - val_loss: 723648128.0000 - val_rmse: 26900.7070\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507073824.0000 - rmse: 22518.2988 - val_loss: 896281024.0000 - val_rmse: 29937.9531\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495028416.0000 - rmse: 22249.2324 - val_loss: 1792120064.0000 - val_rmse: 42333.4375\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480487424.0000 - rmse: 21920.0234 - val_loss: 830144384.0000 - val_rmse: 28812.2266\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503350816.0000 - rmse: 22435.4805 - val_loss: 822876800.0000 - val_rmse: 28685.8281\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476437056.0000 - rmse: 21827.4375 - val_loss: 654723840.0000 - val_rmse: 25587.5703\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479511456.0000 - rmse: 21897.7480 - val_loss: 960599808.0000 - val_rmse: 30993.5430\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498322368.0000 - rmse: 22323.1348 - val_loss: 1781789568.0000 - val_rmse: 42211.2500\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489904576.0000 - rmse: 22133.7891 - val_loss: 955096448.0000 - val_rmse: 30904.6348\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484724512.0000 - rmse: 22016.4590 - val_loss: 936527680.0000 - val_rmse: 30602.7402\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502415232.0000 - rmse: 22414.6211 - val_loss: 937593344.0000 - val_rmse: 30620.1465\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452439104.0000 - rmse: 21270.6133 - val_loss: 779772480.0000 - val_rmse: 27924.4043\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465607584.0000 - rmse: 21577.9414 - val_loss: 645752384.0000 - val_rmse: 25411.6562\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442298816.0000 - rmse: 21030.8984 - val_loss: 788336512.0000 - val_rmse: 28077.3301\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455731264.0000 - rmse: 21347.8633 - val_loss: 943433472.0000 - val_rmse: 30715.3594\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473305088.0000 - rmse: 21755.5762 - val_loss: 556198976.0000 - val_rmse: 23583.8711\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473600160.0000 - rmse: 21762.3574 - val_loss: 760128704.0000 - val_rmse: 27570.4316\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441815424.0000 - rmse: 21019.4062 - val_loss: 613125568.0000 - val_rmse: 24761.3730\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445249792.0000 - rmse: 21100.9414 - val_loss: 820900992.0000 - val_rmse: 28651.3691\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456997280.0000 - rmse: 21377.4941 - val_loss: 1103986560.0000 - val_rmse: 33226.2930\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481929472.0000 - rmse: 21952.8926 - val_loss: 1331388160.0000 - val_rmse: 36488.1914\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427870976.0000 - rmse: 20685.0410 - val_loss: 1772075136.0000 - val_rmse: 42096.0234\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446144192.0000 - rmse: 21122.1250 - val_loss: 644449408.0000 - val_rmse: 25386.0078\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441809280.0000 - rmse: 21019.2598 - val_loss: 1016319744.0000 - val_rmse: 31879.7695\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432990336.0000 - rmse: 20808.4180 - val_loss: 925520192.0000 - val_rmse: 30422.3594\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441096160.0000 - rmse: 21002.2871 - val_loss: 1081997184.0000 - val_rmse: 32893.7227\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412934144.0000 - rmse: 20320.7812 - val_loss: 652355904.0000 - val_rmse: 25541.2598\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378260992.0000 - rmse: 19448.9316 - val_loss: 882082688.0000 - val_rmse: 29699.8770\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426887072.0000 - rmse: 20661.2441 - val_loss: 606659968.0000 - val_rmse: 24630.4668\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456451712.0000 - rmse: 21364.7305 - val_loss: 1365717376.0000 - val_rmse: 36955.6133\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458829152.0000 - rmse: 21420.2969 - val_loss: 708369984.0000 - val_rmse: 26615.2207\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415533312.0000 - rmse: 20384.6348 - val_loss: 553869696.0000 - val_rmse: 23534.4336\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422527296.0000 - rmse: 20555.4668 - val_loss: 1029550592.0000 - val_rmse: 32086.6113\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424643584.0000 - rmse: 20606.8809 - val_loss: 641014720.0000 - val_rmse: 25318.2676\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450254528.0000 - rmse: 21219.2012 - val_loss: 761661568.0000 - val_rmse: 27598.2168\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424526752.0000 - rmse: 20604.0469 - val_loss: 937813184.0000 - val_rmse: 30623.7363\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395595904.0000 - rmse: 19889.5918 - val_loss: 612026048.0000 - val_rmse: 24739.1602\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431618432.0000 - rmse: 20775.4277 - val_loss: 702303104.0000 - val_rmse: 26501.0020\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463958624.0000 - rmse: 21539.6973 - val_loss: 761514176.0000 - val_rmse: 27595.5469\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375154400.0000 - rmse: 19368.9023 - val_loss: 1313349888.0000 - val_rmse: 36240.1680\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385077216.0000 - rmse: 19623.3828 - val_loss: 745061056.0000 - val_rmse: 27295.8066\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422000096.0000 - rmse: 20542.6406 - val_loss: 621354048.0000 - val_rmse: 24926.9746\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407628544.0000 - rmse: 20189.8125 - val_loss: 641688256.0000 - val_rmse: 25331.5645\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397691968.0000 - rmse: 19942.2148 - val_loss: 680769984.0000 - val_rmse: 26091.5684\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347437408.0000 - rmse: 18639.6738 - val_loss: 715974016.0000 - val_rmse: 26757.6895\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362098848.0000 - rmse: 19028.8945 - val_loss: 577857280.0000 - val_rmse: 24038.6621\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378165376.0000 - rmse: 19446.4746 - val_loss: 948613952.0000 - val_rmse: 30799.5762\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473865184.0000 - rmse: 21768.4434 - val_loss: 815224512.0000 - val_rmse: 28552.1367\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407675552.0000 - rmse: 20190.9746 - val_loss: 558153344.0000 - val_rmse: 23625.2695\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383248096.0000 - rmse: 19576.7227 - val_loss: 895412224.0000 - val_rmse: 29923.4375\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403942784.0000 - rmse: 20098.3262 - val_loss: 807092672.0000 - val_rmse: 28409.3770\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400112416.0000 - rmse: 20002.8086 - val_loss: 574894528.0000 - val_rmse: 23976.9590\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406455520.0000 - rmse: 20160.7422 - val_loss: 496142720.0000 - val_rmse: 22274.2598\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388652320.0000 - rmse: 19714.2676 - val_loss: 463219840.0000 - val_rmse: 21522.5410\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394425312.0000 - rmse: 19860.1426 - val_loss: 779149184.0000 - val_rmse: 27913.2422\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393226656.0000 - rmse: 19829.9434 - val_loss: 502752736.0000 - val_rmse: 22422.1484\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421992672.0000 - rmse: 20542.4570 - val_loss: 598513152.0000 - val_rmse: 24464.5273\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327063584.0000 - rmse: 18084.8984 - val_loss: 751434752.0000 - val_rmse: 27412.3105\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338837088.0000 - rmse: 18407.5273 - val_loss: 763809216.0000 - val_rmse: 27637.0977\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391562176.0000 - rmse: 19787.9297 - val_loss: 785678144.0000 - val_rmse: 28029.9492\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358912352.0000 - rmse: 18944.9805 - val_loss: 836920960.0000 - val_rmse: 28929.5840\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391684832.0000 - rmse: 19791.0273 - val_loss: 418577408.0000 - val_rmse: 20459.1641\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342871040.0000 - rmse: 18516.7754 - val_loss: 606453504.0000 - val_rmse: 24626.2754\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357715776.0000 - rmse: 18913.3750 - val_loss: 1016946112.0000 - val_rmse: 31889.5918\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415921472.0000 - rmse: 20394.1523 - val_loss: 571139584.0000 - val_rmse: 23898.5254\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396107424.0000 - rmse: 19902.4453 - val_loss: 475103904.0000 - val_rmse: 21796.8770\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358810880.0000 - rmse: 18942.3027 - val_loss: 496741056.0000 - val_rmse: 22287.6875\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387652416.0000 - rmse: 19688.8887 - val_loss: 952241792.0000 - val_rmse: 30858.4160\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308746656.0000 - rmse: 17571.1875 - val_loss: 666861184.0000 - val_rmse: 25823.6543\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372834688.0000 - rmse: 19308.9277 - val_loss: 609777088.0000 - val_rmse: 24693.6641\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376528384.0000 - rmse: 19404.3359 - val_loss: 620742976.0000 - val_rmse: 24914.7129\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353936896.0000 - rmse: 18813.2090 - val_loss: 561685312.0000 - val_rmse: 23699.9004\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359002976.0000 - rmse: 18947.3730 - val_loss: 562577088.0000 - val_rmse: 23718.7070\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385055744.0000 - rmse: 19622.8359 - val_loss: 516788000.0000 - val_rmse: 22732.9707\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351602560.0000 - rmse: 18751.0664 - val_loss: 390958144.0000 - val_rmse: 19772.6621\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352896480.0000 - rmse: 18785.5371 - val_loss: 608778560.0000 - val_rmse: 24673.4375\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323738496.0000 - rmse: 17992.7344 - val_loss: 456326656.0000 - val_rmse: 21361.8008\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346829792.0000 - rmse: 18623.3652 - val_loss: 524742624.0000 - val_rmse: 22907.2598\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341464256.0000 - rmse: 18478.7520 - val_loss: 603357952.0000 - val_rmse: 24563.3438\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332262816.0000 - rmse: 18228.0762 - val_loss: 631933312.0000 - val_rmse: 25138.2832\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368006400.0000 - rmse: 19183.4922 - val_loss: 571663552.0000 - val_rmse: 23909.4863\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303232640.0000 - rmse: 17413.5762 - val_loss: 598331776.0000 - val_rmse: 24460.8203\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357857280.0000 - rmse: 18917.1152 - val_loss: 1164418048.0000 - val_rmse: 34123.5703\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334054176.0000 - rmse: 18277.1484 - val_loss: 652380224.0000 - val_rmse: 25541.7344\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297892960.0000 - rmse: 17259.5742 - val_loss: 475328672.0000 - val_rmse: 21802.0332\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379502880.0000 - rmse: 19480.8340 - val_loss: 809670720.0000 - val_rmse: 28454.7129\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312592384.0000 - rmse: 17680.2832 - val_loss: 716496256.0000 - val_rmse: 26767.4453\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300215520.0000 - rmse: 17326.7285 - val_loss: 491868352.0000 - val_rmse: 22178.1055\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346899808.0000 - rmse: 18625.2461 - val_loss: 667272320.0000 - val_rmse: 25831.6152\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281480320.0000 - rmse: 16777.3750 - val_loss: 396015424.0000 - val_rmse: 19900.1348\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294776800.0000 - rmse: 17169.0645 - val_loss: 610934016.0000 - val_rmse: 24717.0781\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280997408.0000 - rmse: 16762.9746 - val_loss: 618650880.0000 - val_rmse: 24872.6934\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296011616.0000 - rmse: 17204.9863 - val_loss: 1285359104.0000 - val_rmse: 35851.9062\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314504352.0000 - rmse: 17734.2695 - val_loss: 492161920.0000 - val_rmse: 22184.7227\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331623360.0000 - rmse: 18210.5293 - val_loss: 1037332928.0000 - val_rmse: 32207.6523\n",
      "Epoch 164/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334393056.0000 - rmse: 18286.4180 - val_loss: 653049600.0000 - val_rmse: 25554.8359\n",
      "Epoch 165/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312097984.0000 - rmse: 17666.2949 - val_loss: 565747968.0000 - val_rmse: 23785.4551\n",
      "Epoch 166/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343983520.0000 - rmse: 18546.7910 - val_loss: 711276736.0000 - val_rmse: 26669.7715\n",
      "Epoch 167/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315521568.0000 - rmse: 17762.9219 - val_loss: 824220928.0000 - val_rmse: 28709.2461\n",
      "Epoch 168/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323269792.0000 - rmse: 17979.7031 - val_loss: 697413696.0000 - val_rmse: 26408.5918\n",
      "Epoch 169/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299253152.0000 - rmse: 17298.9355 - val_loss: 599229760.0000 - val_rmse: 24479.1680\n",
      "Epoch 170/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271915840.0000 - rmse: 16489.8711 - val_loss: 1107488768.0000 - val_rmse: 33278.9531\n",
      "Epoch 171/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292777472.0000 - rmse: 17110.7422 - val_loss: 919274112.0000 - val_rmse: 30319.5332\n",
      "Epoch 172/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281526720.0000 - rmse: 16778.7578 - val_loss: 670469888.0000 - val_rmse: 25893.4336\n",
      "Epoch 173/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293730656.0000 - rmse: 17138.5703 - val_loss: 769110144.0000 - val_rmse: 27732.8340\n",
      "Epoch 174/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294319744.0000 - rmse: 17155.7480 - val_loss: 463303936.0000 - val_rmse: 21524.4941\n",
      "Epoch 175/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326448352.0000 - rmse: 18067.8809 - val_loss: 1550662912.0000 - val_rmse: 39378.4570\n",
      "Epoch 176/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287914848.0000 - rmse: 16968.0527 - val_loss: 717783424.0000 - val_rmse: 26791.4785\n",
      "Epoch 177/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249937344.0000 - rmse: 15809.4053 - val_loss: 860910528.0000 - val_rmse: 29341.2773\n",
      "Epoch 178/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269589632.0000 - rmse: 16419.1836 - val_loss: 704001408.0000 - val_rmse: 26533.0234\n",
      "Epoch 179/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274057120.0000 - rmse: 16554.6699 - val_loss: 804918272.0000 - val_rmse: 28371.0801\n",
      "Epoch 180/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256766144.0000 - rmse: 16023.9238 - val_loss: 607952832.0000 - val_rmse: 24656.6992\n",
      "Epoch 181/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314352448.0000 - rmse: 17729.9863 - val_loss: 752468928.0000 - val_rmse: 27431.1660\n",
      "Epoch 182/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281242240.0000 - rmse: 16770.2773 - val_loss: 663580864.0000 - val_rmse: 25760.0625\n",
      "Epoch 183/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281654272.0000 - rmse: 16782.5586 - val_loss: 538505664.0000 - val_rmse: 23205.7246\n",
      "Epoch 184/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291417984.0000 - rmse: 17070.9668 - val_loss: 437812416.0000 - val_rmse: 20923.9648\n",
      "Epoch 185/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245061280.0000 - rmse: 15654.4316 - val_loss: 682273024.0000 - val_rmse: 26120.3555\n",
      "Epoch 186/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255875040.0000 - rmse: 15996.0928 - val_loss: 629599680.0000 - val_rmse: 25091.8203\n",
      "Epoch 187/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279772384.0000 - rmse: 16726.3945 - val_loss: 524185024.0000 - val_rmse: 22895.0879\n",
      "Epoch 188/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257374176.0000 - rmse: 16042.8838 - val_loss: 1052492672.0000 - val_rmse: 32442.1426\n",
      "Epoch 189/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273995232.0000 - rmse: 16552.8008 - val_loss: 580568064.0000 - val_rmse: 24094.9805\n",
      "Epoch 190/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240483408.0000 - rmse: 15507.5264 - val_loss: 531005600.0000 - val_rmse: 23043.5566\n",
      "Epoch 191/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218985312.0000 - rmse: 14798.1494 - val_loss: 477498688.0000 - val_rmse: 21851.7441\n",
      "Epoch 192/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298206304.0000 - rmse: 17268.6484 - val_loss: 756783488.0000 - val_rmse: 27509.6973\n",
      "Epoch 193/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229359584.0000 - rmse: 15144.6201 - val_loss: 519360032.0000 - val_rmse: 22789.4727\n",
      "Epoch 194/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241411488.0000 - rmse: 15537.4209 - val_loss: 584006848.0000 - val_rmse: 24166.2324\n",
      "Epoch 195/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264787936.0000 - rmse: 16272.3047 - val_loss: 571433216.0000 - val_rmse: 23904.6699\n",
      "Epoch 196/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286579648.0000 - rmse: 16928.6621 - val_loss: 487792416.0000 - val_rmse: 22086.0215\n",
      "Epoch 197/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266420896.0000 - rmse: 16322.4043 - val_loss: 433651040.0000 - val_rmse: 20824.2891\n",
      "Epoch 198/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248078880.0000 - rmse: 15750.5176 - val_loss: 384380704.0000 - val_rmse: 19605.6289\n",
      "Epoch 199/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264307600.0000 - rmse: 16257.5371 - val_loss: 417456768.0000 - val_rmse: 20431.7578\n",
      "Epoch 200/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238541040.0000 - rmse: 15444.7705 - val_loss: 555086784.0000 - val_rmse: 23560.2793\n",
      "Epoch 201/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273193088.0000 - rmse: 16528.5508 - val_loss: 612209152.0000 - val_rmse: 24742.8613\n",
      "Epoch 202/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219240928.0000 - rmse: 14806.7852 - val_loss: 572854400.0000 - val_rmse: 23934.3750\n",
      "Epoch 203/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260647632.0000 - rmse: 16144.5850 - val_loss: 593949056.0000 - val_rmse: 24371.0703\n",
      "Epoch 204/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278681376.0000 - rmse: 16693.7500 - val_loss: 416527008.0000 - val_rmse: 20408.9941\n",
      "Epoch 205/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276673920.0000 - rmse: 16633.5176 - val_loss: 443876640.0000 - val_rmse: 21068.3809\n",
      "Epoch 206/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222688832.0000 - rmse: 14922.7598 - val_loss: 513233920.0000 - val_rmse: 22654.6660\n",
      "Epoch 207/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229297296.0000 - rmse: 15142.5654 - val_loss: 517793632.0000 - val_rmse: 22755.0762\n",
      "Epoch 208/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230951104.0000 - rmse: 15197.0742 - val_loss: 1088929024.0000 - val_rmse: 32998.9219\n",
      "Epoch 209/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215584032.0000 - rmse: 14682.7793 - val_loss: 511386080.0000 - val_rmse: 22613.8457\n",
      "Epoch 210/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243976080.0000 - rmse: 15619.7314 - val_loss: 498303616.0000 - val_rmse: 22322.7129\n",
      "Epoch 211/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259827888.0000 - rmse: 16119.1758 - val_loss: 865169664.0000 - val_rmse: 29413.7656\n",
      "Epoch 212/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269063424.0000 - rmse: 16403.1504 - val_loss: 833546432.0000 - val_rmse: 28871.2051\n",
      "Epoch 213/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244403008.0000 - rmse: 15633.3926 - val_loss: 935954880.0000 - val_rmse: 30593.3789\n",
      "Epoch 214/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219603536.0000 - rmse: 14819.0254 - val_loss: 387430336.0000 - val_rmse: 19683.2500\n",
      "Epoch 215/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240000864.0000 - rmse: 15491.9590 - val_loss: 425920448.0000 - val_rmse: 20637.8398\n",
      "Epoch 216/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 213235104.0000 - rmse: 14602.5693 - val_loss: 455469920.0000 - val_rmse: 21341.7422\n",
      "Epoch 217/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220304800.0000 - rmse: 14842.6660 - val_loss: 659565248.0000 - val_rmse: 25682.0020\n",
      "Epoch 218/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 213165824.0000 - rmse: 14600.1963 - val_loss: 526145504.0000 - val_rmse: 22937.8613\n",
      "Epoch 219/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235467472.0000 - rmse: 15344.9473 - val_loss: 374798240.0000 - val_rmse: 19359.7051\n",
      "Epoch 220/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215069008.0000 - rmse: 14665.2295 - val_loss: 637723520.0000 - val_rmse: 25253.1875\n",
      "Epoch 221/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220411488.0000 - rmse: 14846.2607 - val_loss: 493342272.0000 - val_rmse: 22211.3086\n",
      "Epoch 222/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259334144.0000 - rmse: 16103.8535 - val_loss: 671708352.0000 - val_rmse: 25917.3359\n",
      "Epoch 223/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249409440.0000 - rmse: 15792.7012 - val_loss: 444642432.0000 - val_rmse: 21086.5469\n",
      "Epoch 224/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 212026288.0000 - rmse: 14561.1201 - val_loss: 517195264.0000 - val_rmse: 22741.9277\n",
      "Epoch 225/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214565184.0000 - rmse: 14648.0430 - val_loss: 842891776.0000 - val_rmse: 29032.5977\n",
      "Epoch 226/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 225713520.0000 - rmse: 15023.7637 - val_loss: 457512864.0000 - val_rmse: 21389.5488\n",
      "Epoch 227/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256217024.0000 - rmse: 16006.7783 - val_loss: 826285568.0000 - val_rmse: 28745.1836\n",
      "Epoch 228/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216641200.0000 - rmse: 14718.7334 - val_loss: 531799424.0000 - val_rmse: 23060.7754\n",
      "Epoch 229/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226111840.0000 - rmse: 15037.0137 - val_loss: 463195648.0000 - val_rmse: 21521.9805\n",
      "Epoch 230/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253401248.0000 - rmse: 15918.5801 - val_loss: 758384384.0000 - val_rmse: 27538.7773\n",
      "Epoch 231/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 193278832.0000 - rmse: 13902.4736 - val_loss: 611391040.0000 - val_rmse: 24726.3223\n",
      "Epoch 232/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 199613216.0000 - rmse: 14128.4521 - val_loss: 399422816.0000 - val_rmse: 19985.5645\n",
      "Epoch 233/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 213615376.0000 - rmse: 14615.5850 - val_loss: 546222656.0000 - val_rmse: 23371.4062\n",
      "Epoch 234/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215985744.0000 - rmse: 14696.4502 - val_loss: 452262144.0000 - val_rmse: 21266.4551\n",
      "Epoch 235/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220200096.0000 - rmse: 14839.1377 - val_loss: 489523296.0000 - val_rmse: 22125.1738\n",
      "Epoch 236/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234896576.0000 - rmse: 15326.3340 - val_loss: 825034048.0000 - val_rmse: 28723.4043\n",
      "Epoch 237/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 193364736.0000 - rmse: 13905.5625 - val_loss: 432199712.0000 - val_rmse: 20789.4121\n",
      "Epoch 238/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216072928.0000 - rmse: 14699.4189 - val_loss: 410126784.0000 - val_rmse: 20251.5879\n",
      "Epoch 239/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 209377424.0000 - rmse: 14469.8779 - val_loss: 581541504.0000 - val_rmse: 24115.1719\n",
      "Epoch 240/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 208542240.0000 - rmse: 14440.9902 - val_loss: 359058848.0000 - val_rmse: 18948.8477\n",
      "Epoch 241/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210304016.0000 - rmse: 14501.8613 - val_loss: 565790848.0000 - val_rmse: 23786.3574\n",
      "Epoch 242/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210532096.0000 - rmse: 14509.7236 - val_loss: 1175136512.0000 - val_rmse: 34280.2656\n",
      "Epoch 243/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218445248.0000 - rmse: 14779.8926 - val_loss: 605308096.0000 - val_rmse: 24603.0098\n",
      "Epoch 244/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219434928.0000 - rmse: 14813.3350 - val_loss: 639517376.0000 - val_rmse: 25288.6816\n",
      "Epoch 245/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 209782064.0000 - rmse: 14483.8535 - val_loss: 356591264.0000 - val_rmse: 18883.6230\n",
      "Epoch 246/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228253760.0000 - rmse: 15108.0674 - val_loss: 595451328.0000 - val_rmse: 24401.8691\n",
      "Epoch 247/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217191392.0000 - rmse: 14737.4131 - val_loss: 754268288.0000 - val_rmse: 27463.9414\n",
      "Epoch 248/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222614432.0000 - rmse: 14920.2686 - val_loss: 483422368.0000 - val_rmse: 21986.8672\n",
      "Epoch 249/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220181504.0000 - rmse: 14838.5127 - val_loss: 469945600.0000 - val_rmse: 21678.2266\n",
      "Epoch 250/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217643600.0000 - rmse: 14752.7471 - val_loss: 453245248.0000 - val_rmse: 21289.5566\n",
      "Epoch 251/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 194297024.0000 - rmse: 13939.0449 - val_loss: 565657856.0000 - val_rmse: 23783.5586\n",
      "Epoch 252/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 180389104.0000 - rmse: 13430.8994 - val_loss: 879181632.0000 - val_rmse: 29650.9980\n",
      "Epoch 253/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 189016672.0000 - rmse: 13748.3301 - val_loss: 600153344.0000 - val_rmse: 24498.0254\n",
      "Epoch 254/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231160416.0000 - rmse: 15203.9600 - val_loss: 408450688.0000 - val_rmse: 20210.1621\n",
      "Epoch 255/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 187525120.0000 - rmse: 13693.9785 - val_loss: 425521376.0000 - val_rmse: 20628.1699\n",
      "Epoch 256/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 190790512.0000 - rmse: 13812.6924 - val_loss: 645550208.0000 - val_rmse: 25407.6797\n",
      "Epoch 257/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 179145744.0000 - rmse: 13384.5322 - val_loss: 394220256.0000 - val_rmse: 19854.9805\n",
      "Epoch 258/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221532672.0000 - rmse: 14883.9717 - val_loss: 338255744.0000 - val_rmse: 18391.7285\n",
      "Epoch 259/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222262240.0000 - rmse: 14908.4600 - val_loss: 474848576.0000 - val_rmse: 21791.0215\n",
      "Epoch 260/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256069968.0000 - rmse: 16002.1846 - val_loss: 912736960.0000 - val_rmse: 30211.5352\n",
      "Epoch 261/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 187312576.0000 - rmse: 13686.2158 - val_loss: 403787136.0000 - val_rmse: 20094.4551\n",
      "Epoch 262/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230463248.0000 - rmse: 15181.0146 - val_loss: 783895936.0000 - val_rmse: 27998.1426\n",
      "Epoch 263/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 194768096.0000 - rmse: 13955.9326 - val_loss: 555787520.0000 - val_rmse: 23575.1426\n",
      "Epoch 264/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218261424.0000 - rmse: 14773.6709 - val_loss: 412495936.0000 - val_rmse: 20309.9961\n",
      "Epoch 265/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210773632.0000 - rmse: 14518.0420 - val_loss: 339226400.0000 - val_rmse: 18418.0977\n",
      "Epoch 266/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219395216.0000 - rmse: 14811.9932 - val_loss: 561659584.0000 - val_rmse: 23699.3574\n",
      "Epoch 267/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226087824.0000 - rmse: 15036.2129 - val_loss: 445519296.0000 - val_rmse: 21107.3262\n",
      "Epoch 268/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202954704.0000 - rmse: 14246.2168 - val_loss: 855287040.0000 - val_rmse: 29245.2910\n",
      "Epoch 269/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210792048.0000 - rmse: 14518.6768 - val_loss: 364241152.0000 - val_rmse: 19085.1016\n",
      "Epoch 270/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 189930064.0000 - rmse: 13781.5098 - val_loss: 394798752.0000 - val_rmse: 19869.5410\n",
      "Epoch 271/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 205081040.0000 - rmse: 14320.6494 - val_loss: 587512128.0000 - val_rmse: 24238.6484\n",
      "Epoch 272/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207500240.0000 - rmse: 14404.8662 - val_loss: 389976736.0000 - val_rmse: 19747.8281\n",
      "Epoch 273/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217998144.0000 - rmse: 14764.7598 - val_loss: 840282496.0000 - val_rmse: 28987.6270\n",
      "Epoch 274/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 195379504.0000 - rmse: 13977.8203 - val_loss: 453431520.0000 - val_rmse: 21293.9297\n",
      "Epoch 275/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218233984.0000 - rmse: 14772.7432 - val_loss: 428366464.0000 - val_rmse: 20697.0137\n",
      "Epoch 276/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 193129584.0000 - rmse: 13897.1035 - val_loss: 375056832.0000 - val_rmse: 19366.3828\n",
      "Epoch 277/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 195773216.0000 - rmse: 13991.8955 - val_loss: 401474784.0000 - val_rmse: 20036.8340\n",
      "Epoch 278/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206947504.0000 - rmse: 14385.6680 - val_loss: 546087808.0000 - val_rmse: 23368.5195\n",
      "Epoch 279/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 184256128.0000 - rmse: 13574.0967 - val_loss: 412060640.0000 - val_rmse: 20299.2754\n",
      "Epoch 280/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210421360.0000 - rmse: 14505.9053 - val_loss: 414921280.0000 - val_rmse: 20369.6133\n",
      "Epoch 281/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 169218576.0000 - rmse: 13008.4023 - val_loss: 471400608.0000 - val_rmse: 21711.7617\n",
      "Epoch 282/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 184595904.0000 - rmse: 13586.6055 - val_loss: 454920736.0000 - val_rmse: 21328.8711\n",
      "Epoch 283/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224044448.0000 - rmse: 14968.1123 - val_loss: 633105920.0000 - val_rmse: 25161.5957\n",
      "Epoch 284/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 187581088.0000 - rmse: 13696.0225 - val_loss: 438167136.0000 - val_rmse: 20932.4395\n",
      "Epoch 285/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218330096.0000 - rmse: 14775.9941 - val_loss: 512748096.0000 - val_rmse: 22643.9414\n",
      "Epoch 286/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 196964032.0000 - rmse: 14034.3838 - val_loss: 516468256.0000 - val_rmse: 22725.9375\n",
      "Epoch 287/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 191681520.0000 - rmse: 13844.9082 - val_loss: 508531136.0000 - val_rmse: 22550.6348\n",
      "Epoch 288/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 189315456.0000 - rmse: 13759.1943 - val_loss: 446255616.0000 - val_rmse: 21124.7617\n",
      "Epoch 289/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 208869152.0000 - rmse: 14452.3057 - val_loss: 491206656.0000 - val_rmse: 22163.1816\n",
      "Epoch 290/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 200975760.0000 - rmse: 14176.5918 - val_loss: 401625280.0000 - val_rmse: 20040.5898\n",
      "Epoch 291/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 200392672.0000 - rmse: 14156.0098 - val_loss: 552451328.0000 - val_rmse: 23504.2812\n",
      "Epoch 292/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220480368.0000 - rmse: 14848.5791 - val_loss: 517883936.0000 - val_rmse: 22757.0625\n",
      "Epoch 293/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 204471584.0000 - rmse: 14299.3525 - val_loss: 589811328.0000 - val_rmse: 24286.0293\n",
      "Epoch 294/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237190704.0000 - rmse: 15400.9941 - val_loss: 615043648.0000 - val_rmse: 24800.0723\n",
      "Epoch 295/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 187844176.0000 - rmse: 13705.6240 - val_loss: 666800064.0000 - val_rmse: 25822.4688\n",
      "Epoch 296/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 183363248.0000 - rmse: 13541.1670 - val_loss: 519977344.0000 - val_rmse: 22803.0098\n",
      "Epoch 297/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220995472.0000 - rmse: 14865.9150 - val_loss: 394215808.0000 - val_rmse: 19854.8672\n",
      "Epoch 298/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 189202912.0000 - rmse: 13755.1016 - val_loss: 843937792.0000 - val_rmse: 29050.6074\n",
      "Epoch 299/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211920848.0000 - rmse: 14557.5000 - val_loss: 470638976.0000 - val_rmse: 21694.2129\n",
      "Epoch 300/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 223988320.0000 - rmse: 14966.2393 - val_loss: 402242624.0000 - val_rmse: 20055.9844\n",
      "104/104 [==============================] - 0s 671us/step - loss: 507739424.0000 - rmse: 22533.0742\n",
      "[507739424.0, 22533.07421875]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 12447391744.0000 - rmse: 111567.8828 - val_loss: 1812840448.0000 - val_rmse: 42577.4648\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2045563776.0000 - rmse: 45227.9102 - val_loss: 1288751872.0000 - val_rmse: 35899.1914\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1687233664.0000 - rmse: 41075.9492 - val_loss: 1127877120.0000 - val_rmse: 33583.8828\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1554775552.0000 - rmse: 39430.6406 - val_loss: 1115108480.0000 - val_rmse: 33393.2383\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1491986176.0000 - rmse: 38626.2383 - val_loss: 975708736.0000 - val_rmse: 31236.3359\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1398733440.0000 - rmse: 37399.6445 - val_loss: 932356032.0000 - val_rmse: 30534.5059\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1351104512.0000 - rmse: 36757.3750 - val_loss: 925572096.0000 - val_rmse: 30423.2168\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1299048576.0000 - rmse: 36042.3164 - val_loss: 843575936.0000 - val_rmse: 29044.3789\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1267887232.0000 - rmse: 35607.4023 - val_loss: 843807360.0000 - val_rmse: 29048.3633\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1243869312.0000 - rmse: 35268.5312 - val_loss: 815748800.0000 - val_rmse: 28561.3164\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1190450048.0000 - rmse: 34502.8984 - val_loss: 803770112.0000 - val_rmse: 28350.8398\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1118527744.0000 - rmse: 33444.3984 - val_loss: 839498944.0000 - val_rmse: 28974.1074\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1138429312.0000 - rmse: 33740.6172 - val_loss: 785381376.0000 - val_rmse: 28024.6562\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1077991168.0000 - rmse: 32832.7773 - val_loss: 772376640.0000 - val_rmse: 27791.6641\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1023189248.0000 - rmse: 31987.3301 - val_loss: 787610368.0000 - val_rmse: 28064.3965\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1043067328.0000 - rmse: 32296.5527 - val_loss: 777727552.0000 - val_rmse: 27887.7656\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1001340992.0000 - rmse: 31643.9727 - val_loss: 975765952.0000 - val_rmse: 31237.2520\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 940701248.0000 - rmse: 30670.8535 - val_loss: 825575360.0000 - val_rmse: 28732.8281\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 945611200.0000 - rmse: 30750.7910 - val_loss: 838965056.0000 - val_rmse: 28964.8926\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 986987776.0000 - rmse: 31416.3613 - val_loss: 994649088.0000 - val_rmse: 31538.0586\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 918269376.0000 - rmse: 30302.9590 - val_loss: 1132595968.0000 - val_rmse: 33654.0625\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 911089472.0000 - rmse: 30184.2578 - val_loss: 795441472.0000 - val_rmse: 28203.5703\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 881742208.0000 - rmse: 29694.1445 - val_loss: 869848576.0000 - val_rmse: 29493.1953\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 865879040.0000 - rmse: 29425.8203 - val_loss: 751930112.0000 - val_rmse: 27421.3438\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 868616832.0000 - rmse: 29472.3066 - val_loss: 760334592.0000 - val_rmse: 27574.1660\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 859772864.0000 - rmse: 29321.8828 - val_loss: 804790656.0000 - val_rmse: 28368.8320\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 818548032.0000 - rmse: 28610.2793 - val_loss: 857684992.0000 - val_rmse: 29286.2598\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 784414784.0000 - rmse: 28007.4062 - val_loss: 827672704.0000 - val_rmse: 28769.3008\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800413568.0000 - rmse: 28291.5781 - val_loss: 827094912.0000 - val_rmse: 28759.2578\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 791886208.0000 - rmse: 28140.4707 - val_loss: 765035200.0000 - val_rmse: 27659.2695\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 741680960.0000 - rmse: 27233.8203 - val_loss: 782051392.0000 - val_rmse: 27965.1816\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 743078464.0000 - rmse: 27259.4648 - val_loss: 1120592384.0000 - val_rmse: 33475.2500\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 791710912.0000 - rmse: 28137.3574 - val_loss: 784793152.0000 - val_rmse: 28014.1602\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691673600.0000 - rmse: 26299.6875 - val_loss: 1318387584.0000 - val_rmse: 36309.6055\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 670540352.0000 - rmse: 25894.7930 - val_loss: 1111726208.0000 - val_rmse: 33342.5586\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 723039104.0000 - rmse: 26889.3867 - val_loss: 767335168.0000 - val_rmse: 27700.8145\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724964736.0000 - rmse: 26925.1699 - val_loss: 902916544.0000 - val_rmse: 30048.5703\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591034112.0000 - rmse: 24311.1934 - val_loss: 681841664.0000 - val_rmse: 26112.0977\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724139520.0000 - rmse: 26909.8379 - val_loss: 789435968.0000 - val_rmse: 28096.9004\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 633941120.0000 - rmse: 25178.1875 - val_loss: 1099691264.0000 - val_rmse: 33161.5938\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 696822016.0000 - rmse: 26397.3867 - val_loss: 1074524544.0000 - val_rmse: 32779.9414\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 686568064.0000 - rmse: 26202.4434 - val_loss: 895525952.0000 - val_rmse: 29925.3398\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 681412736.0000 - rmse: 26103.8828 - val_loss: 1020757248.0000 - val_rmse: 31949.2910\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589361664.0000 - rmse: 24276.7715 - val_loss: 1614059648.0000 - val_rmse: 40175.3594\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609874816.0000 - rmse: 24695.6426 - val_loss: 869601856.0000 - val_rmse: 29489.0117\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637871296.0000 - rmse: 25256.1133 - val_loss: 888361600.0000 - val_rmse: 29805.3945\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604001536.0000 - rmse: 24576.4434 - val_loss: 1218894848.0000 - val_rmse: 34912.6758\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586000768.0000 - rmse: 24207.4531 - val_loss: 863878144.0000 - val_rmse: 29391.8047\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599441472.0000 - rmse: 24483.4941 - val_loss: 886057600.0000 - val_rmse: 29766.7168\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614609600.0000 - rmse: 24791.3203 - val_loss: 871134720.0000 - val_rmse: 29514.9922\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556714624.0000 - rmse: 23594.7988 - val_loss: 1356173440.0000 - val_rmse: 36826.2578\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585472640.0000 - rmse: 24196.5410 - val_loss: 857754816.0000 - val_rmse: 29287.4512\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526635200.0000 - rmse: 22948.5332 - val_loss: 1028739136.0000 - val_rmse: 32073.9629\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554318528.0000 - rmse: 23543.9688 - val_loss: 1242063872.0000 - val_rmse: 35242.9258\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529356384.0000 - rmse: 23007.7461 - val_loss: 807088640.0000 - val_rmse: 28409.3047\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550831680.0000 - rmse: 23469.8027 - val_loss: 856491520.0000 - val_rmse: 29265.8770\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479301792.0000 - rmse: 21892.9609 - val_loss: 1354845312.0000 - val_rmse: 36808.2227\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513717088.0000 - rmse: 22665.3281 - val_loss: 1105107712.0000 - val_rmse: 33243.1602\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516591104.0000 - rmse: 22728.6387 - val_loss: 970235328.0000 - val_rmse: 31148.6016\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543954304.0000 - rmse: 23322.8281 - val_loss: 776563776.0000 - val_rmse: 27866.8945\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515056768.0000 - rmse: 22694.8613 - val_loss: 1324649984.0000 - val_rmse: 36395.7383\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492578944.0000 - rmse: 22194.1191 - val_loss: 1009346048.0000 - val_rmse: 31770.2070\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458311232.0000 - rmse: 21408.2031 - val_loss: 780418560.0000 - val_rmse: 27935.9727\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458750720.0000 - rmse: 21418.4648 - val_loss: 1131449600.0000 - val_rmse: 33637.0273\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544756480.0000 - rmse: 23340.0195 - val_loss: 2004069248.0000 - val_rmse: 44766.8320\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519801664.0000 - rmse: 22799.1582 - val_loss: 866193792.0000 - val_rmse: 29431.1699\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484375008.0000 - rmse: 22008.5215 - val_loss: 696074176.0000 - val_rmse: 26383.2168\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488481632.0000 - rmse: 22101.6211 - val_loss: 1630232704.0000 - val_rmse: 40376.1406\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527120928.0000 - rmse: 22959.1152 - val_loss: 1046089536.0000 - val_rmse: 32343.3047\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474218784.0000 - rmse: 21776.5645 - val_loss: 740196032.0000 - val_rmse: 27206.5430\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482839456.0000 - rmse: 21973.6074 - val_loss: 786426176.0000 - val_rmse: 28043.2910\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455779232.0000 - rmse: 21348.9824 - val_loss: 962980928.0000 - val_rmse: 31031.9316\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450239584.0000 - rmse: 21218.8496 - val_loss: 830785600.0000 - val_rmse: 28823.3516\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578981952.0000 - rmse: 24062.0430 - val_loss: 1104593664.0000 - val_rmse: 33235.4297\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446654560.0000 - rmse: 21134.2012 - val_loss: 852236544.0000 - val_rmse: 29193.0898\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430458368.0000 - rmse: 20747.4902 - val_loss: 1302666368.0000 - val_rmse: 36092.4688\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485317536.0000 - rmse: 22029.9238 - val_loss: 629130624.0000 - val_rmse: 25082.4746\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492358496.0000 - rmse: 22189.1504 - val_loss: 1095741440.0000 - val_rmse: 33101.9844\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425002688.0000 - rmse: 20615.5938 - val_loss: 977864576.0000 - val_rmse: 31270.8262\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516776352.0000 - rmse: 22732.7148 - val_loss: 735950848.0000 - val_rmse: 27128.4141\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425183360.0000 - rmse: 20619.9727 - val_loss: 1218896000.0000 - val_rmse: 34912.6914\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470670912.0000 - rmse: 21694.9492 - val_loss: 889561536.0000 - val_rmse: 29825.5176\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481825472.0000 - rmse: 21950.5234 - val_loss: 802566272.0000 - val_rmse: 28329.6016\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373932896.0000 - rmse: 19337.3438 - val_loss: 1165812480.0000 - val_rmse: 34143.9961\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436176512.0000 - rmse: 20884.8379 - val_loss: 1054013760.0000 - val_rmse: 32465.5781\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409746624.0000 - rmse: 20242.1992 - val_loss: 775585280.0000 - val_rmse: 27849.3320\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414897536.0000 - rmse: 20369.0332 - val_loss: 1143133824.0000 - val_rmse: 33810.2617\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504351232.0000 - rmse: 22457.7637 - val_loss: 1031761280.0000 - val_rmse: 32121.0391\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451167712.0000 - rmse: 21240.7070 - val_loss: 908147392.0000 - val_rmse: 30135.4844\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460831616.0000 - rmse: 21466.9883 - val_loss: 928639552.0000 - val_rmse: 30473.5879\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384060544.0000 - rmse: 19597.4609 - val_loss: 885518272.0000 - val_rmse: 29757.6582\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419795616.0000 - rmse: 20488.9141 - val_loss: 761547136.0000 - val_rmse: 27596.1406\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406560320.0000 - rmse: 20163.3398 - val_loss: 789734144.0000 - val_rmse: 28102.2090\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466913984.0000 - rmse: 21608.1934 - val_loss: 1517748992.0000 - val_rmse: 38958.2969\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345337120.0000 - rmse: 18583.2480 - val_loss: 1168944768.0000 - val_rmse: 34189.8359\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376975296.0000 - rmse: 19415.8516 - val_loss: 860975808.0000 - val_rmse: 29342.3867\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383647008.0000 - rmse: 19586.9082 - val_loss: 731317952.0000 - val_rmse: 27042.8906\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357510944.0000 - rmse: 18907.9590 - val_loss: 1118893056.0000 - val_rmse: 33449.8594\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464793152.0000 - rmse: 21559.0625 - val_loss: 964525504.0000 - val_rmse: 31056.8105\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389564512.0000 - rmse: 19737.3887 - val_loss: 906559936.0000 - val_rmse: 30109.1309\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443940128.0000 - rmse: 21069.8848 - val_loss: 738234752.0000 - val_rmse: 27170.4766\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349052064.0000 - rmse: 18682.9336 - val_loss: 1750434944.0000 - val_rmse: 41838.1992\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381622464.0000 - rmse: 19535.1582 - val_loss: 1370595712.0000 - val_rmse: 37021.5586\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340934240.0000 - rmse: 18464.4043 - val_loss: 1032758528.0000 - val_rmse: 32136.5605\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372344544.0000 - rmse: 19296.2305 - val_loss: 1611884288.0000 - val_rmse: 40148.2773\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348040992.0000 - rmse: 18655.8555 - val_loss: 1053600576.0000 - val_rmse: 32459.2129\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345766368.0000 - rmse: 18594.7930 - val_loss: 912107008.0000 - val_rmse: 30201.1055\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372543616.0000 - rmse: 19301.3867 - val_loss: 852290752.0000 - val_rmse: 29194.0195\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325817920.0000 - rmse: 18050.4277 - val_loss: 1031091136.0000 - val_rmse: 32110.6074\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362011328.0000 - rmse: 19026.5938 - val_loss: 1004500544.0000 - val_rmse: 31693.8574\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351484992.0000 - rmse: 18747.9297 - val_loss: 918653376.0000 - val_rmse: 30309.2949\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368002816.0000 - rmse: 19183.3984 - val_loss: 723295488.0000 - val_rmse: 26894.1543\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338997984.0000 - rmse: 18411.8965 - val_loss: 904356928.0000 - val_rmse: 30072.5273\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391620896.0000 - rmse: 19789.4121 - val_loss: 914330240.0000 - val_rmse: 30237.8926\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346955872.0000 - rmse: 18626.7500 - val_loss: 1758516864.0000 - val_rmse: 41934.6719\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377321632.0000 - rmse: 19424.7676 - val_loss: 975854208.0000 - val_rmse: 31238.6660\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370435904.0000 - rmse: 19246.7109 - val_loss: 1252115072.0000 - val_rmse: 35385.2383\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380976416.0000 - rmse: 19518.6172 - val_loss: 984175744.0000 - val_rmse: 31371.5762\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393743712.0000 - rmse: 19842.9746 - val_loss: 749980992.0000 - val_rmse: 27385.7793\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276758624.0000 - rmse: 16636.0625 - val_loss: 1168132352.0000 - val_rmse: 34177.9492\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341570976.0000 - rmse: 18481.6367 - val_loss: 1551960448.0000 - val_rmse: 39394.9297\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329558560.0000 - rmse: 18153.7461 - val_loss: 1016618816.0000 - val_rmse: 31884.4609\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300924608.0000 - rmse: 17347.1777 - val_loss: 1220127104.0000 - val_rmse: 34930.3164\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434722400.0000 - rmse: 20849.9980 - val_loss: 921767104.0000 - val_rmse: 30360.6172\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308223872.0000 - rmse: 17556.3047 - val_loss: 943450304.0000 - val_rmse: 30715.6348\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341444352.0000 - rmse: 18478.2129 - val_loss: 1081892736.0000 - val_rmse: 32892.1367\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394916192.0000 - rmse: 19872.4980 - val_loss: 1488425856.0000 - val_rmse: 38580.1211\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319174560.0000 - rmse: 17865.4570 - val_loss: 1382807936.0000 - val_rmse: 37186.1250\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349282144.0000 - rmse: 18689.0918 - val_loss: 1488997632.0000 - val_rmse: 38587.5312\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380005728.0000 - rmse: 19493.7344 - val_loss: 1068387008.0000 - val_rmse: 32686.1875\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324556320.0000 - rmse: 18015.4453 - val_loss: 1859980672.0000 - val_rmse: 43127.4922\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384798848.0000 - rmse: 19616.2891 - val_loss: 1073657088.0000 - val_rmse: 32766.7070\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247070800.0000 - rmse: 15718.4854 - val_loss: 1387192832.0000 - val_rmse: 37245.0391\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388179840.0000 - rmse: 19702.2793 - val_loss: 709907072.0000 - val_rmse: 26644.0801\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388343648.0000 - rmse: 19706.4355 - val_loss: 1602817280.0000 - val_rmse: 40035.1992\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348840384.0000 - rmse: 18677.2676 - val_loss: 990757376.0000 - val_rmse: 31476.2969\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286655168.0000 - rmse: 16930.8926 - val_loss: 1341080960.0000 - val_rmse: 36620.7734\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343635104.0000 - rmse: 18537.3965 - val_loss: 1314983808.0000 - val_rmse: 36262.7070\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386958528.0000 - rmse: 19671.2598 - val_loss: 1051858176.0000 - val_rmse: 32432.3633\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299220384.0000 - rmse: 17297.9863 - val_loss: 1084824064.0000 - val_rmse: 32936.6680\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368481280.0000 - rmse: 19195.8652 - val_loss: 1225527680.0000 - val_rmse: 35007.5391\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370289824.0000 - rmse: 19242.9121 - val_loss: 1181732992.0000 - val_rmse: 34376.3438\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390260512.0000 - rmse: 19755.0098 - val_loss: 787977920.0000 - val_rmse: 28070.9414\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276504672.0000 - rmse: 16628.4297 - val_loss: 1244809728.0000 - val_rmse: 35281.8594\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325394912.0000 - rmse: 18038.7051 - val_loss: 1065112512.0000 - val_rmse: 32636.0625\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270092384.0000 - rmse: 16434.4863 - val_loss: 726788096.0000 - val_rmse: 26959.0078\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410017696.0000 - rmse: 20248.8945 - val_loss: 1048216512.0000 - val_rmse: 32376.1699\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335390304.0000 - rmse: 18313.6641 - val_loss: 726250304.0000 - val_rmse: 26949.0293\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326574528.0000 - rmse: 18071.3730 - val_loss: 1033721600.0000 - val_rmse: 32151.5410\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330798400.0000 - rmse: 18187.8613 - val_loss: 1212231168.0000 - val_rmse: 34817.1094\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377707232.0000 - rmse: 19434.6895 - val_loss: 779025088.0000 - val_rmse: 27911.0195\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282552928.0000 - rmse: 16809.3105 - val_loss: 1412239488.0000 - val_rmse: 37579.7734\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259948272.0000 - rmse: 16122.9092 - val_loss: 962168768.0000 - val_rmse: 31018.8457\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321042688.0000 - rmse: 17917.6621 - val_loss: 893129344.0000 - val_rmse: 29885.2695\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284097600.0000 - rmse: 16855.1934 - val_loss: 941982592.0000 - val_rmse: 30691.7344\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318822368.0000 - rmse: 17855.5957 - val_loss: 612001856.0000 - val_rmse: 24738.6719\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340988832.0000 - rmse: 18465.8828 - val_loss: 588768576.0000 - val_rmse: 24264.5527\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316313440.0000 - rmse: 17785.2012 - val_loss: 1297653888.0000 - val_rmse: 36022.9609\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369314720.0000 - rmse: 19217.5625 - val_loss: 1023790976.0000 - val_rmse: 31996.7324\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306904960.0000 - rmse: 17518.7031 - val_loss: 939765376.0000 - val_rmse: 30655.5938\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294379776.0000 - rmse: 17157.4980 - val_loss: 733422912.0000 - val_rmse: 27081.7812\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291414336.0000 - rmse: 17070.8613 - val_loss: 1252633216.0000 - val_rmse: 35392.5586\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301331136.0000 - rmse: 17358.8906 - val_loss: 826192832.0000 - val_rmse: 28743.5703\n",
      "104/104 [==============================] - 0s 694us/step - loss: 433126784.0000 - rmse: 20811.6973\n",
      "[433126784.0, 20811.697265625]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 14472951808.0000 - rmse: 120303.5781 - val_loss: 2105595264.0000 - val_rmse: 45886.7656\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2414394624.0000 - rmse: 49136.4883 - val_loss: 1565815936.0000 - val_rmse: 39570.3945\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1943333248.0000 - rmse: 44083.2539 - val_loss: 1194634880.0000 - val_rmse: 34563.4922\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1765178752.0000 - rmse: 42014.0312 - val_loss: 1106085376.0000 - val_rmse: 33257.8633\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1643924992.0000 - rmse: 40545.3438 - val_loss: 1021938432.0000 - val_rmse: 31967.7715\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1524641792.0000 - rmse: 39046.6602 - val_loss: 979741824.0000 - val_rmse: 31300.8281\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1417721216.0000 - rmse: 37652.6406 - val_loss: 1006916224.0000 - val_rmse: 31731.9434\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1389911168.0000 - rmse: 37281.5117 - val_loss: 1028255936.0000 - val_rmse: 32066.4297\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1355442816.0000 - rmse: 36816.3398 - val_loss: 922884544.0000 - val_rmse: 30379.0137\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1308360704.0000 - rmse: 36171.2695 - val_loss: 911606208.0000 - val_rmse: 30192.8164\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1277910272.0000 - rmse: 35747.8672 - val_loss: 927768576.0000 - val_rmse: 30459.2930\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1255649024.0000 - rmse: 35435.1367 - val_loss: 994133568.0000 - val_rmse: 31529.8828\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1180783872.0000 - rmse: 34362.5352 - val_loss: 1100742016.0000 - val_rmse: 33177.4336\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1184953088.0000 - rmse: 34423.1484 - val_loss: 1148841344.0000 - val_rmse: 33894.5625\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1205863040.0000 - rmse: 34725.5391 - val_loss: 906249472.0000 - val_rmse: 30103.9785\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1168302720.0000 - rmse: 34180.4453 - val_loss: 892260544.0000 - val_rmse: 29870.7305\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1166909696.0000 - rmse: 34160.0586 - val_loss: 1064738688.0000 - val_rmse: 32630.3340\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1139921280.0000 - rmse: 33762.7188 - val_loss: 915677120.0000 - val_rmse: 30260.1543\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1095347712.0000 - rmse: 33096.0391 - val_loss: 861582592.0000 - val_rmse: 29352.7266\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1141414144.0000 - rmse: 33784.8203 - val_loss: 908552000.0000 - val_rmse: 30142.1973\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1094735232.0000 - rmse: 33086.7812 - val_loss: 859910016.0000 - val_rmse: 29324.2227\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1061868928.0000 - rmse: 32586.3281 - val_loss: 922030080.0000 - val_rmse: 30364.9453\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1021602176.0000 - rmse: 31962.5117 - val_loss: 871412864.0000 - val_rmse: 29519.7012\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 999136192.0000 - rmse: 31609.1152 - val_loss: 932445248.0000 - val_rmse: 30535.9648\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1007350784.0000 - rmse: 31738.7891 - val_loss: 944892352.0000 - val_rmse: 30739.1016\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1013089152.0000 - rmse: 31829.0605 - val_loss: 861555072.0000 - val_rmse: 29352.2578\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1019417344.0000 - rmse: 31928.3164 - val_loss: 884855680.0000 - val_rmse: 29746.5234\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1027465024.0000 - rmse: 32054.0957 - val_loss: 873266944.0000 - val_rmse: 29551.0898\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 949492224.0000 - rmse: 30813.8320 - val_loss: 881390336.0000 - val_rmse: 29688.2188\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 988124032.0000 - rmse: 31434.4375 - val_loss: 877991232.0000 - val_rmse: 29630.9160\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 963980672.0000 - rmse: 31048.0391 - val_loss: 848936832.0000 - val_rmse: 29136.5215\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 974229696.0000 - rmse: 31212.6523 - val_loss: 874780928.0000 - val_rmse: 29576.6953\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 920839040.0000 - rmse: 30345.3301 - val_loss: 884896064.0000 - val_rmse: 29747.2031\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 933535168.0000 - rmse: 30553.8086 - val_loss: 933693312.0000 - val_rmse: 30556.3965\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 911936896.0000 - rmse: 30198.2910 - val_loss: 871451392.0000 - val_rmse: 29520.3555\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 892841088.0000 - rmse: 29880.4453 - val_loss: 1045989696.0000 - val_rmse: 32341.7637\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 945077376.0000 - rmse: 30742.1113 - val_loss: 876605184.0000 - val_rmse: 29607.5195\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 896389184.0000 - rmse: 29939.7578 - val_loss: 873930048.0000 - val_rmse: 29562.3086\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 890046144.0000 - rmse: 29833.6406 - val_loss: 902467712.0000 - val_rmse: 30041.0996\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 898654400.0000 - rmse: 29977.5645 - val_loss: 1101364736.0000 - val_rmse: 33186.8164\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 833909248.0000 - rmse: 28877.4863 - val_loss: 903415872.0000 - val_rmse: 30056.8770\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 852895424.0000 - rmse: 29204.3730 - val_loss: 846335872.0000 - val_rmse: 29091.8516\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 815213696.0000 - rmse: 28551.9453 - val_loss: 939296640.0000 - val_rmse: 30647.9473\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 780844800.0000 - rmse: 27943.5996 - val_loss: 891153152.0000 - val_rmse: 29852.1855\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 816555840.0000 - rmse: 28575.4395 - val_loss: 888866624.0000 - val_rmse: 29813.8672\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 812238272.0000 - rmse: 28499.7930 - val_loss: 840886080.0000 - val_rmse: 28998.0352\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 764917760.0000 - rmse: 27657.1465 - val_loss: 859695488.0000 - val_rmse: 29320.5645\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 778170176.0000 - rmse: 27895.7012 - val_loss: 909306176.0000 - val_rmse: 30154.7012\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 731964672.0000 - rmse: 27054.8457 - val_loss: 1363389696.0000 - val_rmse: 36924.1055\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 710030080.0000 - rmse: 26646.3887 - val_loss: 1182362368.0000 - val_rmse: 34385.4961\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 739835264.0000 - rmse: 27199.9102 - val_loss: 871003904.0000 - val_rmse: 29512.7754\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 712905344.0000 - rmse: 26700.2871 - val_loss: 883922688.0000 - val_rmse: 29730.8379\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704197760.0000 - rmse: 26536.7246 - val_loss: 897699264.0000 - val_rmse: 29961.6289\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 720363456.0000 - rmse: 26839.5859 - val_loss: 898720448.0000 - val_rmse: 29978.6660\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 709473024.0000 - rmse: 26635.9316 - val_loss: 796184384.0000 - val_rmse: 28216.7402\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637512704.0000 - rmse: 25249.0137 - val_loss: 1103503232.0000 - val_rmse: 33219.0195\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642515712.0000 - rmse: 25347.8926 - val_loss: 824525696.0000 - val_rmse: 28714.5547\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 664062208.0000 - rmse: 25769.4023 - val_loss: 838810368.0000 - val_rmse: 28962.2227\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 679040576.0000 - rmse: 26058.4062 - val_loss: 905714624.0000 - val_rmse: 30095.0918\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 655304576.0000 - rmse: 25598.9180 - val_loss: 812883456.0000 - val_rmse: 28511.1113\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589085120.0000 - rmse: 24271.0742 - val_loss: 994615808.0000 - val_rmse: 31537.5293\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 619728640.0000 - rmse: 24894.3457 - val_loss: 825148288.0000 - val_rmse: 28725.3926\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641971968.0000 - rmse: 25337.1641 - val_loss: 833147520.0000 - val_rmse: 28864.2949\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614453312.0000 - rmse: 24788.1680 - val_loss: 1082948480.0000 - val_rmse: 32908.1797\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572180800.0000 - rmse: 23920.3008 - val_loss: 784369536.0000 - val_rmse: 28006.5977\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594919424.0000 - rmse: 24390.9688 - val_loss: 793648640.0000 - val_rmse: 28171.7695\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574499904.0000 - rmse: 23968.7285 - val_loss: 825185856.0000 - val_rmse: 28726.0469\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582207680.0000 - rmse: 24128.9805 - val_loss: 959229248.0000 - val_rmse: 30971.4258\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 638101312.0000 - rmse: 25260.6680 - val_loss: 1003414848.0000 - val_rmse: 31676.7246\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564361024.0000 - rmse: 23756.2832 - val_loss: 892909952.0000 - val_rmse: 29881.5996\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529131584.0000 - rmse: 23002.8594 - val_loss: 894708800.0000 - val_rmse: 29911.6816\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523883328.0000 - rmse: 22888.4980 - val_loss: 796135104.0000 - val_rmse: 28215.8652\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534573856.0000 - rmse: 23120.8535 - val_loss: 835475904.0000 - val_rmse: 28904.5977\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584339712.0000 - rmse: 24173.1191 - val_loss: 829864896.0000 - val_rmse: 28807.3730\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514721696.0000 - rmse: 22687.4766 - val_loss: 754867072.0000 - val_rmse: 27474.8438\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549785344.0000 - rmse: 23447.5020 - val_loss: 819283648.0000 - val_rmse: 28623.1289\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441509696.0000 - rmse: 21012.1309 - val_loss: 908948032.0000 - val_rmse: 30148.7656\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477550176.0000 - rmse: 21852.9199 - val_loss: 811191296.0000 - val_rmse: 28481.4180\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489671360.0000 - rmse: 22128.5176 - val_loss: 822010176.0000 - val_rmse: 28670.7188\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501348160.0000 - rmse: 22390.8047 - val_loss: 883116416.0000 - val_rmse: 29717.2754\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426423040.0000 - rmse: 20650.0117 - val_loss: 1064663168.0000 - val_rmse: 32629.1758\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418473120.0000 - rmse: 20456.6152 - val_loss: 920620224.0000 - val_rmse: 30341.7246\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432930592.0000 - rmse: 20806.9824 - val_loss: 775319040.0000 - val_rmse: 27844.5508\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416797120.0000 - rmse: 20415.6074 - val_loss: 769757888.0000 - val_rmse: 27744.5117\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430927456.0000 - rmse: 20758.7930 - val_loss: 1030503872.0000 - val_rmse: 32101.4629\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413884992.0000 - rmse: 20344.1621 - val_loss: 962213504.0000 - val_rmse: 31019.5664\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449518528.0000 - rmse: 21201.8496 - val_loss: 1053484288.0000 - val_rmse: 32457.4219\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422466592.0000 - rmse: 20553.9922 - val_loss: 851485056.0000 - val_rmse: 29180.2168\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445888800.0000 - rmse: 21116.0781 - val_loss: 1024959936.0000 - val_rmse: 32014.9961\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406994592.0000 - rmse: 20174.1055 - val_loss: 922423232.0000 - val_rmse: 30371.4180\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440365856.0000 - rmse: 20984.8945 - val_loss: 858272384.0000 - val_rmse: 29296.2871\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339080992.0000 - rmse: 18414.1523 - val_loss: 1074008960.0000 - val_rmse: 32772.0742\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426070976.0000 - rmse: 20641.4863 - val_loss: 977861824.0000 - val_rmse: 31270.7793\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356715936.0000 - rmse: 18886.9238 - val_loss: 867672960.0000 - val_rmse: 29456.2891\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406131744.0000 - rmse: 20152.7090 - val_loss: 1035106432.0000 - val_rmse: 32173.0684\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410578880.0000 - rmse: 20262.7461 - val_loss: 866259264.0000 - val_rmse: 29432.2832\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318042976.0000 - rmse: 17833.7578 - val_loss: 922633600.0000 - val_rmse: 30374.8848\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359919424.0000 - rmse: 18971.5410 - val_loss: 1065180224.0000 - val_rmse: 32637.0996\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366836032.0000 - rmse: 19152.9629 - val_loss: 620002176.0000 - val_rmse: 24899.8438\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306791552.0000 - rmse: 17515.4648 - val_loss: 1271049216.0000 - val_rmse: 35651.7773\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344721312.0000 - rmse: 18566.6719 - val_loss: 970329536.0000 - val_rmse: 31150.1074\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327148096.0000 - rmse: 18087.2324 - val_loss: 813166784.0000 - val_rmse: 28516.0801\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299903488.0000 - rmse: 17317.7207 - val_loss: 981463872.0000 - val_rmse: 31328.3242\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299144640.0000 - rmse: 17295.7988 - val_loss: 866689408.0000 - val_rmse: 29439.5879\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334174752.0000 - rmse: 18280.4473 - val_loss: 832480256.0000 - val_rmse: 28852.7324\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338448320.0000 - rmse: 18396.9648 - val_loss: 1080196224.0000 - val_rmse: 32866.3398\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351837760.0000 - rmse: 18757.3379 - val_loss: 1029623616.0000 - val_rmse: 32087.7480\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329153728.0000 - rmse: 18142.5918 - val_loss: 894308544.0000 - val_rmse: 29904.9922\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317821184.0000 - rmse: 17827.5410 - val_loss: 1355608448.0000 - val_rmse: 36818.5898\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304635104.0000 - rmse: 17453.7969 - val_loss: 1321462912.0000 - val_rmse: 36351.9297\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304195200.0000 - rmse: 17441.1914 - val_loss: 879984832.0000 - val_rmse: 29664.5391\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324936032.0000 - rmse: 18025.9805 - val_loss: 1164710784.0000 - val_rmse: 34127.8594\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257616640.0000 - rmse: 16050.4395 - val_loss: 883187328.0000 - val_rmse: 29718.4688\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334230688.0000 - rmse: 18281.9766 - val_loss: 1117589888.0000 - val_rmse: 33430.3711\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309785632.0000 - rmse: 17600.7266 - val_loss: 871605888.0000 - val_rmse: 29522.9727\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341849504.0000 - rmse: 18489.1699 - val_loss: 1195457024.0000 - val_rmse: 34575.3828\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341757024.0000 - rmse: 18486.6699 - val_loss: 1049324288.0000 - val_rmse: 32393.2734\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271607936.0000 - rmse: 16480.5312 - val_loss: 853261632.0000 - val_rmse: 29210.6426\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279585728.0000 - rmse: 16720.8145 - val_loss: 1064424704.0000 - val_rmse: 32625.5215\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292149536.0000 - rmse: 17092.3828 - val_loss: 1236848512.0000 - val_rmse: 35168.8555\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391651200.0000 - rmse: 19790.1777 - val_loss: 1022617344.0000 - val_rmse: 31978.3887\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270345760.0000 - rmse: 16442.1934 - val_loss: 1186389632.0000 - val_rmse: 34444.0078\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310541856.0000 - rmse: 17622.1953 - val_loss: 943730752.0000 - val_rmse: 30720.2012\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262128592.0000 - rmse: 16190.3857 - val_loss: 1059272960.0000 - val_rmse: 32546.4746\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320924640.0000 - rmse: 17914.3691 - val_loss: 1119935360.0000 - val_rmse: 33465.4336\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311101824.0000 - rmse: 17638.0762 - val_loss: 1064735232.0000 - val_rmse: 32630.2793\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257211824.0000 - rmse: 16037.8232 - val_loss: 1228828800.0000 - val_rmse: 35054.6523\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271584992.0000 - rmse: 16479.8340 - val_loss: 978872640.0000 - val_rmse: 31286.9395\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274541728.0000 - rmse: 16569.2988 - val_loss: 848240320.0000 - val_rmse: 29124.5664\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267045696.0000 - rmse: 16341.5322 - val_loss: 1535349632.0000 - val_rmse: 39183.5352\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287390304.0000 - rmse: 16952.5879 - val_loss: 1189629696.0000 - val_rmse: 34491.0078\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270072256.0000 - rmse: 16433.8730 - val_loss: 997946496.0000 - val_rmse: 31590.2891\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285151872.0000 - rmse: 16886.4395 - val_loss: 1452293632.0000 - val_rmse: 38108.9688\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286506560.0000 - rmse: 16926.5020 - val_loss: 939876608.0000 - val_rmse: 30657.4062\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278286720.0000 - rmse: 16681.9277 - val_loss: 1017306688.0000 - val_rmse: 31895.2441\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343877088.0000 - rmse: 18543.9238 - val_loss: 901592896.0000 - val_rmse: 30026.5332\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235063680.0000 - rmse: 15331.7861 - val_loss: 1174383616.0000 - val_rmse: 34269.2812\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296260672.0000 - rmse: 17212.2227 - val_loss: 955554176.0000 - val_rmse: 30912.0371\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297283808.0000 - rmse: 17241.9199 - val_loss: 1022062208.0000 - val_rmse: 31969.7070\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261218832.0000 - rmse: 16162.2646 - val_loss: 1087203968.0000 - val_rmse: 32972.7773\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266448896.0000 - rmse: 16323.2617 - val_loss: 1681634560.0000 - val_rmse: 41007.7383\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260711456.0000 - rmse: 16146.5605 - val_loss: 1093833856.0000 - val_rmse: 33073.1602\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246340800.0000 - rmse: 15695.2461 - val_loss: 1362815616.0000 - val_rmse: 36916.3320\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282056576.0000 - rmse: 16794.5371 - val_loss: 1244220288.0000 - val_rmse: 35273.5078\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232960592.0000 - rmse: 15263.0449 - val_loss: 1292358528.0000 - val_rmse: 35949.3867\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256277840.0000 - rmse: 16008.6797 - val_loss: 1710746752.0000 - val_rmse: 41361.1719\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277058848.0000 - rmse: 16645.0820 - val_loss: 1074978944.0000 - val_rmse: 32786.8711\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246540944.0000 - rmse: 15701.6211 - val_loss: 1073506048.0000 - val_rmse: 32764.4023\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264888576.0000 - rmse: 16275.3965 - val_loss: 1436928384.0000 - val_rmse: 37906.8398\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315495648.0000 - rmse: 17762.1953 - val_loss: 1111846272.0000 - val_rmse: 33344.3594\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258030912.0000 - rmse: 16063.3389 - val_loss: 1123286016.0000 - val_rmse: 33515.4570\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256960720.0000 - rmse: 16029.9922 - val_loss: 1232828032.0000 - val_rmse: 35111.6523\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286467264.0000 - rmse: 16925.3438 - val_loss: 1663293952.0000 - val_rmse: 40783.4961\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247173440.0000 - rmse: 15721.7490 - val_loss: 980495552.0000 - val_rmse: 31312.8652\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250878400.0000 - rmse: 15839.1387 - val_loss: 1188856192.0000 - val_rmse: 34479.7930\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268910272.0000 - rmse: 16398.4824 - val_loss: 1220347904.0000 - val_rmse: 34933.4766\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259903024.0000 - rmse: 16121.5059 - val_loss: 920489792.0000 - val_rmse: 30339.5742\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268455616.0000 - rmse: 16384.6113 - val_loss: 985116288.0000 - val_rmse: 31386.5625\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247894496.0000 - rmse: 15744.6621 - val_loss: 1540203264.0000 - val_rmse: 39245.4219\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227263952.0000 - rmse: 15075.2744 - val_loss: 1100986624.0000 - val_rmse: 33181.1172\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243730608.0000 - rmse: 15611.8730 - val_loss: 915069760.0000 - val_rmse: 30250.1191\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 225294480.0000 - rmse: 15009.8115 - val_loss: 1397507584.0000 - val_rmse: 37383.2539\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228034496.0000 - rmse: 15100.8096 - val_loss: 1350912896.0000 - val_rmse: 36754.7656\n",
      "Epoch 164/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250377984.0000 - rmse: 15823.3350 - val_loss: 1243802240.0000 - val_rmse: 35267.5820\n",
      "Epoch 165/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274833344.0000 - rmse: 16578.0957 - val_loss: 1226611456.0000 - val_rmse: 35023.0117\n",
      "Epoch 166/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249877232.0000 - rmse: 15807.5029 - val_loss: 1023333632.0000 - val_rmse: 31989.5859\n",
      "Epoch 167/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237576384.0000 - rmse: 15413.5127 - val_loss: 1138022656.0000 - val_rmse: 33734.5859\n",
      "Epoch 168/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231127328.0000 - rmse: 15202.8711 - val_loss: 1341517184.0000 - val_rmse: 36626.7266\n",
      "Epoch 169/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227838960.0000 - rmse: 15094.3340 - val_loss: 1510693760.0000 - val_rmse: 38867.6445\n",
      "Epoch 170/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240849920.0000 - rmse: 15519.3389 - val_loss: 1396799232.0000 - val_rmse: 37373.7773\n",
      "Epoch 171/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 205817872.0000 - rmse: 14346.3516 - val_loss: 1392419712.0000 - val_rmse: 37315.1406\n",
      "Epoch 172/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214462512.0000 - rmse: 14644.5371 - val_loss: 1132913536.0000 - val_rmse: 33658.7812\n",
      "Epoch 173/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 204998256.0000 - rmse: 14317.7588 - val_loss: 1639195520.0000 - val_rmse: 40486.9766\n",
      "Epoch 174/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241741856.0000 - rmse: 15548.0488 - val_loss: 1190102784.0000 - val_rmse: 34497.8672\n",
      "Epoch 175/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 208700496.0000 - rmse: 14446.4668 - val_loss: 1177365504.0000 - val_rmse: 34312.7617\n",
      "Epoch 176/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231685056.0000 - rmse: 15221.2002 - val_loss: 1352646656.0000 - val_rmse: 36778.3438\n",
      "Epoch 177/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240117408.0000 - rmse: 15495.7188 - val_loss: 1220153344.0000 - val_rmse: 34930.6953\n",
      "Epoch 178/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211844528.0000 - rmse: 14554.8770 - val_loss: 1501865856.0000 - val_rmse: 38753.9141\n",
      "Epoch 179/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227409632.0000 - rmse: 15080.1055 - val_loss: 1309834368.0000 - val_rmse: 36191.6328\n",
      "Epoch 180/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249284816.0000 - rmse: 15788.7559 - val_loss: 1505401984.0000 - val_rmse: 38799.5117\n",
      "Epoch 181/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230271264.0000 - rmse: 15174.6895 - val_loss: 1316255488.0000 - val_rmse: 36280.2344\n",
      "Epoch 182/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 203629152.0000 - rmse: 14269.8643 - val_loss: 1189651968.0000 - val_rmse: 34491.3320\n",
      "Epoch 183/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220330064.0000 - rmse: 14843.5176 - val_loss: 1207315968.0000 - val_rmse: 34746.4531\n",
      "Epoch 184/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243277600.0000 - rmse: 15597.3574 - val_loss: 1250882304.0000 - val_rmse: 35367.8164\n",
      "Epoch 185/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 212082448.0000 - rmse: 14563.0488 - val_loss: 1548471936.0000 - val_rmse: 39350.6250\n",
      "Epoch 186/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207290256.0000 - rmse: 14397.5732 - val_loss: 1602090880.0000 - val_rmse: 40026.1289\n",
      "Epoch 187/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206296672.0000 - rmse: 14363.0293 - val_loss: 1097412224.0000 - val_rmse: 33127.2109\n",
      "Epoch 188/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 196134912.0000 - rmse: 14004.8154 - val_loss: 1311508224.0000 - val_rmse: 36214.7500\n",
      "Epoch 189/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210296768.0000 - rmse: 14501.6113 - val_loss: 1193539456.0000 - val_rmse: 34547.6406\n",
      "Epoch 190/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206712096.0000 - rmse: 14377.4844 - val_loss: 950451584.0000 - val_rmse: 30829.3945\n",
      "Epoch 191/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218677968.0000 - rmse: 14787.7617 - val_loss: 1273254144.0000 - val_rmse: 35682.6875\n",
      "Epoch 192/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 208530944.0000 - rmse: 14440.5986 - val_loss: 1302016384.0000 - val_rmse: 36083.4648\n",
      "Epoch 193/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 209282608.0000 - rmse: 14466.5996 - val_loss: 1505283328.0000 - val_rmse: 38797.9805\n",
      "Epoch 194/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221879776.0000 - rmse: 14895.6260 - val_loss: 907713024.0000 - val_rmse: 30128.2754\n",
      "Epoch 195/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219302880.0000 - rmse: 14808.8760 - val_loss: 1410990208.0000 - val_rmse: 37563.1484\n",
      "Epoch 196/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 204422848.0000 - rmse: 14297.6494 - val_loss: 1125043072.0000 - val_rmse: 33541.6602\n",
      "Epoch 197/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221696832.0000 - rmse: 14889.4854 - val_loss: 996900928.0000 - val_rmse: 31573.7363\n",
      "Epoch 198/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 183904144.0000 - rmse: 13561.1230 - val_loss: 1218978944.0000 - val_rmse: 34913.8789\n",
      "Epoch 199/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217629440.0000 - rmse: 14752.2676 - val_loss: 1272457984.0000 - val_rmse: 35671.5273\n",
      "Epoch 200/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 225678560.0000 - rmse: 15022.5996 - val_loss: 1134190848.0000 - val_rmse: 33677.7500\n",
      "Epoch 201/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278263040.0000 - rmse: 16681.2168 - val_loss: 1148540544.0000 - val_rmse: 33890.1211\n",
      "Epoch 202/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 203902560.0000 - rmse: 14279.4424 - val_loss: 1281085184.0000 - val_rmse: 35792.2500\n",
      "Epoch 203/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217648384.0000 - rmse: 14752.9082 - val_loss: 929284800.0000 - val_rmse: 30484.1699\n",
      "Epoch 204/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229786736.0000 - rmse: 15158.7158 - val_loss: 1230945536.0000 - val_rmse: 35084.8281\n",
      "Epoch 205/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215985168.0000 - rmse: 14696.4326 - val_loss: 1061817280.0000 - val_rmse: 32585.5371\n",
      "Epoch 206/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211327664.0000 - rmse: 14537.1113 - val_loss: 1024022272.0000 - val_rmse: 32000.3477\n",
      "Epoch 207/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206567552.0000 - rmse: 14372.4561 - val_loss: 1285997952.0000 - val_rmse: 35860.8125\n",
      "Epoch 208/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 198334096.0000 - rmse: 14083.1123 - val_loss: 1077017216.0000 - val_rmse: 32817.9414\n",
      "Epoch 209/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 225199552.0000 - rmse: 15006.6475 - val_loss: 1537120000.0000 - val_rmse: 39206.1211\n",
      "Epoch 210/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 193100752.0000 - rmse: 13896.0674 - val_loss: 1341960320.0000 - val_rmse: 36632.7773\n",
      "Epoch 211/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 197738032.0000 - rmse: 14061.9326 - val_loss: 1255002240.0000 - val_rmse: 35426.0117\n",
      "Epoch 212/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202283872.0000 - rmse: 14222.6504 - val_loss: 2048270720.0000 - val_rmse: 45257.8242\n",
      "Epoch 213/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 198065904.0000 - rmse: 14073.5859 - val_loss: 1268435968.0000 - val_rmse: 35615.1055\n",
      "Epoch 214/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231161920.0000 - rmse: 15204.0068 - val_loss: 1175951488.0000 - val_rmse: 34292.1484\n",
      "Epoch 215/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218486448.0000 - rmse: 14781.2852 - val_loss: 1231586816.0000 - val_rmse: 35093.9688\n",
      "Epoch 216/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 203601280.0000 - rmse: 14268.8887 - val_loss: 1080554880.0000 - val_rmse: 32871.7930\n",
      "Epoch 217/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254124448.0000 - rmse: 15941.2783 - val_loss: 1213359872.0000 - val_rmse: 34833.3164\n",
      "Epoch 218/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 195453088.0000 - rmse: 13980.4521 - val_loss: 1084580992.0000 - val_rmse: 32932.9727\n",
      "Epoch 219/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 192901712.0000 - rmse: 13888.9053 - val_loss: 1394884480.0000 - val_rmse: 37348.1523\n",
      "Epoch 220/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211059280.0000 - rmse: 14527.8770 - val_loss: 1249511552.0000 - val_rmse: 35348.4297\n",
      "Epoch 221/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210995296.0000 - rmse: 14525.6758 - val_loss: 1693887488.0000 - val_rmse: 41156.8633\n",
      "Epoch 222/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220889312.0000 - rmse: 14862.3418 - val_loss: 1076367744.0000 - val_rmse: 32808.0391\n",
      "Epoch 223/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224223360.0000 - rmse: 14974.0879 - val_loss: 1380871168.0000 - val_rmse: 37160.0742\n",
      "Epoch 224/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 199091136.0000 - rmse: 14109.9629 - val_loss: 1009768640.0000 - val_rmse: 31776.8574\n",
      "Epoch 225/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206478608.0000 - rmse: 14369.3623 - val_loss: 1412217856.0000 - val_rmse: 37579.4883\n",
      "Epoch 226/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 195597120.0000 - rmse: 13985.6016 - val_loss: 1157062272.0000 - val_rmse: 34015.6133\n",
      "Epoch 227/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219317920.0000 - rmse: 14809.3838 - val_loss: 1036654336.0000 - val_rmse: 32197.1133\n",
      "Epoch 228/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 189815200.0000 - rmse: 13777.3428 - val_loss: 1220698368.0000 - val_rmse: 34938.4922\n",
      "104/104 [==============================] - 0s 685us/step - loss: 397009120.0000 - rmse: 19925.0859\n",
      "[397009120.0, 19925.0859375]\n",
      "[21168.77734375, 33545.1953125, 22533.07421875, 20811.697265625, 19925.0859375]\n",
      "23596.766015625\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "!python train.py kfold light\n",
    "# epoch 400 p 30 lr 4e-3 (63 32)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 20:41:34.387936: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 20:41:34.387974: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 20:41:34.388283: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 20:41:34.728959: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 17945436160.0000 - rmse: 133960.5781 - val_loss: 3530718976.0000 - val_rmse: 59419.8516\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2836597504.0000 - rmse: 53259.7188 - val_loss: 1341249792.0000 - val_rmse: 36623.0781\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2001559552.0000 - rmse: 44738.7930 - val_loss: 1072029824.0000 - val_rmse: 32741.8672\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1780672256.0000 - rmse: 42198.0117 - val_loss: 985649600.0000 - val_rmse: 31395.0566\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1764143360.0000 - rmse: 42001.7070 - val_loss: 940778112.0000 - val_rmse: 30672.1055\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1694218240.0000 - rmse: 41160.8828 - val_loss: 914864576.0000 - val_rmse: 30246.7285\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1691041408.0000 - rmse: 41122.2734 - val_loss: 872913280.0000 - val_rmse: 29545.1055\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1638732416.0000 - rmse: 40481.2617 - val_loss: 846237248.0000 - val_rmse: 29090.1562\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1550426752.0000 - rmse: 39375.4570 - val_loss: 855298880.0000 - val_rmse: 29245.4941\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1487061248.0000 - rmse: 38562.4336 - val_loss: 817581696.0000 - val_rmse: 28593.3848\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1532188672.0000 - rmse: 39143.1797 - val_loss: 838801152.0000 - val_rmse: 28962.0625\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1337954176.0000 - rmse: 36578.0547 - val_loss: 921190528.0000 - val_rmse: 30351.1211\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1424041472.0000 - rmse: 37736.4727 - val_loss: 828767488.0000 - val_rmse: 28788.3203\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1429862528.0000 - rmse: 37813.5234 - val_loss: 807237568.0000 - val_rmse: 28411.9258\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1409559040.0000 - rmse: 37544.0938 - val_loss: 876493696.0000 - val_rmse: 29605.6367\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1407977344.0000 - rmse: 37523.0234 - val_loss: 823801472.0000 - val_rmse: 28701.9414\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1335207168.0000 - rmse: 36540.4883 - val_loss: 741618304.0000 - val_rmse: 27232.6680\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1315453824.0000 - rmse: 36269.1836 - val_loss: 704568896.0000 - val_rmse: 26543.7148\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1298910720.0000 - rmse: 36040.4023 - val_loss: 717501952.0000 - val_rmse: 26786.2266\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1273555456.0000 - rmse: 35686.9102 - val_loss: 798795072.0000 - val_rmse: 28262.9609\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1234489728.0000 - rmse: 35135.3047 - val_loss: 683390976.0000 - val_rmse: 26141.7480\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1233049088.0000 - rmse: 35114.7969 - val_loss: 695759488.0000 - val_rmse: 26377.2539\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1165031040.0000 - rmse: 34132.5508 - val_loss: 696021888.0000 - val_rmse: 26382.2246\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1115190528.0000 - rmse: 33394.4648 - val_loss: 687566976.0000 - val_rmse: 26221.4980\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1088458240.0000 - rmse: 32991.7852 - val_loss: 667675904.0000 - val_rmse: 25839.4258\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1085604096.0000 - rmse: 32948.5039 - val_loss: 683083264.0000 - val_rmse: 26135.8613\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1084697472.0000 - rmse: 32934.7422 - val_loss: 647236480.0000 - val_rmse: 25440.8398\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1093931904.0000 - rmse: 33074.6406 - val_loss: 729661888.0000 - val_rmse: 27012.2539\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1119984896.0000 - rmse: 33466.1758 - val_loss: 609208320.0000 - val_rmse: 24682.1426\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1103637120.0000 - rmse: 33221.0352 - val_loss: 648015808.0000 - val_rmse: 25456.1543\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1110310656.0000 - rmse: 33321.3242 - val_loss: 628127360.0000 - val_rmse: 25062.4668\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1012949888.0000 - rmse: 31826.8711 - val_loss: 651151168.0000 - val_rmse: 25517.6621\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1028297344.0000 - rmse: 32067.0742 - val_loss: 635427008.0000 - val_rmse: 25207.6777\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1027961280.0000 - rmse: 32061.8320 - val_loss: 543796352.0000 - val_rmse: 23319.4395\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1017642880.0000 - rmse: 31900.5117 - val_loss: 565773632.0000 - val_rmse: 23785.9961\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1043768704.0000 - rmse: 32307.4082 - val_loss: 532698464.0000 - val_rmse: 23080.2598\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 981262656.0000 - rmse: 31325.1113 - val_loss: 646692736.0000 - val_rmse: 25430.1484\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 964547200.0000 - rmse: 31057.1602 - val_loss: 486399264.0000 - val_rmse: 22054.4590\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1009409984.0000 - rmse: 31771.2109 - val_loss: 527700512.0000 - val_rmse: 22971.7305\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 982176512.0000 - rmse: 31339.6934 - val_loss: 505627776.0000 - val_rmse: 22486.1680\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 972892608.0000 - rmse: 31191.2246 - val_loss: 509187328.0000 - val_rmse: 22565.1777\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 993952640.0000 - rmse: 31527.0117 - val_loss: 491707200.0000 - val_rmse: 22174.4707\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 940438656.0000 - rmse: 30666.5684 - val_loss: 557019904.0000 - val_rmse: 23601.2676\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 941921024.0000 - rmse: 30690.7305 - val_loss: 533594016.0000 - val_rmse: 23099.6504\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 965254400.0000 - rmse: 31068.5430 - val_loss: 458743648.0000 - val_rmse: 21418.2988\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 879638144.0000 - rmse: 29658.6914 - val_loss: 508978304.0000 - val_rmse: 22560.5449\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 819880960.0000 - rmse: 28633.5605 - val_loss: 520123712.0000 - val_rmse: 22806.2188\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 895616640.0000 - rmse: 29926.8535 - val_loss: 506497792.0000 - val_rmse: 22505.5039\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 917415232.0000 - rmse: 30288.8613 - val_loss: 526908832.0000 - val_rmse: 22954.4922\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 905547392.0000 - rmse: 30092.3125 - val_loss: 490578944.0000 - val_rmse: 22149.0137\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 868994240.0000 - rmse: 29478.7051 - val_loss: 498968064.0000 - val_rmse: 22337.5898\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 906412672.0000 - rmse: 30106.6855 - val_loss: 494904608.0000 - val_rmse: 22246.4512\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 829203264.0000 - rmse: 28795.8867 - val_loss: 503917280.0000 - val_rmse: 22448.0996\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 833818944.0000 - rmse: 28875.9199 - val_loss: 476605728.0000 - val_rmse: 21831.2988\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 849477056.0000 - rmse: 29145.7871 - val_loss: 467452576.0000 - val_rmse: 21620.6504\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 807276096.0000 - rmse: 28412.6035 - val_loss: 464602208.0000 - val_rmse: 21554.6309\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 835297728.0000 - rmse: 28901.5176 - val_loss: 432651552.0000 - val_rmse: 20800.2754\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 812461632.0000 - rmse: 28503.7109 - val_loss: 462370176.0000 - val_rmse: 21502.7930\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 788974976.0000 - rmse: 28088.6953 - val_loss: 467766976.0000 - val_rmse: 21627.9199\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 879943360.0000 - rmse: 29663.8340 - val_loss: 445960608.0000 - val_rmse: 21117.7773\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 846827136.0000 - rmse: 29100.2910 - val_loss: 520611968.0000 - val_rmse: 22816.9219\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 846528960.0000 - rmse: 29095.1680 - val_loss: 452066208.0000 - val_rmse: 21261.8457\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 792445568.0000 - rmse: 28150.4062 - val_loss: 430732064.0000 - val_rmse: 20754.0840\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 777318080.0000 - rmse: 27880.4219 - val_loss: 475355840.0000 - val_rmse: 21802.6543\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 824853120.0000 - rmse: 28720.2559 - val_loss: 487572832.0000 - val_rmse: 22081.0508\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 792110144.0000 - rmse: 28144.4512 - val_loss: 437184256.0000 - val_rmse: 20908.9492\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 799301952.0000 - rmse: 28271.9258 - val_loss: 473619904.0000 - val_rmse: 21762.8066\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 810568192.0000 - rmse: 28470.4766 - val_loss: 462568096.0000 - val_rmse: 21507.3926\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 793337600.0000 - rmse: 28166.2461 - val_loss: 424338272.0000 - val_rmse: 20599.4707\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 758667136.0000 - rmse: 27543.9121 - val_loss: 490912960.0000 - val_rmse: 22156.5527\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 796597760.0000 - rmse: 28224.0605 - val_loss: 464923872.0000 - val_rmse: 21562.0898\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 726407296.0000 - rmse: 26951.9414 - val_loss: 419491712.0000 - val_rmse: 20481.4941\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 788068992.0000 - rmse: 28072.5664 - val_loss: 407839040.0000 - val_rmse: 20195.0215\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733197632.0000 - rmse: 27077.6172 - val_loss: 452299808.0000 - val_rmse: 21267.3359\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 754872064.0000 - rmse: 27474.9336 - val_loss: 440782304.0000 - val_rmse: 20994.8145\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 750993344.0000 - rmse: 27404.2539 - val_loss: 420632704.0000 - val_rmse: 20509.3281\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706161472.0000 - rmse: 26573.6973 - val_loss: 433651296.0000 - val_rmse: 20824.2910\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 779520064.0000 - rmse: 27919.8828 - val_loss: 396183968.0000 - val_rmse: 19904.3672\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 717915200.0000 - rmse: 26793.9375 - val_loss: 426119328.0000 - val_rmse: 20642.6562\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 740108096.0000 - rmse: 27204.9277 - val_loss: 375167680.0000 - val_rmse: 19369.2422\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 750265728.0000 - rmse: 27390.9785 - val_loss: 369707008.0000 - val_rmse: 19227.7637\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 792827200.0000 - rmse: 28157.1855 - val_loss: 427633248.0000 - val_rmse: 20679.2930\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692700416.0000 - rmse: 26319.2031 - val_loss: 366973984.0000 - val_rmse: 19156.5605\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 747384704.0000 - rmse: 27338.3359 - val_loss: 372734144.0000 - val_rmse: 19306.3223\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 712572928.0000 - rmse: 26694.0586 - val_loss: 450240896.0000 - val_rmse: 21218.8789\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 711011392.0000 - rmse: 26664.7949 - val_loss: 458671776.0000 - val_rmse: 21416.6191\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 674679936.0000 - rmse: 25974.5996 - val_loss: 468172064.0000 - val_rmse: 21637.2812\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706180608.0000 - rmse: 26574.0586 - val_loss: 388509792.0000 - val_rmse: 19710.6484\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730729280.0000 - rmse: 27032.0020 - val_loss: 377199872.0000 - val_rmse: 19421.6309\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704133440.0000 - rmse: 26535.5117 - val_loss: 351940768.0000 - val_rmse: 18760.0801\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 711084928.0000 - rmse: 26666.1758 - val_loss: 361448672.0000 - val_rmse: 19011.8008\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 670687232.0000 - rmse: 25897.6270 - val_loss: 513839360.0000 - val_rmse: 22668.0234\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668475008.0000 - rmse: 25854.8809 - val_loss: 397655552.0000 - val_rmse: 19941.2988\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 709295168.0000 - rmse: 26632.5957 - val_loss: 371784544.0000 - val_rmse: 19281.7109\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651350976.0000 - rmse: 25521.5762 - val_loss: 362899936.0000 - val_rmse: 19049.9297\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704653056.0000 - rmse: 26545.2988 - val_loss: 362619744.0000 - val_rmse: 19042.5742\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721473216.0000 - rmse: 26860.2520 - val_loss: 397831744.0000 - val_rmse: 19945.7148\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 694121536.0000 - rmse: 26346.1855 - val_loss: 368796288.0000 - val_rmse: 19204.0645\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 725350848.0000 - rmse: 26932.3340 - val_loss: 543944512.0000 - val_rmse: 23322.6133\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 683900992.0000 - rmse: 26151.4980 - val_loss: 461705920.0000 - val_rmse: 21487.3418\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 711615296.0000 - rmse: 26676.1191 - val_loss: 355487040.0000 - val_rmse: 18854.3633\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641223040.0000 - rmse: 25322.3789 - val_loss: 361741760.0000 - val_rmse: 19019.5059\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666992512.0000 - rmse: 25826.1953 - val_loss: 343233888.0000 - val_rmse: 18526.5684\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666176128.0000 - rmse: 25810.3867 - val_loss: 343367424.0000 - val_rmse: 18530.1719\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 695258688.0000 - rmse: 26367.7559 - val_loss: 343089632.0000 - val_rmse: 18522.6758\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 689794880.0000 - rmse: 26263.9434 - val_loss: 327800960.0000 - val_rmse: 18105.2715\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 705133120.0000 - rmse: 26554.3398 - val_loss: 330850624.0000 - val_rmse: 18189.2969\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621103360.0000 - rmse: 24921.9434 - val_loss: 338806976.0000 - val_rmse: 18406.7051\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 674666368.0000 - rmse: 25974.3359 - val_loss: 369696256.0000 - val_rmse: 19227.4824\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 669861696.0000 - rmse: 25881.6836 - val_loss: 406655904.0000 - val_rmse: 20165.7070\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642490496.0000 - rmse: 25347.3945 - val_loss: 321528992.0000 - val_rmse: 17931.2266\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623171008.0000 - rmse: 24963.3906 - val_loss: 347306304.0000 - val_rmse: 18636.1523\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 653177984.0000 - rmse: 25557.3438 - val_loss: 335848160.0000 - val_rmse: 18326.1562\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641496960.0000 - rmse: 25327.7891 - val_loss: 341912192.0000 - val_rmse: 18490.8652\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 636842304.0000 - rmse: 25235.7305 - val_loss: 421850432.0000 - val_rmse: 20538.9941\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599269248.0000 - rmse: 24479.9727 - val_loss: 384728800.0000 - val_rmse: 19614.5020\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 735529088.0000 - rmse: 27120.6367 - val_loss: 344086496.0000 - val_rmse: 18549.5645\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647021952.0000 - rmse: 25436.6230 - val_loss: 318619616.0000 - val_rmse: 17849.9160\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610318272.0000 - rmse: 24704.6191 - val_loss: 375895264.0000 - val_rmse: 19388.0137\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 602079168.0000 - rmse: 24537.2988 - val_loss: 343646976.0000 - val_rmse: 18537.7129\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607948608.0000 - rmse: 24656.6094 - val_loss: 340000672.0000 - val_rmse: 18439.1016\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618024448.0000 - rmse: 24860.0938 - val_loss: 351735968.0000 - val_rmse: 18754.6211\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 671060160.0000 - rmse: 25904.8281 - val_loss: 314744384.0000 - val_rmse: 17741.0332\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615889472.0000 - rmse: 24817.1172 - val_loss: 306058656.0000 - val_rmse: 17494.5273\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 667385920.0000 - rmse: 25833.8145 - val_loss: 315632480.0000 - val_rmse: 17766.0449\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 655577216.0000 - rmse: 25604.2402 - val_loss: 353827648.0000 - val_rmse: 18810.3027\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573689408.0000 - rmse: 23951.8125 - val_loss: 326338144.0000 - val_rmse: 18064.8281\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576861824.0000 - rmse: 24017.9473 - val_loss: 335881056.0000 - val_rmse: 18327.0527\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642770624.0000 - rmse: 25352.9199 - val_loss: 362331296.0000 - val_rmse: 19035.0000\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654917184.0000 - rmse: 25591.3477 - val_loss: 308836160.0000 - val_rmse: 17573.7324\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611190336.0000 - rmse: 24722.2637 - val_loss: 300154304.0000 - val_rmse: 17324.9590\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575672384.0000 - rmse: 23993.1719 - val_loss: 354176576.0000 - val_rmse: 18819.5723\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584938752.0000 - rmse: 24185.5039 - val_loss: 320687360.0000 - val_rmse: 17907.7402\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579002624.0000 - rmse: 24062.4688 - val_loss: 316542432.0000 - val_rmse: 17791.6367\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579307136.0000 - rmse: 24068.7969 - val_loss: 316836160.0000 - val_rmse: 17799.8867\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557600256.0000 - rmse: 23613.5566 - val_loss: 393208960.0000 - val_rmse: 19829.4941\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 625209280.0000 - rmse: 25004.1816 - val_loss: 323956832.0000 - val_rmse: 17998.7969\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 595767552.0000 - rmse: 24408.3457 - val_loss: 325047136.0000 - val_rmse: 18029.0586\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601761024.0000 - rmse: 24530.8164 - val_loss: 320722688.0000 - val_rmse: 17908.7266\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598404352.0000 - rmse: 24462.3027 - val_loss: 309820352.0000 - val_rmse: 17601.7109\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575553280.0000 - rmse: 23990.6895 - val_loss: 307851360.0000 - val_rmse: 17545.6875\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576688128.0000 - rmse: 24014.3281 - val_loss: 345010912.0000 - val_rmse: 18574.4648\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577489664.0000 - rmse: 24031.0098 - val_loss: 326591840.0000 - val_rmse: 18071.8496\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560973696.0000 - rmse: 23684.8770 - val_loss: 345236032.0000 - val_rmse: 18580.5254\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560705600.0000 - rmse: 23679.2188 - val_loss: 308718784.0000 - val_rmse: 17570.3887\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607809344.0000 - rmse: 24653.7871 - val_loss: 317528608.0000 - val_rmse: 17819.3262\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597696832.0000 - rmse: 24447.8379 - val_loss: 322263360.0000 - val_rmse: 17951.6895\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526931616.0000 - rmse: 22954.9883 - val_loss: 341513344.0000 - val_rmse: 18480.0781\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580953792.0000 - rmse: 24102.9805 - val_loss: 316985728.0000 - val_rmse: 17804.0879\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546011584.0000 - rmse: 23366.8867 - val_loss: 420263520.0000 - val_rmse: 20500.3262\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573502592.0000 - rmse: 23947.9102 - val_loss: 311178080.0000 - val_rmse: 17640.2363\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538435904.0000 - rmse: 23204.2207 - val_loss: 297968128.0000 - val_rmse: 17261.7480\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531907200.0000 - rmse: 23063.1113 - val_loss: 337973536.0000 - val_rmse: 18384.0508\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550747584.0000 - rmse: 23468.0098 - val_loss: 309397440.0000 - val_rmse: 17589.6934\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583043904.0000 - rmse: 24146.3008 - val_loss: 300800800.0000 - val_rmse: 17343.6055\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538091968.0000 - rmse: 23196.8047 - val_loss: 326806912.0000 - val_rmse: 18077.7949\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503169696.0000 - rmse: 22431.4414 - val_loss: 423767136.0000 - val_rmse: 20585.6035\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543920128.0000 - rmse: 23322.0918 - val_loss: 292696832.0000 - val_rmse: 17108.3789\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580711360.0000 - rmse: 24097.9492 - val_loss: 366923200.0000 - val_rmse: 19155.2344\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544750272.0000 - rmse: 23339.8809 - val_loss: 287437376.0000 - val_rmse: 16953.9746\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541585856.0000 - rmse: 23271.9941 - val_loss: 302838784.0000 - val_rmse: 17402.2578\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563969856.0000 - rmse: 23748.0449 - val_loss: 299615840.0000 - val_rmse: 17309.4102\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 603730304.0000 - rmse: 24570.9199 - val_loss: 316013568.0000 - val_rmse: 17776.7637\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598219968.0000 - rmse: 24458.5332 - val_loss: 299295424.0000 - val_rmse: 17300.1523\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531964704.0000 - rmse: 23064.3535 - val_loss: 377545792.0000 - val_rmse: 19430.5352\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540060224.0000 - rmse: 23239.1914 - val_loss: 310455488.0000 - val_rmse: 17619.7383\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576742336.0000 - rmse: 24015.4590 - val_loss: 315881184.0000 - val_rmse: 17773.0410\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557765376.0000 - rmse: 23617.0547 - val_loss: 372725568.0000 - val_rmse: 19306.0957\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542490560.0000 - rmse: 23291.4238 - val_loss: 316301376.0000 - val_rmse: 17784.8594\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545877504.0000 - rmse: 23364.0195 - val_loss: 309218816.0000 - val_rmse: 17584.6133\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557146496.0000 - rmse: 23603.9453 - val_loss: 296260320.0000 - val_rmse: 17212.2109\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521050752.0000 - rmse: 22826.5312 - val_loss: 304271616.0000 - val_rmse: 17443.3770\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563134464.0000 - rmse: 23730.4512 - val_loss: 298909728.0000 - val_rmse: 17289.0000\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536303072.0000 - rmse: 23158.2129 - val_loss: 321638496.0000 - val_rmse: 17934.2793\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533934336.0000 - rmse: 23107.0137 - val_loss: 399002176.0000 - val_rmse: 19975.0352\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484167872.0000 - rmse: 22003.8105 - val_loss: 297928832.0000 - val_rmse: 17260.6094\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509350176.0000 - rmse: 22568.7852 - val_loss: 317386336.0000 - val_rmse: 17815.3359\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500177760.0000 - rmse: 22364.6504 - val_loss: 296525664.0000 - val_rmse: 17219.9160\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489328480.0000 - rmse: 22120.7676 - val_loss: 441445920.0000 - val_rmse: 21010.6074\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473359552.0000 - rmse: 21756.8223 - val_loss: 437043808.0000 - val_rmse: 20905.5898\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534955360.0000 - rmse: 23129.0996 - val_loss: 330431072.0000 - val_rmse: 18177.7598\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506953760.0000 - rmse: 22515.6309 - val_loss: 324490592.0000 - val_rmse: 18013.6172\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508812224.0000 - rmse: 22556.8613 - val_loss: 321606464.0000 - val_rmse: 17933.3867\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494306240.0000 - rmse: 22232.9941 - val_loss: 316466592.0000 - val_rmse: 17789.5020\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506669824.0000 - rmse: 22509.3242 - val_loss: 313824544.0000 - val_rmse: 17715.0879\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500163552.0000 - rmse: 22364.3340 - val_loss: 290544608.0000 - val_rmse: 17045.3652\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526706912.0000 - rmse: 22950.0938 - val_loss: 370927776.0000 - val_rmse: 19259.4805\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512288640.0000 - rmse: 22633.7891 - val_loss: 313410592.0000 - val_rmse: 17703.4004\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511224224.0000 - rmse: 22610.2637 - val_loss: 374350016.0000 - val_rmse: 19348.1211\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467717728.0000 - rmse: 21626.7773 - val_loss: 333373888.0000 - val_rmse: 18258.5234\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515294112.0000 - rmse: 22700.0879 - val_loss: 322294336.0000 - val_rmse: 17952.5547\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498475744.0000 - rmse: 22326.5684 - val_loss: 335650784.0000 - val_rmse: 18320.7676\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521379264.0000 - rmse: 22833.7266 - val_loss: 402262720.0000 - val_rmse: 20056.4863\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504142176.0000 - rmse: 22453.1094 - val_loss: 306123456.0000 - val_rmse: 17496.3770\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468043456.0000 - rmse: 21634.3066 - val_loss: 534195552.0000 - val_rmse: 23112.6660\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521968256.0000 - rmse: 22846.6211 - val_loss: 290467584.0000 - val_rmse: 17043.1035\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525228704.0000 - rmse: 22917.8652 - val_loss: 326901664.0000 - val_rmse: 18080.4199\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493809952.0000 - rmse: 22221.8301 - val_loss: 308518432.0000 - val_rmse: 17564.6875\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450894432.0000 - rmse: 21234.2715 - val_loss: 304425632.0000 - val_rmse: 17447.7891\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434776160.0000 - rmse: 20851.2812 - val_loss: 303925024.0000 - val_rmse: 17433.4395\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535226080.0000 - rmse: 23134.9531 - val_loss: 292772032.0000 - val_rmse: 17110.5762\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508260288.0000 - rmse: 22544.6230 - val_loss: 325362656.0000 - val_rmse: 18037.8066\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505265664.0000 - rmse: 22478.1094 - val_loss: 323482944.0000 - val_rmse: 17985.6289\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467082240.0000 - rmse: 21612.0820 - val_loss: 327156416.0000 - val_rmse: 18087.4590\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481855872.0000 - rmse: 21951.2129 - val_loss: 345457248.0000 - val_rmse: 18586.4746\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500943264.0000 - rmse: 22381.7578 - val_loss: 353660608.0000 - val_rmse: 18805.8633\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449526880.0000 - rmse: 21202.0449 - val_loss: 289242464.0000 - val_rmse: 17007.1270\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437888576.0000 - rmse: 20925.7812 - val_loss: 313220768.0000 - val_rmse: 17698.0371\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511985568.0000 - rmse: 22627.0938 - val_loss: 484778208.0000 - val_rmse: 22017.6738\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458033632.0000 - rmse: 21401.7148 - val_loss: 302105472.0000 - val_rmse: 17381.1777\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491060704.0000 - rmse: 22159.8848 - val_loss: 364916800.0000 - val_rmse: 19102.7910\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448108096.0000 - rmse: 21168.5586 - val_loss: 313529088.0000 - val_rmse: 17706.7480\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502992256.0000 - rmse: 22427.4844 - val_loss: 334802336.0000 - val_rmse: 18297.6016\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472032384.0000 - rmse: 21726.3027 - val_loss: 298953728.0000 - val_rmse: 17290.2734\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458797920.0000 - rmse: 21419.5625 - val_loss: 308467168.0000 - val_rmse: 17563.2285\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426188128.0000 - rmse: 20644.3164 - val_loss: 378608512.0000 - val_rmse: 19457.8594\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457128960.0000 - rmse: 21380.5684 - val_loss: 337752480.0000 - val_rmse: 18378.0371\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498944128.0000 - rmse: 22337.0488 - val_loss: 303589536.0000 - val_rmse: 17423.8145\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439816192.0000 - rmse: 20971.7910 - val_loss: 318110944.0000 - val_rmse: 17835.6582\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450822784.0000 - rmse: 21232.5840 - val_loss: 336761344.0000 - val_rmse: 18351.0508\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420351008.0000 - rmse: 20502.4609 - val_loss: 298087584.0000 - val_rmse: 17265.2090\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477159552.0000 - rmse: 21843.9805 - val_loss: 311341344.0000 - val_rmse: 17644.8633\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438547712.0000 - rmse: 20941.5254 - val_loss: 331140416.0000 - val_rmse: 18197.2578\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445195520.0000 - rmse: 21099.6523 - val_loss: 309776768.0000 - val_rmse: 17600.4727\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446637568.0000 - rmse: 21133.7969 - val_loss: 311736448.0000 - val_rmse: 17656.0527\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458806656.0000 - rmse: 21419.7695 - val_loss: 335472352.0000 - val_rmse: 18315.8984\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392888160.0000 - rmse: 19821.4023 - val_loss: 322594720.0000 - val_rmse: 17960.9180\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461164736.0000 - rmse: 21474.7402 - val_loss: 302412640.0000 - val_rmse: 17390.0098\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403345440.0000 - rmse: 20083.4590 - val_loss: 329309632.0000 - val_rmse: 18146.8848\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417368352.0000 - rmse: 20429.5918 - val_loss: 308255264.0000 - val_rmse: 17557.1934\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468085920.0000 - rmse: 21635.2891 - val_loss: 320301792.0000 - val_rmse: 17896.9707\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425665888.0000 - rmse: 20631.6641 - val_loss: 322201504.0000 - val_rmse: 17949.9668\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443711968.0000 - rmse: 21064.4688 - val_loss: 309847680.0000 - val_rmse: 17602.4844\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427034912.0000 - rmse: 20664.8145 - val_loss: 339147648.0000 - val_rmse: 18415.9531\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460420096.0000 - rmse: 21457.3984 - val_loss: 312522944.0000 - val_rmse: 17678.3145\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459051232.0000 - rmse: 21425.4766 - val_loss: 296132416.0000 - val_rmse: 17208.4922\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421509984.0000 - rmse: 20530.7012 - val_loss: 305547136.0000 - val_rmse: 17479.9004\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389304448.0000 - rmse: 19730.7910 - val_loss: 298318720.0000 - val_rmse: 17271.9004\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428707904.0000 - rmse: 20705.2578 - val_loss: 332248256.0000 - val_rmse: 18227.6738\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414199904.0000 - rmse: 20351.8965 - val_loss: 347052864.0000 - val_rmse: 18629.3516\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437145920.0000 - rmse: 20908.0293 - val_loss: 316097472.0000 - val_rmse: 17779.1230\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409248288.0000 - rmse: 20229.8809 - val_loss: 353423296.0000 - val_rmse: 18799.5508\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445576992.0000 - rmse: 21108.6895 - val_loss: 330899296.0000 - val_rmse: 18190.6328\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416148160.0000 - rmse: 20399.7070 - val_loss: 303792512.0000 - val_rmse: 17429.6406\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400310816.0000 - rmse: 20007.7617 - val_loss: 391253280.0000 - val_rmse: 19780.1172\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420642336.0000 - rmse: 20509.5645 - val_loss: 347809216.0000 - val_rmse: 18649.6387\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422618688.0000 - rmse: 20557.6895 - val_loss: 376415680.0000 - val_rmse: 19401.4316\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364213920.0000 - rmse: 19084.3848 - val_loss: 331146240.0000 - val_rmse: 18197.4180\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525850848.0000 - rmse: 22931.4297 - val_loss: 307934112.0000 - val_rmse: 17548.0469\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434759744.0000 - rmse: 20850.8867 - val_loss: 303542784.0000 - val_rmse: 17422.4727\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442477920.0000 - rmse: 21035.1543 - val_loss: 476459168.0000 - val_rmse: 21827.9414\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405762784.0000 - rmse: 20143.5508 - val_loss: 341719872.0000 - val_rmse: 18485.6602\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373597600.0000 - rmse: 19328.6680 - val_loss: 320881696.0000 - val_rmse: 17913.1641\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426258496.0000 - rmse: 20646.0234 - val_loss: 371235712.0000 - val_rmse: 19267.4746\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410191648.0000 - rmse: 20253.1836 - val_loss: 338537760.0000 - val_rmse: 18399.3887\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372087840.0000 - rmse: 19289.5723 - val_loss: 307246464.0000 - val_rmse: 17528.4414\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388623712.0000 - rmse: 19713.5371 - val_loss: 294529248.0000 - val_rmse: 17161.8457\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419717888.0000 - rmse: 20487.0137 - val_loss: 317692704.0000 - val_rmse: 17823.9297\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394991360.0000 - rmse: 19874.3828 - val_loss: 322428480.0000 - val_rmse: 17956.2871\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470610560.0000 - rmse: 21693.5508 - val_loss: 369729664.0000 - val_rmse: 19228.3477\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433489824.0000 - rmse: 20820.4121 - val_loss: 339558144.0000 - val_rmse: 18427.0977\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413703392.0000 - rmse: 20339.6934 - val_loss: 328567840.0000 - val_rmse: 18126.4336\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394936256.0000 - rmse: 19873.0000 - val_loss: 309184096.0000 - val_rmse: 17583.6270\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446203168.0000 - rmse: 21123.5176 - val_loss: 303306176.0000 - val_rmse: 17415.6816\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442260768.0000 - rmse: 21029.9922 - val_loss: 362930016.0000 - val_rmse: 19050.7168\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431002912.0000 - rmse: 20760.6055 - val_loss: 383307680.0000 - val_rmse: 19578.2383\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404591328.0000 - rmse: 20114.4512 - val_loss: 478193568.0000 - val_rmse: 21867.6309\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423363360.0000 - rmse: 20575.7910 - val_loss: 356494848.0000 - val_rmse: 18881.0625\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412933664.0000 - rmse: 20320.7637 - val_loss: 517701408.0000 - val_rmse: 22753.0469\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414963328.0000 - rmse: 20370.6406 - val_loss: 320963840.0000 - val_rmse: 17915.4590\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439701120.0000 - rmse: 20969.0469 - val_loss: 330825024.0000 - val_rmse: 18188.5879\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435300768.0000 - rmse: 20863.8555 - val_loss: 355915936.0000 - val_rmse: 18865.7285\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477679168.0000 - rmse: 21855.8691 - val_loss: 348456544.0000 - val_rmse: 18666.9824\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415688736.0000 - rmse: 20388.4414 - val_loss: 315206528.0000 - val_rmse: 17754.0527\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338696416.0000 - rmse: 18403.6992 - val_loss: 476553888.0000 - val_rmse: 21830.1074\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387678336.0000 - rmse: 19689.5449 - val_loss: 364777440.0000 - val_rmse: 19099.1387\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469476256.0000 - rmse: 21667.3926 - val_loss: 325283840.0000 - val_rmse: 18035.6211\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353529888.0000 - rmse: 18802.3867 - val_loss: 335023296.0000 - val_rmse: 18303.6348\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380490304.0000 - rmse: 19506.1523 - val_loss: 305265888.0000 - val_rmse: 17471.8535\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382236128.0000 - rmse: 19550.8516 - val_loss: 380256544.0000 - val_rmse: 19500.1621\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436397952.0000 - rmse: 20890.1348 - val_loss: 297994080.0000 - val_rmse: 17262.4980\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428475936.0000 - rmse: 20699.6562 - val_loss: 309092256.0000 - val_rmse: 17581.0117\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384342912.0000 - rmse: 19604.6602 - val_loss: 322426048.0000 - val_rmse: 17956.2188\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384443104.0000 - rmse: 19607.2109 - val_loss: 325851296.0000 - val_rmse: 18051.3457\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406607680.0000 - rmse: 20164.5098 - val_loss: 399355840.0000 - val_rmse: 19983.8848\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360203136.0000 - rmse: 18979.0098 - val_loss: 714971840.0000 - val_rmse: 26738.9551\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392576320.0000 - rmse: 19813.5332 - val_loss: 347271552.0000 - val_rmse: 18635.2168\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362020032.0000 - rmse: 19026.8164 - val_loss: 325739136.0000 - val_rmse: 18048.2383\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352785568.0000 - rmse: 18782.5801 - val_loss: 301512672.0000 - val_rmse: 17364.1133\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384273920.0000 - rmse: 19602.8984 - val_loss: 314164928.0000 - val_rmse: 17724.6914\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343275648.0000 - rmse: 18527.6914 - val_loss: 326444288.0000 - val_rmse: 18067.7617\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374641888.0000 - rmse: 19355.6641 - val_loss: 310244416.0000 - val_rmse: 17613.7520\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373902624.0000 - rmse: 19336.5566 - val_loss: 347086880.0000 - val_rmse: 18630.2617\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371485152.0000 - rmse: 19273.9473 - val_loss: 316707040.0000 - val_rmse: 17796.2598\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369354720.0000 - rmse: 19218.5977 - val_loss: 330675424.0000 - val_rmse: 18184.4766\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323762048.0000 - rmse: 17993.3828 - val_loss: 318130272.0000 - val_rmse: 17836.1973\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410526112.0000 - rmse: 20261.4355 - val_loss: 305416896.0000 - val_rmse: 17476.1719\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383034272.0000 - rmse: 19571.2559 - val_loss: 331555360.0000 - val_rmse: 18208.6543\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353742304.0000 - rmse: 18808.0312 - val_loss: 313141472.0000 - val_rmse: 17695.7969\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374426848.0000 - rmse: 19350.1055 - val_loss: 445820416.0000 - val_rmse: 21114.4570\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412644064.0000 - rmse: 20313.6348 - val_loss: 371973984.0000 - val_rmse: 19286.6191\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363770976.0000 - rmse: 19072.7734 - val_loss: 367337728.0000 - val_rmse: 19166.0508\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381236576.0000 - rmse: 19525.2734 - val_loss: 317410112.0000 - val_rmse: 17816.0020\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372776160.0000 - rmse: 19307.4062 - val_loss: 403527008.0000 - val_rmse: 20087.9766\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398701088.0000 - rmse: 19967.4961 - val_loss: 347827680.0000 - val_rmse: 18650.1328\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367996704.0000 - rmse: 19183.2324 - val_loss: 309895872.0000 - val_rmse: 17603.8516\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388579872.0000 - rmse: 19712.4238 - val_loss: 441591136.0000 - val_rmse: 21014.0625\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365033760.0000 - rmse: 19105.8496 - val_loss: 341432992.0000 - val_rmse: 18477.8965\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371737344.0000 - rmse: 19280.4863 - val_loss: 311802400.0000 - val_rmse: 17657.9180\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384807872.0000 - rmse: 19616.5156 - val_loss: 427681088.0000 - val_rmse: 20680.4434\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331874400.0000 - rmse: 18217.4121 - val_loss: 426534848.0000 - val_rmse: 20652.7148\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381795712.0000 - rmse: 19539.5859 - val_loss: 306413504.0000 - val_rmse: 17504.6641\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326497280.0000 - rmse: 18069.2266 - val_loss: 317945600.0000 - val_rmse: 17831.0215\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345638112.0000 - rmse: 18591.3359 - val_loss: 446942528.0000 - val_rmse: 21141.0078\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373033888.0000 - rmse: 19314.0801 - val_loss: 468575936.0000 - val_rmse: 21646.6074\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385100256.0000 - rmse: 19623.9648 - val_loss: 305741696.0000 - val_rmse: 17485.4629\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399067104.0000 - rmse: 19976.6543 - val_loss: 344648768.0000 - val_rmse: 18564.7109\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317985056.0000 - rmse: 17832.1289 - val_loss: 340275520.0000 - val_rmse: 18446.5527\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288745856.0000 - rmse: 16992.5176 - val_loss: 328798368.0000 - val_rmse: 18132.7910\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413991584.0000 - rmse: 20346.7754 - val_loss: 362421568.0000 - val_rmse: 19037.3672\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406202336.0000 - rmse: 20154.4551 - val_loss: 340072832.0000 - val_rmse: 18441.0566\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365571648.0000 - rmse: 19119.9199 - val_loss: 362633408.0000 - val_rmse: 19042.9297\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383763488.0000 - rmse: 19589.8750 - val_loss: 309646144.0000 - val_rmse: 17596.7598\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371252736.0000 - rmse: 19267.9121 - val_loss: 473290912.0000 - val_rmse: 21755.2461\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331613632.0000 - rmse: 18210.2520 - val_loss: 327381120.0000 - val_rmse: 18093.6699\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388703808.0000 - rmse: 19715.5684 - val_loss: 311066944.0000 - val_rmse: 17637.0820\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330388064.0000 - rmse: 18176.5723 - val_loss: 316360256.0000 - val_rmse: 17786.5098\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411121024.0000 - rmse: 20276.1094 - val_loss: 605209216.0000 - val_rmse: 24600.9941\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529108224.0000 - rmse: 23002.3398 - val_loss: 323864128.0000 - val_rmse: 17996.2168\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309595360.0000 - rmse: 17595.3145 - val_loss: 329902560.0000 - val_rmse: 18163.2129\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313888160.0000 - rmse: 17716.8828 - val_loss: 392725312.0000 - val_rmse: 19817.2891\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361208896.0000 - rmse: 19005.4902 - val_loss: 337446304.0000 - val_rmse: 18369.7051\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386123328.0000 - rmse: 19650.0156 - val_loss: 362192480.0000 - val_rmse: 19031.3477\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350922752.0000 - rmse: 18732.9238 - val_loss: 326324832.0000 - val_rmse: 18064.4531\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330853824.0000 - rmse: 18189.3809 - val_loss: 424092032.0000 - val_rmse: 20593.4902\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398438304.0000 - rmse: 19960.9121 - val_loss: 390374336.0000 - val_rmse: 19757.8848\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314314272.0000 - rmse: 17728.9043 - val_loss: 356023520.0000 - val_rmse: 18868.5762\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318786880.0000 - rmse: 17854.5938 - val_loss: 604353216.0000 - val_rmse: 24583.5898\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382440096.0000 - rmse: 19556.0645 - val_loss: 367237920.0000 - val_rmse: 19163.4453\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353625504.0000 - rmse: 18804.9258 - val_loss: 312643744.0000 - val_rmse: 17681.7266\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369835648.0000 - rmse: 19231.1016 - val_loss: 300764352.0000 - val_rmse: 17342.5508\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280882976.0000 - rmse: 16759.5547 - val_loss: 332855936.0000 - val_rmse: 18244.3320\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321116512.0000 - rmse: 17919.7148 - val_loss: 341757920.0000 - val_rmse: 18486.6895\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364139232.0000 - rmse: 19082.4238 - val_loss: 331575104.0000 - val_rmse: 18209.1973\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342604160.0000 - rmse: 18509.5625 - val_loss: 316098048.0000 - val_rmse: 17779.1387\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337301536.0000 - rmse: 18365.7637 - val_loss: 317248608.0000 - val_rmse: 17811.4648\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374773920.0000 - rmse: 19359.0684 - val_loss: 323095136.0000 - val_rmse: 17974.8398\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360046752.0000 - rmse: 18974.8906 - val_loss: 416714208.0000 - val_rmse: 20413.5742\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338990304.0000 - rmse: 18411.6816 - val_loss: 320285152.0000 - val_rmse: 17896.5020\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321849280.0000 - rmse: 17940.1484 - val_loss: 290826976.0000 - val_rmse: 17053.6406\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283273888.0000 - rmse: 16830.7305 - val_loss: 295726336.0000 - val_rmse: 17196.6875\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334319328.0000 - rmse: 18284.3945 - val_loss: 299838752.0000 - val_rmse: 17315.8438\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313241216.0000 - rmse: 17698.6113 - val_loss: 352057088.0000 - val_rmse: 18763.1758\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320087584.0000 - rmse: 17890.9824 - val_loss: 329129952.0000 - val_rmse: 18141.9297\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417243552.0000 - rmse: 20426.5332 - val_loss: 291487104.0000 - val_rmse: 17072.9844\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341843648.0000 - rmse: 18489.0039 - val_loss: 397037376.0000 - val_rmse: 19925.7910\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320120064.0000 - rmse: 17891.8906 - val_loss: 389636000.0000 - val_rmse: 19739.1953\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311279552.0000 - rmse: 17643.1035 - val_loss: 323064736.0000 - val_rmse: 17973.9941\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306164000.0000 - rmse: 17497.5332 - val_loss: 328540832.0000 - val_rmse: 18125.6875\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288739936.0000 - rmse: 16992.3418 - val_loss: 324733472.0000 - val_rmse: 18020.3555\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332190336.0000 - rmse: 18226.0801 - val_loss: 344649280.0000 - val_rmse: 18564.7266\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289922336.0000 - rmse: 17027.0957 - val_loss: 321508992.0000 - val_rmse: 17930.6621\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318560512.0000 - rmse: 17848.2578 - val_loss: 302938112.0000 - val_rmse: 17405.1113\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327067456.0000 - rmse: 18084.9980 - val_loss: 295444352.0000 - val_rmse: 17188.4863\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283767712.0000 - rmse: 16845.3965 - val_loss: 321652864.0000 - val_rmse: 17934.6738\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330220416.0000 - rmse: 18171.9590 - val_loss: 308878912.0000 - val_rmse: 17574.9434\n",
      "Epoch 367/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334363456.0000 - rmse: 18285.5957 - val_loss: 311250784.0000 - val_rmse: 17642.2930\n",
      "Epoch 368/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314334048.0000 - rmse: 17729.4609 - val_loss: 365423904.0000 - val_rmse: 19116.0527\n",
      "Epoch 369/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347402464.0000 - rmse: 18638.7266 - val_loss: 453706048.0000 - val_rmse: 21300.3691\n",
      "Epoch 370/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315825280.0000 - rmse: 17771.4648 - val_loss: 353199520.0000 - val_rmse: 18793.5938\n",
      "Epoch 371/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299619008.0000 - rmse: 17309.4961 - val_loss: 322992672.0000 - val_rmse: 17971.9902\n",
      "Epoch 372/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296900736.0000 - rmse: 17230.7988 - val_loss: 359435552.0000 - val_rmse: 18958.7793\n",
      "104/104 [==============================] - 0s 720us/step - loss: 445198464.0000 - rmse: 21099.7148\n",
      "[445198464.0, 21099.71484375]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 18598508544.0000 - rmse: 136376.3438 - val_loss: 3969660672.0000 - val_rmse: 63005.2422\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2788816384.0000 - rmse: 52809.2461 - val_loss: 1466866432.0000 - val_rmse: 38299.6914\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1726855552.0000 - rmse: 41555.4531 - val_loss: 1222195072.0000 - val_rmse: 34959.9062\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1629183232.0000 - rmse: 40363.1406 - val_loss: 1175617536.0000 - val_rmse: 34287.2812\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1500587264.0000 - rmse: 38737.4141 - val_loss: 1187717248.0000 - val_rmse: 34463.2734\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1449925888.0000 - rmse: 38077.8906 - val_loss: 1155467776.0000 - val_rmse: 33992.1719\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1403998848.0000 - rmse: 37469.9727 - val_loss: 1085299456.0000 - val_rmse: 32943.8828\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1368330880.0000 - rmse: 36990.9570 - val_loss: 1056127424.0000 - val_rmse: 32498.1113\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1367168896.0000 - rmse: 36975.2461 - val_loss: 1051718464.0000 - val_rmse: 32430.2090\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1289735040.0000 - rmse: 35912.8789 - val_loss: 1014391040.0000 - val_rmse: 31849.5059\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1277235456.0000 - rmse: 35738.4297 - val_loss: 1081632512.0000 - val_rmse: 32888.1797\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1301992448.0000 - rmse: 36083.1328 - val_loss: 946484608.0000 - val_rmse: 30764.9883\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1241176960.0000 - rmse: 35230.3398 - val_loss: 940944000.0000 - val_rmse: 30674.8105\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1154022144.0000 - rmse: 33970.9023 - val_loss: 918237312.0000 - val_rmse: 30302.4277\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1194987264.0000 - rmse: 34568.5859 - val_loss: 894261184.0000 - val_rmse: 29904.1973\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1143200256.0000 - rmse: 33811.2461 - val_loss: 903028992.0000 - val_rmse: 30050.4414\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1141644672.0000 - rmse: 33788.2344 - val_loss: 876604992.0000 - val_rmse: 29607.5156\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1112131584.0000 - rmse: 33348.6367 - val_loss: 862690048.0000 - val_rmse: 29371.5859\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1076149376.0000 - rmse: 32804.7109 - val_loss: 877309056.0000 - val_rmse: 29619.4023\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1014341248.0000 - rmse: 31848.7246 - val_loss: 874744704.0000 - val_rmse: 29576.0820\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 986941312.0000 - rmse: 31415.6230 - val_loss: 816996480.0000 - val_rmse: 28583.1484\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1048259264.0000 - rmse: 32376.8320 - val_loss: 839964288.0000 - val_rmse: 28982.1367\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1030735808.0000 - rmse: 32105.0723 - val_loss: 791348352.0000 - val_rmse: 28130.9121\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1009732544.0000 - rmse: 31776.2891 - val_loss: 787311488.0000 - val_rmse: 28059.0703\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1028569664.0000 - rmse: 32071.3203 - val_loss: 755104000.0000 - val_rmse: 27479.1543\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 977404800.0000 - rmse: 31263.4707 - val_loss: 749517888.0000 - val_rmse: 27377.3223\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1015070656.0000 - rmse: 31860.1738 - val_loss: 784384192.0000 - val_rmse: 28006.8594\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 957829440.0000 - rmse: 30948.8184 - val_loss: 735374464.0000 - val_rmse: 27117.7891\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 930204352.0000 - rmse: 30499.2500 - val_loss: 709080000.0000 - val_rmse: 26628.5566\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 986274752.0000 - rmse: 31405.0078 - val_loss: 698215744.0000 - val_rmse: 26423.7715\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 943471936.0000 - rmse: 30715.9863 - val_loss: 690794240.0000 - val_rmse: 26282.9609\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 884548608.0000 - rmse: 29741.3613 - val_loss: 681918080.0000 - val_rmse: 26113.5605\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 906086912.0000 - rmse: 30101.2754 - val_loss: 683243904.0000 - val_rmse: 26138.9316\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 921403968.0000 - rmse: 30354.6367 - val_loss: 667093888.0000 - val_rmse: 25828.1602\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 895715456.0000 - rmse: 29928.5039 - val_loss: 672256320.0000 - val_rmse: 25927.9043\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 868066752.0000 - rmse: 29462.9707 - val_loss: 647006080.0000 - val_rmse: 25436.3125\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 883498496.0000 - rmse: 29723.7031 - val_loss: 653778816.0000 - val_rmse: 25569.0977\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 876351936.0000 - rmse: 29603.2383 - val_loss: 639327232.0000 - val_rmse: 25284.9180\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 829540736.0000 - rmse: 28801.7480 - val_loss: 636388800.0000 - val_rmse: 25226.7480\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 830524160.0000 - rmse: 28818.8145 - val_loss: 638744832.0000 - val_rmse: 25273.4004\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 797495936.0000 - rmse: 28239.9668 - val_loss: 616055104.0000 - val_rmse: 24820.4551\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 843107648.0000 - rmse: 29036.3164 - val_loss: 646140288.0000 - val_rmse: 25419.2871\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 809981248.0000 - rmse: 28460.1680 - val_loss: 638508224.0000 - val_rmse: 25268.7188\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 821631424.0000 - rmse: 28664.1133 - val_loss: 620315328.0000 - val_rmse: 24906.1270\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 857783296.0000 - rmse: 29287.9355 - val_loss: 657147264.0000 - val_rmse: 25634.8809\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 824693248.0000 - rmse: 28717.4707 - val_loss: 613673408.0000 - val_rmse: 24772.4297\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 756546176.0000 - rmse: 27505.3828 - val_loss: 604884416.0000 - val_rmse: 24594.3945\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 760372544.0000 - rmse: 27574.8516 - val_loss: 589957760.0000 - val_rmse: 24289.0449\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 769632640.0000 - rmse: 27742.2539 - val_loss: 583787648.0000 - val_rmse: 24161.6973\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 747003456.0000 - rmse: 27331.3633 - val_loss: 577214848.0000 - val_rmse: 24025.2949\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742608448.0000 - rmse: 27250.8418 - val_loss: 595785984.0000 - val_rmse: 24408.7285\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 735701312.0000 - rmse: 27123.8145 - val_loss: 593388736.0000 - val_rmse: 24359.5684\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 690356416.0000 - rmse: 26274.6328 - val_loss: 649688192.0000 - val_rmse: 25488.9824\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 701718400.0000 - rmse: 26489.9648 - val_loss: 634716352.0000 - val_rmse: 25193.5742\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 734452800.0000 - rmse: 27100.7852 - val_loss: 565092736.0000 - val_rmse: 23771.6758\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 746965696.0000 - rmse: 27330.6719 - val_loss: 598230208.0000 - val_rmse: 24458.7422\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718847168.0000 - rmse: 26811.3262 - val_loss: 742778176.0000 - val_rmse: 27253.9531\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698018688.0000 - rmse: 26420.0410 - val_loss: 560963840.0000 - val_rmse: 23684.6719\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 703286528.0000 - rmse: 26519.5469 - val_loss: 564037888.0000 - val_rmse: 23749.4785\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665549504.0000 - rmse: 25798.2441 - val_loss: 569464064.0000 - val_rmse: 23863.4414\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687285440.0000 - rmse: 26216.1289 - val_loss: 549736576.0000 - val_rmse: 23446.4590\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 693491200.0000 - rmse: 26334.2168 - val_loss: 586042880.0000 - val_rmse: 24208.3184\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 710982080.0000 - rmse: 26664.2461 - val_loss: 550810624.0000 - val_rmse: 23469.3516\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 695178944.0000 - rmse: 26366.2441 - val_loss: 575079936.0000 - val_rmse: 23980.8223\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 674071296.0000 - rmse: 25962.8809 - val_loss: 537496448.0000 - val_rmse: 23183.9688\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676092800.0000 - rmse: 26001.7812 - val_loss: 571293120.0000 - val_rmse: 23901.7344\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661807488.0000 - rmse: 25725.6172 - val_loss: 536753376.0000 - val_rmse: 23167.9375\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 673714752.0000 - rmse: 25956.0137 - val_loss: 534411808.0000 - val_rmse: 23117.3457\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 643407040.0000 - rmse: 25365.4688 - val_loss: 544578752.0000 - val_rmse: 23336.2090\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646488640.0000 - rmse: 25426.1406 - val_loss: 531602304.0000 - val_rmse: 23056.5000\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618183360.0000 - rmse: 24863.2891 - val_loss: 612980480.0000 - val_rmse: 24758.4395\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684132864.0000 - rmse: 26155.9316 - val_loss: 509724736.0000 - val_rmse: 22577.0820\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647800192.0000 - rmse: 25451.9160 - val_loss: 542938368.0000 - val_rmse: 23301.0371\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 645118592.0000 - rmse: 25399.1836 - val_loss: 533793856.0000 - val_rmse: 23103.9766\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632135680.0000 - rmse: 25142.3027 - val_loss: 663418112.0000 - val_rmse: 25756.9023\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624654976.0000 - rmse: 24993.0957 - val_loss: 551487616.0000 - val_rmse: 23483.7695\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 643511872.0000 - rmse: 25367.5332 - val_loss: 584566144.0000 - val_rmse: 24177.8008\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 639237632.0000 - rmse: 25283.1484 - val_loss: 513649792.0000 - val_rmse: 22663.8398\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632740032.0000 - rmse: 25154.3184 - val_loss: 527979104.0000 - val_rmse: 22977.7930\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615057856.0000 - rmse: 24800.3574 - val_loss: 558586880.0000 - val_rmse: 23634.4395\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 600921088.0000 - rmse: 24513.6895 - val_loss: 509604288.0000 - val_rmse: 22574.4141\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 595866176.0000 - rmse: 24410.3691 - val_loss: 487591392.0000 - val_rmse: 22081.4688\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616286400.0000 - rmse: 24825.1133 - val_loss: 485074720.0000 - val_rmse: 22024.4082\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566106624.0000 - rmse: 23792.9941 - val_loss: 518639552.0000 - val_rmse: 22773.6562\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587640896.0000 - rmse: 24241.3047 - val_loss: 571142400.0000 - val_rmse: 23898.5840\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 595921344.0000 - rmse: 24411.5000 - val_loss: 476656864.0000 - val_rmse: 21832.4688\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610559680.0000 - rmse: 24709.5039 - val_loss: 479322752.0000 - val_rmse: 21893.4414\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582739648.0000 - rmse: 24139.9980 - val_loss: 497106784.0000 - val_rmse: 22295.8887\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612136256.0000 - rmse: 24741.3867 - val_loss: 477227360.0000 - val_rmse: 21845.5312\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591077120.0000 - rmse: 24312.0762 - val_loss: 489288800.0000 - val_rmse: 22119.8691\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 613668608.0000 - rmse: 24772.3320 - val_loss: 477812832.0000 - val_rmse: 21858.9258\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569037248.0000 - rmse: 23854.4980 - val_loss: 472770048.0000 - val_rmse: 21743.2734\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587993408.0000 - rmse: 24248.5723 - val_loss: 474222304.0000 - val_rmse: 21776.6426\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 595985408.0000 - rmse: 24412.8105 - val_loss: 499714592.0000 - val_rmse: 22354.2949\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597450048.0000 - rmse: 24442.7891 - val_loss: 501508928.0000 - val_rmse: 22394.3926\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562657408.0000 - rmse: 23720.3984 - val_loss: 463932096.0000 - val_rmse: 21539.0781\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570634816.0000 - rmse: 23887.9629 - val_loss: 468384576.0000 - val_rmse: 21642.1914\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519773472.0000 - rmse: 22798.5371 - val_loss: 464112416.0000 - val_rmse: 21543.2656\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607120896.0000 - rmse: 24639.8223 - val_loss: 449554880.0000 - val_rmse: 21202.7051\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570666944.0000 - rmse: 23888.6328 - val_loss: 461214912.0000 - val_rmse: 21475.9121\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574488000.0000 - rmse: 23968.4766 - val_loss: 462321120.0000 - val_rmse: 21501.6523\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554465792.0000 - rmse: 23547.0938 - val_loss: 455543776.0000 - val_rmse: 21343.4668\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578437824.0000 - rmse: 24050.7285 - val_loss: 515371104.0000 - val_rmse: 22701.7852\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543313600.0000 - rmse: 23309.0859 - val_loss: 450755328.0000 - val_rmse: 21230.9961\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510373568.0000 - rmse: 22591.4492 - val_loss: 492823744.0000 - val_rmse: 22199.6309\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543833152.0000 - rmse: 23320.2305 - val_loss: 449329824.0000 - val_rmse: 21197.3984\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547806656.0000 - rmse: 23405.2656 - val_loss: 429155008.0000 - val_rmse: 20716.0547\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553686208.0000 - rmse: 23530.5332 - val_loss: 462508544.0000 - val_rmse: 21506.0098\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530342144.0000 - rmse: 23029.1562 - val_loss: 456157408.0000 - val_rmse: 21357.8418\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508741984.0000 - rmse: 22555.3066 - val_loss: 454849664.0000 - val_rmse: 21327.2031\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517841184.0000 - rmse: 22756.1211 - val_loss: 481129568.0000 - val_rmse: 21934.6621\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483781568.0000 - rmse: 21995.0332 - val_loss: 527103616.0000 - val_rmse: 22958.7363\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552478016.0000 - rmse: 23504.8477 - val_loss: 448008032.0000 - val_rmse: 21166.1973\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544864704.0000 - rmse: 23342.3359 - val_loss: 430456064.0000 - val_rmse: 20747.4316\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535993600.0000 - rmse: 23151.5332 - val_loss: 435961088.0000 - val_rmse: 20879.6777\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518412192.0000 - rmse: 22768.6641 - val_loss: 445453344.0000 - val_rmse: 21105.7637\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484346656.0000 - rmse: 22007.8750 - val_loss: 459571808.0000 - val_rmse: 21437.6230\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523080096.0000 - rmse: 22870.9434 - val_loss: 428542144.0000 - val_rmse: 20701.2578\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541700224.0000 - rmse: 23274.4512 - val_loss: 454965760.0000 - val_rmse: 21329.9238\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507018656.0000 - rmse: 22517.0723 - val_loss: 422785888.0000 - val_rmse: 20561.7559\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521660352.0000 - rmse: 22839.8809 - val_loss: 461558656.0000 - val_rmse: 21483.9141\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484592896.0000 - rmse: 22013.4668 - val_loss: 422783488.0000 - val_rmse: 20561.6973\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500042496.0000 - rmse: 22361.6270 - val_loss: 424348928.0000 - val_rmse: 20599.7285\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505311904.0000 - rmse: 22479.1426 - val_loss: 433241728.0000 - val_rmse: 20814.4570\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503845152.0000 - rmse: 22446.4941 - val_loss: 441114656.0000 - val_rmse: 21002.7266\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507279168.0000 - rmse: 22522.8555 - val_loss: 433018304.0000 - val_rmse: 20809.0879\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493985024.0000 - rmse: 22225.7715 - val_loss: 416801312.0000 - val_rmse: 20415.7090\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503040800.0000 - rmse: 22428.5684 - val_loss: 423785792.0000 - val_rmse: 20586.0547\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484053344.0000 - rmse: 22001.2109 - val_loss: 517756608.0000 - val_rmse: 22754.2656\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526536064.0000 - rmse: 22946.3711 - val_loss: 435989472.0000 - val_rmse: 20880.3594\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487664928.0000 - rmse: 22083.1348 - val_loss: 421169440.0000 - val_rmse: 20522.4102\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508840512.0000 - rmse: 22557.4922 - val_loss: 407474624.0000 - val_rmse: 20185.9961\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512526496.0000 - rmse: 22639.0469 - val_loss: 418676640.0000 - val_rmse: 20461.5879\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496290592.0000 - rmse: 22277.5801 - val_loss: 403983680.0000 - val_rmse: 20099.3438\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520403104.0000 - rmse: 22812.3438 - val_loss: 417837504.0000 - val_rmse: 20441.0723\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508547616.0000 - rmse: 22550.9980 - val_loss: 412734080.0000 - val_rmse: 20315.8535\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523326752.0000 - rmse: 22876.3340 - val_loss: 432628544.0000 - val_rmse: 20799.7227\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487767456.0000 - rmse: 22085.4531 - val_loss: 416500672.0000 - val_rmse: 20408.3457\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475801440.0000 - rmse: 21812.8711 - val_loss: 398794048.0000 - val_rmse: 19969.8262\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487951616.0000 - rmse: 22089.6250 - val_loss: 418296800.0000 - val_rmse: 20452.3027\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484399008.0000 - rmse: 22009.0645 - val_loss: 422021760.0000 - val_rmse: 20543.1641\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462830752.0000 - rmse: 21513.5000 - val_loss: 455152032.0000 - val_rmse: 21334.2910\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459346016.0000 - rmse: 21432.3574 - val_loss: 404268352.0000 - val_rmse: 20106.4199\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490739104.0000 - rmse: 22152.6289 - val_loss: 409525568.0000 - val_rmse: 20236.7344\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525746432.0000 - rmse: 22929.1582 - val_loss: 412552736.0000 - val_rmse: 20311.3906\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496080992.0000 - rmse: 22272.8730 - val_loss: 434887424.0000 - val_rmse: 20853.9531\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480649728.0000 - rmse: 21923.7227 - val_loss: 456275744.0000 - val_rmse: 21360.6113\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469592704.0000 - rmse: 21670.0840 - val_loss: 414233536.0000 - val_rmse: 20352.7246\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484565472.0000 - rmse: 22012.8477 - val_loss: 415029824.0000 - val_rmse: 20372.2793\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515841152.0000 - rmse: 22712.1348 - val_loss: 388548448.0000 - val_rmse: 19711.6289\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476411104.0000 - rmse: 21826.8398 - val_loss: 390612928.0000 - val_rmse: 19763.9277\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457097536.0000 - rmse: 21379.8379 - val_loss: 462932192.0000 - val_rmse: 21515.8574\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485269728.0000 - rmse: 22028.8359 - val_loss: 413823264.0000 - val_rmse: 20342.6445\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481663264.0000 - rmse: 21946.8242 - val_loss: 404150528.0000 - val_rmse: 20103.4922\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455700608.0000 - rmse: 21347.1406 - val_loss: 428209568.0000 - val_rmse: 20693.2207\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446708672.0000 - rmse: 21135.4824 - val_loss: 417243488.0000 - val_rmse: 20426.5371\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468915360.0000 - rmse: 21654.4492 - val_loss: 435584288.0000 - val_rmse: 20870.6543\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472825952.0000 - rmse: 21744.5586 - val_loss: 399983776.0000 - val_rmse: 19999.5918\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437630688.0000 - rmse: 20919.6211 - val_loss: 427704384.0000 - val_rmse: 20681.0137\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412726848.0000 - rmse: 20315.6777 - val_loss: 412406880.0000 - val_rmse: 20307.8008\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485039200.0000 - rmse: 22023.6035 - val_loss: 420292160.0000 - val_rmse: 20501.0254\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445444448.0000 - rmse: 21105.5508 - val_loss: 425319008.0000 - val_rmse: 20623.2598\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427242912.0000 - rmse: 20669.8496 - val_loss: 413201344.0000 - val_rmse: 20327.3516\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448255712.0000 - rmse: 21172.0469 - val_loss: 400920672.0000 - val_rmse: 20023.0020\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444691584.0000 - rmse: 21087.7090 - val_loss: 409662048.0000 - val_rmse: 20240.1074\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427519232.0000 - rmse: 20676.5352 - val_loss: 394823136.0000 - val_rmse: 19870.1543\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394179968.0000 - rmse: 19853.9629 - val_loss: 384949632.0000 - val_rmse: 19620.1309\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432088736.0000 - rmse: 20786.7402 - val_loss: 390332128.0000 - val_rmse: 19756.8203\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487848320.0000 - rmse: 22087.2852 - val_loss: 402109792.0000 - val_rmse: 20052.6699\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457262848.0000 - rmse: 21383.7031 - val_loss: 400915072.0000 - val_rmse: 20022.8613\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437468960.0000 - rmse: 20915.7539 - val_loss: 406972640.0000 - val_rmse: 20173.5605\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472225504.0000 - rmse: 21730.7480 - val_loss: 404094240.0000 - val_rmse: 20102.0938\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421636704.0000 - rmse: 20533.7910 - val_loss: 408551072.0000 - val_rmse: 20212.6445\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439786656.0000 - rmse: 20971.0879 - val_loss: 403658688.0000 - val_rmse: 20091.2559\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454265312.0000 - rmse: 21313.4980 - val_loss: 418646944.0000 - val_rmse: 20460.8613\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424817920.0000 - rmse: 20611.1094 - val_loss: 415103680.0000 - val_rmse: 20374.0898\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436725344.0000 - rmse: 20897.9707 - val_loss: 442426912.0000 - val_rmse: 21033.9434\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398480128.0000 - rmse: 19961.9609 - val_loss: 412825344.0000 - val_rmse: 20318.1016\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419081792.0000 - rmse: 20471.4824 - val_loss: 524548256.0000 - val_rmse: 22903.0156\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412477888.0000 - rmse: 20309.5488 - val_loss: 394965472.0000 - val_rmse: 19873.7363\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392999264.0000 - rmse: 19824.2051 - val_loss: 420049024.0000 - val_rmse: 20495.0938\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442386912.0000 - rmse: 21032.9922 - val_loss: 391519520.0000 - val_rmse: 19786.8477\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425648512.0000 - rmse: 20631.2480 - val_loss: 394140576.0000 - val_rmse: 19852.9707\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397501824.0000 - rmse: 19937.4434 - val_loss: 408376704.0000 - val_rmse: 20208.3301\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414226336.0000 - rmse: 20352.5488 - val_loss: 381737888.0000 - val_rmse: 19538.1113\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445408576.0000 - rmse: 21104.7031 - val_loss: 406522208.0000 - val_rmse: 20162.3926\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413055648.0000 - rmse: 20323.7676 - val_loss: 379118560.0000 - val_rmse: 19470.9629\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392252480.0000 - rmse: 19805.3613 - val_loss: 409600736.0000 - val_rmse: 20238.5898\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387328736.0000 - rmse: 19680.6660 - val_loss: 374015520.0000 - val_rmse: 19339.4785\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401829216.0000 - rmse: 20045.6758 - val_loss: 413455904.0000 - val_rmse: 20333.6133\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387290496.0000 - rmse: 19679.6914 - val_loss: 405884800.0000 - val_rmse: 20146.5801\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400679040.0000 - rmse: 20016.9648 - val_loss: 404527776.0000 - val_rmse: 20112.8711\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411460096.0000 - rmse: 20284.4766 - val_loss: 397436704.0000 - val_rmse: 19935.8125\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367061792.0000 - rmse: 19158.8535 - val_loss: 390439072.0000 - val_rmse: 19759.5273\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378212000.0000 - rmse: 19447.6699 - val_loss: 408142912.0000 - val_rmse: 20202.5430\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383799840.0000 - rmse: 19590.8047 - val_loss: 407595648.0000 - val_rmse: 20188.9941\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378820736.0000 - rmse: 19463.3145 - val_loss: 412724608.0000 - val_rmse: 20315.6211\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415633760.0000 - rmse: 20387.0938 - val_loss: 395366976.0000 - val_rmse: 19883.8340\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402028416.0000 - rmse: 20050.6445 - val_loss: 390664224.0000 - val_rmse: 19765.2266\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347984416.0000 - rmse: 18654.3359 - val_loss: 467027456.0000 - val_rmse: 21610.8164\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383890560.0000 - rmse: 19593.1211 - val_loss: 393232704.0000 - val_rmse: 19830.0918\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387953152.0000 - rmse: 19696.5234 - val_loss: 407317856.0000 - val_rmse: 20182.1133\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403399072.0000 - rmse: 20084.7949 - val_loss: 401536224.0000 - val_rmse: 20038.3652\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383194464.0000 - rmse: 19575.3477 - val_loss: 412212960.0000 - val_rmse: 20303.0234\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365828320.0000 - rmse: 19126.6367 - val_loss: 383558592.0000 - val_rmse: 19584.6484\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375681632.0000 - rmse: 19382.5059 - val_loss: 404836160.0000 - val_rmse: 20120.5391\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409363296.0000 - rmse: 20232.7246 - val_loss: 383129568.0000 - val_rmse: 19573.6914\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368637312.0000 - rmse: 19199.9277 - val_loss: 419304672.0000 - val_rmse: 20476.9277\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401183456.0000 - rmse: 20029.5605 - val_loss: 441899040.0000 - val_rmse: 21021.3926\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453918176.0000 - rmse: 21305.3535 - val_loss: 402494368.0000 - val_rmse: 20062.2598\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370745664.0000 - rmse: 19254.7559 - val_loss: 390088192.0000 - val_rmse: 19750.6465\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379273312.0000 - rmse: 19474.9355 - val_loss: 393016416.0000 - val_rmse: 19824.6387\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408020928.0000 - rmse: 20199.5234 - val_loss: 411016736.0000 - val_rmse: 20273.5449\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375313984.0000 - rmse: 19373.0195 - val_loss: 464985056.0000 - val_rmse: 21563.5078\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383353984.0000 - rmse: 19579.4238 - val_loss: 408050400.0000 - val_rmse: 20200.2559\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348278464.0000 - rmse: 18662.2168 - val_loss: 382700992.0000 - val_rmse: 19562.7422\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381444480.0000 - rmse: 19530.5996 - val_loss: 401166144.0000 - val_rmse: 20029.1289\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367945440.0000 - rmse: 19181.8984 - val_loss: 403851584.0000 - val_rmse: 20096.0566\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429237152.0000 - rmse: 20718.0371 - val_loss: 380120288.0000 - val_rmse: 19496.6719\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380754816.0000 - rmse: 19512.9336 - val_loss: 365814432.0000 - val_rmse: 19126.2734\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349001664.0000 - rmse: 18681.5820 - val_loss: 383644288.0000 - val_rmse: 19586.8359\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343879008.0000 - rmse: 18543.9727 - val_loss: 371124864.0000 - val_rmse: 19264.5977\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363844000.0000 - rmse: 19074.6914 - val_loss: 437852352.0000 - val_rmse: 20924.9199\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347406112.0000 - rmse: 18638.8320 - val_loss: 402876192.0000 - val_rmse: 20071.7734\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384962112.0000 - rmse: 19620.4492 - val_loss: 384354880.0000 - val_rmse: 19604.9668\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433024800.0000 - rmse: 20809.2461 - val_loss: 384519648.0000 - val_rmse: 19609.1699\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329424544.0000 - rmse: 18150.0527 - val_loss: 375879200.0000 - val_rmse: 19387.5996\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391903168.0000 - rmse: 19796.5410 - val_loss: 385616320.0000 - val_rmse: 19637.1113\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390057664.0000 - rmse: 19749.8750 - val_loss: 383683904.0000 - val_rmse: 19587.8477\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320126880.0000 - rmse: 17892.0859 - val_loss: 381578112.0000 - val_rmse: 19534.0234\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401188000.0000 - rmse: 20029.6738 - val_loss: 460297024.0000 - val_rmse: 21454.5312\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343610176.0000 - rmse: 18536.7227 - val_loss: 385087488.0000 - val_rmse: 19623.6426\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377319520.0000 - rmse: 19424.7129 - val_loss: 398367584.0000 - val_rmse: 19959.1445\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347998816.0000 - rmse: 18654.7227 - val_loss: 408787008.0000 - val_rmse: 20218.4785\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370732352.0000 - rmse: 19254.4062 - val_loss: 407081504.0000 - val_rmse: 20176.2578\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350018144.0000 - rmse: 18708.7676 - val_loss: 380661984.0000 - val_rmse: 19510.5566\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351910656.0000 - rmse: 18759.2793 - val_loss: 404593248.0000 - val_rmse: 20114.5000\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363500800.0000 - rmse: 19065.6953 - val_loss: 407352192.0000 - val_rmse: 20182.9648\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361270496.0000 - rmse: 19007.1133 - val_loss: 366339712.0000 - val_rmse: 19140.0000\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362199456.0000 - rmse: 19031.5352 - val_loss: 393745216.0000 - val_rmse: 19843.0117\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343782560.0000 - rmse: 18541.3711 - val_loss: 380143104.0000 - val_rmse: 19497.2539\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350926400.0000 - rmse: 18733.0254 - val_loss: 372086336.0000 - val_rmse: 19289.5352\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333294560.0000 - rmse: 18256.3535 - val_loss: 374455072.0000 - val_rmse: 19350.8398\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360260672.0000 - rmse: 18980.5312 - val_loss: 412402016.0000 - val_rmse: 20307.6797\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329982336.0000 - rmse: 18165.4141 - val_loss: 398679904.0000 - val_rmse: 19966.9688\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316608864.0000 - rmse: 17793.5020 - val_loss: 440798880.0000 - val_rmse: 20995.2090\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311360704.0000 - rmse: 17645.4141 - val_loss: 386843488.0000 - val_rmse: 19668.3340\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313785824.0000 - rmse: 17713.9961 - val_loss: 492027936.0000 - val_rmse: 22181.7012\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336336384.0000 - rmse: 18339.4707 - val_loss: 386514528.0000 - val_rmse: 19659.9688\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323593152.0000 - rmse: 17988.6934 - val_loss: 403588224.0000 - val_rmse: 20089.5000\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327465664.0000 - rmse: 18096.0078 - val_loss: 418716384.0000 - val_rmse: 20462.5566\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307208064.0000 - rmse: 17527.3496 - val_loss: 435139296.0000 - val_rmse: 20859.9902\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317573440.0000 - rmse: 17820.5879 - val_loss: 382243552.0000 - val_rmse: 19551.0469\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347567136.0000 - rmse: 18643.1465 - val_loss: 376519040.0000 - val_rmse: 19404.0938\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301419168.0000 - rmse: 17361.4238 - val_loss: 381325280.0000 - val_rmse: 19527.5469\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331341184.0000 - rmse: 18202.7773 - val_loss: 373176064.0000 - val_rmse: 19317.7617\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358212576.0000 - rmse: 18926.5020 - val_loss: 397896928.0000 - val_rmse: 19947.3496\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341437184.0000 - rmse: 18478.0137 - val_loss: 399552224.0000 - val_rmse: 19988.7988\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350738464.0000 - rmse: 18728.0098 - val_loss: 544259840.0000 - val_rmse: 23329.3730\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344066496.0000 - rmse: 18549.0254 - val_loss: 376248608.0000 - val_rmse: 19397.1250\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316822752.0000 - rmse: 17799.5117 - val_loss: 375424256.0000 - val_rmse: 19375.8633\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305512544.0000 - rmse: 17478.9141 - val_loss: 378553504.0000 - val_rmse: 19456.4492\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321168160.0000 - rmse: 17921.1602 - val_loss: 478935360.0000 - val_rmse: 21884.5879\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305933408.0000 - rmse: 17490.9473 - val_loss: 402428032.0000 - val_rmse: 20060.6055\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324970688.0000 - rmse: 18026.9414 - val_loss: 368770048.0000 - val_rmse: 19203.3828\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315929632.0000 - rmse: 17774.4062 - val_loss: 387735296.0000 - val_rmse: 19690.9922\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381282560.0000 - rmse: 19526.4551 - val_loss: 366489152.0000 - val_rmse: 19143.9023\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321728928.0000 - rmse: 17936.8008 - val_loss: 388713728.0000 - val_rmse: 19715.8203\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316667520.0000 - rmse: 17795.1484 - val_loss: 387032576.0000 - val_rmse: 19673.1387\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314763136.0000 - rmse: 17741.5605 - val_loss: 378094240.0000 - val_rmse: 19444.6406\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335239744.0000 - rmse: 18309.5508 - val_loss: 396355072.0000 - val_rmse: 19908.6641\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300541120.0000 - rmse: 17336.1172 - val_loss: 459215680.0000 - val_rmse: 21429.3145\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317121952.0000 - rmse: 17807.9141 - val_loss: 394562720.0000 - val_rmse: 19863.5996\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350505952.0000 - rmse: 18721.7988 - val_loss: 419017248.0000 - val_rmse: 20469.9062\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298932800.0000 - rmse: 17289.6699 - val_loss: 439117152.0000 - val_rmse: 20955.1191\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333292832.0000 - rmse: 18256.3047 - val_loss: 395452992.0000 - val_rmse: 19885.9980\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317250272.0000 - rmse: 17811.5176 - val_loss: 402970976.0000 - val_rmse: 20074.1309\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294091136.0000 - rmse: 17149.0801 - val_loss: 468922592.0000 - val_rmse: 21654.6172\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316344160.0000 - rmse: 17786.0645 - val_loss: 414912832.0000 - val_rmse: 20369.4062\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265684048.0000 - rmse: 16299.8145 - val_loss: 407237536.0000 - val_rmse: 20180.1250\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296289216.0000 - rmse: 17213.0508 - val_loss: 412433888.0000 - val_rmse: 20308.4648\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309828608.0000 - rmse: 17601.9453 - val_loss: 396558240.0000 - val_rmse: 19913.7676\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334924480.0000 - rmse: 18300.9395 - val_loss: 398472160.0000 - val_rmse: 19961.7637\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309119968.0000 - rmse: 17581.8047 - val_loss: 407010560.0000 - val_rmse: 20174.4980\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336831040.0000 - rmse: 18352.9531 - val_loss: 398810464.0000 - val_rmse: 19970.2363\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332229280.0000 - rmse: 18227.1523 - val_loss: 490322624.0000 - val_rmse: 22143.2285\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314904256.0000 - rmse: 17745.5391 - val_loss: 422637696.0000 - val_rmse: 20558.1504\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342886464.0000 - rmse: 18517.1895 - val_loss: 386622944.0000 - val_rmse: 19662.7266\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330697728.0000 - rmse: 18185.0918 - val_loss: 363804640.0000 - val_rmse: 19073.6602\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316967680.0000 - rmse: 17803.5820 - val_loss: 456389248.0000 - val_rmse: 21363.2637\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304900448.0000 - rmse: 17461.3926 - val_loss: 385028128.0000 - val_rmse: 19622.1289\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294309344.0000 - rmse: 17155.4434 - val_loss: 414691264.0000 - val_rmse: 20363.9668\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324634592.0000 - rmse: 18017.6172 - val_loss: 360203200.0000 - val_rmse: 18979.0176\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269322976.0000 - rmse: 16411.0586 - val_loss: 362266560.0000 - val_rmse: 19033.2988\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278147616.0000 - rmse: 16677.7559 - val_loss: 366911360.0000 - val_rmse: 19154.9258\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305643328.0000 - rmse: 17482.6523 - val_loss: 370335488.0000 - val_rmse: 19244.0977\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265779600.0000 - rmse: 16302.7422 - val_loss: 392505504.0000 - val_rmse: 19811.7461\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299782048.0000 - rmse: 17314.2090 - val_loss: 360019648.0000 - val_rmse: 18974.1777\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280770272.0000 - rmse: 16756.1934 - val_loss: 449934880.0000 - val_rmse: 21211.6660\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314705664.0000 - rmse: 17739.9395 - val_loss: 383886304.0000 - val_rmse: 19593.0117\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316540128.0000 - rmse: 17791.5684 - val_loss: 440040160.0000 - val_rmse: 20977.1309\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287085568.0000 - rmse: 16943.5938 - val_loss: 457205088.0000 - val_rmse: 21382.3516\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305382752.0000 - rmse: 17475.2012 - val_loss: 409173408.0000 - val_rmse: 20228.0312\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273252160.0000 - rmse: 16530.3359 - val_loss: 398707072.0000 - val_rmse: 19967.6484\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284712800.0000 - rmse: 16873.4316 - val_loss: 387401344.0000 - val_rmse: 19682.5098\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317645088.0000 - rmse: 17822.5957 - val_loss: 434271808.0000 - val_rmse: 20839.1855\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266917632.0000 - rmse: 16337.6094 - val_loss: 389267296.0000 - val_rmse: 19729.8555\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260891712.0000 - rmse: 16152.1377 - val_loss: 438587904.0000 - val_rmse: 20942.4844\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293967520.0000 - rmse: 17145.4746 - val_loss: 389671072.0000 - val_rmse: 19740.0840\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295512160.0000 - rmse: 17190.4629 - val_loss: 378197248.0000 - val_rmse: 19447.2910\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295748832.0000 - rmse: 17197.3438 - val_loss: 387015520.0000 - val_rmse: 19672.7051\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264260640.0000 - rmse: 16256.0908 - val_loss: 387038496.0000 - val_rmse: 19673.2891\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273023840.0000 - rmse: 16523.4277 - val_loss: 367149888.0000 - val_rmse: 19161.1504\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242351344.0000 - rmse: 15567.6338 - val_loss: 448165280.0000 - val_rmse: 21169.9102\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276821888.0000 - rmse: 16637.9609 - val_loss: 413049824.0000 - val_rmse: 20323.6250\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296447040.0000 - rmse: 17217.6328 - val_loss: 387072352.0000 - val_rmse: 19674.1523\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247907952.0000 - rmse: 15745.0859 - val_loss: 418574496.0000 - val_rmse: 20459.0879\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286905888.0000 - rmse: 16938.2930 - val_loss: 387816224.0000 - val_rmse: 19693.0469\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248888544.0000 - rmse: 15776.1963 - val_loss: 385761888.0000 - val_rmse: 19640.8203\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300584992.0000 - rmse: 17337.3809 - val_loss: 399865760.0000 - val_rmse: 19996.6387\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266013584.0000 - rmse: 16309.9180 - val_loss: 387085024.0000 - val_rmse: 19674.4746\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239734320.0000 - rmse: 15483.3525 - val_loss: 428537856.0000 - val_rmse: 20701.1523\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251342512.0000 - rmse: 15853.7793 - val_loss: 379215424.0000 - val_rmse: 19473.4512\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255613136.0000 - rmse: 15987.8994 - val_loss: 388927008.0000 - val_rmse: 19721.2285\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242462608.0000 - rmse: 15571.2051 - val_loss: 388882624.0000 - val_rmse: 19720.1035\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253607088.0000 - rmse: 15925.0410 - val_loss: 388970912.0000 - val_rmse: 19722.3418\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259516432.0000 - rmse: 16109.5059 - val_loss: 500653280.0000 - val_rmse: 22375.2793\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258589616.0000 - rmse: 16080.7148 - val_loss: 377730176.0000 - val_rmse: 19435.2773\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277666432.0000 - rmse: 16663.3203 - val_loss: 393765952.0000 - val_rmse: 19843.5312\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265596992.0000 - rmse: 16297.1416 - val_loss: 447897760.0000 - val_rmse: 21163.5898\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250659424.0000 - rmse: 15832.2207 - val_loss: 393262080.0000 - val_rmse: 19830.8340\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248278464.0000 - rmse: 15756.8506 - val_loss: 394756896.0000 - val_rmse: 19868.4844\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238178608.0000 - rmse: 15433.0322 - val_loss: 455541216.0000 - val_rmse: 21343.4082\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251178480.0000 - rmse: 15848.6064 - val_loss: 407491360.0000 - val_rmse: 20186.4121\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239983616.0000 - rmse: 15491.3975 - val_loss: 428561024.0000 - val_rmse: 20701.7129\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266771072.0000 - rmse: 16333.1230 - val_loss: 388778624.0000 - val_rmse: 19717.4629\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243863488.0000 - rmse: 15616.1250 - val_loss: 446206912.0000 - val_rmse: 21123.6074\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248502928.0000 - rmse: 15763.9717 - val_loss: 452151296.0000 - val_rmse: 21263.8457\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245094864.0000 - rmse: 15655.5010 - val_loss: 374637984.0000 - val_rmse: 19355.5625\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234824864.0000 - rmse: 15323.9902 - val_loss: 379377920.0000 - val_rmse: 19477.6211\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255655392.0000 - rmse: 15989.2236 - val_loss: 404203872.0000 - val_rmse: 20104.8184\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279018912.0000 - rmse: 16703.8535 - val_loss: 384985760.0000 - val_rmse: 19621.0488\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237847008.0000 - rmse: 15422.2842 - val_loss: 408450304.0000 - val_rmse: 20210.1504\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270927616.0000 - rmse: 16459.8750 - val_loss: 427478656.0000 - val_rmse: 20675.5547\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261030448.0000 - rmse: 16156.4297 - val_loss: 382478176.0000 - val_rmse: 19557.0469\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236661280.0000 - rmse: 15383.7939 - val_loss: 374080192.0000 - val_rmse: 19341.1465\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 223834672.0000 - rmse: 14961.0986 - val_loss: 381098272.0000 - val_rmse: 19521.7324\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251193072.0000 - rmse: 15849.0674 - val_loss: 388617504.0000 - val_rmse: 19713.3789\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265950320.0000 - rmse: 16307.9756 - val_loss: 385284512.0000 - val_rmse: 19628.6602\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249652928.0000 - rmse: 15800.4043 - val_loss: 362141088.0000 - val_rmse: 19030.0020\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232823696.0000 - rmse: 15258.5547 - val_loss: 384793152.0000 - val_rmse: 19616.1406\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221234096.0000 - rmse: 14873.9365 - val_loss: 392911904.0000 - val_rmse: 19822.0020\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251259744.0000 - rmse: 15851.1680 - val_loss: 404102464.0000 - val_rmse: 20102.2949\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224666912.0000 - rmse: 14988.8896 - val_loss: 377323872.0000 - val_rmse: 19424.8223\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246002224.0000 - rmse: 15684.4512 - val_loss: 373857440.0000 - val_rmse: 19335.3867\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227885088.0000 - rmse: 15095.8584 - val_loss: 390728256.0000 - val_rmse: 19766.8418\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232441472.0000 - rmse: 15246.0254 - val_loss: 341104800.0000 - val_rmse: 18469.0176\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227535808.0000 - rmse: 15084.2842 - val_loss: 360846496.0000 - val_rmse: 18995.9551\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237998752.0000 - rmse: 15427.2041 - val_loss: 375731744.0000 - val_rmse: 19383.7949\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248961632.0000 - rmse: 15778.5146 - val_loss: 361533664.0000 - val_rmse: 19014.0332\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210094368.0000 - rmse: 14494.6250 - val_loss: 404627520.0000 - val_rmse: 20115.3496\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 233862992.0000 - rmse: 15292.5732 - val_loss: 375394688.0000 - val_rmse: 19375.1016\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246752768.0000 - rmse: 15708.3604 - val_loss: 365298528.0000 - val_rmse: 19112.7793\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295550336.0000 - rmse: 17191.5742 - val_loss: 354753120.0000 - val_rmse: 18834.8848\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242416288.0000 - rmse: 15569.7188 - val_loss: 382498272.0000 - val_rmse: 19557.5586\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234352336.0000 - rmse: 15308.5664 - val_loss: 363602240.0000 - val_rmse: 19068.3516\n",
      "Epoch 367/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 209692784.0000 - rmse: 14480.7637 - val_loss: 354652096.0000 - val_rmse: 18832.2031\n",
      "Epoch 368/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264955040.0000 - rmse: 16277.4316 - val_loss: 400819808.0000 - val_rmse: 20020.4805\n",
      "Epoch 369/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239641232.0000 - rmse: 15480.3428 - val_loss: 356448512.0000 - val_rmse: 18879.8398\n",
      "Epoch 370/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242097664.0000 - rmse: 15559.4814 - val_loss: 413254144.0000 - val_rmse: 20328.6484\n",
      "Epoch 371/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261224400.0000 - rmse: 16162.4307 - val_loss: 409209280.0000 - val_rmse: 20228.9180\n",
      "Epoch 372/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272423520.0000 - rmse: 16505.2500 - val_loss: 354166656.0000 - val_rmse: 18819.3125\n",
      "Epoch 373/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244222000.0000 - rmse: 15627.5996 - val_loss: 373571840.0000 - val_rmse: 19328.0020\n",
      "Epoch 374/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235608272.0000 - rmse: 15349.5283 - val_loss: 366261696.0000 - val_rmse: 19137.9609\n",
      "Epoch 375/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234219616.0000 - rmse: 15304.2305 - val_loss: 361242592.0000 - val_rmse: 19006.3770\n",
      "Epoch 376/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220937088.0000 - rmse: 14863.9482 - val_loss: 397701856.0000 - val_rmse: 19942.4590\n",
      "Epoch 377/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 204460160.0000 - rmse: 14298.9482 - val_loss: 362052544.0000 - val_rmse: 19027.6758\n",
      "Epoch 378/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234947648.0000 - rmse: 15327.9961 - val_loss: 391099680.0000 - val_rmse: 19776.2363\n",
      "Epoch 379/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236544816.0000 - rmse: 15380.0049 - val_loss: 362840544.0000 - val_rmse: 19048.3691\n",
      "Epoch 380/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244533728.0000 - rmse: 15637.5703 - val_loss: 351281760.0000 - val_rmse: 18742.5059\n",
      "Epoch 381/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214135104.0000 - rmse: 14633.3496 - val_loss: 336976448.0000 - val_rmse: 18356.9141\n",
      "Epoch 382/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238374656.0000 - rmse: 15439.3799 - val_loss: 387619360.0000 - val_rmse: 19688.0469\n",
      "Epoch 383/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222669296.0000 - rmse: 14922.1025 - val_loss: 363814240.0000 - val_rmse: 19073.9102\n",
      "Epoch 384/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221609296.0000 - rmse: 14886.5439 - val_loss: 351093728.0000 - val_rmse: 18737.4902\n",
      "Epoch 385/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216764400.0000 - rmse: 14722.9150 - val_loss: 368847808.0000 - val_rmse: 19205.4043\n",
      "Epoch 386/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220331136.0000 - rmse: 14843.5498 - val_loss: 365522688.0000 - val_rmse: 19118.6426\n",
      "Epoch 387/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 209391200.0000 - rmse: 14470.3496 - val_loss: 379506304.0000 - val_rmse: 19480.9160\n",
      "Epoch 388/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226390928.0000 - rmse: 15046.2861 - val_loss: 366399296.0000 - val_rmse: 19141.5566\n",
      "Epoch 389/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 194952928.0000 - rmse: 13962.5498 - val_loss: 370460576.0000 - val_rmse: 19247.3496\n",
      "Epoch 390/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228333424.0000 - rmse: 15110.7012 - val_loss: 396176352.0000 - val_rmse: 19904.1738\n",
      "Epoch 391/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226494832.0000 - rmse: 15049.7402 - val_loss: 372570048.0000 - val_rmse: 19302.0684\n",
      "Epoch 392/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 200402144.0000 - rmse: 14156.3418 - val_loss: 359302048.0000 - val_rmse: 18955.2598\n",
      "Epoch 393/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211067456.0000 - rmse: 14528.1562 - val_loss: 383251424.0000 - val_rmse: 19576.8047\n",
      "Epoch 394/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219710256.0000 - rmse: 14822.6182 - val_loss: 396093056.0000 - val_rmse: 19902.0820\n",
      "Epoch 395/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240928512.0000 - rmse: 15521.8662 - val_loss: 383232544.0000 - val_rmse: 19576.3203\n",
      "Epoch 396/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229253376.0000 - rmse: 15141.1084 - val_loss: 405021856.0000 - val_rmse: 20125.1504\n",
      "Epoch 397/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227889168.0000 - rmse: 15095.9922 - val_loss: 402485408.0000 - val_rmse: 20062.0352\n",
      "Epoch 398/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276988672.0000 - rmse: 16642.9727 - val_loss: 388619584.0000 - val_rmse: 19713.4316\n",
      "Epoch 399/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 201228512.0000 - rmse: 14185.5000 - val_loss: 367099552.0000 - val_rmse: 19159.8340\n",
      "Epoch 400/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 204488528.0000 - rmse: 14299.9424 - val_loss: 379485152.0000 - val_rmse: 19480.3750\n",
      "104/104 [==============================] - 0s 653us/step - loss: 1040049472.0000 - rmse: 32249.7969\n",
      "[1040049472.0, 32249.796875]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 17355859968.0000 - rmse: 131741.6406 - val_loss: 3714759936.0000 - val_rmse: 60948.8320\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2588847104.0000 - rmse: 50880.7148 - val_loss: 1537541760.0000 - val_rmse: 39211.5000\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1701489920.0000 - rmse: 41249.1211 - val_loss: 1449561728.0000 - val_rmse: 38073.1094\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1574848896.0000 - rmse: 39684.3672 - val_loss: 1306300672.0000 - val_rmse: 36142.7812\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1490011520.0000 - rmse: 38600.6680 - val_loss: 1248464640.0000 - val_rmse: 35333.6172\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1434422016.0000 - rmse: 37873.7656 - val_loss: 1285989248.0000 - val_rmse: 35860.6914\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1355494784.0000 - rmse: 36817.0430 - val_loss: 1241966336.0000 - val_rmse: 35241.5430\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1385060480.0000 - rmse: 37216.4023 - val_loss: 1103499520.0000 - val_rmse: 33218.9648\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1320330368.0000 - rmse: 36336.3516 - val_loss: 1088353920.0000 - val_rmse: 32990.2070\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1278381184.0000 - rmse: 35754.4531 - val_loss: 1041276928.0000 - val_rmse: 32268.8223\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1195002240.0000 - rmse: 34568.8047 - val_loss: 1073160960.0000 - val_rmse: 32759.1328\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1132361984.0000 - rmse: 33650.5859 - val_loss: 980415680.0000 - val_rmse: 31311.5898\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1148995072.0000 - rmse: 33896.8281 - val_loss: 1048017920.0000 - val_rmse: 32373.1055\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1103816320.0000 - rmse: 33223.7305 - val_loss: 950529792.0000 - val_rmse: 30830.6641\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1055972800.0000 - rmse: 32495.7344 - val_loss: 1074241280.0000 - val_rmse: 32775.6172\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1076201984.0000 - rmse: 32805.5195 - val_loss: 912660992.0000 - val_rmse: 30210.2793\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1050966720.0000 - rmse: 32418.6152 - val_loss: 889794048.0000 - val_rmse: 29829.4160\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1050646464.0000 - rmse: 32413.6777 - val_loss: 916837312.0000 - val_rmse: 30279.3223\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1017705600.0000 - rmse: 31901.4980 - val_loss: 860340416.0000 - val_rmse: 29331.5605\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 950212032.0000 - rmse: 30825.5098 - val_loss: 899682048.0000 - val_rmse: 29994.6992\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 970232768.0000 - rmse: 31148.5566 - val_loss: 851314944.0000 - val_rmse: 29177.3027\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 989453120.0000 - rmse: 31455.5723 - val_loss: 910978944.0000 - val_rmse: 30182.4258\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 902648448.0000 - rmse: 30044.1055 - val_loss: 783248384.0000 - val_rmse: 27986.5742\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 910226496.0000 - rmse: 30169.9609 - val_loss: 793427456.0000 - val_rmse: 28167.8418\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 877224320.0000 - rmse: 29617.9707 - val_loss: 835472448.0000 - val_rmse: 28904.5410\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 900998400.0000 - rmse: 30016.6348 - val_loss: 824492544.0000 - val_rmse: 28713.9766\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 862583168.0000 - rmse: 29369.7637 - val_loss: 804829952.0000 - val_rmse: 28369.5234\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 889072832.0000 - rmse: 29817.3223 - val_loss: 835062144.0000 - val_rmse: 28897.4395\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 827530944.0000 - rmse: 28766.8359 - val_loss: 835430016.0000 - val_rmse: 28903.8047\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 840301312.0000 - rmse: 28987.9492 - val_loss: 729770944.0000 - val_rmse: 27014.2695\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 808341888.0000 - rmse: 28431.3535 - val_loss: 873212992.0000 - val_rmse: 29550.1758\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 870154688.0000 - rmse: 29498.3828 - val_loss: 674498176.0000 - val_rmse: 25971.0996\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 849934144.0000 - rmse: 29153.6309 - val_loss: 662059328.0000 - val_rmse: 25730.5137\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 854964352.0000 - rmse: 29239.7715 - val_loss: 702058240.0000 - val_rmse: 26496.3809\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 797091072.0000 - rmse: 28232.7988 - val_loss: 797150464.0000 - val_rmse: 28233.8516\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 810373504.0000 - rmse: 28467.0605 - val_loss: 622473600.0000 - val_rmse: 24949.4180\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733848832.0000 - rmse: 27089.6406 - val_loss: 626765440.0000 - val_rmse: 25035.2832\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 750620736.0000 - rmse: 27397.4570 - val_loss: 625240704.0000 - val_rmse: 25004.8105\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 829751360.0000 - rmse: 28805.4023 - val_loss: 618750272.0000 - val_rmse: 24874.6914\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 785548224.0000 - rmse: 28027.6328 - val_loss: 619503232.0000 - val_rmse: 24889.8203\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 772153280.0000 - rmse: 27787.6465 - val_loss: 599068672.0000 - val_rmse: 24475.8770\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 748545408.0000 - rmse: 27359.5527 - val_loss: 590475392.0000 - val_rmse: 24299.6973\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 714655232.0000 - rmse: 26733.0332 - val_loss: 568950592.0000 - val_rmse: 23852.6816\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 780806528.0000 - rmse: 27942.9160 - val_loss: 606711232.0000 - val_rmse: 24631.5059\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 722244864.0000 - rmse: 26874.6133 - val_loss: 578463744.0000 - val_rmse: 24051.2715\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724088768.0000 - rmse: 26908.8984 - val_loss: 574621248.0000 - val_rmse: 23971.2559\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721050112.0000 - rmse: 26852.3770 - val_loss: 633787968.0000 - val_rmse: 25175.1426\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704461248.0000 - rmse: 26541.6855 - val_loss: 540573056.0000 - val_rmse: 23250.2246\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 695037696.0000 - rmse: 26363.5664 - val_loss: 578242368.0000 - val_rmse: 24046.6680\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 716798720.0000 - rmse: 26773.0918 - val_loss: 557073408.0000 - val_rmse: 23602.4004\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 701623872.0000 - rmse: 26488.1797 - val_loss: 608050944.0000 - val_rmse: 24658.6855\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 716725568.0000 - rmse: 26771.7305 - val_loss: 549043904.0000 - val_rmse: 23431.6816\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 716931136.0000 - rmse: 26775.5684 - val_loss: 592150016.0000 - val_rmse: 24334.1289\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 731884416.0000 - rmse: 27053.3594 - val_loss: 513487264.0000 - val_rmse: 22660.2559\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 697895872.0000 - rmse: 26417.7168 - val_loss: 768274752.0000 - val_rmse: 27717.7676\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 700316416.0000 - rmse: 26463.4902 - val_loss: 624856512.0000 - val_rmse: 24997.1270\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718908800.0000 - rmse: 26812.4727 - val_loss: 529793312.0000 - val_rmse: 23017.2363\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 683932480.0000 - rmse: 26152.1016 - val_loss: 517027168.0000 - val_rmse: 22738.2305\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685186048.0000 - rmse: 26176.0566 - val_loss: 681721728.0000 - val_rmse: 26109.7988\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656314432.0000 - rmse: 25618.6348 - val_loss: 540273728.0000 - val_rmse: 23243.7871\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 703370560.0000 - rmse: 26521.1309 - val_loss: 519446368.0000 - val_rmse: 22791.3633\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687866432.0000 - rmse: 26227.2051 - val_loss: 558273472.0000 - val_rmse: 23627.8066\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 713480640.0000 - rmse: 26711.0566 - val_loss: 551435840.0000 - val_rmse: 23482.6719\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 653570752.0000 - rmse: 25565.0254 - val_loss: 662394560.0000 - val_rmse: 25737.0254\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649250688.0000 - rmse: 25480.3965 - val_loss: 512633312.0000 - val_rmse: 22641.4062\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648489664.0000 - rmse: 25465.4570 - val_loss: 490506720.0000 - val_rmse: 22147.3828\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 643340416.0000 - rmse: 25364.1543 - val_loss: 470921600.0000 - val_rmse: 21700.7246\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 644127168.0000 - rmse: 25379.6602 - val_loss: 496496192.0000 - val_rmse: 22282.1914\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676831424.0000 - rmse: 26015.9824 - val_loss: 483325472.0000 - val_rmse: 21984.6621\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676156416.0000 - rmse: 26003.0078 - val_loss: 453541696.0000 - val_rmse: 21296.5156\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649194112.0000 - rmse: 25479.2871 - val_loss: 463613952.0000 - val_rmse: 21531.6934\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676234304.0000 - rmse: 26004.5039 - val_loss: 534673728.0000 - val_rmse: 23123.0117\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 672008320.0000 - rmse: 25923.1191 - val_loss: 451393888.0000 - val_rmse: 21246.0293\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646547072.0000 - rmse: 25427.2871 - val_loss: 470178304.0000 - val_rmse: 21683.5938\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623633728.0000 - rmse: 24972.6562 - val_loss: 466684832.0000 - val_rmse: 21602.8848\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623290048.0000 - rmse: 24965.7754 - val_loss: 482814048.0000 - val_rmse: 21973.0273\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 645200320.0000 - rmse: 25400.7910 - val_loss: 500001920.0000 - val_rmse: 22360.7207\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621831744.0000 - rmse: 24936.5527 - val_loss: 481113280.0000 - val_rmse: 21934.2930\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604390400.0000 - rmse: 24584.3535 - val_loss: 462813312.0000 - val_rmse: 21513.0938\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646099520.0000 - rmse: 25418.4844 - val_loss: 459410624.0000 - val_rmse: 21433.8633\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628754752.0000 - rmse: 25074.9805 - val_loss: 477141984.0000 - val_rmse: 21843.5762\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620780224.0000 - rmse: 24915.4590 - val_loss: 500995776.0000 - val_rmse: 22382.9336\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580997888.0000 - rmse: 24103.8945 - val_loss: 470507520.0000 - val_rmse: 21691.1816\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583317056.0000 - rmse: 24151.9570 - val_loss: 499028384.0000 - val_rmse: 22338.9395\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616685632.0000 - rmse: 24833.1543 - val_loss: 530646400.0000 - val_rmse: 23035.7598\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584291072.0000 - rmse: 24172.1113 - val_loss: 510359968.0000 - val_rmse: 22591.1465\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628491008.0000 - rmse: 25069.7207 - val_loss: 510603392.0000 - val_rmse: 22596.5352\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637240832.0000 - rmse: 25243.6289 - val_loss: 443605216.0000 - val_rmse: 21061.9355\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 613826560.0000 - rmse: 24775.5215 - val_loss: 437048096.0000 - val_rmse: 20905.6914\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640099392.0000 - rmse: 25300.1836 - val_loss: 468931296.0000 - val_rmse: 21654.8184\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585606784.0000 - rmse: 24199.3105 - val_loss: 433567968.0000 - val_rmse: 20822.2930\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607181888.0000 - rmse: 24641.0605 - val_loss: 457151136.0000 - val_rmse: 21381.0898\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 608722368.0000 - rmse: 24672.2949 - val_loss: 420689088.0000 - val_rmse: 20510.7031\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573106560.0000 - rmse: 23939.6406 - val_loss: 415586368.0000 - val_rmse: 20385.9316\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601823168.0000 - rmse: 24532.0820 - val_loss: 445304096.0000 - val_rmse: 21102.2266\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591823488.0000 - rmse: 24327.4180 - val_loss: 558041728.0000 - val_rmse: 23622.9043\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620765888.0000 - rmse: 24915.1699 - val_loss: 451658464.0000 - val_rmse: 21252.2559\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561771648.0000 - rmse: 23701.7188 - val_loss: 440587008.0000 - val_rmse: 20990.1621\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585064832.0000 - rmse: 24188.1113 - val_loss: 443378944.0000 - val_rmse: 21056.5625\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584574656.0000 - rmse: 24177.9766 - val_loss: 420514976.0000 - val_rmse: 20506.4570\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536594848.0000 - rmse: 23164.5137 - val_loss: 404259264.0000 - val_rmse: 20106.1973\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572957696.0000 - rmse: 23936.5312 - val_loss: 566217408.0000 - val_rmse: 23795.3223\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564132224.0000 - rmse: 23751.4648 - val_loss: 427173184.0000 - val_rmse: 20668.1641\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560830144.0000 - rmse: 23681.8496 - val_loss: 394978592.0000 - val_rmse: 19874.0645\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580653952.0000 - rmse: 24096.7598 - val_loss: 394741920.0000 - val_rmse: 19868.1113\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547361728.0000 - rmse: 23395.7598 - val_loss: 393058144.0000 - val_rmse: 19825.6914\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559634496.0000 - rmse: 23656.5938 - val_loss: 436366784.0000 - val_rmse: 20889.3887\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583871104.0000 - rmse: 24163.4219 - val_loss: 442505344.0000 - val_rmse: 21035.8066\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540043328.0000 - rmse: 23238.8301 - val_loss: 508418752.0000 - val_rmse: 22548.1406\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574366656.0000 - rmse: 23965.9434 - val_loss: 382146944.0000 - val_rmse: 19548.5742\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 581787264.0000 - rmse: 24120.2656 - val_loss: 425183456.0000 - val_rmse: 20619.9746\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536273696.0000 - rmse: 23157.5781 - val_loss: 401948768.0000 - val_rmse: 20048.6562\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593855488.0000 - rmse: 24369.1484 - val_loss: 376131232.0000 - val_rmse: 19394.0996\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521386976.0000 - rmse: 22833.8965 - val_loss: 379095904.0000 - val_rmse: 19470.3809\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579237568.0000 - rmse: 24067.3516 - val_loss: 552471808.0000 - val_rmse: 23504.7168\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546845632.0000 - rmse: 23384.7266 - val_loss: 449153632.0000 - val_rmse: 21193.2422\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523408064.0000 - rmse: 22878.1074 - val_loss: 381831936.0000 - val_rmse: 19540.5176\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579318912.0000 - rmse: 24069.0391 - val_loss: 424989920.0000 - val_rmse: 20615.2812\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550489280.0000 - rmse: 23462.5020 - val_loss: 433095008.0000 - val_rmse: 20810.9297\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508815872.0000 - rmse: 22556.9434 - val_loss: 380807232.0000 - val_rmse: 19514.2812\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577134080.0000 - rmse: 24023.6133 - val_loss: 374730912.0000 - val_rmse: 19357.9648\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527187648.0000 - rmse: 22960.5645 - val_loss: 432894048.0000 - val_rmse: 20806.1035\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569706112.0000 - rmse: 23868.5156 - val_loss: 384710080.0000 - val_rmse: 19614.0234\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574368000.0000 - rmse: 23965.9688 - val_loss: 436241664.0000 - val_rmse: 20886.3965\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567851136.0000 - rmse: 23829.6230 - val_loss: 365346688.0000 - val_rmse: 19114.0410\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513713888.0000 - rmse: 22665.2559 - val_loss: 410521664.0000 - val_rmse: 20261.3281\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533641632.0000 - rmse: 23100.6836 - val_loss: 422860192.0000 - val_rmse: 20563.5605\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522945216.0000 - rmse: 22867.9922 - val_loss: 436337888.0000 - val_rmse: 20888.6992\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554545856.0000 - rmse: 23548.7969 - val_loss: 360242304.0000 - val_rmse: 18980.0469\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516867168.0000 - rmse: 22734.7109 - val_loss: 494985376.0000 - val_rmse: 22248.2656\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494115520.0000 - rmse: 22228.7070 - val_loss: 458165152.0000 - val_rmse: 21404.7891\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477211328.0000 - rmse: 21845.1641 - val_loss: 472590528.0000 - val_rmse: 21739.1445\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535156096.0000 - rmse: 23133.4395 - val_loss: 444357376.0000 - val_rmse: 21079.7812\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511727648.0000 - rmse: 22621.3965 - val_loss: 443959488.0000 - val_rmse: 21070.3438\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529853568.0000 - rmse: 23018.5469 - val_loss: 444228192.0000 - val_rmse: 21076.7207\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532214976.0000 - rmse: 23069.7852 - val_loss: 448087680.0000 - val_rmse: 21168.0781\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485623744.0000 - rmse: 22036.8711 - val_loss: 454601376.0000 - val_rmse: 21321.3809\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511508640.0000 - rmse: 22616.5547 - val_loss: 389310656.0000 - val_rmse: 19730.9531\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515505792.0000 - rmse: 22704.7480 - val_loss: 386360864.0000 - val_rmse: 19656.0605\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522047232.0000 - rmse: 22848.3457 - val_loss: 367771328.0000 - val_rmse: 19177.3613\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509898816.0000 - rmse: 22580.9375 - val_loss: 389230944.0000 - val_rmse: 19728.9336\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497259648.0000 - rmse: 22299.3164 - val_loss: 437997152.0000 - val_rmse: 20928.3809\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509914464.0000 - rmse: 22581.2832 - val_loss: 365208096.0000 - val_rmse: 19110.4160\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510656704.0000 - rmse: 22597.7090 - val_loss: 354772096.0000 - val_rmse: 18835.3906\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516568736.0000 - rmse: 22728.1426 - val_loss: 399465536.0000 - val_rmse: 19986.6309\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488516512.0000 - rmse: 22102.4062 - val_loss: 366217664.0000 - val_rmse: 19136.8105\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490252544.0000 - rmse: 22141.6445 - val_loss: 440150400.0000 - val_rmse: 20979.7578\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511792960.0000 - rmse: 22622.8359 - val_loss: 346628256.0000 - val_rmse: 18617.9531\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557385408.0000 - rmse: 23609.0078 - val_loss: 468238240.0000 - val_rmse: 21638.8125\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492120256.0000 - rmse: 22183.7793 - val_loss: 372532352.0000 - val_rmse: 19301.0938\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506410880.0000 - rmse: 22503.5723 - val_loss: 357829536.0000 - val_rmse: 18916.3789\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496511296.0000 - rmse: 22282.5312 - val_loss: 412422784.0000 - val_rmse: 20308.1914\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471452960.0000 - rmse: 21712.9648 - val_loss: 386819296.0000 - val_rmse: 19667.7168\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491842752.0000 - rmse: 22177.5254 - val_loss: 340463040.0000 - val_rmse: 18451.6387\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481648416.0000 - rmse: 21946.4863 - val_loss: 365049248.0000 - val_rmse: 19106.2578\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533969984.0000 - rmse: 23107.7891 - val_loss: 364667776.0000 - val_rmse: 19096.2715\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544138432.0000 - rmse: 23326.7734 - val_loss: 340805152.0000 - val_rmse: 18460.9043\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535720704.0000 - rmse: 23145.6387 - val_loss: 375369856.0000 - val_rmse: 19374.4590\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463959616.0000 - rmse: 21539.7207 - val_loss: 379669760.0000 - val_rmse: 19485.1133\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509288704.0000 - rmse: 22567.4219 - val_loss: 345576608.0000 - val_rmse: 18589.6855\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476957856.0000 - rmse: 21839.3613 - val_loss: 348055840.0000 - val_rmse: 18656.2500\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511001216.0000 - rmse: 22605.3320 - val_loss: 373340256.0000 - val_rmse: 19322.0098\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483211488.0000 - rmse: 21982.0684 - val_loss: 357107552.0000 - val_rmse: 18897.2871\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489036928.0000 - rmse: 22114.1777 - val_loss: 386511552.0000 - val_rmse: 19659.8926\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507856832.0000 - rmse: 22535.6719 - val_loss: 341556512.0000 - val_rmse: 18481.2441\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498566368.0000 - rmse: 22328.5977 - val_loss: 328626816.0000 - val_rmse: 18128.0645\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461167296.0000 - rmse: 21474.8047 - val_loss: 350410112.0000 - val_rmse: 18719.2402\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507389792.0000 - rmse: 22525.3105 - val_loss: 365340640.0000 - val_rmse: 19113.8828\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446179968.0000 - rmse: 21122.9688 - val_loss: 330952864.0000 - val_rmse: 18192.1035\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461051680.0000 - rmse: 21472.1113 - val_loss: 360648128.0000 - val_rmse: 18990.7344\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439552672.0000 - rmse: 20965.5098 - val_loss: 318980512.0000 - val_rmse: 17860.0215\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488918464.0000 - rmse: 22111.4961 - val_loss: 319759072.0000 - val_rmse: 17881.8047\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450934784.0000 - rmse: 21235.2207 - val_loss: 349800000.0000 - val_rmse: 18702.9375\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459789696.0000 - rmse: 21442.7012 - val_loss: 473188224.0000 - val_rmse: 21752.8887\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471569888.0000 - rmse: 21715.6602 - val_loss: 317842656.0000 - val_rmse: 17828.1367\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473322240.0000 - rmse: 21755.9668 - val_loss: 317358368.0000 - val_rmse: 17814.5508\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490176352.0000 - rmse: 22139.9238 - val_loss: 366652544.0000 - val_rmse: 19148.1699\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469793344.0000 - rmse: 21674.7129 - val_loss: 341963680.0000 - val_rmse: 18492.2539\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436831904.0000 - rmse: 20900.5195 - val_loss: 313316576.0000 - val_rmse: 17700.7480\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460028608.0000 - rmse: 21448.2715 - val_loss: 375886688.0000 - val_rmse: 19387.7949\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480215104.0000 - rmse: 21913.8066 - val_loss: 326712128.0000 - val_rmse: 18075.1777\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462618784.0000 - rmse: 21508.5703 - val_loss: 395477088.0000 - val_rmse: 19886.6016\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485133760.0000 - rmse: 22025.7500 - val_loss: 355305952.0000 - val_rmse: 18849.5566\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489933440.0000 - rmse: 22134.4375 - val_loss: 325668288.0000 - val_rmse: 18046.2773\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431914688.0000 - rmse: 20782.5527 - val_loss: 337704992.0000 - val_rmse: 18376.7461\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484636608.0000 - rmse: 22014.4590 - val_loss: 325707040.0000 - val_rmse: 18047.3496\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403206208.0000 - rmse: 20079.9922 - val_loss: 364540064.0000 - val_rmse: 19092.9277\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484049632.0000 - rmse: 22001.1250 - val_loss: 409865632.0000 - val_rmse: 20245.1328\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460817504.0000 - rmse: 21466.6523 - val_loss: 371449152.0000 - val_rmse: 19273.0137\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443478656.0000 - rmse: 21058.9277 - val_loss: 394716448.0000 - val_rmse: 19867.4668\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448503040.0000 - rmse: 21177.8867 - val_loss: 354046656.0000 - val_rmse: 18816.1230\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443274400.0000 - rmse: 21054.0781 - val_loss: 346040384.0000 - val_rmse: 18602.1562\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471703584.0000 - rmse: 21718.7344 - val_loss: 344315968.0000 - val_rmse: 18555.7480\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487073920.0000 - rmse: 22069.7461 - val_loss: 357946880.0000 - val_rmse: 18919.4805\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415973888.0000 - rmse: 20395.4316 - val_loss: 441248224.0000 - val_rmse: 21005.9062\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438793696.0000 - rmse: 20947.3984 - val_loss: 385894048.0000 - val_rmse: 19644.1836\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543000576.0000 - rmse: 23302.3691 - val_loss: 427633088.0000 - val_rmse: 20679.2871\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463773344.0000 - rmse: 21535.3945 - val_loss: 430943520.0000 - val_rmse: 20759.1738\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515818432.0000 - rmse: 22711.6309 - val_loss: 383005408.0000 - val_rmse: 19570.5195\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450453312.0000 - rmse: 21223.8809 - val_loss: 506540768.0000 - val_rmse: 22506.4551\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416640352.0000 - rmse: 20411.7656 - val_loss: 377501920.0000 - val_rmse: 19429.4023\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413977184.0000 - rmse: 20346.4238 - val_loss: 337083136.0000 - val_rmse: 18359.8184\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486965376.0000 - rmse: 22067.2871 - val_loss: 412264352.0000 - val_rmse: 20304.2891\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434854976.0000 - rmse: 20853.1719 - val_loss: 340735136.0000 - val_rmse: 18459.0078\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450026176.0000 - rmse: 21213.8145 - val_loss: 382861216.0000 - val_rmse: 19566.8340\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510620064.0000 - rmse: 22596.9004 - val_loss: 346460448.0000 - val_rmse: 18613.4453\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437105952.0000 - rmse: 20907.0762 - val_loss: 352517184.0000 - val_rmse: 18775.4375\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 703955072.0000 - rmse: 26532.1484 - val_loss: 311780384.0000 - val_rmse: 17657.2988\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446630016.0000 - rmse: 21133.6191 - val_loss: 317705312.0000 - val_rmse: 17824.2852\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421679232.0000 - rmse: 20534.8242 - val_loss: 414426816.0000 - val_rmse: 20357.4707\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468463712.0000 - rmse: 21644.0195 - val_loss: 352328864.0000 - val_rmse: 18770.4180\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451500608.0000 - rmse: 21248.5391 - val_loss: 370874752.0000 - val_rmse: 19258.1035\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549743104.0000 - rmse: 23446.5977 - val_loss: 346074496.0000 - val_rmse: 18603.0742\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396546592.0000 - rmse: 19913.4727 - val_loss: 337201568.0000 - val_rmse: 18363.0430\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466362336.0000 - rmse: 21595.4180 - val_loss: 296170368.0000 - val_rmse: 17209.5938\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438111456.0000 - rmse: 20931.1094 - val_loss: 367702272.0000 - val_rmse: 19175.5605\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459495104.0000 - rmse: 21435.8320 - val_loss: 316167776.0000 - val_rmse: 17781.0996\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457938368.0000 - rmse: 21399.4883 - val_loss: 347074816.0000 - val_rmse: 18629.9375\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405738816.0000 - rmse: 20142.9551 - val_loss: 350148032.0000 - val_rmse: 18712.2383\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431870432.0000 - rmse: 20781.4863 - val_loss: 329429568.0000 - val_rmse: 18150.1895\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445232064.0000 - rmse: 21100.5156 - val_loss: 348543360.0000 - val_rmse: 18669.3086\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422226656.0000 - rmse: 20548.1523 - val_loss: 345524064.0000 - val_rmse: 18588.2715\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460292896.0000 - rmse: 21454.4355 - val_loss: 373324416.0000 - val_rmse: 19321.6016\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410253024.0000 - rmse: 20254.6992 - val_loss: 342969696.0000 - val_rmse: 18519.4355\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444917568.0000 - rmse: 21093.0664 - val_loss: 308825824.0000 - val_rmse: 17573.4355\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467248768.0000 - rmse: 21615.9355 - val_loss: 437768000.0000 - val_rmse: 20922.9023\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400048480.0000 - rmse: 20001.2070 - val_loss: 322618720.0000 - val_rmse: 17961.5840\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428704128.0000 - rmse: 20705.1680 - val_loss: 347408992.0000 - val_rmse: 18638.9062\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433896384.0000 - rmse: 20830.1777 - val_loss: 326922656.0000 - val_rmse: 18080.9980\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442095776.0000 - rmse: 21026.0703 - val_loss: 365628736.0000 - val_rmse: 19121.4141\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458564000.0000 - rmse: 21414.0996 - val_loss: 322411616.0000 - val_rmse: 17955.8184\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437402912.0000 - rmse: 20914.1758 - val_loss: 315617440.0000 - val_rmse: 17765.6191\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449655968.0000 - rmse: 21205.0879 - val_loss: 350496704.0000 - val_rmse: 18721.5527\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419966688.0000 - rmse: 20493.0840 - val_loss: 380976928.0000 - val_rmse: 19518.6270\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482720736.0000 - rmse: 21970.9004 - val_loss: 407797472.0000 - val_rmse: 20193.9883\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459635776.0000 - rmse: 21439.1133 - val_loss: 322795040.0000 - val_rmse: 17966.4941\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403417120.0000 - rmse: 20085.2422 - val_loss: 346895808.0000 - val_rmse: 18625.1348\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476227360.0000 - rmse: 21822.6309 - val_loss: 383379264.0000 - val_rmse: 19580.0684\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458910784.0000 - rmse: 21422.1992 - val_loss: 299288224.0000 - val_rmse: 17299.9414\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407658848.0000 - rmse: 20190.5605 - val_loss: 317960768.0000 - val_rmse: 17831.4473\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406245952.0000 - rmse: 20155.5391 - val_loss: 590559616.0000 - val_rmse: 24301.4277\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381204864.0000 - rmse: 19524.4629 - val_loss: 316737600.0000 - val_rmse: 17797.1172\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374798976.0000 - rmse: 19359.7207 - val_loss: 334506240.0000 - val_rmse: 18289.5059\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407884736.0000 - rmse: 20196.1523 - val_loss: 336834784.0000 - val_rmse: 18353.0547\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453972224.0000 - rmse: 21306.6191 - val_loss: 310980128.0000 - val_rmse: 17634.6230\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392600224.0000 - rmse: 19814.1367 - val_loss: 316128576.0000 - val_rmse: 17780.0000\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427930016.0000 - rmse: 20686.4648 - val_loss: 554983424.0000 - val_rmse: 23558.0801\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428212224.0000 - rmse: 20693.2832 - val_loss: 330265856.0000 - val_rmse: 18173.2129\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435139680.0000 - rmse: 20859.9961 - val_loss: 305241024.0000 - val_rmse: 17471.1426\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416276832.0000 - rmse: 20402.8574 - val_loss: 283462464.0000 - val_rmse: 16836.3398\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435074784.0000 - rmse: 20858.4395 - val_loss: 361139744.0000 - val_rmse: 19003.6719\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375051968.0000 - rmse: 19366.2559 - val_loss: 315551904.0000 - val_rmse: 17763.7754\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492947616.0000 - rmse: 22202.4180 - val_loss: 309042880.0000 - val_rmse: 17579.6113\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387856384.0000 - rmse: 19694.0625 - val_loss: 318418368.0000 - val_rmse: 17844.2773\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436451744.0000 - rmse: 20891.4258 - val_loss: 356875200.0000 - val_rmse: 18891.1348\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394407424.0000 - rmse: 19859.6895 - val_loss: 383637216.0000 - val_rmse: 19586.6543\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368339200.0000 - rmse: 19192.1621 - val_loss: 308118208.0000 - val_rmse: 17553.2930\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387454144.0000 - rmse: 19683.8516 - val_loss: 309173376.0000 - val_rmse: 17583.3203\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379079616.0000 - rmse: 19469.9609 - val_loss: 372941600.0000 - val_rmse: 19311.6914\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416046784.0000 - rmse: 20397.2188 - val_loss: 314531616.0000 - val_rmse: 17735.0332\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384810048.0000 - rmse: 19616.5684 - val_loss: 338831776.0000 - val_rmse: 18407.3770\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403050112.0000 - rmse: 20076.1035 - val_loss: 321467840.0000 - val_rmse: 17929.5215\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388677376.0000 - rmse: 19714.8984 - val_loss: 438962848.0000 - val_rmse: 20951.4355\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329968480.0000 - rmse: 18165.0273 - val_loss: 468251520.0000 - val_rmse: 21639.1172\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463241696.0000 - rmse: 21523.0449 - val_loss: 284669120.0000 - val_rmse: 16872.1348\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390232096.0000 - rmse: 19754.2871 - val_loss: 293639936.0000 - val_rmse: 17135.9180\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362508128.0000 - rmse: 19039.6406 - val_loss: 365463104.0000 - val_rmse: 19117.0840\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364979744.0000 - rmse: 19104.4355 - val_loss: 312993920.0000 - val_rmse: 17691.6270\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435773664.0000 - rmse: 20875.1855 - val_loss: 294545888.0000 - val_rmse: 17162.3320\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360913344.0000 - rmse: 18997.7129 - val_loss: 329992160.0000 - val_rmse: 18165.6836\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373641504.0000 - rmse: 19329.8047 - val_loss: 327680192.0000 - val_rmse: 18101.9355\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386895936.0000 - rmse: 19669.6641 - val_loss: 296753216.0000 - val_rmse: 17226.5215\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397185440.0000 - rmse: 19929.5059 - val_loss: 329230336.0000 - val_rmse: 18144.6973\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371900096.0000 - rmse: 19284.7051 - val_loss: 431927264.0000 - val_rmse: 20782.8535\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410557504.0000 - rmse: 20262.2168 - val_loss: 520202304.0000 - val_rmse: 22807.9395\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414748800.0000 - rmse: 20365.3770 - val_loss: 568575040.0000 - val_rmse: 23844.8066\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366240512.0000 - rmse: 19137.4043 - val_loss: 939406016.0000 - val_rmse: 30649.7227\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380306784.0000 - rmse: 19501.4512 - val_loss: 317918112.0000 - val_rmse: 17830.2520\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393562496.0000 - rmse: 19838.4023 - val_loss: 299144000.0000 - val_rmse: 17295.7734\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382982496.0000 - rmse: 19569.9336 - val_loss: 329840672.0000 - val_rmse: 18161.5098\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352125184.0000 - rmse: 18764.9941 - val_loss: 304362304.0000 - val_rmse: 17445.9746\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398969344.0000 - rmse: 19974.2129 - val_loss: 295295552.0000 - val_rmse: 17184.1602\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361827168.0000 - rmse: 19021.7500 - val_loss: 457871776.0000 - val_rmse: 21397.9316\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359730720.0000 - rmse: 18966.5605 - val_loss: 341059264.0000 - val_rmse: 18467.7832\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391783104.0000 - rmse: 19793.5078 - val_loss: 435855584.0000 - val_rmse: 20877.1465\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345140480.0000 - rmse: 18577.9551 - val_loss: 307489280.0000 - val_rmse: 17535.3652\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391716608.0000 - rmse: 19791.8262 - val_loss: 388210112.0000 - val_rmse: 19703.0430\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374142592.0000 - rmse: 19342.7598 - val_loss: 312377216.0000 - val_rmse: 17674.1875\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369240832.0000 - rmse: 19215.6367 - val_loss: 303857856.0000 - val_rmse: 17431.5117\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382330912.0000 - rmse: 19553.2754 - val_loss: 295108864.0000 - val_rmse: 17178.7246\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400701280.0000 - rmse: 20017.5195 - val_loss: 335492640.0000 - val_rmse: 18316.4531\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418400032.0000 - rmse: 20454.8223 - val_loss: 326299808.0000 - val_rmse: 18063.7637\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385897120.0000 - rmse: 19644.2578 - val_loss: 344681376.0000 - val_rmse: 18565.5918\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375559808.0000 - rmse: 19379.3574 - val_loss: 299564352.0000 - val_rmse: 17307.9199\n",
      "104/104 [==============================] - 0s 702us/step - loss: 822911744.0000 - rmse: 28686.4336\n",
      "[822911744.0, 28686.43359375]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 16666465280.0000 - rmse: 129098.6641 - val_loss: 3271045376.0000 - val_rmse: 57193.0547\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2721133568.0000 - rmse: 52164.4844 - val_loss: 1429781760.0000 - val_rmse: 37812.4570\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1814042240.0000 - rmse: 42591.5742 - val_loss: 1146556544.0000 - val_rmse: 33860.8398\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1759556608.0000 - rmse: 41947.0703 - val_loss: 1104232064.0000 - val_rmse: 33229.9883\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1684155648.0000 - rmse: 41038.4648 - val_loss: 1118746624.0000 - val_rmse: 33447.6719\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1627802368.0000 - rmse: 40346.0312 - val_loss: 1107155456.0000 - val_rmse: 33273.9414\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1544533760.0000 - rmse: 39300.5586 - val_loss: 1204149888.0000 - val_rmse: 34700.8633\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1495349248.0000 - rmse: 38669.7461 - val_loss: 1005439936.0000 - val_rmse: 31708.6699\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1512761984.0000 - rmse: 38894.2422 - val_loss: 1048965248.0000 - val_rmse: 32387.7324\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1489077888.0000 - rmse: 38588.5703 - val_loss: 968528000.0000 - val_rmse: 31121.1816\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1420323968.0000 - rmse: 37687.1875 - val_loss: 933910080.0000 - val_rmse: 30559.9434\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1415694208.0000 - rmse: 37625.7109 - val_loss: 934623168.0000 - val_rmse: 30571.6055\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1344699136.0000 - rmse: 36670.1406 - val_loss: 903690944.0000 - val_rmse: 30061.4531\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1339222912.0000 - rmse: 36595.3945 - val_loss: 904440512.0000 - val_rmse: 30073.9180\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1277094400.0000 - rmse: 35736.4570 - val_loss: 850450240.0000 - val_rmse: 29162.4805\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1239248768.0000 - rmse: 35202.9648 - val_loss: 897183616.0000 - val_rmse: 29953.0215\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1228737536.0000 - rmse: 35053.3516 - val_loss: 847443712.0000 - val_rmse: 29110.8867\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1224779520.0000 - rmse: 34996.8516 - val_loss: 869582272.0000 - val_rmse: 29488.6797\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1220733824.0000 - rmse: 34938.9961 - val_loss: 864949696.0000 - val_rmse: 29410.0254\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1236968576.0000 - rmse: 35170.5625 - val_loss: 874754112.0000 - val_rmse: 29576.2402\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1152375296.0000 - rmse: 33946.6523 - val_loss: 825249536.0000 - val_rmse: 28727.1543\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1112606720.0000 - rmse: 33355.7617 - val_loss: 826803520.0000 - val_rmse: 28754.1895\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1182340608.0000 - rmse: 34385.1797 - val_loss: 767497664.0000 - val_rmse: 27703.7461\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1085775360.0000 - rmse: 32951.1016 - val_loss: 768514688.0000 - val_rmse: 27722.0957\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1058312448.0000 - rmse: 32531.7148 - val_loss: 800993152.0000 - val_rmse: 28301.8203\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1107475968.0000 - rmse: 33278.7617 - val_loss: 763360064.0000 - val_rmse: 27628.9707\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1028813760.0000 - rmse: 32075.1250 - val_loss: 782382016.0000 - val_rmse: 27971.0918\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1001825280.0000 - rmse: 31651.6230 - val_loss: 1275413376.0000 - val_rmse: 35712.9297\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1015116288.0000 - rmse: 31860.8887 - val_loss: 730540352.0000 - val_rmse: 27028.5078\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1033003392.0000 - rmse: 32140.3691 - val_loss: 844330880.0000 - val_rmse: 29057.3691\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 985603072.0000 - rmse: 31394.3145 - val_loss: 794614976.0000 - val_rmse: 28188.9141\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 985065152.0000 - rmse: 31385.7480 - val_loss: 1136754560.0000 - val_rmse: 33715.7891\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1021787520.0000 - rmse: 31965.4082 - val_loss: 689959040.0000 - val_rmse: 26267.0684\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 984482880.0000 - rmse: 31376.4707 - val_loss: 738937344.0000 - val_rmse: 27183.4004\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 922789568.0000 - rmse: 30377.4512 - val_loss: 749450560.0000 - val_rmse: 27376.0918\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 958542848.0000 - rmse: 30960.3418 - val_loss: 665940096.0000 - val_rmse: 25805.8145\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 955403648.0000 - rmse: 30909.6035 - val_loss: 641052096.0000 - val_rmse: 25319.0059\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 985405312.0000 - rmse: 31391.1660 - val_loss: 694861632.0000 - val_rmse: 26360.2266\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 918329152.0000 - rmse: 30303.9434 - val_loss: 634889280.0000 - val_rmse: 25197.0059\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 917476032.0000 - rmse: 30289.8652 - val_loss: 672705216.0000 - val_rmse: 25936.5605\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 876421440.0000 - rmse: 29604.4141 - val_loss: 666798784.0000 - val_rmse: 25822.4453\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 940188416.0000 - rmse: 30662.4902 - val_loss: 669763200.0000 - val_rmse: 25879.7832\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 896756096.0000 - rmse: 29945.8848 - val_loss: 659827904.0000 - val_rmse: 25687.1152\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 862619584.0000 - rmse: 29370.3848 - val_loss: 637899904.0000 - val_rmse: 25256.6797\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 882789056.0000 - rmse: 29711.7637 - val_loss: 613948544.0000 - val_rmse: 24777.9844\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 878907968.0000 - rmse: 29646.3809 - val_loss: 607007872.0000 - val_rmse: 24637.5254\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 824173696.0000 - rmse: 28708.4258 - val_loss: 630293056.0000 - val_rmse: 25105.6367\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 892093440.0000 - rmse: 29867.9336 - val_loss: 698855040.0000 - val_rmse: 26435.8672\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 842982208.0000 - rmse: 29034.1523 - val_loss: 633145472.0000 - val_rmse: 25162.3789\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 775039680.0000 - rmse: 27839.5352 - val_loss: 687501696.0000 - val_rmse: 26220.2520\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 853351936.0000 - rmse: 29212.1855 - val_loss: 622035200.0000 - val_rmse: 24940.6328\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 852209408.0000 - rmse: 29192.6230 - val_loss: 632107264.0000 - val_rmse: 25141.7422\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 805263424.0000 - rmse: 28377.1621 - val_loss: 581822016.0000 - val_rmse: 24120.9863\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 806953920.0000 - rmse: 28406.9316 - val_loss: 593216192.0000 - val_rmse: 24356.0293\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 828092096.0000 - rmse: 28776.5879 - val_loss: 595986368.0000 - val_rmse: 24412.8301\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 839352128.0000 - rmse: 28971.5723 - val_loss: 557312128.0000 - val_rmse: 23607.4570\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 809643264.0000 - rmse: 28454.2285 - val_loss: 559163840.0000 - val_rmse: 23646.6445\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 793788160.0000 - rmse: 28174.2441 - val_loss: 545277952.0000 - val_rmse: 23351.1836\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 723588032.0000 - rmse: 26899.5918 - val_loss: 881516800.0000 - val_rmse: 29690.3477\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 775157120.0000 - rmse: 27841.6426 - val_loss: 519510752.0000 - val_rmse: 22792.7773\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 753335104.0000 - rmse: 27446.9492 - val_loss: 537643008.0000 - val_rmse: 23187.1289\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751019840.0000 - rmse: 27404.7402 - val_loss: 528511616.0000 - val_rmse: 22989.3789\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 757668480.0000 - rmse: 27525.7793 - val_loss: 520156896.0000 - val_rmse: 22806.9434\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704044096.0000 - rmse: 26533.8281 - val_loss: 563481920.0000 - val_rmse: 23737.7734\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 792978688.0000 - rmse: 28159.8750 - val_loss: 532172768.0000 - val_rmse: 23068.8672\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721235712.0000 - rmse: 26855.8301 - val_loss: 584191616.0000 - val_rmse: 24170.0527\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 678617280.0000 - rmse: 26050.2812 - val_loss: 515323872.0000 - val_rmse: 22700.7441\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 701291072.0000 - rmse: 26481.9004 - val_loss: 512388096.0000 - val_rmse: 22635.9883\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 702828608.0000 - rmse: 26510.9141 - val_loss: 517575968.0000 - val_rmse: 22750.2949\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 740944064.0000 - rmse: 27220.2871 - val_loss: 492116608.0000 - val_rmse: 22183.6973\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 700276544.0000 - rmse: 26462.7383 - val_loss: 495462784.0000 - val_rmse: 22258.9902\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 720125056.0000 - rmse: 26835.1426 - val_loss: 544780544.0000 - val_rmse: 23340.5332\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 662481088.0000 - rmse: 25738.7051 - val_loss: 515397600.0000 - val_rmse: 22702.3672\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692779328.0000 - rmse: 26320.6992 - val_loss: 593665088.0000 - val_rmse: 24365.2402\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730554560.0000 - rmse: 27028.7695 - val_loss: 486112736.0000 - val_rmse: 22047.9648\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 653731968.0000 - rmse: 25568.1816 - val_loss: 477223488.0000 - val_rmse: 21845.4414\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730792256.0000 - rmse: 27033.1680 - val_loss: 523091328.0000 - val_rmse: 22871.1855\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666483648.0000 - rmse: 25816.3438 - val_loss: 485446784.0000 - val_rmse: 22032.8555\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704638784.0000 - rmse: 26545.0312 - val_loss: 526661920.0000 - val_rmse: 22949.1133\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676234112.0000 - rmse: 26004.5020 - val_loss: 462647136.0000 - val_rmse: 21509.2324\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706191936.0000 - rmse: 26574.2695 - val_loss: 463698272.0000 - val_rmse: 21533.6543\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706854208.0000 - rmse: 26586.7266 - val_loss: 468959168.0000 - val_rmse: 21655.4648\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641427840.0000 - rmse: 25326.4238 - val_loss: 457597824.0000 - val_rmse: 21391.5332\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646885376.0000 - rmse: 25433.9395 - val_loss: 517179296.0000 - val_rmse: 22741.5742\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 619908096.0000 - rmse: 24897.9512 - val_loss: 568042752.0000 - val_rmse: 23833.6445\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 636806976.0000 - rmse: 25235.0332 - val_loss: 524868800.0000 - val_rmse: 22910.0137\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 700508288.0000 - rmse: 26467.1152 - val_loss: 621834112.0000 - val_rmse: 24936.5977\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 633839808.0000 - rmse: 25176.1738 - val_loss: 485874624.0000 - val_rmse: 22042.5625\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654044288.0000 - rmse: 25574.2891 - val_loss: 441697344.0000 - val_rmse: 21016.5957\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 636657088.0000 - rmse: 25232.0625 - val_loss: 450603136.0000 - val_rmse: 21227.4121\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 660042816.0000 - rmse: 25691.2930 - val_loss: 449891584.0000 - val_rmse: 21210.6465\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571012160.0000 - rmse: 23895.8574 - val_loss: 457861984.0000 - val_rmse: 21397.7070\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 675812160.0000 - rmse: 25996.3867 - val_loss: 512279744.0000 - val_rmse: 22633.5957\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 603780032.0000 - rmse: 24571.9336 - val_loss: 447982880.0000 - val_rmse: 21165.6055\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574283840.0000 - rmse: 23964.2207 - val_loss: 431634400.0000 - val_rmse: 20775.8125\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 644618560.0000 - rmse: 25389.3379 - val_loss: 446814464.0000 - val_rmse: 21137.9844\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 688204864.0000 - rmse: 26233.6582 - val_loss: 444490784.0000 - val_rmse: 21082.9492\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665539392.0000 - rmse: 25798.0488 - val_loss: 444601792.0000 - val_rmse: 21085.5781\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648223744.0000 - rmse: 25460.2363 - val_loss: 431952480.0000 - val_rmse: 20783.4648\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 613875584.0000 - rmse: 24776.5098 - val_loss: 424709600.0000 - val_rmse: 20608.4805\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621239232.0000 - rmse: 24924.6719 - val_loss: 422310784.0000 - val_rmse: 20550.1992\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596508160.0000 - rmse: 24423.5137 - val_loss: 432020320.0000 - val_rmse: 20785.0977\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661843712.0000 - rmse: 25726.3223 - val_loss: 414896832.0000 - val_rmse: 20369.0156\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584545280.0000 - rmse: 24177.3672 - val_loss: 408842304.0000 - val_rmse: 20219.8477\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592640512.0000 - rmse: 24344.2070 - val_loss: 415477472.0000 - val_rmse: 20383.2598\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583121024.0000 - rmse: 24147.8945 - val_loss: 467230656.0000 - val_rmse: 21615.5195\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584416640.0000 - rmse: 24174.7090 - val_loss: 535853792.0000 - val_rmse: 23148.5137\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591602880.0000 - rmse: 24322.8887 - val_loss: 480520256.0000 - val_rmse: 21920.7715\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550121088.0000 - rmse: 23454.6582 - val_loss: 412460672.0000 - val_rmse: 20309.1250\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540578304.0000 - rmse: 23250.3359 - val_loss: 400457248.0000 - val_rmse: 20011.4258\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616294144.0000 - rmse: 24825.2676 - val_loss: 401467328.0000 - val_rmse: 20036.6465\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590403520.0000 - rmse: 24298.2188 - val_loss: 472422208.0000 - val_rmse: 21735.2734\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558431872.0000 - rmse: 23631.1621 - val_loss: 388496288.0000 - val_rmse: 19710.3066\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596285888.0000 - rmse: 24418.9629 - val_loss: 408193856.0000 - val_rmse: 20203.8047\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566988288.0000 - rmse: 23811.5137 - val_loss: 381828608.0000 - val_rmse: 19540.4316\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534611616.0000 - rmse: 23121.6680 - val_loss: 508931488.0000 - val_rmse: 22559.5098\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610829184.0000 - rmse: 24714.9590 - val_loss: 403602048.0000 - val_rmse: 20089.8457\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538306560.0000 - rmse: 23201.4336 - val_loss: 389050784.0000 - val_rmse: 19724.3672\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594609792.0000 - rmse: 24384.6172 - val_loss: 384652416.0000 - val_rmse: 19612.5547\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 631023744.0000 - rmse: 25120.1836 - val_loss: 420933856.0000 - val_rmse: 20516.6719\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564779904.0000 - rmse: 23765.0977 - val_loss: 455694112.0000 - val_rmse: 21346.9902\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536208640.0000 - rmse: 23156.1777 - val_loss: 398302368.0000 - val_rmse: 19957.5117\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536170816.0000 - rmse: 23155.3633 - val_loss: 417004640.0000 - val_rmse: 20420.6895\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578115456.0000 - rmse: 24044.0273 - val_loss: 419402944.0000 - val_rmse: 20479.3262\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536260192.0000 - rmse: 23157.2910 - val_loss: 406659584.0000 - val_rmse: 20165.8008\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506986304.0000 - rmse: 22516.3535 - val_loss: 474122368.0000 - val_rmse: 21774.3477\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525066144.0000 - rmse: 22914.3184 - val_loss: 386358560.0000 - val_rmse: 19656.0039\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520949120.0000 - rmse: 22824.3047 - val_loss: 474851776.0000 - val_rmse: 21791.0938\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576984128.0000 - rmse: 24020.4922 - val_loss: 384259296.0000 - val_rmse: 19602.5312\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501533568.0000 - rmse: 22394.9414 - val_loss: 391339072.0000 - val_rmse: 19782.2891\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526476928.0000 - rmse: 22945.0820 - val_loss: 376016352.0000 - val_rmse: 19391.1387\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514328800.0000 - rmse: 22678.8164 - val_loss: 409609376.0000 - val_rmse: 20238.8066\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515597440.0000 - rmse: 22706.7695 - val_loss: 422151648.0000 - val_rmse: 20546.3262\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502015200.0000 - rmse: 22405.6934 - val_loss: 381137344.0000 - val_rmse: 19522.7363\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522434528.0000 - rmse: 22856.8223 - val_loss: 379185408.0000 - val_rmse: 19472.6816\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590083648.0000 - rmse: 24291.6367 - val_loss: 368545728.0000 - val_rmse: 19197.5430\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488129856.0000 - rmse: 22093.6582 - val_loss: 404117376.0000 - val_rmse: 20102.6680\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527401216.0000 - rmse: 22965.2148 - val_loss: 484051808.0000 - val_rmse: 22001.1758\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511876160.0000 - rmse: 22624.6797 - val_loss: 462464128.0000 - val_rmse: 21504.9785\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509374144.0000 - rmse: 22569.3164 - val_loss: 403029760.0000 - val_rmse: 20075.5977\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531952416.0000 - rmse: 23064.0879 - val_loss: 407125024.0000 - val_rmse: 20177.3340\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504975520.0000 - rmse: 22471.6562 - val_loss: 402028640.0000 - val_rmse: 20050.6504\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485367776.0000 - rmse: 22031.0605 - val_loss: 428315712.0000 - val_rmse: 20695.7891\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482678528.0000 - rmse: 21969.9414 - val_loss: 405793792.0000 - val_rmse: 20144.3223\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474662880.0000 - rmse: 21786.7578 - val_loss: 393922304.0000 - val_rmse: 19847.4727\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495202176.0000 - rmse: 22253.1387 - val_loss: 376068416.0000 - val_rmse: 19392.4785\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507953248.0000 - rmse: 22537.8145 - val_loss: 397100288.0000 - val_rmse: 19927.3730\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488669888.0000 - rmse: 22105.8770 - val_loss: 482202944.0000 - val_rmse: 21959.1172\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483032512.0000 - rmse: 21978.0000 - val_loss: 405913632.0000 - val_rmse: 20147.2949\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502105280.0000 - rmse: 22407.7051 - val_loss: 365169888.0000 - val_rmse: 19109.4160\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489280512.0000 - rmse: 22119.6836 - val_loss: 372578560.0000 - val_rmse: 19302.2910\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493710784.0000 - rmse: 22219.6016 - val_loss: 354267360.0000 - val_rmse: 18821.9883\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453376032.0000 - rmse: 21292.6250 - val_loss: 369390752.0000 - val_rmse: 19219.5371\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412358848.0000 - rmse: 20306.6152 - val_loss: 440445376.0000 - val_rmse: 20986.7871\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438272704.0000 - rmse: 20934.9609 - val_loss: 405893504.0000 - val_rmse: 20146.7969\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476388832.0000 - rmse: 21826.3320 - val_loss: 500757152.0000 - val_rmse: 22377.6016\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453085920.0000 - rmse: 21285.8125 - val_loss: 397743648.0000 - val_rmse: 19943.5098\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476315744.0000 - rmse: 21824.6562 - val_loss: 364503840.0000 - val_rmse: 19091.9805\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446408768.0000 - rmse: 21128.3828 - val_loss: 402034560.0000 - val_rmse: 20050.7930\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474302656.0000 - rmse: 21778.4883 - val_loss: 366485472.0000 - val_rmse: 19143.8066\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443953792.0000 - rmse: 21070.2070 - val_loss: 490591200.0000 - val_rmse: 22149.2910\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466088864.0000 - rmse: 21589.0898 - val_loss: 359491744.0000 - val_rmse: 18960.2656\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418862944.0000 - rmse: 20466.1387 - val_loss: 365097696.0000 - val_rmse: 19107.5273\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471266368.0000 - rmse: 21708.6680 - val_loss: 396412736.0000 - val_rmse: 19910.1113\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427923328.0000 - rmse: 20686.3047 - val_loss: 420758496.0000 - val_rmse: 20512.3965\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452457664.0000 - rmse: 21271.0488 - val_loss: 362076384.0000 - val_rmse: 19028.3008\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421364864.0000 - rmse: 20527.1719 - val_loss: 414023904.0000 - val_rmse: 20347.5742\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415860192.0000 - rmse: 20392.6465 - val_loss: 351218496.0000 - val_rmse: 18740.8223\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427453792.0000 - rmse: 20674.9551 - val_loss: 368159936.0000 - val_rmse: 19187.4902\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394694432.0000 - rmse: 19866.9141 - val_loss: 362453856.0000 - val_rmse: 19038.2168\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447233760.0000 - rmse: 21147.9004 - val_loss: 405416992.0000 - val_rmse: 20134.9688\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409708192.0000 - rmse: 20241.2461 - val_loss: 424942720.0000 - val_rmse: 20614.1348\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403595840.0000 - rmse: 20089.6895 - val_loss: 368000256.0000 - val_rmse: 19183.3301\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396112768.0000 - rmse: 19902.5820 - val_loss: 365317472.0000 - val_rmse: 19113.2773\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384509728.0000 - rmse: 19608.9180 - val_loss: 355034048.0000 - val_rmse: 18842.3438\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438044192.0000 - rmse: 20929.5020 - val_loss: 365432192.0000 - val_rmse: 19116.2773\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399623456.0000 - rmse: 19990.5801 - val_loss: 365521216.0000 - val_rmse: 19118.6055\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425481344.0000 - rmse: 20627.1973 - val_loss: 394249696.0000 - val_rmse: 19855.7188\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462205760.0000 - rmse: 21498.9688 - val_loss: 432113376.0000 - val_rmse: 20787.3340\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365269536.0000 - rmse: 19112.0234 - val_loss: 363505568.0000 - val_rmse: 19065.8203\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413556128.0000 - rmse: 20336.0762 - val_loss: 358016544.0000 - val_rmse: 18921.3203\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353057184.0000 - rmse: 18789.8125 - val_loss: 358947424.0000 - val_rmse: 18945.9043\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424806912.0000 - rmse: 20610.8438 - val_loss: 373879200.0000 - val_rmse: 19335.9531\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353426048.0000 - rmse: 18799.6270 - val_loss: 349413120.0000 - val_rmse: 18692.5918\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433026912.0000 - rmse: 20809.2949 - val_loss: 342489568.0000 - val_rmse: 18506.4707\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409318880.0000 - rmse: 20231.6250 - val_loss: 349584192.0000 - val_rmse: 18697.1699\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397460096.0000 - rmse: 19936.4004 - val_loss: 580413696.0000 - val_rmse: 24091.7734\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389439616.0000 - rmse: 19734.2227 - val_loss: 360923840.0000 - val_rmse: 18997.9922\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392378944.0000 - rmse: 19808.5547 - val_loss: 358408160.0000 - val_rmse: 18931.6680\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395069792.0000 - rmse: 19876.3613 - val_loss: 386661920.0000 - val_rmse: 19663.7168\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381335744.0000 - rmse: 19527.8184 - val_loss: 381521952.0000 - val_rmse: 19532.5840\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399886592.0000 - rmse: 19997.1621 - val_loss: 393796064.0000 - val_rmse: 19844.2930\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395104480.0000 - rmse: 19877.2344 - val_loss: 432967392.0000 - val_rmse: 20807.8672\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336706176.0000 - rmse: 18349.5527 - val_loss: 352689856.0000 - val_rmse: 18780.0352\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389214112.0000 - rmse: 19728.5059 - val_loss: 385842880.0000 - val_rmse: 19642.8809\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362045888.0000 - rmse: 19027.5000 - val_loss: 430675328.0000 - val_rmse: 20752.7168\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324890144.0000 - rmse: 18024.7051 - val_loss: 403324960.0000 - val_rmse: 20082.9492\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343408512.0000 - rmse: 18531.2773 - val_loss: 379521408.0000 - val_rmse: 19481.3027\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362969600.0000 - rmse: 19051.7578 - val_loss: 509597952.0000 - val_rmse: 22574.2734\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394494976.0000 - rmse: 19861.8926 - val_loss: 338466944.0000 - val_rmse: 18397.4668\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374600064.0000 - rmse: 19354.5820 - val_loss: 371857344.0000 - val_rmse: 19283.5996\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349343744.0000 - rmse: 18690.7383 - val_loss: 506501248.0000 - val_rmse: 22505.5781\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347755712.0000 - rmse: 18648.2051 - val_loss: 352199584.0000 - val_rmse: 18766.9785\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364018432.0000 - rmse: 19079.2637 - val_loss: 357581536.0000 - val_rmse: 18909.8223\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411924000.0000 - rmse: 20295.9082 - val_loss: 348639072.0000 - val_rmse: 18671.8750\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351502624.0000 - rmse: 18748.4004 - val_loss: 609787200.0000 - val_rmse: 24693.8672\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382659104.0000 - rmse: 19561.6719 - val_loss: 560112192.0000 - val_rmse: 23666.6855\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385973600.0000 - rmse: 19646.2051 - val_loss: 588955072.0000 - val_rmse: 24268.3945\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394331104.0000 - rmse: 19857.7695 - val_loss: 361554272.0000 - val_rmse: 19014.5762\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342839744.0000 - rmse: 18515.9277 - val_loss: 356065376.0000 - val_rmse: 18869.6914\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329644960.0000 - rmse: 18156.1230 - val_loss: 439699424.0000 - val_rmse: 20969.0059\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347828576.0000 - rmse: 18650.1602 - val_loss: 389607488.0000 - val_rmse: 19738.4746\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357482880.0000 - rmse: 18907.2148 - val_loss: 354338656.0000 - val_rmse: 18823.8828\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377711488.0000 - rmse: 19434.7969 - val_loss: 370071584.0000 - val_rmse: 19237.2422\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356490240.0000 - rmse: 18880.9453 - val_loss: 422260960.0000 - val_rmse: 20548.9863\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331123840.0000 - rmse: 18196.8047 - val_loss: 381138112.0000 - val_rmse: 19522.7539\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336911552.0000 - rmse: 18355.1465 - val_loss: 349471456.0000 - val_rmse: 18694.1523\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359994112.0000 - rmse: 18973.5059 - val_loss: 364923648.0000 - val_rmse: 19102.9707\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312486048.0000 - rmse: 17677.2695 - val_loss: 507869696.0000 - val_rmse: 22535.9629\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376280416.0000 - rmse: 19397.9453 - val_loss: 400731744.0000 - val_rmse: 20018.2812\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286492128.0000 - rmse: 16926.0742 - val_loss: 426648800.0000 - val_rmse: 20655.4727\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354961472.0000 - rmse: 18840.4180 - val_loss: 352525952.0000 - val_rmse: 18775.6719\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324739104.0000 - rmse: 18020.5156 - val_loss: 404679488.0000 - val_rmse: 20116.6445\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317609184.0000 - rmse: 17821.5879 - val_loss: 494741024.0000 - val_rmse: 22242.7715\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385334208.0000 - rmse: 19629.9297 - val_loss: 492432384.0000 - val_rmse: 22190.8145\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371280288.0000 - rmse: 19268.6289 - val_loss: 427553600.0000 - val_rmse: 20677.3672\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325126624.0000 - rmse: 18031.2637 - val_loss: 427014272.0000 - val_rmse: 20664.3223\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381378528.0000 - rmse: 19528.9102 - val_loss: 350444864.0000 - val_rmse: 18720.1699\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366291232.0000 - rmse: 19138.7324 - val_loss: 386414432.0000 - val_rmse: 19657.4238\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316055680.0000 - rmse: 17777.9492 - val_loss: 368307776.0000 - val_rmse: 19191.3398\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376296352.0000 - rmse: 19398.3555 - val_loss: 366268512.0000 - val_rmse: 19138.1387\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330798432.0000 - rmse: 18187.8613 - val_loss: 399088896.0000 - val_rmse: 19977.2070\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320491392.0000 - rmse: 17902.2676 - val_loss: 386041856.0000 - val_rmse: 19647.9414\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355341056.0000 - rmse: 18850.4883 - val_loss: 521592256.0000 - val_rmse: 22838.3926\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354372544.0000 - rmse: 18824.7812 - val_loss: 353566496.0000 - val_rmse: 18803.3594\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320936768.0000 - rmse: 17914.7051 - val_loss: 381311840.0000 - val_rmse: 19527.2051\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333780864.0000 - rmse: 18269.6680 - val_loss: 402664128.0000 - val_rmse: 20066.4902\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381146720.0000 - rmse: 19522.9766 - val_loss: 424228864.0000 - val_rmse: 20596.8125\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344403168.0000 - rmse: 18558.0977 - val_loss: 373469568.0000 - val_rmse: 19325.3574\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323294400.0000 - rmse: 17980.3848 - val_loss: 335670464.0000 - val_rmse: 18321.3086\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370797280.0000 - rmse: 19256.0918 - val_loss: 368878592.0000 - val_rmse: 19206.2090\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349120096.0000 - rmse: 18684.7539 - val_loss: 421328192.0000 - val_rmse: 20526.2773\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381704544.0000 - rmse: 19537.2539 - val_loss: 358790752.0000 - val_rmse: 18941.7676\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337415584.0000 - rmse: 18368.8711 - val_loss: 534140064.0000 - val_rmse: 23111.4668\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319096736.0000 - rmse: 17863.2754 - val_loss: 375683136.0000 - val_rmse: 19382.5430\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319817312.0000 - rmse: 17883.4336 - val_loss: 363338656.0000 - val_rmse: 19061.4375\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294416416.0000 - rmse: 17158.5645 - val_loss: 353305760.0000 - val_rmse: 18796.4258\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332683168.0000 - rmse: 18239.5996 - val_loss: 364405088.0000 - val_rmse: 19089.3926\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337001312.0000 - rmse: 18357.5918 - val_loss: 392316832.0000 - val_rmse: 19806.9863\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348130528.0000 - rmse: 18658.2520 - val_loss: 387587136.0000 - val_rmse: 19687.2305\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313921376.0000 - rmse: 17717.8242 - val_loss: 479481120.0000 - val_rmse: 21897.0547\n",
      "104/104 [==============================] - 0s 740us/step - loss: 616048384.0000 - rmse: 24820.3203\n",
      "[616048384.0, 24820.3203125]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 17700227072.0000 - rmse: 133042.2031 - val_loss: 3363376384.0000 - val_rmse: 57994.6250\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2987843840.0000 - rmse: 54661.1719 - val_loss: 1381629952.0000 - val_rmse: 37170.2812\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2038450688.0000 - rmse: 45149.2031 - val_loss: 1200969984.0000 - val_rmse: 34655.0156\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1926225536.0000 - rmse: 43888.7852 - val_loss: 1137043840.0000 - val_rmse: 33720.0781\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1793016704.0000 - rmse: 42344.0273 - val_loss: 1085423488.0000 - val_rmse: 32945.7656\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1723504640.0000 - rmse: 41515.1133 - val_loss: 1108943616.0000 - val_rmse: 33300.8047\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1694308736.0000 - rmse: 41161.9805 - val_loss: 1031752192.0000 - val_rmse: 32120.8965\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1680777344.0000 - rmse: 40997.2852 - val_loss: 1002807808.0000 - val_rmse: 31667.1406\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1528745216.0000 - rmse: 39099.1719 - val_loss: 966464000.0000 - val_rmse: 31088.0039\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1566828800.0000 - rmse: 39583.1875 - val_loss: 934118016.0000 - val_rmse: 30563.3438\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1523966848.0000 - rmse: 39038.0195 - val_loss: 988026368.0000 - val_rmse: 31432.8828\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1435339392.0000 - rmse: 37885.8711 - val_loss: 912774912.0000 - val_rmse: 30212.1641\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1487643008.0000 - rmse: 38569.9727 - val_loss: 915996480.0000 - val_rmse: 30265.4336\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1371032832.0000 - rmse: 37027.4570 - val_loss: 857800128.0000 - val_rmse: 29288.2246\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1330937600.0000 - rmse: 36482.0156 - val_loss: 901251072.0000 - val_rmse: 30020.8438\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1301206528.0000 - rmse: 36072.2422 - val_loss: 828912064.0000 - val_rmse: 28790.8320\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1350417280.0000 - rmse: 36748.0234 - val_loss: 806344576.0000 - val_rmse: 28396.2051\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1316846464.0000 - rmse: 36288.3789 - val_loss: 808442944.0000 - val_rmse: 28433.1309\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1280314880.0000 - rmse: 35781.4883 - val_loss: 765389312.0000 - val_rmse: 27665.6699\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1211916160.0000 - rmse: 34812.5859 - val_loss: 752226880.0000 - val_rmse: 27426.7539\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1223125376.0000 - rmse: 34973.2070 - val_loss: 796016576.0000 - val_rmse: 28213.7656\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1119101312.0000 - rmse: 33452.9727 - val_loss: 730233472.0000 - val_rmse: 27022.8301\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1161435264.0000 - rmse: 34079.8359 - val_loss: 719929472.0000 - val_rmse: 26831.5020\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1127040896.0000 - rmse: 33571.4297 - val_loss: 698754624.0000 - val_rmse: 26433.9668\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1112874624.0000 - rmse: 33359.7734 - val_loss: 689919552.0000 - val_rmse: 26266.3184\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1036263488.0000 - rmse: 32191.0449 - val_loss: 756163072.0000 - val_rmse: 27498.4180\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1126149504.0000 - rmse: 33558.1484 - val_loss: 729561728.0000 - val_rmse: 27010.4004\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1082456576.0000 - rmse: 32900.7031 - val_loss: 682091456.0000 - val_rmse: 26116.8789\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1011008768.0000 - rmse: 31796.3633 - val_loss: 668607488.0000 - val_rmse: 25857.4434\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1005479232.0000 - rmse: 31709.2910 - val_loss: 656436352.0000 - val_rmse: 25621.0117\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1010712320.0000 - rmse: 31791.6992 - val_loss: 727289024.0000 - val_rmse: 26968.2949\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1063643328.0000 - rmse: 32613.5449 - val_loss: 632594624.0000 - val_rmse: 25151.4316\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 995023552.0000 - rmse: 31543.9941 - val_loss: 613527232.0000 - val_rmse: 24769.4785\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 968712064.0000 - rmse: 31124.1367 - val_loss: 639483968.0000 - val_rmse: 25288.0176\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 943813888.0000 - rmse: 30721.5547 - val_loss: 675051584.0000 - val_rmse: 25981.7539\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1007115328.0000 - rmse: 31735.0801 - val_loss: 604824256.0000 - val_rmse: 24593.1738\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 973778688.0000 - rmse: 31205.4258 - val_loss: 607501760.0000 - val_rmse: 24647.5469\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 919225664.0000 - rmse: 30318.7344 - val_loss: 618651520.0000 - val_rmse: 24872.7070\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 993897152.0000 - rmse: 31526.1328 - val_loss: 625033984.0000 - val_rmse: 25000.6777\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1003358080.0000 - rmse: 31675.8281 - val_loss: 607500608.0000 - val_rmse: 24647.5273\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 920756224.0000 - rmse: 30343.9648 - val_loss: 661434752.0000 - val_rmse: 25718.3711\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1003103616.0000 - rmse: 31671.8086 - val_loss: 576232768.0000 - val_rmse: 24004.8477\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 936628608.0000 - rmse: 30604.3887 - val_loss: 589516864.0000 - val_rmse: 24279.9648\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 878871232.0000 - rmse: 29645.7598 - val_loss: 571928192.0000 - val_rmse: 23915.0195\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 901167936.0000 - rmse: 30019.4590 - val_loss: 586249792.0000 - val_rmse: 24212.5938\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 891253184.0000 - rmse: 29853.8633 - val_loss: 603855296.0000 - val_rmse: 24573.4648\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 897862144.0000 - rmse: 29964.3438 - val_loss: 612602368.0000 - val_rmse: 24750.8008\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 890187776.0000 - rmse: 29836.0137 - val_loss: 578279616.0000 - val_rmse: 24047.4434\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 890703232.0000 - rmse: 29844.6504 - val_loss: 600232448.0000 - val_rmse: 24499.6387\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 864120832.0000 - rmse: 29395.9316 - val_loss: 556748032.0000 - val_rmse: 23595.5078\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 833523712.0000 - rmse: 28870.8086 - val_loss: 566528128.0000 - val_rmse: 23801.8496\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 870313728.0000 - rmse: 29501.0781 - val_loss: 539813184.0000 - val_rmse: 23233.8789\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 858856832.0000 - rmse: 29306.2578 - val_loss: 673868288.0000 - val_rmse: 25958.9727\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 812397440.0000 - rmse: 28502.5859 - val_loss: 550282304.0000 - val_rmse: 23458.0957\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 788246720.0000 - rmse: 28075.7305 - val_loss: 558865856.0000 - val_rmse: 23640.3418\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 821555456.0000 - rmse: 28662.7871 - val_loss: 563741632.0000 - val_rmse: 23743.2422\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 844625728.0000 - rmse: 29062.4434 - val_loss: 585265088.0000 - val_rmse: 24192.2500\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 773722816.0000 - rmse: 27815.8711 - val_loss: 556237056.0000 - val_rmse: 23584.6777\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 867141632.0000 - rmse: 29447.2695 - val_loss: 554133632.0000 - val_rmse: 23540.0410\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 862025792.0000 - rmse: 29360.2754 - val_loss: 524471552.0000 - val_rmse: 22901.3438\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 770742784.0000 - rmse: 27762.2539 - val_loss: 521944800.0000 - val_rmse: 22846.1094\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 790763712.0000 - rmse: 28120.5195 - val_loss: 526058432.0000 - val_rmse: 22935.9609\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751856832.0000 - rmse: 27420.0020 - val_loss: 522407232.0000 - val_rmse: 22856.2266\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 760732864.0000 - rmse: 27581.3848 - val_loss: 520734496.0000 - val_rmse: 22819.6055\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 743512448.0000 - rmse: 27267.4219 - val_loss: 539640448.0000 - val_rmse: 23230.1582\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 779260416.0000 - rmse: 27915.2344 - val_loss: 664265728.0000 - val_rmse: 25773.3516\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 712595392.0000 - rmse: 26694.4805 - val_loss: 500718304.0000 - val_rmse: 22376.7324\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 819903488.0000 - rmse: 28633.9551 - val_loss: 500522464.0000 - val_rmse: 22372.3594\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 712825216.0000 - rmse: 26698.7832 - val_loss: 496652832.0000 - val_rmse: 22285.7090\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698625728.0000 - rmse: 26431.5273 - val_loss: 486478688.0000 - val_rmse: 22056.2617\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 725507904.0000 - rmse: 26935.2520 - val_loss: 507108672.0000 - val_rmse: 22519.0703\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 726613376.0000 - rmse: 26955.7656 - val_loss: 528250368.0000 - val_rmse: 22983.6953\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 745251968.0000 - rmse: 27299.3008 - val_loss: 489698528.0000 - val_rmse: 22129.1309\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 734363456.0000 - rmse: 27099.1406 - val_loss: 581512896.0000 - val_rmse: 24114.5762\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 786727936.0000 - rmse: 28048.6680 - val_loss: 514904384.0000 - val_rmse: 22691.5039\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 710882304.0000 - rmse: 26662.3730 - val_loss: 473663712.0000 - val_rmse: 21763.8125\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692787584.0000 - rmse: 26320.8574 - val_loss: 630457472.0000 - val_rmse: 25108.9102\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 627730048.0000 - rmse: 25054.5410 - val_loss: 505354400.0000 - val_rmse: 22480.0859\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704004992.0000 - rmse: 26533.0918 - val_loss: 484539232.0000 - val_rmse: 22012.2500\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730941248.0000 - rmse: 27035.9219 - val_loss: 475131936.0000 - val_rmse: 21797.5195\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684140608.0000 - rmse: 26156.0801 - val_loss: 468279616.0000 - val_rmse: 21639.7676\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656532736.0000 - rmse: 25622.8926 - val_loss: 462518080.0000 - val_rmse: 21506.2305\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 710349696.0000 - rmse: 26652.3848 - val_loss: 509093856.0000 - val_rmse: 22563.1055\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640863616.0000 - rmse: 25315.2812 - val_loss: 480016768.0000 - val_rmse: 21909.2832\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724784064.0000 - rmse: 26921.8125 - val_loss: 453237440.0000 - val_rmse: 21289.3711\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 644380352.0000 - rmse: 25384.6445 - val_loss: 607299776.0000 - val_rmse: 24643.4512\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 653630784.0000 - rmse: 25566.2031 - val_loss: 455782464.0000 - val_rmse: 21349.0625\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630093760.0000 - rmse: 25101.6660 - val_loss: 463680960.0000 - val_rmse: 21533.2500\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585374464.0000 - rmse: 24194.5117 - val_loss: 492844160.0000 - val_rmse: 22200.0918\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632541120.0000 - rmse: 25150.3672 - val_loss: 443754496.0000 - val_rmse: 21065.4766\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626655936.0000 - rmse: 25033.0938 - val_loss: 446079584.0000 - val_rmse: 21120.5938\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634839296.0000 - rmse: 25196.0156 - val_loss: 444556576.0000 - val_rmse: 21084.5078\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 608997248.0000 - rmse: 24677.8652 - val_loss: 441708064.0000 - val_rmse: 21016.8477\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668729856.0000 - rmse: 25859.8105 - val_loss: 456512576.0000 - val_rmse: 21366.1504\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589051968.0000 - rmse: 24270.3906 - val_loss: 454675552.0000 - val_rmse: 21323.1191\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616342144.0000 - rmse: 24826.2344 - val_loss: 432185312.0000 - val_rmse: 20789.0625\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594491008.0000 - rmse: 24382.1836 - val_loss: 484086752.0000 - val_rmse: 22001.9707\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592784256.0000 - rmse: 24347.1562 - val_loss: 453571040.0000 - val_rmse: 21297.2012\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576524416.0000 - rmse: 24010.9199 - val_loss: 471742496.0000 - val_rmse: 21719.6309\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599372864.0000 - rmse: 24482.0898 - val_loss: 546650240.0000 - val_rmse: 23380.5488\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607010688.0000 - rmse: 24637.5859 - val_loss: 466151616.0000 - val_rmse: 21590.5430\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 639043584.0000 - rmse: 25279.3066 - val_loss: 424418240.0000 - val_rmse: 20601.4082\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622768960.0000 - rmse: 24955.3379 - val_loss: 570630144.0000 - val_rmse: 23887.8633\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632016256.0000 - rmse: 25139.9316 - val_loss: 467041280.0000 - val_rmse: 21611.1348\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562498432.0000 - rmse: 23717.0449 - val_loss: 500440064.0000 - val_rmse: 22370.5156\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598810688.0000 - rmse: 24470.6055 - val_loss: 458674112.0000 - val_rmse: 21416.6738\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598861760.0000 - rmse: 24471.6484 - val_loss: 440704096.0000 - val_rmse: 20992.9512\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589026944.0000 - rmse: 24269.8770 - val_loss: 470372192.0000 - val_rmse: 21688.0625\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610779904.0000 - rmse: 24713.9590 - val_loss: 501997568.0000 - val_rmse: 22405.2988\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592713792.0000 - rmse: 24345.7109 - val_loss: 471308960.0000 - val_rmse: 21709.6484\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544429440.0000 - rmse: 23333.0098 - val_loss: 485554368.0000 - val_rmse: 22035.2969\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628441088.0000 - rmse: 25068.7227 - val_loss: 509195424.0000 - val_rmse: 22565.3555\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562683840.0000 - rmse: 23720.9531 - val_loss: 426945248.0000 - val_rmse: 20662.6504\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587615296.0000 - rmse: 24240.7754 - val_loss: 435914848.0000 - val_rmse: 20878.5723\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554429120.0000 - rmse: 23546.3164 - val_loss: 407312576.0000 - val_rmse: 20181.9824\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552745152.0000 - rmse: 23510.5293 - val_loss: 423943264.0000 - val_rmse: 20589.8789\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559127488.0000 - rmse: 23645.8750 - val_loss: 407778400.0000 - val_rmse: 20193.5215\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527626912.0000 - rmse: 22970.1289 - val_loss: 404397984.0000 - val_rmse: 20109.6465\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534674304.0000 - rmse: 23123.0234 - val_loss: 409344032.0000 - val_rmse: 20232.2500\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554579456.0000 - rmse: 23549.5098 - val_loss: 402451328.0000 - val_rmse: 20061.1855\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623219328.0000 - rmse: 24964.3594 - val_loss: 412025920.0000 - val_rmse: 20298.4199\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527308352.0000 - rmse: 22963.1953 - val_loss: 432027680.0000 - val_rmse: 20785.2734\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495203360.0000 - rmse: 22253.1621 - val_loss: 379678816.0000 - val_rmse: 19485.3457\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493030624.0000 - rmse: 22204.2910 - val_loss: 410916800.0000 - val_rmse: 20271.0781\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542329088.0000 - rmse: 23287.9590 - val_loss: 401793280.0000 - val_rmse: 20044.7793\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547633792.0000 - rmse: 23401.5762 - val_loss: 449396320.0000 - val_rmse: 21198.9668\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569633152.0000 - rmse: 23866.9883 - val_loss: 502174240.0000 - val_rmse: 22409.2402\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580469184.0000 - rmse: 24092.9258 - val_loss: 419994624.0000 - val_rmse: 20493.7676\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495320960.0000 - rmse: 22255.8047 - val_loss: 450097312.0000 - val_rmse: 21215.4941\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520463680.0000 - rmse: 22813.6699 - val_loss: 426982976.0000 - val_rmse: 20663.5645\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521403808.0000 - rmse: 22834.2656 - val_loss: 446175744.0000 - val_rmse: 21122.8691\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537149760.0000 - rmse: 23176.4902 - val_loss: 434688224.0000 - val_rmse: 20849.1738\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579332928.0000 - rmse: 24069.3301 - val_loss: 451481632.0000 - val_rmse: 21248.0938\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525411936.0000 - rmse: 22921.8613 - val_loss: 416201184.0000 - val_rmse: 20401.0039\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566689408.0000 - rmse: 23805.2363 - val_loss: 454117536.0000 - val_rmse: 21310.0312\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500481568.0000 - rmse: 22371.4434 - val_loss: 418861376.0000 - val_rmse: 20466.0977\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515476032.0000 - rmse: 22704.0918 - val_loss: 414215008.0000 - val_rmse: 20352.2695\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502258656.0000 - rmse: 22411.1250 - val_loss: 423804992.0000 - val_rmse: 20586.5215\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509020032.0000 - rmse: 22561.4707 - val_loss: 408643328.0000 - val_rmse: 20214.9277\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486342912.0000 - rmse: 22053.1777 - val_loss: 414289856.0000 - val_rmse: 20354.1074\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484713408.0000 - rmse: 22016.2031 - val_loss: 431459808.0000 - val_rmse: 20771.6074\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448784704.0000 - rmse: 21184.5371 - val_loss: 432246080.0000 - val_rmse: 20790.5273\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470483456.0000 - rmse: 21690.6270 - val_loss: 426768352.0000 - val_rmse: 20658.3711\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504569952.0000 - rmse: 22462.6309 - val_loss: 427581408.0000 - val_rmse: 20678.0371\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498791104.0000 - rmse: 22333.6289 - val_loss: 428493568.0000 - val_rmse: 20700.0840\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468174624.0000 - rmse: 21637.3398 - val_loss: 406919872.0000 - val_rmse: 20172.2500\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480483840.0000 - rmse: 21919.9395 - val_loss: 406447104.0000 - val_rmse: 20160.5293\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519171296.0000 - rmse: 22785.3281 - val_loss: 413327744.0000 - val_rmse: 20330.4609\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434517952.0000 - rmse: 20845.0898 - val_loss: 432309952.0000 - val_rmse: 20792.0625\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502923168.0000 - rmse: 22425.9453 - val_loss: 407918464.0000 - val_rmse: 20196.9883\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446169888.0000 - rmse: 21122.7305 - val_loss: 425350848.0000 - val_rmse: 20624.0332\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496617984.0000 - rmse: 22284.9219 - val_loss: 442070368.0000 - val_rmse: 21025.4648\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523275232.0000 - rmse: 22875.2070 - val_loss: 469590432.0000 - val_rmse: 21670.0332\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512561760.0000 - rmse: 22639.8242 - val_loss: 404259328.0000 - val_rmse: 20106.1992\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467052256.0000 - rmse: 21611.3887 - val_loss: 511222848.0000 - val_rmse: 22610.2363\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493659680.0000 - rmse: 22218.4512 - val_loss: 445736320.0000 - val_rmse: 21112.4668\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489425824.0000 - rmse: 22122.9668 - val_loss: 415827040.0000 - val_rmse: 20391.8320\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472465760.0000 - rmse: 21736.2734 - val_loss: 417561568.0000 - val_rmse: 20434.3203\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495095424.0000 - rmse: 22250.7363 - val_loss: 476987264.0000 - val_rmse: 21840.0332\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475427552.0000 - rmse: 21804.2969 - val_loss: 402861600.0000 - val_rmse: 20071.4102\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493204288.0000 - rmse: 22208.1992 - val_loss: 401106976.0000 - val_rmse: 20027.6504\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448496864.0000 - rmse: 21177.7402 - val_loss: 457594208.0000 - val_rmse: 21391.4473\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465555200.0000 - rmse: 21576.7246 - val_loss: 437055072.0000 - val_rmse: 20905.8574\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433046240.0000 - rmse: 20809.7598 - val_loss: 426428768.0000 - val_rmse: 20650.1504\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483029088.0000 - rmse: 21977.9199 - val_loss: 420901216.0000 - val_rmse: 20515.8750\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446105440.0000 - rmse: 21121.2031 - val_loss: 473330304.0000 - val_rmse: 21756.1504\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475420736.0000 - rmse: 21804.1426 - val_loss: 451372864.0000 - val_rmse: 21245.5332\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504936992.0000 - rmse: 22470.8008 - val_loss: 410485632.0000 - val_rmse: 20260.4434\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480432000.0000 - rmse: 21918.7578 - val_loss: 449633184.0000 - val_rmse: 21204.5527\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413985568.0000 - rmse: 20346.6348 - val_loss: 417941280.0000 - val_rmse: 20443.6074\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436017792.0000 - rmse: 20881.0352 - val_loss: 410208256.0000 - val_rmse: 20253.5977\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439552832.0000 - rmse: 20965.5137 - val_loss: 413700800.0000 - val_rmse: 20339.6328\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431734656.0000 - rmse: 20778.2227 - val_loss: 435593120.0000 - val_rmse: 20870.8652\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437966912.0000 - rmse: 20927.6543 - val_loss: 437344416.0000 - val_rmse: 20912.7754\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428299584.0000 - rmse: 20695.3965 - val_loss: 415349344.0000 - val_rmse: 20380.1172\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436723008.0000 - rmse: 20897.9141 - val_loss: 419817536.0000 - val_rmse: 20489.4453\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401255232.0000 - rmse: 20031.3516 - val_loss: 570983872.0000 - val_rmse: 23895.2676\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445323904.0000 - rmse: 21102.6953 - val_loss: 467735776.0000 - val_rmse: 21627.1953\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438123488.0000 - rmse: 20931.3965 - val_loss: 415436288.0000 - val_rmse: 20382.2500\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392352000.0000 - rmse: 19807.8730 - val_loss: 427434688.0000 - val_rmse: 20674.4883\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438206592.0000 - rmse: 20933.3809 - val_loss: 492684544.0000 - val_rmse: 22196.4941\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453747424.0000 - rmse: 21301.3438 - val_loss: 418559712.0000 - val_rmse: 20458.7266\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416236864.0000 - rmse: 20401.8809 - val_loss: 430317568.0000 - val_rmse: 20744.0938\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419679776.0000 - rmse: 20486.0859 - val_loss: 415100128.0000 - val_rmse: 20374.0000\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420882880.0000 - rmse: 20515.4277 - val_loss: 419111840.0000 - val_rmse: 20472.2148\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393274976.0000 - rmse: 19831.1582 - val_loss: 509256352.0000 - val_rmse: 22566.7031\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444068096.0000 - rmse: 21072.9180 - val_loss: 437465152.0000 - val_rmse: 20915.6621\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354447360.0000 - rmse: 18826.7676 - val_loss: 440946240.0000 - val_rmse: 20998.7168\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384963424.0000 - rmse: 19620.4824 - val_loss: 420344256.0000 - val_rmse: 20502.2949\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432452704.0000 - rmse: 20795.4941 - val_loss: 438399008.0000 - val_rmse: 20937.9746\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431323552.0000 - rmse: 20768.3262 - val_loss: 433536800.0000 - val_rmse: 20821.5410\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389072768.0000 - rmse: 19724.9238 - val_loss: 462595040.0000 - val_rmse: 21508.0176\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434983968.0000 - rmse: 20856.2637 - val_loss: 447532736.0000 - val_rmse: 21154.9648\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377377856.0000 - rmse: 19426.2109 - val_loss: 515093600.0000 - val_rmse: 22695.6680\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389085952.0000 - rmse: 19725.2559 - val_loss: 431244096.0000 - val_rmse: 20766.4160\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432285760.0000 - rmse: 20791.4805 - val_loss: 458287552.0000 - val_rmse: 21407.6484\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419387680.0000 - rmse: 20478.9531 - val_loss: 536541088.0000 - val_rmse: 23163.3535\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366975872.0000 - rmse: 19156.6094 - val_loss: 512245472.0000 - val_rmse: 22632.8379\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407414112.0000 - rmse: 20184.4961 - val_loss: 456052448.0000 - val_rmse: 21355.3809\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441965440.0000 - rmse: 21022.9688 - val_loss: 426554560.0000 - val_rmse: 20653.1934\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366817984.0000 - rmse: 19152.4883 - val_loss: 418762496.0000 - val_rmse: 20463.6816\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375428192.0000 - rmse: 19375.9668 - val_loss: 463002496.0000 - val_rmse: 21517.4883\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370706496.0000 - rmse: 19253.7363 - val_loss: 415171136.0000 - val_rmse: 20375.7441\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382845632.0000 - rmse: 19566.4395 - val_loss: 590214400.0000 - val_rmse: 24294.3242\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501129344.0000 - rmse: 22385.9141 - val_loss: 435590272.0000 - val_rmse: 20870.7949\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399779072.0000 - rmse: 19994.4727 - val_loss: 525322272.0000 - val_rmse: 22919.9043\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488174976.0000 - rmse: 22094.6797 - val_loss: 558961280.0000 - val_rmse: 23642.3574\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391591520.0000 - rmse: 19788.6680 - val_loss: 420628800.0000 - val_rmse: 20509.2324\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441577760.0000 - rmse: 21013.7500 - val_loss: 436999840.0000 - val_rmse: 20904.5371\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393180672.0000 - rmse: 19828.7812 - val_loss: 412546560.0000 - val_rmse: 20311.2383\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404325824.0000 - rmse: 20107.8496 - val_loss: 634091904.0000 - val_rmse: 25181.1797\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386496800.0000 - rmse: 19659.5176 - val_loss: 433409920.0000 - val_rmse: 20818.4961\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395015264.0000 - rmse: 19874.9863 - val_loss: 546319040.0000 - val_rmse: 23373.4668\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402685472.0000 - rmse: 20067.0195 - val_loss: 420042816.0000 - val_rmse: 20494.9414\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359108288.0000 - rmse: 18950.1484 - val_loss: 692007808.0000 - val_rmse: 26306.0410\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361469568.0000 - rmse: 19012.3477 - val_loss: 509514528.0000 - val_rmse: 22572.4238\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375900736.0000 - rmse: 19388.1543 - val_loss: 460815872.0000 - val_rmse: 21466.6172\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383114496.0000 - rmse: 19573.3066 - val_loss: 535103648.0000 - val_rmse: 23132.3047\n",
      "104/104 [==============================] - 0s 718us/step - loss: 389523968.0000 - rmse: 19736.3574\n",
      "[389523968.0, 19736.357421875]\n",
      "[21099.71484375, 32249.796875, 28686.43359375, 24820.3203125, 19736.357421875]\n",
      "25318.524609375\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "!python train.py kfold light\n",
    "# epoch 300 p 20 lr 4e-3 (63 32)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 20:51:46.619774: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 20:51:46.619812: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 20:51:46.620116: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 20:51:47.127334: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 17970907136.0000 - rmse: 134055.6094 - val_loss: 3426963200.0000 - val_rmse: 58540.2695\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2821986560.0000 - rmse: 53122.3750 - val_loss: 1284130048.0000 - val_rmse: 35834.7617\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1907853440.0000 - rmse: 43678.9805 - val_loss: 1064753280.0000 - val_rmse: 32630.5566\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1820235904.0000 - rmse: 42664.2227 - val_loss: 979078912.0000 - val_rmse: 31290.2363\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1719368960.0000 - rmse: 41465.2734 - val_loss: 936186944.0000 - val_rmse: 30597.1699\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1710427392.0000 - rmse: 41357.3125 - val_loss: 911987840.0000 - val_rmse: 30199.1367\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1697372544.0000 - rmse: 41199.1797 - val_loss: 920693440.0000 - val_rmse: 30342.9297\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1604040064.0000 - rmse: 40050.4688 - val_loss: 865520192.0000 - val_rmse: 29419.7246\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1570448640.0000 - rmse: 39628.8867 - val_loss: 878268672.0000 - val_rmse: 29635.5977\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1535089536.0000 - rmse: 39180.2188 - val_loss: 816295488.0000 - val_rmse: 28570.8848\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1539903616.0000 - rmse: 39241.6055 - val_loss: 802145408.0000 - val_rmse: 28322.1699\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1472663680.0000 - rmse: 38375.3008 - val_loss: 890955200.0000 - val_rmse: 29848.8730\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1413891072.0000 - rmse: 37601.7422 - val_loss: 819329728.0000 - val_rmse: 28623.9355\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1363718016.0000 - rmse: 36928.5547 - val_loss: 958594752.0000 - val_rmse: 30961.1816\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1409887360.0000 - rmse: 37548.4688 - val_loss: 759898752.0000 - val_rmse: 27566.2617\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1370111872.0000 - rmse: 37015.0234 - val_loss: 735764224.0000 - val_rmse: 27124.9746\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1323625472.0000 - rmse: 36381.6641 - val_loss: 811390080.0000 - val_rmse: 28484.9102\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1360298752.0000 - rmse: 36882.2266 - val_loss: 744279680.0000 - val_rmse: 27281.4883\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1225892864.0000 - rmse: 35012.7500 - val_loss: 756871168.0000 - val_rmse: 27511.2891\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1261609600.0000 - rmse: 35519.1406 - val_loss: 709137344.0000 - val_rmse: 26629.6309\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1268855936.0000 - rmse: 35621.0039 - val_loss: 741690432.0000 - val_rmse: 27233.9922\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1246876800.0000 - rmse: 35311.1406 - val_loss: 708370752.0000 - val_rmse: 26615.2344\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1202115712.0000 - rmse: 34671.5391 - val_loss: 796619968.0000 - val_rmse: 28224.4551\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1232219904.0000 - rmse: 35102.9883 - val_loss: 711781120.0000 - val_rmse: 26679.2246\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1187368192.0000 - rmse: 34458.2070 - val_loss: 768708096.0000 - val_rmse: 27725.5859\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1198259072.0000 - rmse: 34615.8789 - val_loss: 988067136.0000 - val_rmse: 31433.5312\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1166937984.0000 - rmse: 34160.4727 - val_loss: 1013235008.0000 - val_rmse: 31831.3496\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1096026752.0000 - rmse: 33106.2891 - val_loss: 713663104.0000 - val_rmse: 26714.4707\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1193981696.0000 - rmse: 34554.0391 - val_loss: 740888000.0000 - val_rmse: 27219.2559\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1176425984.0000 - rmse: 34299.0664 - val_loss: 673014976.0000 - val_rmse: 25942.5293\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1119831424.0000 - rmse: 33463.8828 - val_loss: 811757184.0000 - val_rmse: 28491.3496\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1107443456.0000 - rmse: 33278.2734 - val_loss: 691548032.0000 - val_rmse: 26297.3008\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1067881536.0000 - rmse: 32678.4551 - val_loss: 698014912.0000 - val_rmse: 26419.9707\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1021714624.0000 - rmse: 31964.2695 - val_loss: 705650368.0000 - val_rmse: 26564.0801\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1060270656.0000 - rmse: 32561.7949 - val_loss: 662093504.0000 - val_rmse: 25731.1758\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1074431488.0000 - rmse: 32778.5234 - val_loss: 664538496.0000 - val_rmse: 25778.6426\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1064923072.0000 - rmse: 32633.1543 - val_loss: 610124928.0000 - val_rmse: 24700.7051\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 986392768.0000 - rmse: 31406.8906 - val_loss: 694818048.0000 - val_rmse: 26359.3984\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1060814592.0000 - rmse: 32570.1465 - val_loss: 645912512.0000 - val_rmse: 25414.8086\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1041612480.0000 - rmse: 32274.0195 - val_loss: 631565248.0000 - val_rmse: 25130.9590\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 996575104.0000 - rmse: 31568.5781 - val_loss: 580549056.0000 - val_rmse: 24094.5820\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1030344768.0000 - rmse: 32098.9824 - val_loss: 556564416.0000 - val_rmse: 23591.6133\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 969203392.0000 - rmse: 31132.0293 - val_loss: 589644288.0000 - val_rmse: 24282.5918\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 940828800.0000 - rmse: 30672.9297 - val_loss: 635595392.0000 - val_rmse: 25211.0156\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 963456256.0000 - rmse: 31039.5918 - val_loss: 610833920.0000 - val_rmse: 24715.0527\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 989873920.0000 - rmse: 31462.2617 - val_loss: 631620544.0000 - val_rmse: 25132.0605\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 937102592.0000 - rmse: 30612.1289 - val_loss: 560934656.0000 - val_rmse: 23684.0566\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 964262336.0000 - rmse: 31052.5723 - val_loss: 541388224.0000 - val_rmse: 23267.7500\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 969044608.0000 - rmse: 31129.4805 - val_loss: 635986368.0000 - val_rmse: 25218.7695\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 927773632.0000 - rmse: 30459.3750 - val_loss: 635415552.0000 - val_rmse: 25207.4473\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 929125760.0000 - rmse: 30481.5625 - val_loss: 570441856.0000 - val_rmse: 23883.9199\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 895907136.0000 - rmse: 29931.7070 - val_loss: 600785536.0000 - val_rmse: 24510.9258\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 966156416.0000 - rmse: 31083.0547 - val_loss: 513737632.0000 - val_rmse: 22665.7773\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 909779712.0000 - rmse: 30162.5547 - val_loss: 571255488.0000 - val_rmse: 23900.9512\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 849900992.0000 - rmse: 29153.0586 - val_loss: 586466624.0000 - val_rmse: 24217.0684\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 960347264.0000 - rmse: 30989.4707 - val_loss: 489996160.0000 - val_rmse: 22135.8555\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 833779584.0000 - rmse: 28875.2383 - val_loss: 575336960.0000 - val_rmse: 23986.1816\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 896417984.0000 - rmse: 29940.2383 - val_loss: 490218272.0000 - val_rmse: 22140.8691\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 887741888.0000 - rmse: 29794.9961 - val_loss: 465302432.0000 - val_rmse: 21570.8691\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 871726016.0000 - rmse: 29525.0059 - val_loss: 565448384.0000 - val_rmse: 23779.1543\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 903632896.0000 - rmse: 30060.4844 - val_loss: 566376896.0000 - val_rmse: 23798.6738\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 923939456.0000 - rmse: 30396.3711 - val_loss: 474825472.0000 - val_rmse: 21790.4883\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 860800512.0000 - rmse: 29339.4004 - val_loss: 571978240.0000 - val_rmse: 23916.0645\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 836384768.0000 - rmse: 28920.3145 - val_loss: 480478368.0000 - val_rmse: 21919.8145\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 841885440.0000 - rmse: 29015.2617 - val_loss: 447913408.0000 - val_rmse: 21163.9609\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 834162560.0000 - rmse: 28881.8691 - val_loss: 454454432.0000 - val_rmse: 21317.9355\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 881709888.0000 - rmse: 29693.5977 - val_loss: 439236000.0000 - val_rmse: 20957.9551\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 853349120.0000 - rmse: 29212.1367 - val_loss: 617757952.0000 - val_rmse: 24854.7324\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 887352128.0000 - rmse: 29788.4492 - val_loss: 462452864.0000 - val_rmse: 21504.7148\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 870763840.0000 - rmse: 29508.7070 - val_loss: 451062944.0000 - val_rmse: 21238.2383\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 810872000.0000 - rmse: 28475.8125 - val_loss: 497088416.0000 - val_rmse: 22295.4785\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 828957376.0000 - rmse: 28791.6191 - val_loss: 491980512.0000 - val_rmse: 22180.6309\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 715180480.0000 - rmse: 26742.8555 - val_loss: 482647136.0000 - val_rmse: 21969.2266\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 831153408.0000 - rmse: 28829.7305 - val_loss: 492490880.0000 - val_rmse: 22192.1328\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 840299648.0000 - rmse: 28987.9199 - val_loss: 432495392.0000 - val_rmse: 20796.5215\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 796454976.0000 - rmse: 28221.5332 - val_loss: 519243136.0000 - val_rmse: 22786.9062\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 783781056.0000 - rmse: 27996.0898 - val_loss: 512683360.0000 - val_rmse: 22642.5117\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 780505664.0000 - rmse: 27937.5293 - val_loss: 438185792.0000 - val_rmse: 20932.8848\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 825078016.0000 - rmse: 28724.1699 - val_loss: 401570112.0000 - val_rmse: 20039.2090\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 828871680.0000 - rmse: 28790.1289 - val_loss: 439225888.0000 - val_rmse: 20957.7129\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 857909312.0000 - rmse: 29290.0859 - val_loss: 419522816.0000 - val_rmse: 20482.2539\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 758656896.0000 - rmse: 27543.7266 - val_loss: 396618080.0000 - val_rmse: 19915.2695\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 767229760.0000 - rmse: 27698.9102 - val_loss: 407934912.0000 - val_rmse: 20197.3945\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730846464.0000 - rmse: 27034.1699 - val_loss: 402984288.0000 - val_rmse: 20074.4648\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 763060032.0000 - rmse: 27623.5371 - val_loss: 436067904.0000 - val_rmse: 20882.2383\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751000896.0000 - rmse: 27404.3945 - val_loss: 407432768.0000 - val_rmse: 20184.9609\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 810253888.0000 - rmse: 28464.9570 - val_loss: 397544160.0000 - val_rmse: 19938.5078\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 764650112.0000 - rmse: 27652.3047 - val_loss: 416943136.0000 - val_rmse: 20419.1816\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 714478272.0000 - rmse: 26729.7246 - val_loss: 390463744.0000 - val_rmse: 19760.1523\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 820409792.0000 - rmse: 28642.7949 - val_loss: 405728352.0000 - val_rmse: 20142.6973\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 761576832.0000 - rmse: 27596.6816 - val_loss: 391826208.0000 - val_rmse: 19794.5977\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 744030464.0000 - rmse: 27276.9199 - val_loss: 487578304.0000 - val_rmse: 22081.1738\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730198336.0000 - rmse: 27022.1816 - val_loss: 377273280.0000 - val_rmse: 19423.5215\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 788939264.0000 - rmse: 28088.0605 - val_loss: 404527424.0000 - val_rmse: 20112.8633\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 776943680.0000 - rmse: 27873.7090 - val_loss: 480312224.0000 - val_rmse: 21916.0254\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 784088128.0000 - rmse: 28001.5723 - val_loss: 379520512.0000 - val_rmse: 19481.2832\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 738662528.0000 - rmse: 27178.3438 - val_loss: 477634080.0000 - val_rmse: 21854.8359\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733948160.0000 - rmse: 27091.4746 - val_loss: 453790816.0000 - val_rmse: 21302.3652\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 717322240.0000 - rmse: 26782.8672 - val_loss: 372792000.0000 - val_rmse: 19307.8164\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 699085184.0000 - rmse: 26440.2148 - val_loss: 388830880.0000 - val_rmse: 19718.7910\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 693478208.0000 - rmse: 26333.9727 - val_loss: 376866720.0000 - val_rmse: 19413.0527\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691195648.0000 - rmse: 26290.5996 - val_loss: 414363232.0000 - val_rmse: 20355.9102\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 779747840.0000 - rmse: 27923.9609 - val_loss: 359796096.0000 - val_rmse: 18968.2871\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751047232.0000 - rmse: 27405.2363 - val_loss: 392141664.0000 - val_rmse: 19802.5645\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654445696.0000 - rmse: 25582.1328 - val_loss: 394724064.0000 - val_rmse: 19867.6602\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 745978176.0000 - rmse: 27312.5996 - val_loss: 403948320.0000 - val_rmse: 20098.4629\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 737874048.0000 - rmse: 27163.8340 - val_loss: 377866976.0000 - val_rmse: 19438.7988\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 759302144.0000 - rmse: 27555.4336 - val_loss: 672877440.0000 - val_rmse: 25939.8789\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 725225024.0000 - rmse: 26930.0020 - val_loss: 359766528.0000 - val_rmse: 18967.5098\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 743096576.0000 - rmse: 27259.7949 - val_loss: 350549312.0000 - val_rmse: 18722.9590\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718152832.0000 - rmse: 26798.3711 - val_loss: 338320864.0000 - val_rmse: 18393.4980\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668358912.0000 - rmse: 25852.6367 - val_loss: 340274720.0000 - val_rmse: 18446.5332\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665053952.0000 - rmse: 25788.6367 - val_loss: 381777728.0000 - val_rmse: 19539.1309\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 749367232.0000 - rmse: 27374.5703 - val_loss: 338784832.0000 - val_rmse: 18406.1055\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 749688448.0000 - rmse: 27380.4336 - val_loss: 367249216.0000 - val_rmse: 19163.7441\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 673630400.0000 - rmse: 25954.3867 - val_loss: 401060864.0000 - val_rmse: 20026.5000\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590488768.0000 - rmse: 24299.9707 - val_loss: 378888512.0000 - val_rmse: 19465.0547\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 693213504.0000 - rmse: 26328.9414 - val_loss: 387586336.0000 - val_rmse: 19687.2090\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 717023552.0000 - rmse: 26777.2930 - val_loss: 377442240.0000 - val_rmse: 19427.8711\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642856192.0000 - rmse: 25354.6055 - val_loss: 517608960.0000 - val_rmse: 22751.0215\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607357568.0000 - rmse: 24644.6250 - val_loss: 373378624.0000 - val_rmse: 19323.0039\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 673179200.0000 - rmse: 25945.6953 - val_loss: 328695072.0000 - val_rmse: 18129.9453\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665210304.0000 - rmse: 25791.6680 - val_loss: 326396928.0000 - val_rmse: 18066.4570\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 669998848.0000 - rmse: 25884.3320 - val_loss: 349655232.0000 - val_rmse: 18699.0664\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618979456.0000 - rmse: 24879.2969 - val_loss: 385910144.0000 - val_rmse: 19644.5898\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630503040.0000 - rmse: 25109.8164 - val_loss: 367545728.0000 - val_rmse: 19171.4805\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 617710656.0000 - rmse: 24853.7832 - val_loss: 366620576.0000 - val_rmse: 19147.3320\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 686624832.0000 - rmse: 26203.5254 - val_loss: 326567168.0000 - val_rmse: 18071.1660\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 660588352.0000 - rmse: 25701.9082 - val_loss: 321525312.0000 - val_rmse: 17931.1250\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642808384.0000 - rmse: 25353.6641 - val_loss: 416252384.0000 - val_rmse: 20402.2617\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 638182080.0000 - rmse: 25262.2656 - val_loss: 346618496.0000 - val_rmse: 18617.6914\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629107008.0000 - rmse: 25082.0059 - val_loss: 342991936.0000 - val_rmse: 18520.0371\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629982336.0000 - rmse: 25099.4453 - val_loss: 310845440.0000 - val_rmse: 17630.8066\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586799040.0000 - rmse: 24223.9355 - val_loss: 316587136.0000 - val_rmse: 17792.8906\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 690525312.0000 - rmse: 26277.8477 - val_loss: 311357824.0000 - val_rmse: 17645.3301\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 669067392.0000 - rmse: 25866.3359 - val_loss: 348914528.0000 - val_rmse: 18679.2500\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594966080.0000 - rmse: 24391.9238 - val_loss: 301972256.0000 - val_rmse: 17377.3438\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573064320.0000 - rmse: 23938.7598 - val_loss: 319089056.0000 - val_rmse: 17863.0586\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 636134720.0000 - rmse: 25221.7070 - val_loss: 348171552.0000 - val_rmse: 18659.3516\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589499648.0000 - rmse: 24279.6094 - val_loss: 294823104.0000 - val_rmse: 17170.4062\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624872128.0000 - rmse: 24997.4395 - val_loss: 311444256.0000 - val_rmse: 17647.7793\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572030656.0000 - rmse: 23917.1582 - val_loss: 356643296.0000 - val_rmse: 18884.9961\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580762688.0000 - rmse: 24099.0176 - val_loss: 321501152.0000 - val_rmse: 17930.4473\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562009792.0000 - rmse: 23706.7422 - val_loss: 365474112.0000 - val_rmse: 19117.3730\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587631104.0000 - rmse: 24241.0996 - val_loss: 401649248.0000 - val_rmse: 20041.1855\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 638608960.0000 - rmse: 25270.7109 - val_loss: 454745632.0000 - val_rmse: 21324.7598\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 600067968.0000 - rmse: 24496.2812 - val_loss: 338317728.0000 - val_rmse: 18393.4121\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607617216.0000 - rmse: 24649.8906 - val_loss: 293106016.0000 - val_rmse: 17120.3340\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 625941632.0000 - rmse: 25018.8223 - val_loss: 326444896.0000 - val_rmse: 18067.7812\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630076224.0000 - rmse: 25101.3184 - val_loss: 325309888.0000 - val_rmse: 18036.3457\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 703861376.0000 - rmse: 26530.3828 - val_loss: 304390432.0000 - val_rmse: 17446.7871\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569978112.0000 - rmse: 23874.2109 - val_loss: 393041760.0000 - val_rmse: 19825.2754\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628598912.0000 - rmse: 25071.8730 - val_loss: 274619552.0000 - val_rmse: 16571.6426\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582041344.0000 - rmse: 24125.5293 - val_loss: 309130112.0000 - val_rmse: 17582.0918\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531869472.0000 - rmse: 23062.2910 - val_loss: 320729632.0000 - val_rmse: 17908.9238\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560365440.0000 - rmse: 23672.0371 - val_loss: 340781312.0000 - val_rmse: 18460.2578\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532784704.0000 - rmse: 23082.1289 - val_loss: 302079840.0000 - val_rmse: 17380.4395\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527138496.0000 - rmse: 22959.4922 - val_loss: 303805408.0000 - val_rmse: 17430.0098\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558293760.0000 - rmse: 23628.2402 - val_loss: 274196000.0000 - val_rmse: 16558.8613\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533515968.0000 - rmse: 23097.9629 - val_loss: 388011904.0000 - val_rmse: 19698.0117\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601559360.0000 - rmse: 24526.7051 - val_loss: 293124736.0000 - val_rmse: 17120.8809\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565206528.0000 - rmse: 23774.0703 - val_loss: 279100480.0000 - val_rmse: 16706.2949\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563976896.0000 - rmse: 23748.1934 - val_loss: 296747712.0000 - val_rmse: 17226.3613\n",
      "Epoch 164/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553690624.0000 - rmse: 23530.6270 - val_loss: 277298336.0000 - val_rmse: 16652.2734\n",
      "Epoch 165/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523606368.0000 - rmse: 22882.4453 - val_loss: 310105728.0000 - val_rmse: 17609.8125\n",
      "Epoch 166/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557528512.0000 - rmse: 23612.0391 - val_loss: 299740160.0000 - val_rmse: 17312.9980\n",
      "Epoch 167/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546007680.0000 - rmse: 23366.8047 - val_loss: 287828448.0000 - val_rmse: 16965.5020\n",
      "Epoch 168/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572748864.0000 - rmse: 23932.1660 - val_loss: 309700096.0000 - val_rmse: 17598.2949\n",
      "Epoch 169/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577234432.0000 - rmse: 24025.6992 - val_loss: 291299136.0000 - val_rmse: 17067.4824\n",
      "Epoch 170/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553261440.0000 - rmse: 23521.5078 - val_loss: 357874432.0000 - val_rmse: 18917.5664\n",
      "Epoch 171/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586248256.0000 - rmse: 24212.5625 - val_loss: 482420480.0000 - val_rmse: 21964.0703\n",
      "Epoch 172/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512490432.0000 - rmse: 22638.2500 - val_loss: 308376288.0000 - val_rmse: 17560.6406\n",
      "Epoch 173/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491533152.0000 - rmse: 22170.5449 - val_loss: 311530976.0000 - val_rmse: 17650.2344\n",
      "Epoch 174/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554097984.0000 - rmse: 23539.2793 - val_loss: 284026080.0000 - val_rmse: 16853.0684\n",
      "Epoch 175/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532876032.0000 - rmse: 23084.1035 - val_loss: 291516288.0000 - val_rmse: 17073.8438\n",
      "Epoch 176/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496603456.0000 - rmse: 22284.5996 - val_loss: 302914304.0000 - val_rmse: 17404.4277\n",
      "Epoch 177/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502322816.0000 - rmse: 22412.5566 - val_loss: 268716256.0000 - val_rmse: 16392.5605\n",
      "Epoch 178/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556723392.0000 - rmse: 23594.9824 - val_loss: 289482752.0000 - val_rmse: 17014.1875\n",
      "Epoch 179/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479920480.0000 - rmse: 21907.0840 - val_loss: 272745920.0000 - val_rmse: 16515.0137\n",
      "Epoch 180/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479475552.0000 - rmse: 21896.9258 - val_loss: 285453344.0000 - val_rmse: 16895.3574\n",
      "Epoch 181/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521556384.0000 - rmse: 22837.6055 - val_loss: 393101856.0000 - val_rmse: 19826.7891\n",
      "Epoch 182/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479536064.0000 - rmse: 21898.3086 - val_loss: 463649344.0000 - val_rmse: 21532.5137\n",
      "Epoch 183/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490703104.0000 - rmse: 22151.8164 - val_loss: 260065264.0000 - val_rmse: 16126.5332\n",
      "Epoch 184/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460655808.0000 - rmse: 21462.8906 - val_loss: 286956384.0000 - val_rmse: 16939.7832\n",
      "Epoch 185/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492643488.0000 - rmse: 22195.5684 - val_loss: 285410272.0000 - val_rmse: 16894.0859\n",
      "Epoch 186/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505307712.0000 - rmse: 22479.0449 - val_loss: 371185696.0000 - val_rmse: 19266.1758\n",
      "Epoch 187/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461265728.0000 - rmse: 21477.0938 - val_loss: 366429888.0000 - val_rmse: 19142.3555\n",
      "Epoch 188/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518749824.0000 - rmse: 22776.0781 - val_loss: 326345568.0000 - val_rmse: 18065.0332\n",
      "Epoch 189/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566103872.0000 - rmse: 23792.9355 - val_loss: 279867712.0000 - val_rmse: 16729.2402\n",
      "Epoch 190/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463602752.0000 - rmse: 21531.4297 - val_loss: 295079584.0000 - val_rmse: 17177.8770\n",
      "Epoch 191/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507680288.0000 - rmse: 22531.7578 - val_loss: 343858944.0000 - val_rmse: 18543.4297\n",
      "Epoch 192/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564910144.0000 - rmse: 23767.8359 - val_loss: 389887680.0000 - val_rmse: 19745.5684\n",
      "Epoch 193/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578911808.0000 - rmse: 24060.5820 - val_loss: 277842336.0000 - val_rmse: 16668.5977\n",
      "Epoch 194/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446241280.0000 - rmse: 21124.4199 - val_loss: 275706304.0000 - val_rmse: 16604.4004\n",
      "Epoch 195/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524944384.0000 - rmse: 22911.6621 - val_loss: 319278368.0000 - val_rmse: 17868.3555\n",
      "Epoch 196/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487888768.0000 - rmse: 22088.1973 - val_loss: 336822464.0000 - val_rmse: 18352.7188\n",
      "Epoch 197/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492946912.0000 - rmse: 22202.4043 - val_loss: 287884576.0000 - val_rmse: 16967.1562\n",
      "Epoch 198/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542121088.0000 - rmse: 23283.4883 - val_loss: 277772160.0000 - val_rmse: 16666.4941\n",
      "Epoch 199/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466144672.0000 - rmse: 21590.3809 - val_loss: 336407488.0000 - val_rmse: 18341.4082\n",
      "Epoch 200/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442545952.0000 - rmse: 21036.7734 - val_loss: 312837824.0000 - val_rmse: 17687.2168\n",
      "Epoch 201/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469224768.0000 - rmse: 21661.5898 - val_loss: 293883552.0000 - val_rmse: 17143.0293\n",
      "Epoch 202/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 528889504.0000 - rmse: 22997.5938 - val_loss: 279489056.0000 - val_rmse: 16717.9199\n",
      "Epoch 203/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469996448.0000 - rmse: 21679.3965 - val_loss: 318868992.0000 - val_rmse: 17856.8965\n",
      "Epoch 204/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441236544.0000 - rmse: 21005.6289 - val_loss: 298018816.0000 - val_rmse: 17263.2188\n",
      "Epoch 205/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511406336.0000 - rmse: 22614.2910 - val_loss: 314103840.0000 - val_rmse: 17722.9688\n",
      "Epoch 206/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541057280.0000 - rmse: 23260.6348 - val_loss: 314201952.0000 - val_rmse: 17725.7344\n",
      "Epoch 207/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468159456.0000 - rmse: 21636.9883 - val_loss: 320553536.0000 - val_rmse: 17904.0059\n",
      "Epoch 208/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477624352.0000 - rmse: 21854.6152 - val_loss: 295523648.0000 - val_rmse: 17190.7969\n",
      "Epoch 209/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446351040.0000 - rmse: 21127.0156 - val_loss: 304167456.0000 - val_rmse: 17440.3906\n",
      "Epoch 210/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448702816.0000 - rmse: 21182.6016 - val_loss: 299671520.0000 - val_rmse: 17311.0176\n",
      "Epoch 211/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480662400.0000 - rmse: 21924.0098 - val_loss: 268973216.0000 - val_rmse: 16400.3984\n",
      "Epoch 212/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447340256.0000 - rmse: 21150.4141 - val_loss: 401122272.0000 - val_rmse: 20028.0332\n",
      "Epoch 213/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425124288.0000 - rmse: 20618.5371 - val_loss: 314965056.0000 - val_rmse: 17747.2520\n",
      "Epoch 214/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471648896.0000 - rmse: 21717.4746 - val_loss: 314538976.0000 - val_rmse: 17735.2402\n",
      "Epoch 215/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481269312.0000 - rmse: 21937.8477 - val_loss: 278144672.0000 - val_rmse: 16677.6660\n",
      "Epoch 216/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465187904.0000 - rmse: 21568.2109 - val_loss: 265973504.0000 - val_rmse: 16308.6855\n",
      "Epoch 217/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393145120.0000 - rmse: 19827.8828 - val_loss: 323687424.0000 - val_rmse: 17991.3105\n",
      "Epoch 218/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489587264.0000 - rmse: 22126.6152 - val_loss: 306224288.0000 - val_rmse: 17499.2617\n",
      "Epoch 219/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446879072.0000 - rmse: 21139.5098 - val_loss: 277245184.0000 - val_rmse: 16650.6758\n",
      "Epoch 220/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437952096.0000 - rmse: 20927.3008 - val_loss: 250957264.0000 - val_rmse: 15841.6250\n",
      "Epoch 221/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431211264.0000 - rmse: 20765.6230 - val_loss: 267492304.0000 - val_rmse: 16355.1875\n",
      "Epoch 222/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430547264.0000 - rmse: 20749.6289 - val_loss: 285147424.0000 - val_rmse: 16886.3027\n",
      "Epoch 223/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456420224.0000 - rmse: 21363.9922 - val_loss: 271913696.0000 - val_rmse: 16489.7988\n",
      "Epoch 224/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449541248.0000 - rmse: 21202.3828 - val_loss: 345918240.0000 - val_rmse: 18598.8672\n",
      "Epoch 225/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408726240.0000 - rmse: 20216.9766 - val_loss: 331376544.0000 - val_rmse: 18203.7441\n",
      "Epoch 226/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424460032.0000 - rmse: 20602.4199 - val_loss: 413105408.0000 - val_rmse: 20324.9883\n",
      "Epoch 227/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421775168.0000 - rmse: 20537.1602 - val_loss: 270901696.0000 - val_rmse: 16459.0859\n",
      "Epoch 228/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442411360.0000 - rmse: 21033.5723 - val_loss: 309213376.0000 - val_rmse: 17584.4570\n",
      "Epoch 229/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474705760.0000 - rmse: 21787.7402 - val_loss: 378757536.0000 - val_rmse: 19461.6875\n",
      "Epoch 230/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458521056.0000 - rmse: 21413.0996 - val_loss: 292691488.0000 - val_rmse: 17108.2227\n",
      "Epoch 231/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436906304.0000 - rmse: 20902.3008 - val_loss: 274520384.0000 - val_rmse: 16568.6504\n",
      "Epoch 232/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432283360.0000 - rmse: 20791.4199 - val_loss: 279033408.0000 - val_rmse: 16704.2871\n",
      "Epoch 233/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418606336.0000 - rmse: 20459.8672 - val_loss: 281719040.0000 - val_rmse: 16784.4805\n",
      "Epoch 234/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414519008.0000 - rmse: 20359.7363 - val_loss: 388430304.0000 - val_rmse: 19708.6289\n",
      "Epoch 235/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406741984.0000 - rmse: 20167.8398 - val_loss: 289577152.0000 - val_rmse: 17016.9609\n",
      "Epoch 236/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419284992.0000 - rmse: 20476.4434 - val_loss: 552260288.0000 - val_rmse: 23500.2168\n",
      "Epoch 237/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418945376.0000 - rmse: 20468.1504 - val_loss: 285525952.0000 - val_rmse: 16897.5059\n",
      "104/104 [==============================] - 0s 694us/step - loss: 522288896.0000 - rmse: 22853.6367\n",
      "[522288896.0, 22853.63671875]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 15865152512.0000 - rmse: 125956.9453 - val_loss: 2437162240.0000 - val_rmse: 49367.6250\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2287486464.0000 - rmse: 47827.6758 - val_loss: 1334424704.0000 - val_rmse: 36529.7773\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1658049280.0000 - rmse: 40719.1523 - val_loss: 1218899584.0000 - val_rmse: 34912.7422\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1539463424.0000 - rmse: 39235.9961 - val_loss: 1139106688.0000 - val_rmse: 33750.6562\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1504195712.0000 - rmse: 38783.9609 - val_loss: 1137592448.0000 - val_rmse: 33728.2148\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1416527872.0000 - rmse: 37636.7891 - val_loss: 1241577600.0000 - val_rmse: 35236.0273\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1342448512.0000 - rmse: 36639.4375 - val_loss: 1088567296.0000 - val_rmse: 32993.4414\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1348176512.0000 - rmse: 36717.5234 - val_loss: 1191381760.0000 - val_rmse: 34516.3984\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1340567680.0000 - rmse: 36613.7617 - val_loss: 1016322688.0000 - val_rmse: 31879.8164\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1276930944.0000 - rmse: 35734.1719 - val_loss: 1022947200.0000 - val_rmse: 31983.5449\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1241801856.0000 - rmse: 35239.2109 - val_loss: 948806656.0000 - val_rmse: 30802.7051\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1208859520.0000 - rmse: 34768.6562 - val_loss: 935607936.0000 - val_rmse: 30587.7090\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1133521152.0000 - rmse: 33667.8047 - val_loss: 923234624.0000 - val_rmse: 30384.7754\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1176766336.0000 - rmse: 34304.0273 - val_loss: 887815872.0000 - val_rmse: 29796.2383\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1125541632.0000 - rmse: 33549.0898 - val_loss: 861592320.0000 - val_rmse: 29352.8926\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1081840384.0000 - rmse: 32891.3438 - val_loss: 849792064.0000 - val_rmse: 29151.1934\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1067559040.0000 - rmse: 32673.5215 - val_loss: 834320128.0000 - val_rmse: 28884.5977\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1047937024.0000 - rmse: 32371.8555 - val_loss: 850352192.0000 - val_rmse: 29160.7969\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1057112000.0000 - rmse: 32513.2598 - val_loss: 978718592.0000 - val_rmse: 31284.4785\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1089312384.0000 - rmse: 33004.7344 - val_loss: 799669824.0000 - val_rmse: 28278.4336\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1030229056.0000 - rmse: 32097.1816 - val_loss: 771084608.0000 - val_rmse: 27768.4082\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 974179456.0000 - rmse: 31211.8477 - val_loss: 790485952.0000 - val_rmse: 28115.5820\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 976782848.0000 - rmse: 31253.5254 - val_loss: 739212736.0000 - val_rmse: 27188.4668\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1040370816.0000 - rmse: 32254.7793 - val_loss: 869649856.0000 - val_rmse: 29489.8262\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 940908992.0000 - rmse: 30674.2402 - val_loss: 740791744.0000 - val_rmse: 27217.4883\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 944621504.0000 - rmse: 30734.6934 - val_loss: 712923584.0000 - val_rmse: 26700.6289\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 942872128.0000 - rmse: 30706.2227 - val_loss: 701374336.0000 - val_rmse: 26483.4707\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 875843328.0000 - rmse: 29594.6484 - val_loss: 676689216.0000 - val_rmse: 26013.2500\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1003881856.0000 - rmse: 31684.0938 - val_loss: 682566912.0000 - val_rmse: 26125.9824\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 898122624.0000 - rmse: 29968.6934 - val_loss: 720518656.0000 - val_rmse: 26842.4766\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 910294848.0000 - rmse: 30171.0898 - val_loss: 641969536.0000 - val_rmse: 25337.1152\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 899638080.0000 - rmse: 29993.9668 - val_loss: 731787200.0000 - val_rmse: 27051.5664\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 899586560.0000 - rmse: 29993.1055 - val_loss: 656436736.0000 - val_rmse: 25621.0195\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 854485440.0000 - rmse: 29231.5820 - val_loss: 646285824.0000 - val_rmse: 25422.1504\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 803476288.0000 - rmse: 28345.6562 - val_loss: 632165248.0000 - val_rmse: 25142.8965\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 841115584.0000 - rmse: 29001.9922 - val_loss: 723364480.0000 - val_rmse: 26895.4355\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 860251328.0000 - rmse: 29330.0391 - val_loss: 649322816.0000 - val_rmse: 25481.8125\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 760969664.0000 - rmse: 27585.6777 - val_loss: 771060608.0000 - val_rmse: 27767.9766\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 819154560.0000 - rmse: 28620.8750 - val_loss: 601745472.0000 - val_rmse: 24530.5000\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 812142592.0000 - rmse: 28498.1152 - val_loss: 599959040.0000 - val_rmse: 24494.0605\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 833901696.0000 - rmse: 28877.3535 - val_loss: 602121664.0000 - val_rmse: 24538.1660\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 806417984.0000 - rmse: 28397.5000 - val_loss: 702157248.0000 - val_rmse: 26498.2461\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 834758272.0000 - rmse: 28892.1816 - val_loss: 582604608.0000 - val_rmse: 24137.2012\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 787324928.0000 - rmse: 28059.3105 - val_loss: 597289536.0000 - val_rmse: 24439.5059\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 748396992.0000 - rmse: 27356.8457 - val_loss: 579125440.0000 - val_rmse: 24065.0234\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 781918656.0000 - rmse: 27962.8066 - val_loss: 577995840.0000 - val_rmse: 24041.5430\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 754716864.0000 - rmse: 27472.1094 - val_loss: 566920832.0000 - val_rmse: 23810.0977\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742288064.0000 - rmse: 27244.9609 - val_loss: 590886976.0000 - val_rmse: 24308.1660\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 708136000.0000 - rmse: 26610.8242 - val_loss: 564703552.0000 - val_rmse: 23763.4922\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 728813440.0000 - rmse: 26996.5430 - val_loss: 619986112.0000 - val_rmse: 24899.5195\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 744740992.0000 - rmse: 27289.9414 - val_loss: 573583552.0000 - val_rmse: 23949.6035\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 726939904.0000 - rmse: 26961.8223 - val_loss: 562147392.0000 - val_rmse: 23709.6465\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718304064.0000 - rmse: 26801.1934 - val_loss: 556435776.0000 - val_rmse: 23588.8887\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680408704.0000 - rmse: 26084.6445 - val_loss: 553563392.0000 - val_rmse: 23527.9258\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 700869376.0000 - rmse: 26473.9336 - val_loss: 561282048.0000 - val_rmse: 23691.3906\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 717587072.0000 - rmse: 26787.8164 - val_loss: 529056000.0000 - val_rmse: 23001.2148\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 759359488.0000 - rmse: 27556.4766 - val_loss: 571062592.0000 - val_rmse: 23896.9141\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661028608.0000 - rmse: 25710.4746 - val_loss: 552134336.0000 - val_rmse: 23497.5371\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 653770560.0000 - rmse: 25568.9355 - val_loss: 559656576.0000 - val_rmse: 23657.0605\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 635137408.0000 - rmse: 25201.9336 - val_loss: 526007680.0000 - val_rmse: 22934.8555\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 695604928.0000 - rmse: 26374.3242 - val_loss: 529013984.0000 - val_rmse: 23000.3027\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 660461120.0000 - rmse: 25699.4375 - val_loss: 594728384.0000 - val_rmse: 24387.0527\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665270336.0000 - rmse: 25792.8340 - val_loss: 527880160.0000 - val_rmse: 22975.6426\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 631225152.0000 - rmse: 25124.1953 - val_loss: 536880960.0000 - val_rmse: 23170.6914\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666394560.0000 - rmse: 25814.6172 - val_loss: 506630144.0000 - val_rmse: 22508.4434\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652368384.0000 - rmse: 25541.5020 - val_loss: 510537376.0000 - val_rmse: 22595.0723\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630317632.0000 - rmse: 25106.1270 - val_loss: 508195040.0000 - val_rmse: 22543.1797\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628391168.0000 - rmse: 25067.7324 - val_loss: 507584544.0000 - val_rmse: 22529.6348\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 662307456.0000 - rmse: 25735.3320 - val_loss: 511697376.0000 - val_rmse: 22620.7266\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642312192.0000 - rmse: 25343.8770 - val_loss: 500273728.0000 - val_rmse: 22366.7969\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614860160.0000 - rmse: 24796.3730 - val_loss: 503153120.0000 - val_rmse: 22431.0742\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565104512.0000 - rmse: 23771.9258 - val_loss: 512654400.0000 - val_rmse: 22641.8711\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592822720.0000 - rmse: 24347.9492 - val_loss: 485972288.0000 - val_rmse: 22044.7773\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654398912.0000 - rmse: 25581.2188 - val_loss: 501041888.0000 - val_rmse: 22383.9648\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598064128.0000 - rmse: 24455.3477 - val_loss: 485269408.0000 - val_rmse: 22028.8281\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550379840.0000 - rmse: 23460.1738 - val_loss: 522397152.0000 - val_rmse: 22856.0078\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614694464.0000 - rmse: 24793.0293 - val_loss: 493170976.0000 - val_rmse: 22207.4512\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592595392.0000 - rmse: 24343.2832 - val_loss: 488337792.0000 - val_rmse: 22098.3652\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606336640.0000 - rmse: 24623.9043 - val_loss: 496150016.0000 - val_rmse: 22274.4238\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601479040.0000 - rmse: 24525.0664 - val_loss: 513021440.0000 - val_rmse: 22649.9727\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569885696.0000 - rmse: 23872.2793 - val_loss: 472223680.0000 - val_rmse: 21730.7070\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 613320512.0000 - rmse: 24765.3066 - val_loss: 493158912.0000 - val_rmse: 22207.1816\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624494272.0000 - rmse: 24989.8809 - val_loss: 481872960.0000 - val_rmse: 21951.6035\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591754688.0000 - rmse: 24326.0078 - val_loss: 475112640.0000 - val_rmse: 21797.0762\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561814464.0000 - rmse: 23702.6211 - val_loss: 479351936.0000 - val_rmse: 21894.1055\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604903104.0000 - rmse: 24594.7773 - val_loss: 476807296.0000 - val_rmse: 21835.9141\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565456384.0000 - rmse: 23779.3242 - val_loss: 485040352.0000 - val_rmse: 22023.6289\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562516480.0000 - rmse: 23717.4297 - val_loss: 469893408.0000 - val_rmse: 21677.0254\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545440896.0000 - rmse: 23354.6758 - val_loss: 484261568.0000 - val_rmse: 22005.9434\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576088256.0000 - rmse: 24001.8379 - val_loss: 472866752.0000 - val_rmse: 21745.4980\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 581061888.0000 - rmse: 24105.2246 - val_loss: 447000096.0000 - val_rmse: 21142.3750\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537727936.0000 - rmse: 23188.9609 - val_loss: 474724320.0000 - val_rmse: 21788.1680\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579147392.0000 - rmse: 24065.4805 - val_loss: 478710976.0000 - val_rmse: 21879.4629\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555301120.0000 - rmse: 23564.8262 - val_loss: 446255200.0000 - val_rmse: 21124.7500\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582782080.0000 - rmse: 24140.8789 - val_loss: 447131072.0000 - val_rmse: 21145.4727\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554604608.0000 - rmse: 23550.0430 - val_loss: 458671840.0000 - val_rmse: 21416.6211\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565699328.0000 - rmse: 23784.4336 - val_loss: 464085664.0000 - val_rmse: 21542.6445\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539106176.0000 - rmse: 23218.6562 - val_loss: 477347392.0000 - val_rmse: 21848.2793\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562960000.0000 - rmse: 23726.7754 - val_loss: 454576096.0000 - val_rmse: 21320.7871\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557123264.0000 - rmse: 23603.4551 - val_loss: 431750912.0000 - val_rmse: 20778.6152\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596449024.0000 - rmse: 24422.3027 - val_loss: 439718752.0000 - val_rmse: 20969.4707\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550088512.0000 - rmse: 23453.9648 - val_loss: 448880096.0000 - val_rmse: 21186.7871\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573458560.0000 - rmse: 23946.9941 - val_loss: 436181312.0000 - val_rmse: 20884.9512\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560073152.0000 - rmse: 23665.8652 - val_loss: 436732384.0000 - val_rmse: 20898.1406\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524301568.0000 - rmse: 22897.6309 - val_loss: 439780096.0000 - val_rmse: 20970.9316\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523167520.0000 - rmse: 22872.8516 - val_loss: 441340192.0000 - val_rmse: 21008.0977\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533956800.0000 - rmse: 23107.5020 - val_loss: 437728384.0000 - val_rmse: 20921.9590\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524701120.0000 - rmse: 22906.3535 - val_loss: 435880544.0000 - val_rmse: 20877.7520\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507769760.0000 - rmse: 22533.7461 - val_loss: 434630272.0000 - val_rmse: 20847.7871\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529618496.0000 - rmse: 23013.4395 - val_loss: 449907520.0000 - val_rmse: 21211.0215\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516717664.0000 - rmse: 22731.4219 - val_loss: 424631456.0000 - val_rmse: 20606.5859\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538175872.0000 - rmse: 23198.6172 - val_loss: 427230592.0000 - val_rmse: 20669.5547\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537163648.0000 - rmse: 23176.7891 - val_loss: 436847968.0000 - val_rmse: 20900.9062\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482852640.0000 - rmse: 21973.9082 - val_loss: 466619168.0000 - val_rmse: 21601.3672\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518202976.0000 - rmse: 22764.0723 - val_loss: 455942464.0000 - val_rmse: 21352.8086\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518582752.0000 - rmse: 22772.4121 - val_loss: 423375904.0000 - val_rmse: 20576.0977\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471143680.0000 - rmse: 21705.8438 - val_loss: 431296288.0000 - val_rmse: 20767.6719\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533527776.0000 - rmse: 23098.2148 - val_loss: 428811648.0000 - val_rmse: 20707.7656\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490987104.0000 - rmse: 22158.2266 - val_loss: 420108960.0000 - val_rmse: 20496.5566\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519132640.0000 - rmse: 22784.4785 - val_loss: 451203488.0000 - val_rmse: 21241.5488\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500359424.0000 - rmse: 22368.7109 - val_loss: 463253664.0000 - val_rmse: 21523.3281\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488809952.0000 - rmse: 22109.0469 - val_loss: 413214432.0000 - val_rmse: 20327.6738\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491728032.0000 - rmse: 22174.9395 - val_loss: 452230688.0000 - val_rmse: 21265.7148\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485141216.0000 - rmse: 22025.9180 - val_loss: 443399840.0000 - val_rmse: 21057.0586\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471578176.0000 - rmse: 21715.8496 - val_loss: 430007840.0000 - val_rmse: 20736.6289\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473538496.0000 - rmse: 21760.9395 - val_loss: 442797152.0000 - val_rmse: 21042.7441\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481126240.0000 - rmse: 21934.5879 - val_loss: 426420256.0000 - val_rmse: 20649.9434\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491614784.0000 - rmse: 22172.3848 - val_loss: 440064800.0000 - val_rmse: 20977.7188\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485771296.0000 - rmse: 22040.2188 - val_loss: 415466944.0000 - val_rmse: 20383.0039\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478182176.0000 - rmse: 21867.3730 - val_loss: 456832832.0000 - val_rmse: 21373.6465\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438701984.0000 - rmse: 20945.2109 - val_loss: 434797312.0000 - val_rmse: 20851.7910\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460304160.0000 - rmse: 21454.6973 - val_loss: 411353856.0000 - val_rmse: 20281.8594\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487086720.0000 - rmse: 22070.0391 - val_loss: 430594976.0000 - val_rmse: 20750.7793\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470039360.0000 - rmse: 21680.3887 - val_loss: 403559360.0000 - val_rmse: 20088.7832\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453692352.0000 - rmse: 21300.0527 - val_loss: 452262624.0000 - val_rmse: 21266.4629\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489098080.0000 - rmse: 22115.5605 - val_loss: 409024992.0000 - val_rmse: 20224.3633\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447485056.0000 - rmse: 21153.8379 - val_loss: 398831840.0000 - val_rmse: 19970.7715\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462209248.0000 - rmse: 21499.0508 - val_loss: 440672992.0000 - val_rmse: 20992.2109\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448202976.0000 - rmse: 21170.8008 - val_loss: 405330720.0000 - val_rmse: 20132.8223\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427359328.0000 - rmse: 20672.6699 - val_loss: 533581024.0000 - val_rmse: 23099.3711\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473208256.0000 - rmse: 21753.3477 - val_loss: 432262432.0000 - val_rmse: 20790.9199\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465134976.0000 - rmse: 21566.9863 - val_loss: 403897504.0000 - val_rmse: 20097.1992\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466957984.0000 - rmse: 21609.2090 - val_loss: 422013344.0000 - val_rmse: 20542.9609\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434207072.0000 - rmse: 20837.6348 - val_loss: 433025440.0000 - val_rmse: 20809.2617\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450648832.0000 - rmse: 21228.4902 - val_loss: 466790400.0000 - val_rmse: 21605.3301\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458543936.0000 - rmse: 21413.6367 - val_loss: 439114688.0000 - val_rmse: 20955.0605\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474152064.0000 - rmse: 21775.0293 - val_loss: 402663360.0000 - val_rmse: 20066.4727\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429230560.0000 - rmse: 20717.8770 - val_loss: 416976896.0000 - val_rmse: 20420.0098\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421340576.0000 - rmse: 20526.5781 - val_loss: 474443776.0000 - val_rmse: 21781.7266\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429069984.0000 - rmse: 20714.0020 - val_loss: 400133472.0000 - val_rmse: 20003.3340\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431724064.0000 - rmse: 20777.9688 - val_loss: 400243840.0000 - val_rmse: 20006.0938\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431504320.0000 - rmse: 20772.6797 - val_loss: 402958784.0000 - val_rmse: 20073.8320\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421368928.0000 - rmse: 20527.2695 - val_loss: 426401248.0000 - val_rmse: 20649.4824\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461059104.0000 - rmse: 21472.2852 - val_loss: 424918208.0000 - val_rmse: 20613.5410\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420570496.0000 - rmse: 20507.8125 - val_loss: 415419840.0000 - val_rmse: 20381.8477\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421474816.0000 - rmse: 20529.8477 - val_loss: 456545152.0000 - val_rmse: 21366.9141\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440363488.0000 - rmse: 20984.8379 - val_loss: 548879424.0000 - val_rmse: 23428.1738\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378324416.0000 - rmse: 19450.5625 - val_loss: 412244000.0000 - val_rmse: 20303.7910\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428033536.0000 - rmse: 20688.9707 - val_loss: 452427008.0000 - val_rmse: 21270.3281\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405885920.0000 - rmse: 20146.6074 - val_loss: 429732576.0000 - val_rmse: 20729.9902\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377936416.0000 - rmse: 19440.5840 - val_loss: 385072096.0000 - val_rmse: 19623.2520\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453963936.0000 - rmse: 21306.4258 - val_loss: 424222240.0000 - val_rmse: 20596.6523\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392443328.0000 - rmse: 19810.1797 - val_loss: 386922368.0000 - val_rmse: 19670.3398\n",
      "Epoch 164/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425148480.0000 - rmse: 20619.1270 - val_loss: 430105376.0000 - val_rmse: 20738.9785\n",
      "Epoch 165/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388789312.0000 - rmse: 19717.7402 - val_loss: 445689536.0000 - val_rmse: 21111.3594\n",
      "Epoch 166/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413520320.0000 - rmse: 20335.1953 - val_loss: 390616800.0000 - val_rmse: 19764.0254\n",
      "Epoch 167/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394323264.0000 - rmse: 19857.5703 - val_loss: 414599488.0000 - val_rmse: 20361.7148\n",
      "Epoch 168/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380108896.0000 - rmse: 19496.3809 - val_loss: 416587456.0000 - val_rmse: 20410.4707\n",
      "Epoch 169/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372387040.0000 - rmse: 19297.3301 - val_loss: 394866336.0000 - val_rmse: 19871.2422\n",
      "Epoch 170/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402058496.0000 - rmse: 20051.3965 - val_loss: 386739552.0000 - val_rmse: 19665.6914\n",
      "Epoch 171/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425769824.0000 - rmse: 20634.1855 - val_loss: 407629760.0000 - val_rmse: 20189.8398\n",
      "Epoch 172/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403078240.0000 - rmse: 20076.8066 - val_loss: 381768928.0000 - val_rmse: 19538.9062\n",
      "Epoch 173/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387962784.0000 - rmse: 19696.7695 - val_loss: 392446240.0000 - val_rmse: 19810.2539\n",
      "Epoch 174/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370754400.0000 - rmse: 19254.9805 - val_loss: 409194400.0000 - val_rmse: 20228.5508\n",
      "Epoch 175/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377012800.0000 - rmse: 19416.8164 - val_loss: 419667968.0000 - val_rmse: 20485.7969\n",
      "Epoch 176/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350985984.0000 - rmse: 18734.6172 - val_loss: 370288064.0000 - val_rmse: 19242.8672\n",
      "Epoch 177/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392205792.0000 - rmse: 19804.1836 - val_loss: 420087296.0000 - val_rmse: 20496.0293\n",
      "Epoch 178/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368529824.0000 - rmse: 19197.1270 - val_loss: 382885248.0000 - val_rmse: 19567.4512\n",
      "Epoch 179/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351967776.0000 - rmse: 18760.8027 - val_loss: 382060864.0000 - val_rmse: 19546.3711\n",
      "Epoch 180/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409323488.0000 - rmse: 20231.7422 - val_loss: 390235552.0000 - val_rmse: 19754.3770\n",
      "Epoch 181/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364806112.0000 - rmse: 19099.8965 - val_loss: 409491040.0000 - val_rmse: 20235.8809\n",
      "Epoch 182/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402991520.0000 - rmse: 20074.6465 - val_loss: 385621920.0000 - val_rmse: 19637.2559\n",
      "Epoch 183/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372732800.0000 - rmse: 19306.2871 - val_loss: 428086272.0000 - val_rmse: 20690.2422\n",
      "Epoch 184/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401623040.0000 - rmse: 20040.5332 - val_loss: 430434656.0000 - val_rmse: 20746.9160\n",
      "Epoch 185/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368137696.0000 - rmse: 19186.9121 - val_loss: 401941440.0000 - val_rmse: 20048.4746\n",
      "Epoch 186/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406646080.0000 - rmse: 20165.4648 - val_loss: 378958592.0000 - val_rmse: 19466.8555\n",
      "Epoch 187/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350778240.0000 - rmse: 18729.0742 - val_loss: 388628064.0000 - val_rmse: 19713.6465\n",
      "Epoch 188/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338820928.0000 - rmse: 18407.0879 - val_loss: 372635616.0000 - val_rmse: 19303.7695\n",
      "Epoch 189/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348374432.0000 - rmse: 18664.7871 - val_loss: 411143136.0000 - val_rmse: 20276.6621\n",
      "Epoch 190/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363421824.0000 - rmse: 19063.6211 - val_loss: 373267520.0000 - val_rmse: 19320.1289\n",
      "Epoch 191/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343772320.0000 - rmse: 18541.0977 - val_loss: 367255008.0000 - val_rmse: 19163.8945\n",
      "Epoch 192/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356369024.0000 - rmse: 18877.7363 - val_loss: 375958880.0000 - val_rmse: 19389.6562\n",
      "Epoch 193/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340111456.0000 - rmse: 18442.1074 - val_loss: 366169792.0000 - val_rmse: 19135.5605\n",
      "Epoch 194/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350686272.0000 - rmse: 18726.6172 - val_loss: 368290688.0000 - val_rmse: 19190.8984\n",
      "Epoch 195/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334415392.0000 - rmse: 18287.0254 - val_loss: 386236064.0000 - val_rmse: 19652.8848\n",
      "Epoch 196/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361392384.0000 - rmse: 19010.3184 - val_loss: 365666048.0000 - val_rmse: 19122.3926\n",
      "Epoch 197/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325330912.0000 - rmse: 18036.9297 - val_loss: 387887680.0000 - val_rmse: 19694.8633\n",
      "Epoch 198/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332970624.0000 - rmse: 18247.4805 - val_loss: 409700704.0000 - val_rmse: 20241.0625\n",
      "Epoch 199/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325488256.0000 - rmse: 18041.2910 - val_loss: 372500672.0000 - val_rmse: 19300.2754\n",
      "Epoch 200/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328474816.0000 - rmse: 18123.8711 - val_loss: 369506048.0000 - val_rmse: 19222.5391\n",
      "Epoch 201/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340820800.0000 - rmse: 18461.3301 - val_loss: 380164736.0000 - val_rmse: 19497.8105\n",
      "Epoch 202/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364272416.0000 - rmse: 19085.9199 - val_loss: 370206688.0000 - val_rmse: 19240.7539\n",
      "Epoch 203/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351922432.0000 - rmse: 18759.5938 - val_loss: 404103552.0000 - val_rmse: 20102.3242\n",
      "Epoch 204/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362435328.0000 - rmse: 19037.7324 - val_loss: 388540768.0000 - val_rmse: 19711.4336\n",
      "Epoch 205/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332342688.0000 - rmse: 18230.2637 - val_loss: 387089568.0000 - val_rmse: 19674.5879\n",
      "Epoch 206/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335867648.0000 - rmse: 18326.6875 - val_loss: 377828832.0000 - val_rmse: 19437.8184\n",
      "Epoch 207/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346625472.0000 - rmse: 18617.8770 - val_loss: 387427072.0000 - val_rmse: 19683.1641\n",
      "Epoch 208/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331591872.0000 - rmse: 18209.6621 - val_loss: 355609600.0000 - val_rmse: 18857.6113\n",
      "Epoch 209/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318194464.0000 - rmse: 17838.0039 - val_loss: 480612736.0000 - val_rmse: 21922.8770\n",
      "Epoch 210/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316171040.0000 - rmse: 17781.1953 - val_loss: 374744928.0000 - val_rmse: 19358.3262\n",
      "Epoch 211/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323286112.0000 - rmse: 17980.1562 - val_loss: 364276192.0000 - val_rmse: 19086.0176\n",
      "Epoch 212/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357872064.0000 - rmse: 18917.5039 - val_loss: 374675968.0000 - val_rmse: 19356.5430\n",
      "Epoch 213/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341789248.0000 - rmse: 18487.5391 - val_loss: 374876640.0000 - val_rmse: 19361.7266\n",
      "Epoch 214/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294021792.0000 - rmse: 17147.0605 - val_loss: 424188224.0000 - val_rmse: 20595.8301\n",
      "Epoch 215/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332651680.0000 - rmse: 18238.7383 - val_loss: 389520448.0000 - val_rmse: 19736.2695\n",
      "Epoch 216/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300087552.0000 - rmse: 17323.0312 - val_loss: 381421408.0000 - val_rmse: 19530.0098\n",
      "Epoch 217/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321477664.0000 - rmse: 17929.7930 - val_loss: 375851712.0000 - val_rmse: 19386.8945\n",
      "Epoch 218/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326359520.0000 - rmse: 18065.4180 - val_loss: 375466336.0000 - val_rmse: 19376.9492\n",
      "Epoch 219/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284912192.0000 - rmse: 16879.3398 - val_loss: 350526976.0000 - val_rmse: 18722.3613\n",
      "Epoch 220/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367705536.0000 - rmse: 19175.6484 - val_loss: 411510848.0000 - val_rmse: 20285.7285\n",
      "Epoch 221/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320505696.0000 - rmse: 17902.6699 - val_loss: 393329184.0000 - val_rmse: 19832.5254\n",
      "Epoch 222/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322395936.0000 - rmse: 17955.3848 - val_loss: 351240480.0000 - val_rmse: 18741.4062\n",
      "Epoch 223/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284136672.0000 - rmse: 16856.3516 - val_loss: 361272064.0000 - val_rmse: 19007.1562\n",
      "Epoch 224/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306448672.0000 - rmse: 17505.6699 - val_loss: 378289088.0000 - val_rmse: 19449.6523\n",
      "Epoch 225/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331282816.0000 - rmse: 18201.1719 - val_loss: 394831392.0000 - val_rmse: 19870.3633\n",
      "Epoch 226/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303328256.0000 - rmse: 17416.3184 - val_loss: 397741696.0000 - val_rmse: 19943.4609\n",
      "Epoch 227/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328864800.0000 - rmse: 18134.6289 - val_loss: 360207616.0000 - val_rmse: 18979.1328\n",
      "Epoch 228/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329367808.0000 - rmse: 18148.4902 - val_loss: 371861920.0000 - val_rmse: 19283.7168\n",
      "Epoch 229/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301513184.0000 - rmse: 17364.1309 - val_loss: 472943968.0000 - val_rmse: 21747.2715\n",
      "Epoch 230/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312590016.0000 - rmse: 17680.2109 - val_loss: 397845856.0000 - val_rmse: 19946.0703\n",
      "Epoch 231/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385298688.0000 - rmse: 19629.0215 - val_loss: 356487104.0000 - val_rmse: 18880.8633\n",
      "Epoch 232/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295125632.0000 - rmse: 17179.2148 - val_loss: 420054016.0000 - val_rmse: 20495.2168\n",
      "Epoch 233/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293142560.0000 - rmse: 17121.4043 - val_loss: 341374816.0000 - val_rmse: 18476.3281\n",
      "Epoch 234/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318221088.0000 - rmse: 17838.7480 - val_loss: 372146624.0000 - val_rmse: 19291.0996\n",
      "Epoch 235/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301147872.0000 - rmse: 17353.6074 - val_loss: 433532256.0000 - val_rmse: 20821.4355\n",
      "Epoch 236/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319284416.0000 - rmse: 17868.5293 - val_loss: 344632000.0000 - val_rmse: 18564.2637\n",
      "Epoch 237/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300315872.0000 - rmse: 17329.6211 - val_loss: 355948000.0000 - val_rmse: 18866.5801\n",
      "Epoch 238/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339021856.0000 - rmse: 18412.5430 - val_loss: 351446784.0000 - val_rmse: 18746.9121\n",
      "Epoch 239/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315743776.0000 - rmse: 17769.1758 - val_loss: 429973280.0000 - val_rmse: 20735.7930\n",
      "Epoch 240/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288496352.0000 - rmse: 16985.1758 - val_loss: 353259712.0000 - val_rmse: 18795.2012\n",
      "Epoch 241/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293149440.0000 - rmse: 17121.6055 - val_loss: 452601088.0000 - val_rmse: 21274.4199\n",
      "Epoch 242/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268304576.0000 - rmse: 16380.0010 - val_loss: 365197440.0000 - val_rmse: 19110.1367\n",
      "Epoch 243/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307691136.0000 - rmse: 17541.1250 - val_loss: 353260320.0000 - val_rmse: 18795.2188\n",
      "Epoch 244/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313989920.0000 - rmse: 17719.7598 - val_loss: 359079616.0000 - val_rmse: 18949.3926\n",
      "Epoch 245/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261156912.0000 - rmse: 16160.3486 - val_loss: 354503520.0000 - val_rmse: 18828.2598\n",
      "Epoch 246/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295242784.0000 - rmse: 17182.6270 - val_loss: 401339584.0000 - val_rmse: 20033.4590\n",
      "Epoch 247/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306955360.0000 - rmse: 17520.1387 - val_loss: 416467616.0000 - val_rmse: 20407.5352\n",
      "Epoch 248/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252581504.0000 - rmse: 15892.8076 - val_loss: 371500672.0000 - val_rmse: 19274.3477\n",
      "Epoch 249/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290650496.0000 - rmse: 17048.4727 - val_loss: 369915552.0000 - val_rmse: 19233.1855\n",
      "Epoch 250/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307694496.0000 - rmse: 17541.2188 - val_loss: 392757632.0000 - val_rmse: 19818.1113\n",
      "Epoch 251/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277059008.0000 - rmse: 16645.0859 - val_loss: 352408608.0000 - val_rmse: 18772.5469\n",
      "Epoch 252/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289666336.0000 - rmse: 17019.5840 - val_loss: 373615712.0000 - val_rmse: 19329.1387\n",
      "Epoch 253/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304883360.0000 - rmse: 17460.9062 - val_loss: 356661440.0000 - val_rmse: 18885.4785\n",
      "Epoch 254/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309655616.0000 - rmse: 17597.0293 - val_loss: 372328832.0000 - val_rmse: 19295.8223\n",
      "Epoch 255/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290454336.0000 - rmse: 17042.7188 - val_loss: 385309920.0000 - val_rmse: 19629.3105\n",
      "Epoch 256/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269435488.0000 - rmse: 16414.4883 - val_loss: 354073120.0000 - val_rmse: 18816.8262\n",
      "Epoch 257/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274543520.0000 - rmse: 16569.3516 - val_loss: 334865696.0000 - val_rmse: 18299.3320\n",
      "Epoch 258/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323359776.0000 - rmse: 17982.2031 - val_loss: 358426112.0000 - val_rmse: 18932.1406\n",
      "Epoch 259/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265209456.0000 - rmse: 16285.2490 - val_loss: 366492704.0000 - val_rmse: 19143.9941\n",
      "Epoch 260/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305187712.0000 - rmse: 17469.6191 - val_loss: 392129408.0000 - val_rmse: 19802.2539\n",
      "Epoch 261/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264207872.0000 - rmse: 16254.4688 - val_loss: 400410016.0000 - val_rmse: 20010.2441\n",
      "Epoch 262/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279789344.0000 - rmse: 16726.8984 - val_loss: 387953856.0000 - val_rmse: 19696.5410\n",
      "Epoch 263/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305698336.0000 - rmse: 17484.2285 - val_loss: 361586016.0000 - val_rmse: 19015.4121\n",
      "Epoch 264/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343247552.0000 - rmse: 18526.9375 - val_loss: 366958016.0000 - val_rmse: 19156.1465\n",
      "Epoch 265/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267692368.0000 - rmse: 16361.3018 - val_loss: 377957888.0000 - val_rmse: 19441.1367\n",
      "Epoch 266/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282199808.0000 - rmse: 16798.8008 - val_loss: 399342976.0000 - val_rmse: 19983.5645\n",
      "Epoch 267/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254554016.0000 - rmse: 15954.7471 - val_loss: 427502720.0000 - val_rmse: 20676.1367\n",
      "Epoch 268/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278358144.0000 - rmse: 16684.0684 - val_loss: 386919456.0000 - val_rmse: 19670.2656\n",
      "104/104 [==============================] - 0s 719us/step - loss: 798730432.0000 - rmse: 28261.8164\n",
      "[798730432.0, 28261.81640625]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 17514971136.0000 - rmse: 132344.1406 - val_loss: 4100417024.0000 - val_rmse: 64034.5000\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2764255488.0000 - rmse: 52576.1875 - val_loss: 1564495104.0000 - val_rmse: 39553.6992\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1678855168.0000 - rmse: 40973.8359 - val_loss: 1374592256.0000 - val_rmse: 37075.4922\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1562024320.0000 - rmse: 39522.4531 - val_loss: 1362027136.0000 - val_rmse: 36905.6523\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1498589824.0000 - rmse: 38711.6250 - val_loss: 1248012544.0000 - val_rmse: 35327.2227\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1426102272.0000 - rmse: 37763.7695 - val_loss: 1190422528.0000 - val_rmse: 34502.5000\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1337473664.0000 - rmse: 36571.4883 - val_loss: 1163932416.0000 - val_rmse: 34116.4531\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1246962176.0000 - rmse: 35312.3516 - val_loss: 1101561088.0000 - val_rmse: 33189.7734\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1265556992.0000 - rmse: 35574.6680 - val_loss: 1157979136.0000 - val_rmse: 34029.0938\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1170544128.0000 - rmse: 34213.2148 - val_loss: 1091251840.0000 - val_rmse: 33034.1016\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1250250752.0000 - rmse: 35358.8828 - val_loss: 1038593280.0000 - val_rmse: 32227.2129\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1190701696.0000 - rmse: 34506.5430 - val_loss: 983290560.0000 - val_rmse: 31357.4629\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1135876096.0000 - rmse: 33702.7617 - val_loss: 963947328.0000 - val_rmse: 31047.5000\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1084197504.0000 - rmse: 32927.1562 - val_loss: 953905088.0000 - val_rmse: 30885.3516\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1050383040.0000 - rmse: 32409.6133 - val_loss: 946748736.0000 - val_rmse: 30769.2832\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 982911104.0000 - rmse: 31351.4141 - val_loss: 1007728320.0000 - val_rmse: 31744.7363\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1015809792.0000 - rmse: 31871.7715 - val_loss: 851997888.0000 - val_rmse: 29189.0020\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 966967232.0000 - rmse: 31096.0938 - val_loss: 826929664.0000 - val_rmse: 28756.3828\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 973525888.0000 - rmse: 31201.3770 - val_loss: 896617216.0000 - val_rmse: 29943.5645\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 962158272.0000 - rmse: 31018.6738 - val_loss: 877641216.0000 - val_rmse: 29625.0098\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 937448768.0000 - rmse: 30617.7832 - val_loss: 831761024.0000 - val_rmse: 28840.2656\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 901905344.0000 - rmse: 30031.7363 - val_loss: 872207488.0000 - val_rmse: 29533.1582\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 949234176.0000 - rmse: 30809.6445 - val_loss: 810960128.0000 - val_rmse: 28477.3574\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 960283072.0000 - rmse: 30988.4316 - val_loss: 722592192.0000 - val_rmse: 26881.0742\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 889109056.0000 - rmse: 29817.9316 - val_loss: 692532032.0000 - val_rmse: 26316.0000\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 881876544.0000 - rmse: 29696.4043 - val_loss: 683367040.0000 - val_rmse: 26141.2891\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 843335808.0000 - rmse: 29040.2441 - val_loss: 671719488.0000 - val_rmse: 25917.5488\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 843097152.0000 - rmse: 29036.1348 - val_loss: 685898432.0000 - val_rmse: 26189.6621\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 831215936.0000 - rmse: 28830.8145 - val_loss: 663017024.0000 - val_rmse: 25749.1133\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 859834816.0000 - rmse: 29322.9375 - val_loss: 718702144.0000 - val_rmse: 26808.6191\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 850614784.0000 - rmse: 29165.3008 - val_loss: 632941376.0000 - val_rmse: 25158.3262\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 819996032.0000 - rmse: 28635.5723 - val_loss: 642114368.0000 - val_rmse: 25339.9746\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 841400576.0000 - rmse: 29006.9062 - val_loss: 660566848.0000 - val_rmse: 25701.4922\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 825528768.0000 - rmse: 28732.0137 - val_loss: 592342784.0000 - val_rmse: 24338.0938\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 780669440.0000 - rmse: 27940.4629 - val_loss: 737678208.0000 - val_rmse: 27160.2305\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 775704128.0000 - rmse: 27851.4648 - val_loss: 625969152.0000 - val_rmse: 25019.3750\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 768001088.0000 - rmse: 27712.8301 - val_loss: 585253632.0000 - val_rmse: 24192.0137\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 773582016.0000 - rmse: 27813.3398 - val_loss: 652752128.0000 - val_rmse: 25549.0137\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 818490240.0000 - rmse: 28609.2656 - val_loss: 630266176.0000 - val_rmse: 25105.1016\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 755728640.0000 - rmse: 27490.5176 - val_loss: 635597824.0000 - val_rmse: 25211.0605\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 798523200.0000 - rmse: 28258.1523 - val_loss: 554144064.0000 - val_rmse: 23540.2637\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 805979136.0000 - rmse: 28389.7695 - val_loss: 615118272.0000 - val_rmse: 24801.5762\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 746937600.0000 - rmse: 27330.1582 - val_loss: 596036544.0000 - val_rmse: 24413.8574\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 775458816.0000 - rmse: 27847.0605 - val_loss: 644023232.0000 - val_rmse: 25377.6094\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 745099200.0000 - rmse: 27296.5000 - val_loss: 601797888.0000 - val_rmse: 24531.5684\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 766727936.0000 - rmse: 27689.8535 - val_loss: 560318912.0000 - val_rmse: 23671.0547\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 737761344.0000 - rmse: 27161.7598 - val_loss: 514889760.0000 - val_rmse: 22691.1797\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 736399680.0000 - rmse: 27136.6836 - val_loss: 533964768.0000 - val_rmse: 23107.6758\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 663923136.0000 - rmse: 25766.7051 - val_loss: 541665152.0000 - val_rmse: 23273.6973\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704615872.0000 - rmse: 26544.5996 - val_loss: 526688864.0000 - val_rmse: 22949.6992\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 719723072.0000 - rmse: 26827.6543 - val_loss: 576564928.0000 - val_rmse: 24011.7656\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692087488.0000 - rmse: 26307.5527 - val_loss: 518866656.0000 - val_rmse: 22778.6426\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742953920.0000 - rmse: 27257.1816 - val_loss: 507207776.0000 - val_rmse: 22521.2695\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 693958912.0000 - rmse: 26343.0996 - val_loss: 498244064.0000 - val_rmse: 22321.3809\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676502464.0000 - rmse: 26009.6602 - val_loss: 512043104.0000 - val_rmse: 22628.3691\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704424960.0000 - rmse: 26541.0039 - val_loss: 511600320.0000 - val_rmse: 22618.5801\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 711841600.0000 - rmse: 26680.3594 - val_loss: 504009344.0000 - val_rmse: 22450.1523\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 657481984.0000 - rmse: 25641.4102 - val_loss: 529509504.0000 - val_rmse: 23011.0684\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706732864.0000 - rmse: 26584.4473 - val_loss: 511198656.0000 - val_rmse: 22609.6992\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 659419008.0000 - rmse: 25679.1543 - val_loss: 560351616.0000 - val_rmse: 23671.7441\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684648448.0000 - rmse: 26165.7852 - val_loss: 548379776.0000 - val_rmse: 23417.5078\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 672774528.0000 - rmse: 25937.8945 - val_loss: 485685600.0000 - val_rmse: 22038.2734\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668455296.0000 - rmse: 25854.5000 - val_loss: 501530336.0000 - val_rmse: 22394.8711\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654927232.0000 - rmse: 25591.5449 - val_loss: 482686080.0000 - val_rmse: 21970.1152\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 662167552.0000 - rmse: 25732.6133 - val_loss: 497036192.0000 - val_rmse: 22294.3066\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 644438976.0000 - rmse: 25385.7988 - val_loss: 528787488.0000 - val_rmse: 22995.3770\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621763712.0000 - rmse: 24935.1895 - val_loss: 460915200.0000 - val_rmse: 21468.9336\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615222528.0000 - rmse: 24803.6777 - val_loss: 517930688.0000 - val_rmse: 22758.0879\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 600398144.0000 - rmse: 24503.0234 - val_loss: 480498880.0000 - val_rmse: 21920.2832\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654062528.0000 - rmse: 25574.6406 - val_loss: 497020000.0000 - val_rmse: 22293.9434\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 643461568.0000 - rmse: 25366.5430 - val_loss: 472055968.0000 - val_rmse: 21726.8438\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649489344.0000 - rmse: 25485.0801 - val_loss: 507043904.0000 - val_rmse: 22517.6348\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 635476544.0000 - rmse: 25208.6602 - val_loss: 571056000.0000 - val_rmse: 23896.7754\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630654656.0000 - rmse: 25112.8359 - val_loss: 490533440.0000 - val_rmse: 22147.9863\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 617129664.0000 - rmse: 24842.0938 - val_loss: 466354880.0000 - val_rmse: 21595.2500\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 675023616.0000 - rmse: 25981.2148 - val_loss: 514451136.0000 - val_rmse: 22681.5137\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604808768.0000 - rmse: 24592.8594 - val_loss: 522687776.0000 - val_rmse: 22862.3633\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652664896.0000 - rmse: 25547.3047 - val_loss: 480668416.0000 - val_rmse: 21924.1484\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624942144.0000 - rmse: 24998.8398 - val_loss: 476012096.0000 - val_rmse: 21817.6973\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554057024.0000 - rmse: 23538.4141 - val_loss: 570526272.0000 - val_rmse: 23885.6895\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596411264.0000 - rmse: 24421.5312 - val_loss: 496341024.0000 - val_rmse: 22278.7090\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629122560.0000 - rmse: 25082.3145 - val_loss: 441043200.0000 - val_rmse: 21001.0254\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605860864.0000 - rmse: 24614.2383 - val_loss: 489744768.0000 - val_rmse: 22130.1777\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596640960.0000 - rmse: 24426.2324 - val_loss: 461747360.0000 - val_rmse: 21488.3047\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646988032.0000 - rmse: 25435.9531 - val_loss: 422692544.0000 - val_rmse: 20559.4844\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624779904.0000 - rmse: 24995.5918 - val_loss: 428548480.0000 - val_rmse: 20701.4082\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586960000.0000 - rmse: 24227.2559 - val_loss: 453384160.0000 - val_rmse: 21292.8164\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642323648.0000 - rmse: 25344.1016 - val_loss: 435255936.0000 - val_rmse: 20862.7871\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563313600.0000 - rmse: 23734.2227 - val_loss: 554309312.0000 - val_rmse: 23543.7715\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 679754176.0000 - rmse: 26072.0957 - val_loss: 420241376.0000 - val_rmse: 20499.7891\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 633477248.0000 - rmse: 25168.9707 - val_loss: 405254176.0000 - val_rmse: 20130.9219\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 619488640.0000 - rmse: 24889.5273 - val_loss: 429722144.0000 - val_rmse: 20729.7363\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609884736.0000 - rmse: 24695.8418 - val_loss: 409523136.0000 - val_rmse: 20236.6758\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576064960.0000 - rmse: 24001.3496 - val_loss: 414151744.0000 - val_rmse: 20350.7148\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628395392.0000 - rmse: 25067.8125 - val_loss: 414043232.0000 - val_rmse: 20348.0488\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575537728.0000 - rmse: 23990.3652 - val_loss: 437439840.0000 - val_rmse: 20915.0605\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567198528.0000 - rmse: 23815.9297 - val_loss: 444436320.0000 - val_rmse: 21081.6562\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598760448.0000 - rmse: 24469.5801 - val_loss: 431672576.0000 - val_rmse: 20776.7266\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583143872.0000 - rmse: 24148.3672 - val_loss: 429828672.0000 - val_rmse: 20732.3047\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598567808.0000 - rmse: 24465.6445 - val_loss: 397502944.0000 - val_rmse: 19937.4727\n",
      "104/104 [==============================] - 0s 706us/step - loss: 962229440.0000 - rmse: 31019.8203\n",
      "[962229440.0, 31019.8203125]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 15902659584.0000 - rmse: 126105.7500 - val_loss: 2665721088.0000 - val_rmse: 51630.6211\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2520849920.0000 - rmse: 50208.0664 - val_loss: 1349754624.0000 - val_rmse: 36739.0078\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1820030080.0000 - rmse: 42661.8125 - val_loss: 1135515648.0000 - val_rmse: 33697.4102\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1704867968.0000 - rmse: 41290.0469 - val_loss: 1121752960.0000 - val_rmse: 33492.5820\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1621031680.0000 - rmse: 40262.0391 - val_loss: 1081600768.0000 - val_rmse: 32887.6992\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1677640960.0000 - rmse: 40959.0156 - val_loss: 1078081536.0000 - val_rmse: 32834.1523\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1590813696.0000 - rmse: 39885.0078 - val_loss: 1036462464.0000 - val_rmse: 32194.1367\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1559093504.0000 - rmse: 39485.3555 - val_loss: 1089097600.0000 - val_rmse: 33001.4805\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1462293120.0000 - rmse: 38239.9414 - val_loss: 1004906368.0000 - val_rmse: 31700.2578\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1442794496.0000 - rmse: 37984.1367 - val_loss: 994087744.0000 - val_rmse: 31529.1543\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1442190720.0000 - rmse: 37976.1875 - val_loss: 933583936.0000 - val_rmse: 30554.6055\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1393862784.0000 - rmse: 37334.4727 - val_loss: 918360448.0000 - val_rmse: 30304.4590\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1355162624.0000 - rmse: 36812.5352 - val_loss: 899441216.0000 - val_rmse: 29990.6855\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1265167872.0000 - rmse: 35569.1992 - val_loss: 881924480.0000 - val_rmse: 29697.2129\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1285116672.0000 - rmse: 35848.5234 - val_loss: 865054848.0000 - val_rmse: 29411.8145\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1278686208.0000 - rmse: 35758.7227 - val_loss: 864144320.0000 - val_rmse: 29396.3301\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1259462016.0000 - rmse: 35488.8984 - val_loss: 864643456.0000 - val_rmse: 29404.8203\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1208912384.0000 - rmse: 34769.4180 - val_loss: 857380288.0000 - val_rmse: 29281.0566\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1217177216.0000 - rmse: 34888.0664 - val_loss: 831916416.0000 - val_rmse: 28842.9609\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1130487296.0000 - rmse: 33622.7188 - val_loss: 810274944.0000 - val_rmse: 28465.3281\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1217319168.0000 - rmse: 34890.1016 - val_loss: 842454080.0000 - val_rmse: 29025.0566\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1104070272.0000 - rmse: 33227.5547 - val_loss: 903161792.0000 - val_rmse: 30052.6504\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1124918912.0000 - rmse: 33539.8086 - val_loss: 988225984.0000 - val_rmse: 31436.0586\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1106646528.0000 - rmse: 33266.2969 - val_loss: 869987712.0000 - val_rmse: 29495.5547\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1085800192.0000 - rmse: 32951.4805 - val_loss: 814953536.0000 - val_rmse: 28547.3867\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1062042752.0000 - rmse: 32588.9980 - val_loss: 787578624.0000 - val_rmse: 28063.8281\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1058169024.0000 - rmse: 32529.5098 - val_loss: 832288960.0000 - val_rmse: 28849.4160\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1055931456.0000 - rmse: 32495.0977 - val_loss: 771823104.0000 - val_rmse: 27781.7031\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1037198208.0000 - rmse: 32205.5605 - val_loss: 774670976.0000 - val_rmse: 27832.9102\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 999356416.0000 - rmse: 31612.5957 - val_loss: 840636480.0000 - val_rmse: 28993.7324\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 998769728.0000 - rmse: 31603.3164 - val_loss: 896143936.0000 - val_rmse: 29935.6621\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1016326272.0000 - rmse: 31879.8711 - val_loss: 859132800.0000 - val_rmse: 29310.9648\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 949483584.0000 - rmse: 30813.6914 - val_loss: 909454080.0000 - val_rmse: 30157.1543\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 981251584.0000 - rmse: 31324.9355 - val_loss: 744501248.0000 - val_rmse: 27285.5488\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 982521664.0000 - rmse: 31345.2012 - val_loss: 752988800.0000 - val_rmse: 27440.6387\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 923731520.0000 - rmse: 30392.9492 - val_loss: 719686016.0000 - val_rmse: 26826.9629\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 951039360.0000 - rmse: 30838.9258 - val_loss: 753908480.0000 - val_rmse: 27457.3945\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 901221248.0000 - rmse: 30020.3477 - val_loss: 713815872.0000 - val_rmse: 26717.3320\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 928521664.0000 - rmse: 30471.6504 - val_loss: 736293312.0000 - val_rmse: 27134.7246\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 955200256.0000 - rmse: 30906.3145 - val_loss: 731660224.0000 - val_rmse: 27049.2188\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 914255744.0000 - rmse: 30236.6602 - val_loss: 621643520.0000 - val_rmse: 24932.7754\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 869046784.0000 - rmse: 29479.5957 - val_loss: 656531200.0000 - val_rmse: 25622.8633\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 956848448.0000 - rmse: 30932.9648 - val_loss: 678675520.0000 - val_rmse: 26051.4023\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 828837312.0000 - rmse: 28789.5332 - val_loss: 663496448.0000 - val_rmse: 25758.4238\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 848673152.0000 - rmse: 29131.9961 - val_loss: 599219328.0000 - val_rmse: 24478.9551\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 801791488.0000 - rmse: 28315.9199 - val_loss: 596501824.0000 - val_rmse: 24423.3828\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 845427328.0000 - rmse: 29076.2324 - val_loss: 709501696.0000 - val_rmse: 26636.4707\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 906075136.0000 - rmse: 30101.0820 - val_loss: 597857152.0000 - val_rmse: 24451.1172\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 850984512.0000 - rmse: 29171.6348 - val_loss: 710532544.0000 - val_rmse: 26655.8145\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 801374656.0000 - rmse: 28308.5586 - val_loss: 580615168.0000 - val_rmse: 24095.9570\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 821606400.0000 - rmse: 28663.6758 - val_loss: 614264384.0000 - val_rmse: 24784.3555\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 806849728.0000 - rmse: 28405.0996 - val_loss: 634316992.0000 - val_rmse: 25185.6484\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 819443520.0000 - rmse: 28625.9219 - val_loss: 640518400.0000 - val_rmse: 25308.4629\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 840006080.0000 - rmse: 28982.8555 - val_loss: 579875648.0000 - val_rmse: 24080.6055\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 759114752.0000 - rmse: 27552.0352 - val_loss: 645984704.0000 - val_rmse: 25416.2266\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 764618880.0000 - rmse: 27651.7422 - val_loss: 592801088.0000 - val_rmse: 24347.5039\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 781638656.0000 - rmse: 27957.7988 - val_loss: 669039296.0000 - val_rmse: 25865.7930\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 841003584.0000 - rmse: 29000.0586 - val_loss: 600807168.0000 - val_rmse: 24511.3672\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 735226304.0000 - rmse: 27115.0547 - val_loss: 533013088.0000 - val_rmse: 23087.0742\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800207616.0000 - rmse: 28287.9395 - val_loss: 577212160.0000 - val_rmse: 24025.2383\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 741938880.0000 - rmse: 27238.5547 - val_loss: 576880512.0000 - val_rmse: 24018.3340\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 812003712.0000 - rmse: 28495.6777 - val_loss: 536238816.0000 - val_rmse: 23156.8262\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 702555520.0000 - rmse: 26505.7598 - val_loss: 534453760.0000 - val_rmse: 23118.2539\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 735792192.0000 - rmse: 27125.4883 - val_loss: 548145856.0000 - val_rmse: 23412.5137\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 753008512.0000 - rmse: 27441.0000 - val_loss: 575782848.0000 - val_rmse: 23995.4746\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 759513216.0000 - rmse: 27559.2676 - val_loss: 504854752.0000 - val_rmse: 22468.9688\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 737521792.0000 - rmse: 27157.3516 - val_loss: 515961888.0000 - val_rmse: 22714.7930\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 773477504.0000 - rmse: 27811.4609 - val_loss: 527977216.0000 - val_rmse: 22977.7539\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 727944576.0000 - rmse: 26980.4453 - val_loss: 509535616.0000 - val_rmse: 22572.8926\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 710083584.0000 - rmse: 26647.3926 - val_loss: 541354688.0000 - val_rmse: 23267.0293\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 681714752.0000 - rmse: 26109.6660 - val_loss: 495621632.0000 - val_rmse: 22262.5605\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 707408256.0000 - rmse: 26597.1465 - val_loss: 519209472.0000 - val_rmse: 22786.1680\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 679186432.0000 - rmse: 26061.2051 - val_loss: 509837536.0000 - val_rmse: 22579.5781\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 703909632.0000 - rmse: 26531.2949 - val_loss: 526302816.0000 - val_rmse: 22941.2891\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 681455296.0000 - rmse: 26104.6953 - val_loss: 520915072.0000 - val_rmse: 22823.5625\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 701530176.0000 - rmse: 26486.4141 - val_loss: 531157408.0000 - val_rmse: 23046.8496\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751041600.0000 - rmse: 27405.1387 - val_loss: 501184864.0000 - val_rmse: 22387.1562\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 695481152.0000 - rmse: 26371.9746 - val_loss: 513464064.0000 - val_rmse: 22659.7422\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 675683648.0000 - rmse: 25993.9102 - val_loss: 513105024.0000 - val_rmse: 22651.8203\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687670656.0000 - rmse: 26223.4746 - val_loss: 498886752.0000 - val_rmse: 22335.7695\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 716378496.0000 - rmse: 26765.2461 - val_loss: 522722752.0000 - val_rmse: 22863.1250\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648622848.0000 - rmse: 25468.0703 - val_loss: 473423680.0000 - val_rmse: 21758.3008\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 700449600.0000 - rmse: 26466.0078 - val_loss: 524626976.0000 - val_rmse: 22904.7344\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 667774016.0000 - rmse: 25841.3223 - val_loss: 566186624.0000 - val_rmse: 23794.6738\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647378496.0000 - rmse: 25443.6309 - val_loss: 466676800.0000 - val_rmse: 21602.7012\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611619712.0000 - rmse: 24730.9473 - val_loss: 464402752.0000 - val_rmse: 21550.0039\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680885184.0000 - rmse: 26093.7773 - val_loss: 473615872.0000 - val_rmse: 21762.7168\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607298560.0000 - rmse: 24643.4277 - val_loss: 529390048.0000 - val_rmse: 23008.4766\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685143232.0000 - rmse: 26175.2402 - val_loss: 627185088.0000 - val_rmse: 25043.6641\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665724096.0000 - rmse: 25801.6270 - val_loss: 474676032.0000 - val_rmse: 21787.0586\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614362816.0000 - rmse: 24786.3398 - val_loss: 473422688.0000 - val_rmse: 21758.2734\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 644072192.0000 - rmse: 25378.5762 - val_loss: 443591488.0000 - val_rmse: 21061.6094\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 664282240.0000 - rmse: 25773.6719 - val_loss: 478305568.0000 - val_rmse: 21870.1953\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661485056.0000 - rmse: 25719.3477 - val_loss: 473523360.0000 - val_rmse: 21760.5898\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 655410688.0000 - rmse: 25600.9902 - val_loss: 444790304.0000 - val_rmse: 21090.0488\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622153024.0000 - rmse: 24942.9922 - val_loss: 594471744.0000 - val_rmse: 24381.7891\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578969728.0000 - rmse: 24061.7891 - val_loss: 446369632.0000 - val_rmse: 21127.4590\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621962432.0000 - rmse: 24939.1719 - val_loss: 474075136.0000 - val_rmse: 21773.2637\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622611328.0000 - rmse: 24952.1777 - val_loss: 452862944.0000 - val_rmse: 21280.5742\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 631210880.0000 - rmse: 25123.9082 - val_loss: 437466496.0000 - val_rmse: 20915.6992\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630880320.0000 - rmse: 25117.3301 - val_loss: 422660352.0000 - val_rmse: 20558.7031\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614492736.0000 - rmse: 24788.9629 - val_loss: 445336160.0000 - val_rmse: 21102.9863\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563167424.0000 - rmse: 23731.1484 - val_loss: 431146304.0000 - val_rmse: 20764.0605\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616567552.0000 - rmse: 24830.7754 - val_loss: 443380992.0000 - val_rmse: 21056.6113\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605774208.0000 - rmse: 24612.4785 - val_loss: 506623456.0000 - val_rmse: 22508.2969\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648089280.0000 - rmse: 25457.5977 - val_loss: 445685472.0000 - val_rmse: 21111.2637\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606102080.0000 - rmse: 24619.1348 - val_loss: 469619136.0000 - val_rmse: 21670.6953\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590464320.0000 - rmse: 24299.4707 - val_loss: 453665952.0000 - val_rmse: 21299.4316\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575023552.0000 - rmse: 23979.6445 - val_loss: 441321952.0000 - val_rmse: 21007.6621\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585201792.0000 - rmse: 24190.9414 - val_loss: 485922400.0000 - val_rmse: 22043.6445\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571219072.0000 - rmse: 23900.1875 - val_loss: 459126912.0000 - val_rmse: 21427.2441\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 608911936.0000 - rmse: 24676.1406 - val_loss: 432112832.0000 - val_rmse: 20787.3223\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594287360.0000 - rmse: 24378.0078 - val_loss: 498307520.0000 - val_rmse: 22322.8008\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610538048.0000 - rmse: 24709.0664 - val_loss: 422828160.0000 - val_rmse: 20562.7832\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605017920.0000 - rmse: 24597.1094 - val_loss: 414340512.0000 - val_rmse: 20355.3535\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605169024.0000 - rmse: 24600.1816 - val_loss: 398477344.0000 - val_rmse: 19961.8945\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549111104.0000 - rmse: 23433.1191 - val_loss: 440785024.0000 - val_rmse: 20994.8789\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562125504.0000 - rmse: 23709.1836 - val_loss: 409423840.0000 - val_rmse: 20234.2227\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564634048.0000 - rmse: 23762.0254 - val_loss: 412730976.0000 - val_rmse: 20315.7773\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570446144.0000 - rmse: 23884.0117 - val_loss: 414026784.0000 - val_rmse: 20347.6465\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571669568.0000 - rmse: 23909.6113 - val_loss: 484791296.0000 - val_rmse: 22017.9746\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555349184.0000 - rmse: 23565.8457 - val_loss: 398352224.0000 - val_rmse: 19958.7598\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568789760.0000 - rmse: 23849.3105 - val_loss: 409891040.0000 - val_rmse: 20245.7637\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507680960.0000 - rmse: 22531.7754 - val_loss: 482688800.0000 - val_rmse: 21970.1777\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560891968.0000 - rmse: 23683.1582 - val_loss: 647302784.0000 - val_rmse: 25442.1445\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558616384.0000 - rmse: 23635.0645 - val_loss: 444422400.0000 - val_rmse: 21081.3262\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537891648.0000 - rmse: 23192.4902 - val_loss: 570175808.0000 - val_rmse: 23878.3496\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563203456.0000 - rmse: 23731.9062 - val_loss: 454100448.0000 - val_rmse: 21309.6328\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554927552.0000 - rmse: 23556.8965 - val_loss: 392233600.0000 - val_rmse: 19804.8867\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532010720.0000 - rmse: 23065.3555 - val_loss: 391236448.0000 - val_rmse: 19779.6953\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 528181888.0000 - rmse: 22982.2051 - val_loss: 392544768.0000 - val_rmse: 19812.7402\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579390656.0000 - rmse: 24070.5312 - val_loss: 379323168.0000 - val_rmse: 19476.2188\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539886464.0000 - rmse: 23235.4551 - val_loss: 407910368.0000 - val_rmse: 20196.7852\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516347040.0000 - rmse: 22723.2695 - val_loss: 428476896.0000 - val_rmse: 20699.6797\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512369152.0000 - rmse: 22635.5703 - val_loss: 404358784.0000 - val_rmse: 20108.6719\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508368128.0000 - rmse: 22547.0176 - val_loss: 397747552.0000 - val_rmse: 19943.6055\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 528771360.0000 - rmse: 22995.0273 - val_loss: 380025920.0000 - val_rmse: 19494.2500\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501726976.0000 - rmse: 22399.2598 - val_loss: 445587616.0000 - val_rmse: 21108.9434\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507324096.0000 - rmse: 22523.8535 - val_loss: 427329824.0000 - val_rmse: 20671.9551\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542156096.0000 - rmse: 23284.2422 - val_loss: 423112224.0000 - val_rmse: 20569.6895\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484610176.0000 - rmse: 22013.8613 - val_loss: 508193696.0000 - val_rmse: 22543.1504\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500318816.0000 - rmse: 22367.8047 - val_loss: 387612416.0000 - val_rmse: 19687.8730\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 528384576.0000 - rmse: 22986.6113 - val_loss: 454468096.0000 - val_rmse: 21318.2559\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507749504.0000 - rmse: 22533.2949 - val_loss: 397457728.0000 - val_rmse: 19936.3398\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530588480.0000 - rmse: 23034.5039 - val_loss: 450010464.0000 - val_rmse: 21213.4473\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502420128.0000 - rmse: 22414.7266 - val_loss: 386257696.0000 - val_rmse: 19653.4375\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473177536.0000 - rmse: 21752.6406 - val_loss: 395744000.0000 - val_rmse: 19893.3125\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459992640.0000 - rmse: 21447.4336 - val_loss: 424581856.0000 - val_rmse: 20605.3809\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489937024.0000 - rmse: 22134.5156 - val_loss: 399513536.0000 - val_rmse: 19987.8340\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541188608.0000 - rmse: 23263.4590 - val_loss: 436959744.0000 - val_rmse: 20903.5781\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535058176.0000 - rmse: 23131.3203 - val_loss: 426002656.0000 - val_rmse: 20639.8301\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529742400.0000 - rmse: 23016.1328 - val_loss: 395272928.0000 - val_rmse: 19881.4668\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470669920.0000 - rmse: 21694.9277 - val_loss: 386619552.0000 - val_rmse: 19662.6387\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533530368.0000 - rmse: 23098.2715 - val_loss: 405601568.0000 - val_rmse: 20139.5508\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504467104.0000 - rmse: 22460.3418 - val_loss: 391795104.0000 - val_rmse: 19793.8125\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496202496.0000 - rmse: 22275.6035 - val_loss: 414508800.0000 - val_rmse: 20359.4863\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522820640.0000 - rmse: 22865.2695 - val_loss: 478911072.0000 - val_rmse: 21884.0352\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551437184.0000 - rmse: 23482.6934 - val_loss: 411254592.0000 - val_rmse: 20279.4121\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502671744.0000 - rmse: 22420.3398 - val_loss: 377478624.0000 - val_rmse: 19428.8047\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523982656.0000 - rmse: 22890.6641 - val_loss: 387625184.0000 - val_rmse: 19688.1973\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464971104.0000 - rmse: 21563.1875 - val_loss: 377112576.0000 - val_rmse: 19419.3828\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498238592.0000 - rmse: 22321.2559 - val_loss: 382372480.0000 - val_rmse: 19554.3438\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585307072.0000 - rmse: 24193.1172 - val_loss: 395658592.0000 - val_rmse: 19891.1660\n",
      "Epoch 164/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487423072.0000 - rmse: 22077.6582 - val_loss: 372034720.0000 - val_rmse: 19288.1992\n",
      "Epoch 165/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465764896.0000 - rmse: 21581.5840 - val_loss: 596098688.0000 - val_rmse: 24415.1289\n",
      "Epoch 166/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468439072.0000 - rmse: 21643.4531 - val_loss: 442185088.0000 - val_rmse: 21028.1953\n",
      "Epoch 167/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472739712.0000 - rmse: 21742.5762 - val_loss: 359976544.0000 - val_rmse: 18973.0449\n",
      "Epoch 168/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495059744.0000 - rmse: 22249.9355 - val_loss: 401273216.0000 - val_rmse: 20031.8027\n",
      "104/104 [==============================] - 0s 709us/step - loss: 513108256.0000 - rmse: 22651.8906\n",
      "[513108256.0, 22651.890625]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 17179744256.0000 - rmse: 131071.5234 - val_loss: 3037083648.0000 - val_rmse: 55109.7422\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2960847616.0000 - rmse: 54413.6719 - val_loss: 1313119104.0000 - val_rmse: 36236.9844\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2036576384.0000 - rmse: 45128.4414 - val_loss: 1183556224.0000 - val_rmse: 34402.8516\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1848632832.0000 - rmse: 42995.7305 - val_loss: 1135546112.0000 - val_rmse: 33697.8633\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1722199168.0000 - rmse: 41499.3867 - val_loss: 1163706880.0000 - val_rmse: 34113.1445\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1770575488.0000 - rmse: 42078.2031 - val_loss: 1092486784.0000 - val_rmse: 33052.7891\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1725906944.0000 - rmse: 41544.0352 - val_loss: 1190556928.0000 - val_rmse: 34504.4492\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1640987264.0000 - rmse: 40509.0977 - val_loss: 1070104512.0000 - val_rmse: 32712.4492\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1541062272.0000 - rmse: 39256.3672 - val_loss: 987420736.0000 - val_rmse: 31423.2520\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1479671168.0000 - rmse: 38466.4922 - val_loss: 936311040.0000 - val_rmse: 30599.1992\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1443726336.0000 - rmse: 37996.3984 - val_loss: 932717760.0000 - val_rmse: 30540.4277\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1449982720.0000 - rmse: 38078.6367 - val_loss: 905779712.0000 - val_rmse: 30096.1738\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1487055744.0000 - rmse: 38562.3633 - val_loss: 873089600.0000 - val_rmse: 29548.0859\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1341089536.0000 - rmse: 36620.8906 - val_loss: 862924544.0000 - val_rmse: 29375.5742\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1481075968.0000 - rmse: 38484.7500 - val_loss: 842469824.0000 - val_rmse: 29025.3301\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1335460992.0000 - rmse: 36543.9570 - val_loss: 868681536.0000 - val_rmse: 29473.4023\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1228648960.0000 - rmse: 35052.0898 - val_loss: 769515456.0000 - val_rmse: 27740.1406\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1235676288.0000 - rmse: 35152.1875 - val_loss: 765182656.0000 - val_rmse: 27661.9355\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1244782336.0000 - rmse: 35281.4727 - val_loss: 755348352.0000 - val_rmse: 27483.5996\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1216534784.0000 - rmse: 34878.8555 - val_loss: 737105856.0000 - val_rmse: 27149.6914\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1260955392.0000 - rmse: 35509.9336 - val_loss: 776272192.0000 - val_rmse: 27861.6602\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1126691840.0000 - rmse: 33566.2305 - val_loss: 742203136.0000 - val_rmse: 27243.4023\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1109524992.0000 - rmse: 33309.5312 - val_loss: 768936896.0000 - val_rmse: 27729.7109\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1137429504.0000 - rmse: 33725.7969 - val_loss: 797010944.0000 - val_rmse: 28231.3809\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1030593344.0000 - rmse: 32102.8555 - val_loss: 740286720.0000 - val_rmse: 27208.2090\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1088246528.0000 - rmse: 32988.5781 - val_loss: 683075008.0000 - val_rmse: 26135.7012\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1113576192.0000 - rmse: 33370.2891 - val_loss: 678190592.0000 - val_rmse: 26042.0918\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 977944000.0000 - rmse: 31272.0957 - val_loss: 707689344.0000 - val_rmse: 26602.4297\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1035076544.0000 - rmse: 32172.6055 - val_loss: 653365504.0000 - val_rmse: 25561.0137\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1011397504.0000 - rmse: 31802.4746 - val_loss: 637428032.0000 - val_rmse: 25247.3359\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 999398976.0000 - rmse: 31613.2715 - val_loss: 622912576.0000 - val_rmse: 24958.2168\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1046602432.0000 - rmse: 32351.2344 - val_loss: 664420608.0000 - val_rmse: 25776.3574\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 966315968.0000 - rmse: 31085.6211 - val_loss: 629685312.0000 - val_rmse: 25093.5293\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1006753536.0000 - rmse: 31729.3789 - val_loss: 891939584.0000 - val_rmse: 29865.3535\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1005583680.0000 - rmse: 31710.9375 - val_loss: 690361728.0000 - val_rmse: 26274.7344\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 979355392.0000 - rmse: 31294.6504 - val_loss: 648479744.0000 - val_rmse: 25465.2637\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 989244672.0000 - rmse: 31452.2598 - val_loss: 610765952.0000 - val_rmse: 24713.6777\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 947809216.0000 - rmse: 30786.5078 - val_loss: 607698688.0000 - val_rmse: 24651.5430\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 932961856.0000 - rmse: 30544.4219 - val_loss: 682753216.0000 - val_rmse: 26129.5469\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 975119360.0000 - rmse: 31226.9004 - val_loss: 748417792.0000 - val_rmse: 27357.2227\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 919377088.0000 - rmse: 30321.2324 - val_loss: 614655680.0000 - val_rmse: 24792.2500\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 933117376.0000 - rmse: 30546.9707 - val_loss: 665031808.0000 - val_rmse: 25788.2070\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 958471168.0000 - rmse: 30959.1855 - val_loss: 682053376.0000 - val_rmse: 26116.1484\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 843440512.0000 - rmse: 29042.0449 - val_loss: 578206400.0000 - val_rmse: 24045.9219\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 936612864.0000 - rmse: 30604.1289 - val_loss: 592381568.0000 - val_rmse: 24338.8887\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 888113856.0000 - rmse: 29801.2363 - val_loss: 597513536.0000 - val_rmse: 24444.0879\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 873159808.0000 - rmse: 29549.2773 - val_loss: 637982592.0000 - val_rmse: 25258.3164\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 888503552.0000 - rmse: 29807.7734 - val_loss: 576326336.0000 - val_rmse: 24006.7969\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 900151360.0000 - rmse: 30002.5234 - val_loss: 631462464.0000 - val_rmse: 25128.9141\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 863240256.0000 - rmse: 29380.9492 - val_loss: 546170624.0000 - val_rmse: 23370.2930\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 886339072.0000 - rmse: 29771.4453 - val_loss: 558262272.0000 - val_rmse: 23627.5723\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 836864192.0000 - rmse: 28928.6035 - val_loss: 576841216.0000 - val_rmse: 24017.5156\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 892249024.0000 - rmse: 29870.5371 - val_loss: 598769472.0000 - val_rmse: 24469.7656\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 861038720.0000 - rmse: 29343.4590 - val_loss: 552305408.0000 - val_rmse: 23501.1777\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 808153216.0000 - rmse: 28428.0312 - val_loss: 591154304.0000 - val_rmse: 24313.6641\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 812973952.0000 - rmse: 28512.6973 - val_loss: 538411584.0000 - val_rmse: 23203.6953\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 833763328.0000 - rmse: 28874.9551 - val_loss: 551413248.0000 - val_rmse: 23482.1875\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 804373824.0000 - rmse: 28361.4844 - val_loss: 526743904.0000 - val_rmse: 22950.8984\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 802892160.0000 - rmse: 28335.3496 - val_loss: 528190080.0000 - val_rmse: 22982.3848\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 831234176.0000 - rmse: 28831.1289 - val_loss: 527785632.0000 - val_rmse: 22973.5820\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 825523648.0000 - rmse: 28731.9277 - val_loss: 533671264.0000 - val_rmse: 23101.3262\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 803991296.0000 - rmse: 28354.7383 - val_loss: 544407552.0000 - val_rmse: 23332.5430\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 785956032.0000 - rmse: 28034.9062 - val_loss: 540543424.0000 - val_rmse: 23249.5879\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 796281984.0000 - rmse: 28218.4668 - val_loss: 509841312.0000 - val_rmse: 22579.6641\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 723425216.0000 - rmse: 26896.5645 - val_loss: 559277632.0000 - val_rmse: 23649.0508\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742376960.0000 - rmse: 27246.5938 - val_loss: 513162432.0000 - val_rmse: 22653.0859\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 732717952.0000 - rmse: 27068.7617 - val_loss: 511995712.0000 - val_rmse: 22627.3203\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 850104256.0000 - rmse: 29156.5469 - val_loss: 517911616.0000 - val_rmse: 22757.6699\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 735508736.0000 - rmse: 27120.2617 - val_loss: 500814496.0000 - val_rmse: 22378.8828\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 756957824.0000 - rmse: 27512.8672 - val_loss: 499609088.0000 - val_rmse: 22351.9355\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 728810816.0000 - rmse: 26996.4961 - val_loss: 593003904.0000 - val_rmse: 24351.6680\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 754909632.0000 - rmse: 27475.6152 - val_loss: 501935616.0000 - val_rmse: 22403.9180\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 771894720.0000 - rmse: 27782.9902 - val_loss: 555240256.0000 - val_rmse: 23563.5332\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 769678336.0000 - rmse: 27743.0742 - val_loss: 482883264.0000 - val_rmse: 21974.6055\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724813760.0000 - rmse: 26922.3633 - val_loss: 562929216.0000 - val_rmse: 23726.1270\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724139968.0000 - rmse: 26909.8457 - val_loss: 479739264.0000 - val_rmse: 21902.9492\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 722217216.0000 - rmse: 26874.0977 - val_loss: 497043232.0000 - val_rmse: 22294.4629\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 735569920.0000 - rmse: 27121.3906 - val_loss: 511481792.0000 - val_rmse: 22615.9629\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680925888.0000 - rmse: 26094.5547 - val_loss: 461185312.0000 - val_rmse: 21475.2227\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 725105728.0000 - rmse: 26927.7852 - val_loss: 622652544.0000 - val_rmse: 24953.0059\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782511296.0000 - rmse: 27973.4023 - val_loss: 491293632.0000 - val_rmse: 22165.1445\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 701111232.0000 - rmse: 26478.5000 - val_loss: 487473024.0000 - val_rmse: 22078.7891\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668923968.0000 - rmse: 25863.5625 - val_loss: 526369696.0000 - val_rmse: 22942.7441\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 747697408.0000 - rmse: 27344.0547 - val_loss: 449527296.0000 - val_rmse: 21202.0566\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 655757888.0000 - rmse: 25607.7676 - val_loss: 473417984.0000 - val_rmse: 21758.1660\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 644300224.0000 - rmse: 25383.0664 - val_loss: 464605600.0000 - val_rmse: 21554.7109\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 708931776.0000 - rmse: 26625.7695 - val_loss: 464830240.0000 - val_rmse: 21559.9199\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 636813824.0000 - rmse: 25235.1680 - val_loss: 458991648.0000 - val_rmse: 21424.0859\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 674184704.0000 - rmse: 25965.0625 - val_loss: 444104512.0000 - val_rmse: 21073.7852\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684519808.0000 - rmse: 26163.3301 - val_loss: 454250912.0000 - val_rmse: 21313.1602\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618805120.0000 - rmse: 24875.7930 - val_loss: 473294336.0000 - val_rmse: 21755.3242\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579621184.0000 - rmse: 24075.3203 - val_loss: 466868704.0000 - val_rmse: 21607.1426\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 679159744.0000 - rmse: 26060.6914 - val_loss: 532155840.0000 - val_rmse: 23068.5000\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 675273472.0000 - rmse: 25986.0234 - val_loss: 477049152.0000 - val_rmse: 21841.4531\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616599552.0000 - rmse: 24831.4219 - val_loss: 450579712.0000 - val_rmse: 21226.8613\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616536192.0000 - rmse: 24830.1465 - val_loss: 592280256.0000 - val_rmse: 24336.8066\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668755776.0000 - rmse: 25860.3105 - val_loss: 490059808.0000 - val_rmse: 22137.2910\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 672949824.0000 - rmse: 25941.2734 - val_loss: 421780960.0000 - val_rmse: 20537.3047\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594781760.0000 - rmse: 24388.1445 - val_loss: 452368128.0000 - val_rmse: 21268.9453\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 650320256.0000 - rmse: 25501.3770 - val_loss: 425715296.0000 - val_rmse: 20632.8672\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612676160.0000 - rmse: 24752.2949 - val_loss: 444394208.0000 - val_rmse: 21080.6582\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618979840.0000 - rmse: 24879.3047 - val_loss: 428547840.0000 - val_rmse: 20701.3945\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616941888.0000 - rmse: 24838.3145 - val_loss: 433324640.0000 - val_rmse: 20816.4473\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 600650624.0000 - rmse: 24508.1738 - val_loss: 464241920.0000 - val_rmse: 21546.2715\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612809600.0000 - rmse: 24754.9922 - val_loss: 481095904.0000 - val_rmse: 21933.8945\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566245824.0000 - rmse: 23795.9199 - val_loss: 422840192.0000 - val_rmse: 20563.0762\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536265600.0000 - rmse: 23157.4062 - val_loss: 455912544.0000 - val_rmse: 21352.1055\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592726592.0000 - rmse: 24345.9766 - val_loss: 425876928.0000 - val_rmse: 20636.7812\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628027648.0000 - rmse: 25060.4766 - val_loss: 419617600.0000 - val_rmse: 20484.5664\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546450624.0000 - rmse: 23376.2812 - val_loss: 419436032.0000 - val_rmse: 20480.1367\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616637504.0000 - rmse: 24832.1855 - val_loss: 476799552.0000 - val_rmse: 21835.7363\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562445696.0000 - rmse: 23715.9375 - val_loss: 441010048.0000 - val_rmse: 21000.2363\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612588800.0000 - rmse: 24750.5293 - val_loss: 424523584.0000 - val_rmse: 20603.9688\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605380928.0000 - rmse: 24604.4883 - val_loss: 413563872.0000 - val_rmse: 20336.2676\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551433664.0000 - rmse: 23482.6250 - val_loss: 438421120.0000 - val_rmse: 20938.5059\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553182464.0000 - rmse: 23519.8301 - val_loss: 442219808.0000 - val_rmse: 21029.0195\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541853056.0000 - rmse: 23277.7363 - val_loss: 649139968.0000 - val_rmse: 25478.2246\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538582784.0000 - rmse: 23207.3828 - val_loss: 450461536.0000 - val_rmse: 21224.0762\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 581865280.0000 - rmse: 24121.8828 - val_loss: 394680736.0000 - val_rmse: 19866.5703\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568515072.0000 - rmse: 23843.5508 - val_loss: 435066336.0000 - val_rmse: 20858.2422\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575419456.0000 - rmse: 23987.9004 - val_loss: 427378336.0000 - val_rmse: 20673.1309\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541442176.0000 - rmse: 23268.9082 - val_loss: 429162400.0000 - val_rmse: 20716.2344\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542063104.0000 - rmse: 23282.2480 - val_loss: 456048704.0000 - val_rmse: 21355.2969\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518379744.0000 - rmse: 22767.9531 - val_loss: 556950464.0000 - val_rmse: 23599.7930\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538797568.0000 - rmse: 23212.0098 - val_loss: 460951776.0000 - val_rmse: 21469.7871\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505695392.0000 - rmse: 22487.6719 - val_loss: 438639616.0000 - val_rmse: 20943.7227\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558361088.0000 - rmse: 23629.6621 - val_loss: 385668416.0000 - val_rmse: 19638.4414\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564196800.0000 - rmse: 23752.8262 - val_loss: 428068768.0000 - val_rmse: 20689.8203\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498053344.0000 - rmse: 22317.1074 - val_loss: 379038048.0000 - val_rmse: 19468.8965\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532674688.0000 - rmse: 23079.7441 - val_loss: 585368512.0000 - val_rmse: 24194.3867\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491185920.0000 - rmse: 22162.7109 - val_loss: 420692544.0000 - val_rmse: 20510.7891\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548465088.0000 - rmse: 23419.3301 - val_loss: 505878080.0000 - val_rmse: 22491.7324\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559291520.0000 - rmse: 23649.3438 - val_loss: 394665760.0000 - val_rmse: 19866.1934\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500080640.0000 - rmse: 22362.4805 - val_loss: 615889792.0000 - val_rmse: 24817.1250\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523448960.0000 - rmse: 22879.0039 - val_loss: 420711552.0000 - val_rmse: 20511.2520\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486996832.0000 - rmse: 22068.0020 - val_loss: 393652288.0000 - val_rmse: 19840.6680\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482107424.0000 - rmse: 21956.9434 - val_loss: 402053632.0000 - val_rmse: 20051.2715\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454885824.0000 - rmse: 21328.0508 - val_loss: 437261984.0000 - val_rmse: 20910.8066\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480214880.0000 - rmse: 21913.8008 - val_loss: 464384384.0000 - val_rmse: 21549.5781\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553183872.0000 - rmse: 23519.8594 - val_loss: 403612480.0000 - val_rmse: 20090.1074\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532303296.0000 - rmse: 23071.6914 - val_loss: 412781312.0000 - val_rmse: 20317.0176\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559863104.0000 - rmse: 23661.4219 - val_loss: 408380992.0000 - val_rmse: 20208.4355\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533912800.0000 - rmse: 23106.5527 - val_loss: 432783904.0000 - val_rmse: 20803.4570\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494859520.0000 - rmse: 22245.4375 - val_loss: 412616000.0000 - val_rmse: 20312.9492\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524780928.0000 - rmse: 22908.0957 - val_loss: 458176064.0000 - val_rmse: 21405.0449\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500252672.0000 - rmse: 22366.3262 - val_loss: 407020960.0000 - val_rmse: 20174.7578\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493652448.0000 - rmse: 22218.2871 - val_loss: 424423136.0000 - val_rmse: 20601.5293\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486784896.0000 - rmse: 22063.1992 - val_loss: 443532896.0000 - val_rmse: 21060.2188\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519842752.0000 - rmse: 22800.0547 - val_loss: 496608096.0000 - val_rmse: 22284.7031\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424119104.0000 - rmse: 20594.1504 - val_loss: 441847424.0000 - val_rmse: 21020.1621\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437476000.0000 - rmse: 20915.9238 - val_loss: 399960320.0000 - val_rmse: 19999.0039\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473275072.0000 - rmse: 21754.8848 - val_loss: 401060512.0000 - val_rmse: 20026.4922\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466445056.0000 - rmse: 21597.3359 - val_loss: 408189696.0000 - val_rmse: 20203.7031\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486691840.0000 - rmse: 22061.0918 - val_loss: 399206848.0000 - val_rmse: 19980.1582\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489897568.0000 - rmse: 22133.6289 - val_loss: 394857024.0000 - val_rmse: 19871.0078\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472376608.0000 - rmse: 21734.2246 - val_loss: 482003520.0000 - val_rmse: 21954.5762\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471146560.0000 - rmse: 21705.9062 - val_loss: 397525664.0000 - val_rmse: 19938.0410\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474742912.0000 - rmse: 21788.5898 - val_loss: 436178208.0000 - val_rmse: 20884.8770\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468472640.0000 - rmse: 21644.2246 - val_loss: 451058304.0000 - val_rmse: 21238.1309\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456528192.0000 - rmse: 21366.5176 - val_loss: 434397696.0000 - val_rmse: 20842.2070\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419670656.0000 - rmse: 20485.8633 - val_loss: 459267008.0000 - val_rmse: 21430.5137\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434112352.0000 - rmse: 20835.3613 - val_loss: 424886048.0000 - val_rmse: 20612.7637\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482174144.0000 - rmse: 21958.4609 - val_loss: 423781536.0000 - val_rmse: 20585.9531\n",
      "Epoch 164/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490263904.0000 - rmse: 22141.9023 - val_loss: 481439712.0000 - val_rmse: 21941.7285\n",
      "Epoch 165/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424023232.0000 - rmse: 20591.8203 - val_loss: 443935072.0000 - val_rmse: 21069.7637\n",
      "Epoch 166/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420302432.0000 - rmse: 20501.2773 - val_loss: 405885216.0000 - val_rmse: 20146.5898\n",
      "Epoch 167/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504058656.0000 - rmse: 22451.2500 - val_loss: 427948896.0000 - val_rmse: 20686.9219\n",
      "Epoch 168/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485288064.0000 - rmse: 22029.2520 - val_loss: 503566624.0000 - val_rmse: 22440.2871\n",
      "Epoch 169/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442788288.0000 - rmse: 21042.5332 - val_loss: 405313344.0000 - val_rmse: 20132.3926\n",
      "Epoch 170/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416900320.0000 - rmse: 20418.1328 - val_loss: 429547488.0000 - val_rmse: 20725.5234\n",
      "Epoch 171/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459083904.0000 - rmse: 21426.2402 - val_loss: 405149760.0000 - val_rmse: 20128.3281\n",
      "Epoch 172/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417034304.0000 - rmse: 20421.4141 - val_loss: 485591552.0000 - val_rmse: 22036.1406\n",
      "Epoch 173/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440346720.0000 - rmse: 20984.4355 - val_loss: 400426656.0000 - val_rmse: 20010.6602\n",
      "Epoch 174/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404315168.0000 - rmse: 20107.5859 - val_loss: 409167296.0000 - val_rmse: 20227.8809\n",
      "Epoch 175/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437606656.0000 - rmse: 20919.0469 - val_loss: 429015040.0000 - val_rmse: 20712.6758\n",
      "Epoch 176/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430909472.0000 - rmse: 20758.3555 - val_loss: 437243648.0000 - val_rmse: 20910.3691\n",
      "Epoch 177/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418191296.0000 - rmse: 20449.7227 - val_loss: 440214816.0000 - val_rmse: 20981.2930\n",
      "Epoch 178/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400500992.0000 - rmse: 20012.5176 - val_loss: 518571104.0000 - val_rmse: 22772.1523\n",
      "Epoch 179/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399646624.0000 - rmse: 19991.1582 - val_loss: 464096928.0000 - val_rmse: 21542.9062\n",
      "Epoch 180/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403798592.0000 - rmse: 20094.7363 - val_loss: 443794528.0000 - val_rmse: 21066.4297\n",
      "Epoch 181/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411874976.0000 - rmse: 20294.6973 - val_loss: 408414368.0000 - val_rmse: 20209.2617\n",
      "Epoch 182/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410654688.0000 - rmse: 20264.6094 - val_loss: 486548896.0000 - val_rmse: 22057.8516\n",
      "Epoch 183/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401653856.0000 - rmse: 20041.2988 - val_loss: 391014624.0000 - val_rmse: 19774.0879\n",
      "Epoch 184/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420955072.0000 - rmse: 20517.1875 - val_loss: 452612064.0000 - val_rmse: 21274.6797\n",
      "Epoch 185/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398748192.0000 - rmse: 19968.6758 - val_loss: 396792672.0000 - val_rmse: 19919.6523\n",
      "Epoch 186/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394442496.0000 - rmse: 19860.5723 - val_loss: 400767904.0000 - val_rmse: 20019.1855\n",
      "Epoch 187/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450466208.0000 - rmse: 21224.1855 - val_loss: 412345760.0000 - val_rmse: 20306.2949\n",
      "Epoch 188/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403244992.0000 - rmse: 20080.9570 - val_loss: 426638432.0000 - val_rmse: 20655.2246\n",
      "Epoch 189/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361771968.0000 - rmse: 19020.2988 - val_loss: 474671904.0000 - val_rmse: 21786.9648\n",
      "Epoch 190/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420642816.0000 - rmse: 20509.5762 - val_loss: 475845600.0000 - val_rmse: 21813.8828\n",
      "Epoch 191/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379195904.0000 - rmse: 19472.9512 - val_loss: 400719456.0000 - val_rmse: 20017.9746\n",
      "Epoch 192/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374077120.0000 - rmse: 19341.0703 - val_loss: 400789024.0000 - val_rmse: 20019.7109\n",
      "Epoch 193/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383036000.0000 - rmse: 19571.3047 - val_loss: 411542240.0000 - val_rmse: 20286.5000\n",
      "Epoch 194/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461074816.0000 - rmse: 21472.6504 - val_loss: 399651648.0000 - val_rmse: 19991.2852\n",
      "Epoch 195/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395827232.0000 - rmse: 19895.4043 - val_loss: 419540832.0000 - val_rmse: 20482.6934\n",
      "Epoch 196/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397696384.0000 - rmse: 19942.3223 - val_loss: 441904320.0000 - val_rmse: 21021.5156\n",
      "Epoch 197/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355530688.0000 - rmse: 18855.5215 - val_loss: 450683840.0000 - val_rmse: 21229.3125\n",
      "Epoch 198/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420378272.0000 - rmse: 20503.1250 - val_loss: 433709952.0000 - val_rmse: 20825.7012\n",
      "Epoch 199/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382087680.0000 - rmse: 19547.0605 - val_loss: 467915744.0000 - val_rmse: 21631.3574\n",
      "Epoch 200/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387101248.0000 - rmse: 19674.8848 - val_loss: 427532288.0000 - val_rmse: 20676.8496\n",
      "Epoch 201/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351354272.0000 - rmse: 18744.4434 - val_loss: 435513696.0000 - val_rmse: 20868.9629\n",
      "Epoch 202/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383085088.0000 - rmse: 19572.5566 - val_loss: 441132128.0000 - val_rmse: 21003.1426\n",
      "Epoch 203/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361264032.0000 - rmse: 19006.9434 - val_loss: 531505696.0000 - val_rmse: 23054.4023\n",
      "Epoch 204/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363617152.0000 - rmse: 19068.7441 - val_loss: 426475296.0000 - val_rmse: 20651.2734\n",
      "Epoch 205/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349523936.0000 - rmse: 18695.5547 - val_loss: 404139712.0000 - val_rmse: 20103.2207\n",
      "Epoch 206/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330915392.0000 - rmse: 18191.0781 - val_loss: 440995040.0000 - val_rmse: 20999.8789\n",
      "Epoch 207/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371435456.0000 - rmse: 19272.6582 - val_loss: 442027104.0000 - val_rmse: 21024.4375\n",
      "Epoch 208/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362235296.0000 - rmse: 19032.4766 - val_loss: 407068800.0000 - val_rmse: 20175.9434\n",
      "Epoch 209/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390672224.0000 - rmse: 19765.4277 - val_loss: 432099424.0000 - val_rmse: 20786.9980\n",
      "Epoch 210/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331906720.0000 - rmse: 18218.3027 - val_loss: 475985664.0000 - val_rmse: 21817.0918\n",
      "Epoch 211/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374376288.0000 - rmse: 19348.8027 - val_loss: 400229792.0000 - val_rmse: 20005.7402\n",
      "Epoch 212/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353416384.0000 - rmse: 18799.3672 - val_loss: 397149632.0000 - val_rmse: 19928.6094\n",
      "Epoch 213/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349289920.0000 - rmse: 18689.2949 - val_loss: 395039904.0000 - val_rmse: 19875.6074\n",
      "Epoch 214/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344307488.0000 - rmse: 18555.5215 - val_loss: 556121600.0000 - val_rmse: 23582.2285\n",
      "Epoch 215/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361606240.0000 - rmse: 19015.9395 - val_loss: 414887904.0000 - val_rmse: 20368.7930\n",
      "Epoch 216/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388575072.0000 - rmse: 19712.3027 - val_loss: 455295744.0000 - val_rmse: 21337.6562\n",
      "Epoch 217/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351754304.0000 - rmse: 18755.1113 - val_loss: 476043840.0000 - val_rmse: 21818.4258\n",
      "Epoch 218/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336740736.0000 - rmse: 18350.4902 - val_loss: 471900416.0000 - val_rmse: 21723.2656\n",
      "Epoch 219/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363199200.0000 - rmse: 19057.7832 - val_loss: 445029536.0000 - val_rmse: 21095.7207\n",
      "Epoch 220/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403264640.0000 - rmse: 20081.4473 - val_loss: 483575520.0000 - val_rmse: 21990.3477\n",
      "Epoch 221/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377668800.0000 - rmse: 19433.7012 - val_loss: 473367104.0000 - val_rmse: 21756.9961\n",
      "Epoch 222/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343835584.0000 - rmse: 18542.8008 - val_loss: 454803200.0000 - val_rmse: 21326.1113\n",
      "Epoch 223/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353933056.0000 - rmse: 18813.1055 - val_loss: 425652640.0000 - val_rmse: 20631.3477\n",
      "Epoch 224/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352393600.0000 - rmse: 18772.1465 - val_loss: 407492224.0000 - val_rmse: 20186.4316\n",
      "Epoch 225/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415439264.0000 - rmse: 20382.3242 - val_loss: 413962368.0000 - val_rmse: 20346.0605\n",
      "Epoch 226/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370743872.0000 - rmse: 19254.7070 - val_loss: 420730560.0000 - val_rmse: 20511.7148\n",
      "104/104 [==============================] - 0s 682us/step - loss: 385426208.0000 - rmse: 19632.2715\n",
      "[385426208.0, 19632.271484375]\n",
      "[22853.63671875, 28261.81640625, 31019.8203125, 22651.890625, 19632.271484375]\n",
      "24883.887109375\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "!python train.py kfold light\n",
    "# epoch 200 p 20 lr 4e-3 (63 32)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 20:58:38.531921: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 20:58:38.531961: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 20:58:38.532265: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 20:58:38.770406: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 19445624832.0000 - rmse: 139447.5625 - val_loss: 4793297408.0000 - val_rmse: 69233.6406\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 3111383040.0000 - rmse: 55779.7734 - val_loss: 1463961856.0000 - val_rmse: 38261.7539\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1979567872.0000 - rmse: 44492.3359 - val_loss: 1084172928.0000 - val_rmse: 32926.7812\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1741943552.0000 - rmse: 41736.5977 - val_loss: 984849024.0000 - val_rmse: 31382.3047\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1764245120.0000 - rmse: 42002.9180 - val_loss: 954003136.0000 - val_rmse: 30886.9395\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1700270080.0000 - rmse: 41234.3320 - val_loss: 910748608.0000 - val_rmse: 30178.6094\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1622676096.0000 - rmse: 40282.4531 - val_loss: 892692224.0000 - val_rmse: 29877.9551\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1572703360.0000 - rmse: 39657.3242 - val_loss: 859206144.0000 - val_rmse: 29312.2188\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1557996416.0000 - rmse: 39471.4648 - val_loss: 843345984.0000 - val_rmse: 29040.4180\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1529289088.0000 - rmse: 39106.1250 - val_loss: 838842112.0000 - val_rmse: 28962.7715\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1545517824.0000 - rmse: 39313.0742 - val_loss: 839744384.0000 - val_rmse: 28978.3418\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1492773376.0000 - rmse: 38636.4258 - val_loss: 810620800.0000 - val_rmse: 28471.4004\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1469437184.0000 - rmse: 38333.2383 - val_loss: 755613952.0000 - val_rmse: 27488.4316\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1404290816.0000 - rmse: 37473.8672 - val_loss: 792584256.0000 - val_rmse: 28152.8730\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1320583424.0000 - rmse: 36339.8320 - val_loss: 861139584.0000 - val_rmse: 29345.1777\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1352944256.0000 - rmse: 36782.3906 - val_loss: 815883776.0000 - val_rmse: 28563.6797\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1409171712.0000 - rmse: 37538.9375 - val_loss: 1002159936.0000 - val_rmse: 31656.9102\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1296849536.0000 - rmse: 36011.7969 - val_loss: 740760960.0000 - val_rmse: 27216.9238\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1291868288.0000 - rmse: 35942.5703 - val_loss: 721715392.0000 - val_rmse: 26864.7578\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1321906944.0000 - rmse: 36358.0352 - val_loss: 883940608.0000 - val_rmse: 29731.1367\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1253853312.0000 - rmse: 35409.7930 - val_loss: 796435712.0000 - val_rmse: 28221.1934\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1261947648.0000 - rmse: 35523.9023 - val_loss: 697228992.0000 - val_rmse: 26405.0938\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1224189312.0000 - rmse: 34988.4180 - val_loss: 767194304.0000 - val_rmse: 27698.2734\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1243966976.0000 - rmse: 35269.9141 - val_loss: 776869440.0000 - val_rmse: 27872.3770\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1166379392.0000 - rmse: 34152.2969 - val_loss: 866495744.0000 - val_rmse: 29436.2949\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1175927424.0000 - rmse: 34291.7969 - val_loss: 816077824.0000 - val_rmse: 28567.0742\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1142245888.0000 - rmse: 33797.1250 - val_loss: 696943744.0000 - val_rmse: 26399.6895\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1124300544.0000 - rmse: 33530.5898 - val_loss: 908117696.0000 - val_rmse: 30134.9883\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1147860864.0000 - rmse: 33880.0898 - val_loss: 718848960.0000 - val_rmse: 26811.3555\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1053028224.0000 - rmse: 32450.3965 - val_loss: 643700672.0000 - val_rmse: 25371.2559\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1087214592.0000 - rmse: 32972.9375 - val_loss: 611523328.0000 - val_rmse: 24728.9961\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1074086400.0000 - rmse: 32773.2578 - val_loss: 612511040.0000 - val_rmse: 24748.9570\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1032660800.0000 - rmse: 32135.0371 - val_loss: 700515392.0000 - val_rmse: 26467.2520\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1087825664.0000 - rmse: 32982.2031 - val_loss: 695116928.0000 - val_rmse: 26365.0684\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1030246336.0000 - rmse: 32097.4492 - val_loss: 635393856.0000 - val_rmse: 25207.0195\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1028832704.0000 - rmse: 32075.4199 - val_loss: 596574016.0000 - val_rmse: 24424.8633\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 961837952.0000 - rmse: 31013.5098 - val_loss: 570945664.0000 - val_rmse: 23894.4648\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1014067904.0000 - rmse: 31844.4336 - val_loss: 556626432.0000 - val_rmse: 23592.9297\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 909724416.0000 - rmse: 30161.6387 - val_loss: 663532352.0000 - val_rmse: 25759.1211\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1014653056.0000 - rmse: 31853.6191 - val_loss: 598788096.0000 - val_rmse: 24470.1465\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 965205632.0000 - rmse: 31067.7559 - val_loss: 581535232.0000 - val_rmse: 24115.0371\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1011097216.0000 - rmse: 31797.7520 - val_loss: 524793824.0000 - val_rmse: 22908.3750\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 994563840.0000 - rmse: 31536.7070 - val_loss: 517099232.0000 - val_rmse: 22739.8145\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 955151808.0000 - rmse: 30905.5273 - val_loss: 587230912.0000 - val_rmse: 24232.8477\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 901667968.0000 - rmse: 30027.7852 - val_loss: 633459968.0000 - val_rmse: 25168.6289\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 931899520.0000 - rmse: 30527.0273 - val_loss: 616752128.0000 - val_rmse: 24834.4922\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 949532672.0000 - rmse: 30814.4863 - val_loss: 503730688.0000 - val_rmse: 22443.9434\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 924130816.0000 - rmse: 30399.5195 - val_loss: 586994944.0000 - val_rmse: 24227.9766\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1008381824.0000 - rmse: 31755.0273 - val_loss: 555119232.0000 - val_rmse: 23560.9668\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 967010688.0000 - rmse: 31096.7949 - val_loss: 500752704.0000 - val_rmse: 22377.5039\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 902835904.0000 - rmse: 30047.2285 - val_loss: 503390720.0000 - val_rmse: 22436.3691\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 935231296.0000 - rmse: 30581.5488 - val_loss: 623371904.0000 - val_rmse: 24967.4141\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 974838144.0000 - rmse: 31222.3965 - val_loss: 558974272.0000 - val_rmse: 23642.6348\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 885990912.0000 - rmse: 29765.5977 - val_loss: 484068288.0000 - val_rmse: 22001.5488\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 899021440.0000 - rmse: 29983.6855 - val_loss: 488964160.0000 - val_rmse: 22112.5332\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 882327488.0000 - rmse: 29703.9980 - val_loss: 500354496.0000 - val_rmse: 22368.6035\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 913162816.0000 - rmse: 30218.5820 - val_loss: 586004800.0000 - val_rmse: 24207.5332\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 889748544.0000 - rmse: 29828.6504 - val_loss: 463978752.0000 - val_rmse: 21540.1621\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 896071872.0000 - rmse: 29934.4570 - val_loss: 460330144.0000 - val_rmse: 21455.3027\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 891377216.0000 - rmse: 29855.9414 - val_loss: 533550528.0000 - val_rmse: 23098.7109\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 827075904.0000 - rmse: 28758.9277 - val_loss: 481396320.0000 - val_rmse: 21940.7422\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 862680064.0000 - rmse: 29371.4141 - val_loss: 461422656.0000 - val_rmse: 21480.7500\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 900384832.0000 - rmse: 30006.4141 - val_loss: 535113568.0000 - val_rmse: 23132.5176\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 858333504.0000 - rmse: 29297.3262 - val_loss: 496551840.0000 - val_rmse: 22283.4395\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 837487872.0000 - rmse: 28939.3828 - val_loss: 444246656.0000 - val_rmse: 21077.1582\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 859328576.0000 - rmse: 29314.3066 - val_loss: 525444480.0000 - val_rmse: 22922.5723\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 851856768.0000 - rmse: 29186.5840 - val_loss: 427418688.0000 - val_rmse: 20674.1035\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 847683328.0000 - rmse: 29114.9980 - val_loss: 428511040.0000 - val_rmse: 20700.5059\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 852455040.0000 - rmse: 29196.8320 - val_loss: 453478080.0000 - val_rmse: 21295.0215\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 781243584.0000 - rmse: 27950.7344 - val_loss: 459499232.0000 - val_rmse: 21435.9336\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 735422144.0000 - rmse: 27118.6680 - val_loss: 463860256.0000 - val_rmse: 21537.4121\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 794906560.0000 - rmse: 28194.0840 - val_loss: 448722848.0000 - val_rmse: 21183.0781\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 792681408.0000 - rmse: 28154.5938 - val_loss: 549672448.0000 - val_rmse: 23445.0898\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 799396992.0000 - rmse: 28273.6074 - val_loss: 423334272.0000 - val_rmse: 20575.0840\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800295488.0000 - rmse: 28289.4922 - val_loss: 485688704.0000 - val_rmse: 22038.3418\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 767808768.0000 - rmse: 27709.3633 - val_loss: 515369280.0000 - val_rmse: 22701.7422\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 760530496.0000 - rmse: 27577.7168 - val_loss: 521368416.0000 - val_rmse: 22833.4902\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 832870912.0000 - rmse: 28859.5000 - val_loss: 426682944.0000 - val_rmse: 20656.3027\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 771895488.0000 - rmse: 27783.0020 - val_loss: 440677280.0000 - val_rmse: 20992.3125\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 768820032.0000 - rmse: 27727.6016 - val_loss: 494817984.0000 - val_rmse: 22244.5020\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 783913856.0000 - rmse: 27998.4590 - val_loss: 444847712.0000 - val_rmse: 21091.4102\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 787324416.0000 - rmse: 28059.3027 - val_loss: 456900352.0000 - val_rmse: 21375.2246\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 711259968.0000 - rmse: 26669.4551 - val_loss: 465986624.0000 - val_rmse: 21586.7227\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 795374208.0000 - rmse: 28202.3770 - val_loss: 397213440.0000 - val_rmse: 19930.2090\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 770330368.0000 - rmse: 27754.8262 - val_loss: 512664352.0000 - val_rmse: 22642.0898\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 768331904.0000 - rmse: 27718.7988 - val_loss: 406111424.0000 - val_rmse: 20152.2031\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 835665600.0000 - rmse: 28907.8789 - val_loss: 412084128.0000 - val_rmse: 20299.8535\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 762507712.0000 - rmse: 27613.5391 - val_loss: 386447424.0000 - val_rmse: 19658.2656\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 769172352.0000 - rmse: 27733.9570 - val_loss: 392680928.0000 - val_rmse: 19816.1777\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 743916736.0000 - rmse: 27274.8340 - val_loss: 541629568.0000 - val_rmse: 23272.9355\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 746307456.0000 - rmse: 27318.6270 - val_loss: 463216352.0000 - val_rmse: 21522.4590\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 794531136.0000 - rmse: 28187.4258 - val_loss: 407421312.0000 - val_rmse: 20184.6777\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 735619968.0000 - rmse: 27122.3125 - val_loss: 412680160.0000 - val_rmse: 20314.5293\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 716686208.0000 - rmse: 26770.9941 - val_loss: 685820608.0000 - val_rmse: 26188.1738\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 770139776.0000 - rmse: 27751.3887 - val_loss: 403384160.0000 - val_rmse: 20084.4258\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 725668096.0000 - rmse: 26938.2246 - val_loss: 398013472.0000 - val_rmse: 19950.2715\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 743208000.0000 - rmse: 27261.8398 - val_loss: 413549504.0000 - val_rmse: 20335.9121\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 728370688.0000 - rmse: 26988.3418 - val_loss: 378137792.0000 - val_rmse: 19445.7617\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 747142528.0000 - rmse: 27333.9082 - val_loss: 394407840.0000 - val_rmse: 19859.7031\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 688247808.0000 - rmse: 26234.4766 - val_loss: 437234112.0000 - val_rmse: 20910.1426\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 741315392.0000 - rmse: 27227.1035 - val_loss: 380372768.0000 - val_rmse: 19503.1426\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 697452288.0000 - rmse: 26409.3203 - val_loss: 393514112.0000 - val_rmse: 19837.1875\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 728754688.0000 - rmse: 26995.4551 - val_loss: 386777152.0000 - val_rmse: 19666.6484\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 776289472.0000 - rmse: 27861.9688 - val_loss: 370230208.0000 - val_rmse: 19241.3633\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 683448640.0000 - rmse: 26142.8516 - val_loss: 376595360.0000 - val_rmse: 19406.0625\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 673248000.0000 - rmse: 25947.0215 - val_loss: 355659168.0000 - val_rmse: 18858.9219\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680217536.0000 - rmse: 26080.9785 - val_loss: 388528384.0000 - val_rmse: 19711.1191\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 674788864.0000 - rmse: 25976.6953 - val_loss: 401736032.0000 - val_rmse: 20043.3496\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698259392.0000 - rmse: 26424.5977 - val_loss: 351918016.0000 - val_rmse: 18759.4727\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 709046080.0000 - rmse: 26627.9180 - val_loss: 361135392.0000 - val_rmse: 19003.5605\n",
      "Epoch 111/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 737666688.0000 - rmse: 27160.0195 - val_loss: 408995296.0000 - val_rmse: 20223.6289\n",
      "Epoch 112/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 707757824.0000 - rmse: 26603.7168 - val_loss: 381406720.0000 - val_rmse: 19529.6328\n",
      "Epoch 113/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684239744.0000 - rmse: 26157.9746 - val_loss: 376803968.0000 - val_rmse: 19411.4375\n",
      "Epoch 114/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 671027392.0000 - rmse: 25904.1914 - val_loss: 373829920.0000 - val_rmse: 19334.6777\n",
      "Epoch 115/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684892544.0000 - rmse: 26170.4512 - val_loss: 334932224.0000 - val_rmse: 18301.1504\n",
      "Epoch 116/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 700032256.0000 - rmse: 26458.1172 - val_loss: 339703392.0000 - val_rmse: 18431.0430\n",
      "Epoch 117/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649018048.0000 - rmse: 25475.8301 - val_loss: 420333440.0000 - val_rmse: 20502.0312\n",
      "Epoch 118/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 695428864.0000 - rmse: 26370.9824 - val_loss: 341147232.0000 - val_rmse: 18470.1699\n",
      "Epoch 119/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676424448.0000 - rmse: 26008.1562 - val_loss: 369741824.0000 - val_rmse: 19228.6660\n",
      "Epoch 120/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 664889856.0000 - rmse: 25785.4551 - val_loss: 425973536.0000 - val_rmse: 20639.1230\n",
      "Epoch 121/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 672401600.0000 - rmse: 25930.7051 - val_loss: 431211424.0000 - val_rmse: 20765.6289\n",
      "Epoch 122/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 657264384.0000 - rmse: 25637.1660 - val_loss: 358474912.0000 - val_rmse: 18933.4316\n",
      "Epoch 123/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 683986368.0000 - rmse: 26153.1309 - val_loss: 316295264.0000 - val_rmse: 17784.6875\n",
      "Epoch 124/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 762311808.0000 - rmse: 27609.9922 - val_loss: 326188704.0000 - val_rmse: 18060.6914\n",
      "Epoch 125/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751623168.0000 - rmse: 27415.7441 - val_loss: 335546016.0000 - val_rmse: 18317.9121\n",
      "Epoch 126/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 715431616.0000 - rmse: 26747.5508 - val_loss: 341259456.0000 - val_rmse: 18473.2070\n",
      "Epoch 127/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 619988352.0000 - rmse: 24899.5605 - val_loss: 321643200.0000 - val_rmse: 17934.4102\n",
      "Epoch 128/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651924544.0000 - rmse: 25532.8105 - val_loss: 407227680.0000 - val_rmse: 20179.8750\n",
      "Epoch 129/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607300480.0000 - rmse: 24643.4629 - val_loss: 316942432.0000 - val_rmse: 17802.8730\n",
      "Epoch 130/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652013888.0000 - rmse: 25534.5625 - val_loss: 315885472.0000 - val_rmse: 17773.1641\n",
      "Epoch 131/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665730560.0000 - rmse: 25801.7520 - val_loss: 319155296.0000 - val_rmse: 17864.9141\n",
      "Epoch 132/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 650829696.0000 - rmse: 25511.3613 - val_loss: 345636160.0000 - val_rmse: 18591.2871\n",
      "Epoch 133/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 663170752.0000 - rmse: 25752.1016 - val_loss: 338572736.0000 - val_rmse: 18400.3418\n",
      "Epoch 134/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 608331200.0000 - rmse: 24664.3672 - val_loss: 308673536.0000 - val_rmse: 17569.1055\n",
      "Epoch 135/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656472128.0000 - rmse: 25621.7070 - val_loss: 304999104.0000 - val_rmse: 17464.2207\n",
      "Epoch 136/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 653611200.0000 - rmse: 25565.8184 - val_loss: 311264928.0000 - val_rmse: 17642.6992\n",
      "Epoch 137/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 602106560.0000 - rmse: 24537.8555 - val_loss: 337750912.0000 - val_rmse: 18377.9980\n",
      "Epoch 138/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616284096.0000 - rmse: 24825.0664 - val_loss: 314899680.0000 - val_rmse: 17745.4082\n",
      "Epoch 139/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647841472.0000 - rmse: 25452.7266 - val_loss: 370846976.0000 - val_rmse: 19257.3848\n",
      "Epoch 140/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 658125504.0000 - rmse: 25653.9551 - val_loss: 355857408.0000 - val_rmse: 18864.1777\n",
      "Epoch 141/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623230400.0000 - rmse: 24964.5820 - val_loss: 298686304.0000 - val_rmse: 17282.5391\n",
      "Epoch 142/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558484736.0000 - rmse: 23632.2793 - val_loss: 313163488.0000 - val_rmse: 17696.4219\n",
      "Epoch 143/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577460672.0000 - rmse: 24030.4082 - val_loss: 361534400.0000 - val_rmse: 19014.0547\n",
      "Epoch 144/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 644101184.0000 - rmse: 25379.1465 - val_loss: 312086560.0000 - val_rmse: 17665.9668\n",
      "Epoch 145/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641416064.0000 - rmse: 25326.1895 - val_loss: 294470112.0000 - val_rmse: 17160.1270\n",
      "Epoch 146/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584980736.0000 - rmse: 24186.3711 - val_loss: 335534848.0000 - val_rmse: 18317.6055\n",
      "Epoch 147/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561688000.0000 - rmse: 23699.9570 - val_loss: 470250784.0000 - val_rmse: 21685.2617\n",
      "Epoch 148/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634450944.0000 - rmse: 25188.3086 - val_loss: 323653760.0000 - val_rmse: 17990.3770\n",
      "Epoch 149/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 650653184.0000 - rmse: 25507.9023 - val_loss: 359504672.0000 - val_rmse: 18960.6035\n",
      "Epoch 150/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580242688.0000 - rmse: 24088.2246 - val_loss: 318359808.0000 - val_rmse: 17842.6367\n",
      "Epoch 151/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589598976.0000 - rmse: 24281.6562 - val_loss: 332718016.0000 - val_rmse: 18240.5566\n",
      "Epoch 152/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 688372608.0000 - rmse: 26236.8535 - val_loss: 302537536.0000 - val_rmse: 17393.6035\n",
      "Epoch 153/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 600378752.0000 - rmse: 24502.6230 - val_loss: 331818592.0000 - val_rmse: 18215.8809\n",
      "Epoch 154/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604879104.0000 - rmse: 24594.2852 - val_loss: 308738976.0000 - val_rmse: 17570.9668\n",
      "Epoch 155/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604481472.0000 - rmse: 24586.2031 - val_loss: 352677344.0000 - val_rmse: 18779.7031\n",
      "Epoch 156/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578526144.0000 - rmse: 24052.5684 - val_loss: 319467200.0000 - val_rmse: 17873.6406\n",
      "Epoch 157/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 602870144.0000 - rmse: 24553.4121 - val_loss: 310846688.0000 - val_rmse: 17630.8379\n",
      "Epoch 158/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584789376.0000 - rmse: 24182.4180 - val_loss: 316544320.0000 - val_rmse: 17791.6855\n",
      "Epoch 159/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558535296.0000 - rmse: 23633.3477 - val_loss: 390893248.0000 - val_rmse: 19771.0176\n",
      "Epoch 160/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547772736.0000 - rmse: 23404.5391 - val_loss: 310096512.0000 - val_rmse: 17609.5527\n",
      "Epoch 161/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580714624.0000 - rmse: 24098.0176 - val_loss: 288969280.0000 - val_rmse: 16999.0918\n",
      "Epoch 162/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589595584.0000 - rmse: 24281.5820 - val_loss: 290131488.0000 - val_rmse: 17033.2422\n",
      "Epoch 163/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547679808.0000 - rmse: 23402.5566 - val_loss: 364687424.0000 - val_rmse: 19096.7871\n",
      "Epoch 164/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524277536.0000 - rmse: 22897.1016 - val_loss: 283484864.0000 - val_rmse: 16837.0039\n",
      "Epoch 165/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548960640.0000 - rmse: 23429.9082 - val_loss: 296961376.0000 - val_rmse: 17232.5625\n",
      "Epoch 166/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521985888.0000 - rmse: 22847.0078 - val_loss: 336579136.0000 - val_rmse: 18346.0879\n",
      "Epoch 167/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576396032.0000 - rmse: 24008.2461 - val_loss: 270035808.0000 - val_rmse: 16432.7617\n",
      "Epoch 168/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542650048.0000 - rmse: 23294.8477 - val_loss: 346808992.0000 - val_rmse: 18622.8027\n",
      "Epoch 169/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489418144.0000 - rmse: 22122.7910 - val_loss: 317768800.0000 - val_rmse: 17826.0664\n",
      "Epoch 170/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590996416.0000 - rmse: 24310.4121 - val_loss: 309171776.0000 - val_rmse: 17583.2773\n",
      "Epoch 171/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 600661824.0000 - rmse: 24508.4023 - val_loss: 288137664.0000 - val_rmse: 16974.6152\n",
      "Epoch 172/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538535744.0000 - rmse: 23206.3711 - val_loss: 298224224.0000 - val_rmse: 17269.1660\n",
      "Epoch 173/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545151488.0000 - rmse: 23348.4746 - val_loss: 385671392.0000 - val_rmse: 19638.5117\n",
      "Epoch 174/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560201472.0000 - rmse: 23668.5703 - val_loss: 313390528.0000 - val_rmse: 17702.8359\n",
      "Epoch 175/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592509760.0000 - rmse: 24341.5234 - val_loss: 341167712.0000 - val_rmse: 18470.7188\n",
      "Epoch 176/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570215680.0000 - rmse: 23879.1875 - val_loss: 323051200.0000 - val_rmse: 17973.6211\n",
      "Epoch 177/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521803168.0000 - rmse: 22843.0078 - val_loss: 324034976.0000 - val_rmse: 18000.9648\n",
      "Epoch 178/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530817760.0000 - rmse: 23039.4785 - val_loss: 348663520.0000 - val_rmse: 18672.5293\n",
      "Epoch 179/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506262240.0000 - rmse: 22500.2676 - val_loss: 304806528.0000 - val_rmse: 17458.7051\n",
      "Epoch 180/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554196032.0000 - rmse: 23541.3652 - val_loss: 382033152.0000 - val_rmse: 19545.6641\n",
      "Epoch 181/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534522272.0000 - rmse: 23119.7324 - val_loss: 279474080.0000 - val_rmse: 16717.4707\n",
      "Epoch 182/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498693248.0000 - rmse: 22331.4355 - val_loss: 293693824.0000 - val_rmse: 17137.4922\n",
      "Epoch 183/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525981120.0000 - rmse: 22934.2754 - val_loss: 344909088.0000 - val_rmse: 18571.7227\n",
      "Epoch 184/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556909056.0000 - rmse: 23598.9180 - val_loss: 270752192.0000 - val_rmse: 16454.5449\n",
      "Epoch 185/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526925984.0000 - rmse: 22954.8633 - val_loss: 286456320.0000 - val_rmse: 16925.0156\n",
      "Epoch 186/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493227936.0000 - rmse: 22208.7305 - val_loss: 299734624.0000 - val_rmse: 17312.8418\n",
      "Epoch 187/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529849856.0000 - rmse: 23018.4629 - val_loss: 300647488.0000 - val_rmse: 17339.1855\n",
      "Epoch 188/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508509440.0000 - rmse: 22550.1523 - val_loss: 292992384.0000 - val_rmse: 17117.0156\n",
      "Epoch 189/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520498816.0000 - rmse: 22814.4395 - val_loss: 278350432.0000 - val_rmse: 16683.8301\n",
      "104/104 [==============================] - 0s 675us/step - loss: 501570368.0000 - rmse: 22395.7637\n",
      "[501570368.0, 22395.763671875]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 17782841344.0000 - rmse: 133352.3281 - val_loss: 3369649408.0000 - val_rmse: 58048.6797\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2732299520.0000 - rmse: 52271.4023 - val_loss: 1413159296.0000 - val_rmse: 37592.0117\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1715853952.0000 - rmse: 41422.8672 - val_loss: 1214517632.0000 - val_rmse: 34849.9297\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1599613312.0000 - rmse: 39995.1641 - val_loss: 1156564352.0000 - val_rmse: 34008.2969\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1491179136.0000 - rmse: 38615.7891 - val_loss: 1121180544.0000 - val_rmse: 33484.0352\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1464434432.0000 - rmse: 38267.9297 - val_loss: 1093371776.0000 - val_rmse: 33066.1719\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1406883840.0000 - rmse: 37508.4492 - val_loss: 1026660672.0000 - val_rmse: 32041.5469\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1374891008.0000 - rmse: 37079.5234 - val_loss: 1002310272.0000 - val_rmse: 31659.2832\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1332633856.0000 - rmse: 36505.2578 - val_loss: 951144320.0000 - val_rmse: 30840.6270\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1225286400.0000 - rmse: 35004.0898 - val_loss: 929939584.0000 - val_rmse: 30494.9102\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1220919040.0000 - rmse: 34941.6523 - val_loss: 948743488.0000 - val_rmse: 30801.6777\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1167810688.0000 - rmse: 34173.2461 - val_loss: 872954432.0000 - val_rmse: 29545.8027\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1103351552.0000 - rmse: 33216.7344 - val_loss: 844137472.0000 - val_rmse: 29054.0449\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1101470848.0000 - rmse: 33188.4141 - val_loss: 833720832.0000 - val_rmse: 28874.2227\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1172008448.0000 - rmse: 34234.6094 - val_loss: 823315392.0000 - val_rmse: 28693.4727\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1054574720.0000 - rmse: 32474.2148 - val_loss: 932880128.0000 - val_rmse: 30543.0859\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1031892544.0000 - rmse: 32123.0820 - val_loss: 833327040.0000 - val_rmse: 28867.4043\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1060968896.0000 - rmse: 32572.5176 - val_loss: 777401664.0000 - val_rmse: 27881.9219\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1011668736.0000 - rmse: 31806.7402 - val_loss: 788297920.0000 - val_rmse: 28076.6406\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 952597120.0000 - rmse: 30864.1699 - val_loss: 755844864.0000 - val_rmse: 27492.6309\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 997901888.0000 - rmse: 31589.5859 - val_loss: 793571264.0000 - val_rmse: 28170.3926\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 963805632.0000 - rmse: 31045.2188 - val_loss: 754166144.0000 - val_rmse: 27462.0840\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 962004544.0000 - rmse: 31016.1973 - val_loss: 712143936.0000 - val_rmse: 26686.0234\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 997771904.0000 - rmse: 31587.5273 - val_loss: 707783936.0000 - val_rmse: 26604.2070\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 953650816.0000 - rmse: 30881.2363 - val_loss: 706416256.0000 - val_rmse: 26578.4922\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 889346816.0000 - rmse: 29821.9180 - val_loss: 681235264.0000 - val_rmse: 26100.4805\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 923609344.0000 - rmse: 30390.9414 - val_loss: 681479360.0000 - val_rmse: 26105.1582\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 926405824.0000 - rmse: 30436.9160 - val_loss: 823524288.0000 - val_rmse: 28697.1113\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 935384320.0000 - rmse: 30584.0527 - val_loss: 713791168.0000 - val_rmse: 26716.8711\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 901186304.0000 - rmse: 30019.7637 - val_loss: 660969344.0000 - val_rmse: 25709.3223\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 935868608.0000 - rmse: 30591.9668 - val_loss: 653151744.0000 - val_rmse: 25556.8320\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 926874688.0000 - rmse: 30444.6172 - val_loss: 650922432.0000 - val_rmse: 25513.1797\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 879208960.0000 - rmse: 29651.4570 - val_loss: 644500800.0000 - val_rmse: 25387.0195\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 871778816.0000 - rmse: 29525.9004 - val_loss: 645521088.0000 - val_rmse: 25407.1074\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 854335424.0000 - rmse: 29229.0137 - val_loss: 649338752.0000 - val_rmse: 25482.1250\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 859868288.0000 - rmse: 29323.5117 - val_loss: 623017856.0000 - val_rmse: 24960.3242\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 878308416.0000 - rmse: 29636.2676 - val_loss: 684603136.0000 - val_rmse: 26164.9199\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 822798528.0000 - rmse: 28684.4629 - val_loss: 703878784.0000 - val_rmse: 26530.7129\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 860450368.0000 - rmse: 29333.4316 - val_loss: 648191616.0000 - val_rmse: 25459.6055\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 828014464.0000 - rmse: 28775.2402 - val_loss: 618039360.0000 - val_rmse: 24860.3965\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 802552768.0000 - rmse: 28329.3594 - val_loss: 606877760.0000 - val_rmse: 24634.8887\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 748857664.0000 - rmse: 27365.2617 - val_loss: 606823680.0000 - val_rmse: 24633.7891\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 813457280.0000 - rmse: 28521.1719 - val_loss: 657960832.0000 - val_rmse: 25650.7461\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 819941632.0000 - rmse: 28634.6211 - val_loss: 600413056.0000 - val_rmse: 24503.3242\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 745154624.0000 - rmse: 27297.5176 - val_loss: 610314752.0000 - val_rmse: 24704.5488\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 814532928.0000 - rmse: 28540.0195 - val_loss: 609785088.0000 - val_rmse: 24693.8223\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 766164480.0000 - rmse: 27679.6758 - val_loss: 701782784.0000 - val_rmse: 26491.1816\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 769479744.0000 - rmse: 27739.4941 - val_loss: 622107520.0000 - val_rmse: 24942.0801\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 774959424.0000 - rmse: 27838.0898 - val_loss: 579916736.0000 - val_rmse: 24081.4570\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 775909760.0000 - rmse: 27855.1562 - val_loss: 595523904.0000 - val_rmse: 24403.3594\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 773683136.0000 - rmse: 27815.1562 - val_loss: 609655296.0000 - val_rmse: 24691.1973\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 764268160.0000 - rmse: 27645.3965 - val_loss: 578703616.0000 - val_rmse: 24056.2559\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 745397952.0000 - rmse: 27301.9727 - val_loss: 608044736.0000 - val_rmse: 24658.5605\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 715068480.0000 - rmse: 26740.7617 - val_loss: 575701120.0000 - val_rmse: 23993.7695\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 747181824.0000 - rmse: 27334.6250 - val_loss: 624829952.0000 - val_rmse: 24996.5957\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 760120832.0000 - rmse: 27570.2891 - val_loss: 570074112.0000 - val_rmse: 23876.2227\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 673828864.0000 - rmse: 25958.2109 - val_loss: 569041856.0000 - val_rmse: 23854.5977\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730437184.0000 - rmse: 27026.5996 - val_loss: 608140480.0000 - val_rmse: 24660.5020\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 740537280.0000 - rmse: 27212.8145 - val_loss: 578432192.0000 - val_rmse: 24050.6152\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742473600.0000 - rmse: 27248.3691 - val_loss: 573201600.0000 - val_rmse: 23941.6270\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 712861120.0000 - rmse: 26699.4570 - val_loss: 629453184.0000 - val_rmse: 25088.9023\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 678476864.0000 - rmse: 26047.5879 - val_loss: 555156800.0000 - val_rmse: 23561.7637\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698353856.0000 - rmse: 26426.3848 - val_loss: 563187776.0000 - val_rmse: 23731.5742\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656269632.0000 - rmse: 25617.7598 - val_loss: 547782720.0000 - val_rmse: 23404.7559\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666311040.0000 - rmse: 25812.9980 - val_loss: 564908096.0000 - val_rmse: 23767.7949\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654126144.0000 - rmse: 25575.8848 - val_loss: 556048000.0000 - val_rmse: 23580.6680\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 669738880.0000 - rmse: 25879.3105 - val_loss: 581012480.0000 - val_rmse: 24104.1973\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640896192.0000 - rmse: 25315.9258 - val_loss: 535715392.0000 - val_rmse: 23145.5234\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 689819456.0000 - rmse: 26264.4121 - val_loss: 565287872.0000 - val_rmse: 23775.7812\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685416448.0000 - rmse: 26180.4590 - val_loss: 533622112.0000 - val_rmse: 23100.2598\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 663037312.0000 - rmse: 25749.5078 - val_loss: 516119072.0000 - val_rmse: 22718.2500\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640882304.0000 - rmse: 25315.6504 - val_loss: 529685504.0000 - val_rmse: 23014.8926\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632301184.0000 - rmse: 25145.5938 - val_loss: 529894560.0000 - val_rmse: 23019.4375\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680488576.0000 - rmse: 26086.1738 - val_loss: 505304832.0000 - val_rmse: 22478.9844\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 633357248.0000 - rmse: 25166.5879 - val_loss: 504066688.0000 - val_rmse: 22451.4277\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628467392.0000 - rmse: 25069.2500 - val_loss: 527703040.0000 - val_rmse: 22971.7852\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 650324672.0000 - rmse: 25501.4590 - val_loss: 512583488.0000 - val_rmse: 22640.3047\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 617297728.0000 - rmse: 24845.4766 - val_loss: 521996832.0000 - val_rmse: 22847.2441\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621123200.0000 - rmse: 24922.3418 - val_loss: 511700928.0000 - val_rmse: 22620.8027\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580438848.0000 - rmse: 24092.2949 - val_loss: 504366784.0000 - val_rmse: 22458.1074\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585882240.0000 - rmse: 24205.0039 - val_loss: 498635648.0000 - val_rmse: 22330.1465\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 608782784.0000 - rmse: 24673.5195 - val_loss: 512164416.0000 - val_rmse: 22631.0469\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624320320.0000 - rmse: 24986.4023 - val_loss: 477671296.0000 - val_rmse: 21855.6914\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592898112.0000 - rmse: 24349.4961 - val_loss: 472846752.0000 - val_rmse: 21745.0371\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542562176.0000 - rmse: 23292.9609 - val_loss: 467120960.0000 - val_rmse: 21612.9785\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591285184.0000 - rmse: 24316.3555 - val_loss: 482146560.0000 - val_rmse: 21957.8340\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569663680.0000 - rmse: 23867.6270 - val_loss: 485792928.0000 - val_rmse: 22040.7090\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590525760.0000 - rmse: 24300.7344 - val_loss: 476536032.0000 - val_rmse: 21829.7031\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604925056.0000 - rmse: 24595.2207 - val_loss: 489145344.0000 - val_rmse: 22116.6289\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 581925696.0000 - rmse: 24123.1367 - val_loss: 466226752.0000 - val_rmse: 21592.2812\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582898048.0000 - rmse: 24143.2812 - val_loss: 491830272.0000 - val_rmse: 22177.2441\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565290112.0000 - rmse: 23775.8281 - val_loss: 468673536.0000 - val_rmse: 21648.8652\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592094720.0000 - rmse: 24332.9961 - val_loss: 476176544.0000 - val_rmse: 21821.4668\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584299456.0000 - rmse: 24172.2852 - val_loss: 458169696.0000 - val_rmse: 21404.8965\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510174688.0000 - rmse: 22587.0469 - val_loss: 446201952.0000 - val_rmse: 21123.4902\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552311296.0000 - rmse: 23501.3008 - val_loss: 437030496.0000 - val_rmse: 20905.2715\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551556928.0000 - rmse: 23485.2480 - val_loss: 460011040.0000 - val_rmse: 21447.8672\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560123648.0000 - rmse: 23666.9297 - val_loss: 496086240.0000 - val_rmse: 22272.9922\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497474784.0000 - rmse: 22304.1426 - val_loss: 511771104.0000 - val_rmse: 22622.3555\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559916736.0000 - rmse: 23662.5566 - val_loss: 449341984.0000 - val_rmse: 21197.6855\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539984640.0000 - rmse: 23237.5664 - val_loss: 451121984.0000 - val_rmse: 21239.6309\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524201120.0000 - rmse: 22895.4375 - val_loss: 449886112.0000 - val_rmse: 21210.5156\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521522784.0000 - rmse: 22836.8730 - val_loss: 486488320.0000 - val_rmse: 22056.4766\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543209856.0000 - rmse: 23306.8613 - val_loss: 508553632.0000 - val_rmse: 22551.1328\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478725568.0000 - rmse: 21879.7969 - val_loss: 429891968.0000 - val_rmse: 20733.8320\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501966336.0000 - rmse: 22404.5996 - val_loss: 423815456.0000 - val_rmse: 20586.7754\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509120992.0000 - rmse: 22563.7070 - val_loss: 471221216.0000 - val_rmse: 21707.6289\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521827904.0000 - rmse: 22843.5488 - val_loss: 473433728.0000 - val_rmse: 21758.5312\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495744032.0000 - rmse: 22265.3086 - val_loss: 462109376.0000 - val_rmse: 21496.7266\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493297728.0000 - rmse: 22210.3047 - val_loss: 442092864.0000 - val_rmse: 21026.0020\n",
      "Epoch 111/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469330848.0000 - rmse: 21664.0410 - val_loss: 461881152.0000 - val_rmse: 21491.4160\n",
      "Epoch 112/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498086752.0000 - rmse: 22317.8516 - val_loss: 460003936.0000 - val_rmse: 21447.6992\n",
      "Epoch 113/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474959680.0000 - rmse: 21793.5684 - val_loss: 445180256.0000 - val_rmse: 21099.2910\n",
      "Epoch 114/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491759808.0000 - rmse: 22175.6523 - val_loss: 448887488.0000 - val_rmse: 21186.9609\n",
      "Epoch 115/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547714752.0000 - rmse: 23403.3047 - val_loss: 425013088.0000 - val_rmse: 20615.8418\n",
      "Epoch 116/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483958080.0000 - rmse: 21999.0430 - val_loss: 446037088.0000 - val_rmse: 21119.5879\n",
      "Epoch 117/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478937568.0000 - rmse: 21884.6387 - val_loss: 475299264.0000 - val_rmse: 21801.3574\n",
      "Epoch 118/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495514592.0000 - rmse: 22260.1543 - val_loss: 460905376.0000 - val_rmse: 21468.7031\n",
      "Epoch 119/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467856576.0000 - rmse: 21629.9863 - val_loss: 417585568.0000 - val_rmse: 20434.9082\n",
      "Epoch 120/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445604896.0000 - rmse: 21109.3516 - val_loss: 418417120.0000 - val_rmse: 20455.2441\n",
      "Epoch 121/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483851904.0000 - rmse: 21996.6309 - val_loss: 425200416.0000 - val_rmse: 20620.3867\n",
      "Epoch 122/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458601632.0000 - rmse: 21414.9805 - val_loss: 430565792.0000 - val_rmse: 20750.0762\n",
      "Epoch 123/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499121248.0000 - rmse: 22341.0176 - val_loss: 444159872.0000 - val_rmse: 21075.0977\n",
      "Epoch 124/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480064032.0000 - rmse: 21910.3633 - val_loss: 411491904.0000 - val_rmse: 20285.2617\n",
      "Epoch 125/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453467936.0000 - rmse: 21294.7852 - val_loss: 474961568.0000 - val_rmse: 21793.6113\n",
      "Epoch 126/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470515008.0000 - rmse: 21691.3555 - val_loss: 396348928.0000 - val_rmse: 19908.5117\n",
      "Epoch 127/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425369344.0000 - rmse: 20624.4805 - val_loss: 441864736.0000 - val_rmse: 21020.5762\n",
      "Epoch 128/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442467648.0000 - rmse: 21034.9121 - val_loss: 391083680.0000 - val_rmse: 19775.8320\n",
      "Epoch 129/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487195968.0000 - rmse: 22072.5117 - val_loss: 399555776.0000 - val_rmse: 19988.8887\n",
      "Epoch 130/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460671008.0000 - rmse: 21463.2441 - val_loss: 402883136.0000 - val_rmse: 20071.9453\n",
      "Epoch 131/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455791168.0000 - rmse: 21349.2637 - val_loss: 435302208.0000 - val_rmse: 20863.8945\n",
      "Epoch 132/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432940384.0000 - rmse: 20807.2188 - val_loss: 409029888.0000 - val_rmse: 20224.4844\n",
      "Epoch 133/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419822080.0000 - rmse: 20489.5566 - val_loss: 537575488.0000 - val_rmse: 23185.6719\n",
      "Epoch 134/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457779776.0000 - rmse: 21395.7852 - val_loss: 415550816.0000 - val_rmse: 20385.0605\n",
      "Epoch 135/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426088800.0000 - rmse: 20641.9160 - val_loss: 393934496.0000 - val_rmse: 19847.7812\n",
      "Epoch 136/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443771168.0000 - rmse: 21065.8750 - val_loss: 411154496.0000 - val_rmse: 20276.9414\n",
      "Epoch 137/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446398784.0000 - rmse: 21128.1465 - val_loss: 436544448.0000 - val_rmse: 20893.6426\n",
      "Epoch 138/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430371008.0000 - rmse: 20745.3809 - val_loss: 412307008.0000 - val_rmse: 20305.3418\n",
      "Epoch 139/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423303680.0000 - rmse: 20574.3438 - val_loss: 389235072.0000 - val_rmse: 19729.0391\n",
      "Epoch 140/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435206752.0000 - rmse: 20861.6074 - val_loss: 378477632.0000 - val_rmse: 19454.4980\n",
      "Epoch 141/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441675168.0000 - rmse: 21016.0645 - val_loss: 377002976.0000 - val_rmse: 19416.5605\n",
      "Epoch 142/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461191456.0000 - rmse: 21475.3672 - val_loss: 387358496.0000 - val_rmse: 19681.4219\n",
      "Epoch 143/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457999264.0000 - rmse: 21400.9141 - val_loss: 399163232.0000 - val_rmse: 19979.0684\n",
      "Epoch 144/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422403904.0000 - rmse: 20552.4648 - val_loss: 396403904.0000 - val_rmse: 19909.8906\n",
      "Epoch 145/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401933312.0000 - rmse: 20048.2715 - val_loss: 383534208.0000 - val_rmse: 19584.0254\n",
      "Epoch 146/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435725952.0000 - rmse: 20874.0449 - val_loss: 464276832.0000 - val_rmse: 21547.0801\n",
      "Epoch 147/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393868736.0000 - rmse: 19846.1250 - val_loss: 384618880.0000 - val_rmse: 19611.6992\n",
      "Epoch 148/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462451392.0000 - rmse: 21504.6816 - val_loss: 405123200.0000 - val_rmse: 20127.6699\n",
      "Epoch 149/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408780512.0000 - rmse: 20218.3164 - val_loss: 401576448.0000 - val_rmse: 20039.3711\n",
      "Epoch 150/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430692160.0000 - rmse: 20753.1211 - val_loss: 383230080.0000 - val_rmse: 19576.2598\n",
      "Epoch 151/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413045632.0000 - rmse: 20323.5195 - val_loss: 462117408.0000 - val_rmse: 21496.9160\n",
      "Epoch 152/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436643456.0000 - rmse: 20896.0117 - val_loss: 402762464.0000 - val_rmse: 20068.9395\n",
      "Epoch 153/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414835584.0000 - rmse: 20367.5117 - val_loss: 409693504.0000 - val_rmse: 20240.8828\n",
      "Epoch 154/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387701632.0000 - rmse: 19690.1367 - val_loss: 462151616.0000 - val_rmse: 21497.7090\n",
      "Epoch 155/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434831616.0000 - rmse: 20852.6133 - val_loss: 406599424.0000 - val_rmse: 20164.3066\n",
      "Epoch 156/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439303424.0000 - rmse: 20959.5645 - val_loss: 387865504.0000 - val_rmse: 19694.2969\n",
      "Epoch 157/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399348832.0000 - rmse: 19983.7109 - val_loss: 381493856.0000 - val_rmse: 19531.8652\n",
      "Epoch 158/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397493152.0000 - rmse: 19937.2266 - val_loss: 418688160.0000 - val_rmse: 20461.8672\n",
      "Epoch 159/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439288640.0000 - rmse: 20959.2109 - val_loss: 376786496.0000 - val_rmse: 19410.9844\n",
      "Epoch 160/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379845312.0000 - rmse: 19489.6172 - val_loss: 378994016.0000 - val_rmse: 19467.7656\n",
      "Epoch 161/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400371072.0000 - rmse: 20009.2715 - val_loss: 380981024.0000 - val_rmse: 19518.7324\n",
      "Epoch 162/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407913792.0000 - rmse: 20196.8730 - val_loss: 380154720.0000 - val_rmse: 19497.5527\n",
      "Epoch 163/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380961632.0000 - rmse: 19518.2344 - val_loss: 379524704.0000 - val_rmse: 19481.3926\n",
      "Epoch 164/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395359424.0000 - rmse: 19883.6465 - val_loss: 394016544.0000 - val_rmse: 19849.8457\n",
      "Epoch 165/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365525024.0000 - rmse: 19118.7070 - val_loss: 400190080.0000 - val_rmse: 20004.7480\n",
      "Epoch 166/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402570272.0000 - rmse: 20064.1484 - val_loss: 376619264.0000 - val_rmse: 19406.6777\n",
      "Epoch 167/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432195552.0000 - rmse: 20789.3086 - val_loss: 412429024.0000 - val_rmse: 20308.3457\n",
      "Epoch 168/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390053152.0000 - rmse: 19749.7617 - val_loss: 380911136.0000 - val_rmse: 19516.9414\n",
      "Epoch 169/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379205984.0000 - rmse: 19473.2090 - val_loss: 393911392.0000 - val_rmse: 19847.1973\n",
      "Epoch 170/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369625408.0000 - rmse: 19225.6406 - val_loss: 379109984.0000 - val_rmse: 19470.7422\n",
      "Epoch 171/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404539296.0000 - rmse: 20113.1582 - val_loss: 388412704.0000 - val_rmse: 19708.1855\n",
      "Epoch 172/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416209312.0000 - rmse: 20401.2051 - val_loss: 365412480.0000 - val_rmse: 19115.7637\n",
      "Epoch 173/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354219104.0000 - rmse: 18820.7031 - val_loss: 363018272.0000 - val_rmse: 19053.0371\n",
      "Epoch 174/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396645568.0000 - rmse: 19915.9590 - val_loss: 368772736.0000 - val_rmse: 19203.4531\n",
      "Epoch 175/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376634656.0000 - rmse: 19407.0742 - val_loss: 405168192.0000 - val_rmse: 20128.7871\n",
      "Epoch 176/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359463616.0000 - rmse: 18959.5215 - val_loss: 370180448.0000 - val_rmse: 19240.0703\n",
      "Epoch 177/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394526880.0000 - rmse: 19862.6973 - val_loss: 396163296.0000 - val_rmse: 19903.8477\n",
      "Epoch 178/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412557824.0000 - rmse: 20311.5176 - val_loss: 407099872.0000 - val_rmse: 20176.7109\n",
      "Epoch 179/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373981088.0000 - rmse: 19338.5859 - val_loss: 370907136.0000 - val_rmse: 19258.9453\n",
      "Epoch 180/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344217184.0000 - rmse: 18553.0879 - val_loss: 376493888.0000 - val_rmse: 19403.4473\n",
      "Epoch 181/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364670272.0000 - rmse: 19096.3379 - val_loss: 378990720.0000 - val_rmse: 19467.6797\n",
      "Epoch 182/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394484960.0000 - rmse: 19861.6406 - val_loss: 542356608.0000 - val_rmse: 23288.5488\n",
      "Epoch 183/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367643584.0000 - rmse: 19174.0293 - val_loss: 413090176.0000 - val_rmse: 20324.6172\n",
      "Epoch 184/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348818880.0000 - rmse: 18676.6895 - val_loss: 383306016.0000 - val_rmse: 19578.1992\n",
      "Epoch 185/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389186592.0000 - rmse: 19727.8086 - val_loss: 380532640.0000 - val_rmse: 19507.2422\n",
      "Epoch 186/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338743104.0000 - rmse: 18404.9707 - val_loss: 392654400.0000 - val_rmse: 19815.5078\n",
      "Epoch 187/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339771232.0000 - rmse: 18432.8789 - val_loss: 370407040.0000 - val_rmse: 19245.9590\n",
      "Epoch 188/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367877344.0000 - rmse: 19180.1250 - val_loss: 401779616.0000 - val_rmse: 20044.4395\n",
      "Epoch 189/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372273664.0000 - rmse: 19294.3906 - val_loss: 376086048.0000 - val_rmse: 19392.9336\n",
      "Epoch 190/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325160416.0000 - rmse: 18032.2012 - val_loss: 399915968.0000 - val_rmse: 19997.8965\n",
      "Epoch 191/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368176000.0000 - rmse: 19187.9102 - val_loss: 358246880.0000 - val_rmse: 18927.4082\n",
      "Epoch 192/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350840928.0000 - rmse: 18730.7441 - val_loss: 461158496.0000 - val_rmse: 21474.5977\n",
      "Epoch 193/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371527648.0000 - rmse: 19275.0469 - val_loss: 387551616.0000 - val_rmse: 19686.3262\n",
      "Epoch 194/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304555328.0000 - rmse: 17451.5098 - val_loss: 376685408.0000 - val_rmse: 19408.3809\n",
      "Epoch 195/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367564512.0000 - rmse: 19171.9688 - val_loss: 358644896.0000 - val_rmse: 18937.9180\n",
      "Epoch 196/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319971008.0000 - rmse: 17887.7285 - val_loss: 386775072.0000 - val_rmse: 19666.5938\n",
      "Epoch 197/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374844576.0000 - rmse: 19360.9004 - val_loss: 383207136.0000 - val_rmse: 19575.6738\n",
      "Epoch 198/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337573504.0000 - rmse: 18373.1680 - val_loss: 417122208.0000 - val_rmse: 20423.5684\n",
      "Epoch 199/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354247872.0000 - rmse: 18821.4707 - val_loss: 403493984.0000 - val_rmse: 20087.1562\n",
      "Epoch 200/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340055008.0000 - rmse: 18440.5762 - val_loss: 379348320.0000 - val_rmse: 19476.8613\n",
      "104/104 [==============================] - 0s 689us/step - loss: 875605440.0000 - rmse: 29590.6289\n",
      "[875605440.0, 29590.62890625]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 17545371648.0000 - rmse: 132458.9375 - val_loss: 3791540992.0000 - val_rmse: 61575.4883\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2604470016.0000 - rmse: 51034.0078 - val_loss: 1539852160.0000 - val_rmse: 39240.9492\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1676231552.0000 - rmse: 40941.8086 - val_loss: 1287640704.0000 - val_rmse: 35883.7109\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1546888960.0000 - rmse: 39330.5078 - val_loss: 1315429376.0000 - val_rmse: 36268.8477\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1477213568.0000 - rmse: 38434.5352 - val_loss: 1220379008.0000 - val_rmse: 34933.9219\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1400023808.0000 - rmse: 37416.8906 - val_loss: 1204637440.0000 - val_rmse: 34707.8867\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1385854592.0000 - rmse: 37227.0664 - val_loss: 1169556352.0000 - val_rmse: 34198.7773\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1329801728.0000 - rmse: 36466.4453 - val_loss: 1136937600.0000 - val_rmse: 33718.5039\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1303982720.0000 - rmse: 36110.6992 - val_loss: 1102223360.0000 - val_rmse: 33199.7500\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1225073920.0000 - rmse: 35001.0547 - val_loss: 1082125056.0000 - val_rmse: 32895.6680\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1246979584.0000 - rmse: 35312.5977 - val_loss: 1051346048.0000 - val_rmse: 32424.4648\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1225045632.0000 - rmse: 35000.6484 - val_loss: 1063258048.0000 - val_rmse: 32607.6387\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1198835456.0000 - rmse: 34624.2031 - val_loss: 1212161024.0000 - val_rmse: 34816.1016\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1143049344.0000 - rmse: 33809.0117 - val_loss: 1028212800.0000 - val_rmse: 32065.7578\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1097354368.0000 - rmse: 33126.3359 - val_loss: 997307840.0000 - val_rmse: 31580.1816\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1074985344.0000 - rmse: 32786.9688 - val_loss: 1045295232.0000 - val_rmse: 32331.0254\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1100838528.0000 - rmse: 33178.8867 - val_loss: 953998016.0000 - val_rmse: 30886.8555\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1070587136.0000 - rmse: 32719.8281 - val_loss: 1062402560.0000 - val_rmse: 32594.5176\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1066467456.0000 - rmse: 32656.8125 - val_loss: 971635904.0000 - val_rmse: 31171.0723\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1035341184.0000 - rmse: 32176.7168 - val_loss: 1142230656.0000 - val_rmse: 33796.8984\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1063711104.0000 - rmse: 32614.5840 - val_loss: 935555136.0000 - val_rmse: 30586.8418\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1010546240.0000 - rmse: 31789.0898 - val_loss: 1014684096.0000 - val_rmse: 31854.1055\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 900949248.0000 - rmse: 30015.8164 - val_loss: 866687936.0000 - val_rmse: 29439.5625\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 976256384.0000 - rmse: 31245.0977 - val_loss: 873827264.0000 - val_rmse: 29560.5703\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 965750080.0000 - rmse: 31076.5156 - val_loss: 824935680.0000 - val_rmse: 28721.6934\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1000806528.0000 - rmse: 31635.5254 - val_loss: 842412928.0000 - val_rmse: 29024.3477\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 889422592.0000 - rmse: 29823.1895 - val_loss: 819600320.0000 - val_rmse: 28628.6621\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 905916736.0000 - rmse: 30098.4473 - val_loss: 881023936.0000 - val_rmse: 29682.0449\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 899545280.0000 - rmse: 29992.4180 - val_loss: 848705088.0000 - val_rmse: 29132.5430\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 871368320.0000 - rmse: 29518.9473 - val_loss: 795410560.0000 - val_rmse: 28203.0234\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 898353152.0000 - rmse: 29972.5391 - val_loss: 814110784.0000 - val_rmse: 28532.6230\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 858133120.0000 - rmse: 29293.9102 - val_loss: 793780160.0000 - val_rmse: 28174.1035\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 895791104.0000 - rmse: 29929.7695 - val_loss: 871769088.0000 - val_rmse: 29525.7344\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 833762624.0000 - rmse: 28874.9473 - val_loss: 779444928.0000 - val_rmse: 27918.5391\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 852572224.0000 - rmse: 29198.8398 - val_loss: 784395648.0000 - val_rmse: 28007.0625\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 833434304.0000 - rmse: 28869.2598 - val_loss: 744660288.0000 - val_rmse: 27288.4629\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 834323648.0000 - rmse: 28884.6582 - val_loss: 709102144.0000 - val_rmse: 26628.9688\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 810919872.0000 - rmse: 28476.6523 - val_loss: 733372416.0000 - val_rmse: 27080.8477\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 824604160.0000 - rmse: 28715.9180 - val_loss: 697734464.0000 - val_rmse: 26414.6621\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 775134784.0000 - rmse: 27841.2422 - val_loss: 782672448.0000 - val_rmse: 27976.2832\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 774021824.0000 - rmse: 27821.2480 - val_loss: 784815104.0000 - val_rmse: 28014.5488\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 842592576.0000 - rmse: 29027.4434 - val_loss: 752302144.0000 - val_rmse: 27428.1230\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 743229248.0000 - rmse: 27262.2305 - val_loss: 818782016.0000 - val_rmse: 28614.3672\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782203648.0000 - rmse: 27967.8984 - val_loss: 759002112.0000 - val_rmse: 27549.9902\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 824549376.0000 - rmse: 28714.9668 - val_loss: 716759232.0000 - val_rmse: 26772.3555\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 734095872.0000 - rmse: 27094.2012 - val_loss: 689217728.0000 - val_rmse: 26252.9570\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 735946240.0000 - rmse: 27128.3281 - val_loss: 784920384.0000 - val_rmse: 28016.4277\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721255872.0000 - rmse: 26856.2070 - val_loss: 774606976.0000 - val_rmse: 27831.7617\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 768782784.0000 - rmse: 27726.9316 - val_loss: 651353024.0000 - val_rmse: 25521.6172\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 737142656.0000 - rmse: 27150.3691 - val_loss: 736790400.0000 - val_rmse: 27143.8828\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733493952.0000 - rmse: 27083.0898 - val_loss: 741727936.0000 - val_rmse: 27234.6816\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 760871552.0000 - rmse: 27583.8984 - val_loss: 693331328.0000 - val_rmse: 26331.1836\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718610560.0000 - rmse: 26806.9121 - val_loss: 604224256.0000 - val_rmse: 24580.9727\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742752832.0000 - rmse: 27253.4902 - val_loss: 648714880.0000 - val_rmse: 25469.8809\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698299200.0000 - rmse: 26425.3516 - val_loss: 610031936.0000 - val_rmse: 24698.8242\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 671011648.0000 - rmse: 25903.8906 - val_loss: 599019648.0000 - val_rmse: 24474.8770\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677582400.0000 - rmse: 26030.4102 - val_loss: 594003712.0000 - val_rmse: 24372.1895\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687206144.0000 - rmse: 26214.6152 - val_loss: 633837312.0000 - val_rmse: 25176.1250\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 696606912.0000 - rmse: 26393.3086 - val_loss: 579778496.0000 - val_rmse: 24078.5879\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 713287936.0000 - rmse: 26707.4512 - val_loss: 605501632.0000 - val_rmse: 24606.9414\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 768972672.0000 - rmse: 27730.3535 - val_loss: 785195136.0000 - val_rmse: 28021.3320\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 749579136.0000 - rmse: 27378.4414 - val_loss: 600422400.0000 - val_rmse: 24503.5137\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 708157120.0000 - rmse: 26611.2207 - val_loss: 605135872.0000 - val_rmse: 24599.5078\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 672294208.0000 - rmse: 25928.6348 - val_loss: 604809728.0000 - val_rmse: 24592.8789\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 689464256.0000 - rmse: 26257.6504 - val_loss: 599590784.0000 - val_rmse: 24486.5410\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706069056.0000 - rmse: 26571.9590 - val_loss: 605842624.0000 - val_rmse: 24613.8691\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742149248.0000 - rmse: 27242.4160 - val_loss: 624210816.0000 - val_rmse: 24984.2109\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692636416.0000 - rmse: 26317.9824 - val_loss: 578984320.0000 - val_rmse: 24062.0898\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729117568.0000 - rmse: 27002.1777 - val_loss: 579129792.0000 - val_rmse: 24065.1152\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 670170368.0000 - rmse: 25887.6484 - val_loss: 663219456.0000 - val_rmse: 25753.0469\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677143104.0000 - rmse: 26021.9727 - val_loss: 541835648.0000 - val_rmse: 23277.3633\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 695945408.0000 - rmse: 26380.7754 - val_loss: 550752192.0000 - val_rmse: 23468.1055\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 675157312.0000 - rmse: 25983.7871 - val_loss: 565002944.0000 - val_rmse: 23769.7891\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647249088.0000 - rmse: 25441.0898 - val_loss: 636496064.0000 - val_rmse: 25228.8711\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 670259072.0000 - rmse: 25889.3613 - val_loss: 559233728.0000 - val_rmse: 23648.1191\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 705353536.0000 - rmse: 26558.4922 - val_loss: 568229504.0000 - val_rmse: 23837.5625\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 657592320.0000 - rmse: 25643.5625 - val_loss: 565762240.0000 - val_rmse: 23785.7559\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654889920.0000 - rmse: 25590.8164 - val_loss: 539292096.0000 - val_rmse: 23222.6602\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706195456.0000 - rmse: 26574.3379 - val_loss: 538120000.0000 - val_rmse: 23197.4102\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652050432.0000 - rmse: 25535.2754 - val_loss: 578942976.0000 - val_rmse: 24061.2324\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604099264.0000 - rmse: 24578.4316 - val_loss: 645123008.0000 - val_rmse: 25399.2695\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646289536.0000 - rmse: 25422.2227 - val_loss: 540711360.0000 - val_rmse: 23253.1973\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 662966912.0000 - rmse: 25748.1406 - val_loss: 535363008.0000 - val_rmse: 23137.9121\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 657429760.0000 - rmse: 25640.3926 - val_loss: 590212992.0000 - val_rmse: 24294.2988\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 681903808.0000 - rmse: 26113.2852 - val_loss: 521147200.0000 - val_rmse: 22828.6465\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 671434368.0000 - rmse: 25912.0488 - val_loss: 525558144.0000 - val_rmse: 22925.0527\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606481088.0000 - rmse: 24626.8340 - val_loss: 598399808.0000 - val_rmse: 24462.2109\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632709312.0000 - rmse: 25153.7129 - val_loss: 609905216.0000 - val_rmse: 24696.2559\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 633468288.0000 - rmse: 25168.7930 - val_loss: 689038528.0000 - val_rmse: 26249.5430\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 670654208.0000 - rmse: 25896.9922 - val_loss: 609297344.0000 - val_rmse: 24683.9473\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598018496.0000 - rmse: 24454.4160 - val_loss: 514745760.0000 - val_rmse: 22688.0078\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 631947072.0000 - rmse: 25138.5547 - val_loss: 537582976.0000 - val_rmse: 23185.8340\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668147200.0000 - rmse: 25848.5391 - val_loss: 494249792.0000 - val_rmse: 22231.7285\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611454912.0000 - rmse: 24727.6133 - val_loss: 511277856.0000 - val_rmse: 22611.4531\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 639500992.0000 - rmse: 25288.3535 - val_loss: 489347264.0000 - val_rmse: 22121.1914\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624795904.0000 - rmse: 24995.9160 - val_loss: 506068608.0000 - val_rmse: 22495.9668\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 678172032.0000 - rmse: 26041.7344 - val_loss: 498226240.0000 - val_rmse: 22320.9805\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 613274624.0000 - rmse: 24764.3809 - val_loss: 633639104.0000 - val_rmse: 25172.1875\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615807232.0000 - rmse: 24815.4629 - val_loss: 603146432.0000 - val_rmse: 24559.0391\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 631121728.0000 - rmse: 25122.1328 - val_loss: 505031616.0000 - val_rmse: 22472.9082\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593063360.0000 - rmse: 24352.8906 - val_loss: 474298624.0000 - val_rmse: 21778.3945\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623701248.0000 - rmse: 24974.0078 - val_loss: 556373248.0000 - val_rmse: 23587.5605\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560659648.0000 - rmse: 23678.2500 - val_loss: 503985504.0000 - val_rmse: 22449.6191\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 603355392.0000 - rmse: 24563.2891 - val_loss: 484966208.0000 - val_rmse: 22021.9453\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583466752.0000 - rmse: 24155.0547 - val_loss: 554307712.0000 - val_rmse: 23543.7402\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611585152.0000 - rmse: 24730.2461 - val_loss: 561467328.0000 - val_rmse: 23695.2988\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586956288.0000 - rmse: 24227.1797 - val_loss: 554232576.0000 - val_rmse: 23542.1445\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605626944.0000 - rmse: 24609.4863 - val_loss: 466918656.0000 - val_rmse: 21608.2969\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563586240.0000 - rmse: 23739.9668 - val_loss: 481338208.0000 - val_rmse: 21939.4160\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563739776.0000 - rmse: 23743.2031 - val_loss: 495050752.0000 - val_rmse: 22249.7344\n",
      "Epoch 111/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 600472576.0000 - rmse: 24504.5410 - val_loss: 444758400.0000 - val_rmse: 21089.2949\n",
      "Epoch 112/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609205696.0000 - rmse: 24682.0898 - val_loss: 483497312.0000 - val_rmse: 21988.5703\n",
      "Epoch 113/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560130048.0000 - rmse: 23667.0645 - val_loss: 471394944.0000 - val_rmse: 21711.6289\n",
      "Epoch 114/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626480768.0000 - rmse: 25029.5957 - val_loss: 466153536.0000 - val_rmse: 21590.5859\n",
      "Epoch 115/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 603752512.0000 - rmse: 24571.3730 - val_loss: 645870592.0000 - val_rmse: 25413.9805\n",
      "Epoch 116/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555630336.0000 - rmse: 23571.8105 - val_loss: 461394880.0000 - val_rmse: 21480.0996\n",
      "Epoch 117/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584484992.0000 - rmse: 24176.1230 - val_loss: 496024000.0000 - val_rmse: 22271.5918\n",
      "Epoch 118/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579927552.0000 - rmse: 24081.6816 - val_loss: 501062624.0000 - val_rmse: 22384.4258\n",
      "Epoch 119/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575382464.0000 - rmse: 23987.1289 - val_loss: 494755712.0000 - val_rmse: 22243.1035\n",
      "Epoch 120/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569315776.0000 - rmse: 23860.3359 - val_loss: 465349728.0000 - val_rmse: 21571.9648\n",
      "Epoch 121/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583315648.0000 - rmse: 24151.9258 - val_loss: 437056928.0000 - val_rmse: 20905.9043\n",
      "Epoch 122/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 617781120.0000 - rmse: 24855.2031 - val_loss: 463709248.0000 - val_rmse: 21533.9062\n",
      "Epoch 123/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545902272.0000 - rmse: 23364.5508 - val_loss: 503578688.0000 - val_rmse: 22440.5547\n",
      "Epoch 124/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585421504.0000 - rmse: 24195.4844 - val_loss: 532644512.0000 - val_rmse: 23079.0898\n",
      "Epoch 125/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539682560.0000 - rmse: 23231.0684 - val_loss: 552633984.0000 - val_rmse: 23508.1660\n",
      "Epoch 126/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594027712.0000 - rmse: 24372.6816 - val_loss: 440920000.0000 - val_rmse: 20998.0938\n",
      "Epoch 127/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546876672.0000 - rmse: 23385.3906 - val_loss: 470645632.0000 - val_rmse: 21694.3652\n",
      "Epoch 128/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535291232.0000 - rmse: 23136.3594 - val_loss: 510385888.0000 - val_rmse: 22591.7188\n",
      "Epoch 129/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542723840.0000 - rmse: 23296.4336 - val_loss: 602049792.0000 - val_rmse: 24536.7012\n",
      "Epoch 130/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565871872.0000 - rmse: 23788.0605 - val_loss: 500800480.0000 - val_rmse: 22378.5684\n",
      "Epoch 131/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524464640.0000 - rmse: 22901.1875 - val_loss: 503842048.0000 - val_rmse: 22446.4258\n",
      "Epoch 132/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525458176.0000 - rmse: 22922.8711 - val_loss: 420400416.0000 - val_rmse: 20503.6660\n",
      "Epoch 133/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582562688.0000 - rmse: 24136.3320 - val_loss: 468296032.0000 - val_rmse: 21640.1465\n",
      "Epoch 134/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549683200.0000 - rmse: 23445.3223 - val_loss: 428002816.0000 - val_rmse: 20688.2266\n",
      "Epoch 135/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557528000.0000 - rmse: 23612.0293 - val_loss: 423666432.0000 - val_rmse: 20583.1562\n",
      "Epoch 136/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526883552.0000 - rmse: 22953.9434 - val_loss: 471399840.0000 - val_rmse: 21711.7422\n",
      "Epoch 137/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535122272.0000 - rmse: 23132.7090 - val_loss: 473983808.0000 - val_rmse: 21771.1660\n",
      "Epoch 138/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527093056.0000 - rmse: 22958.5059 - val_loss: 419909184.0000 - val_rmse: 20491.6816\n",
      "Epoch 139/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564317568.0000 - rmse: 23755.3672 - val_loss: 402677280.0000 - val_rmse: 20066.8164\n",
      "Epoch 140/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534545152.0000 - rmse: 23120.2305 - val_loss: 417831904.0000 - val_rmse: 20440.9355\n",
      "Epoch 141/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556483072.0000 - rmse: 23589.8906 - val_loss: 415310880.0000 - val_rmse: 20379.1738\n",
      "Epoch 142/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539455744.0000 - rmse: 23226.1836 - val_loss: 439309504.0000 - val_rmse: 20959.7070\n",
      "Epoch 143/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513643872.0000 - rmse: 22663.7070 - val_loss: 483786144.0000 - val_rmse: 21995.1348\n",
      "Epoch 144/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537987392.0000 - rmse: 23194.5547 - val_loss: 460971488.0000 - val_rmse: 21470.2441\n",
      "Epoch 145/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516109696.0000 - rmse: 22718.0469 - val_loss: 449020480.0000 - val_rmse: 21190.0996\n",
      "Epoch 146/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570434304.0000 - rmse: 23883.7617 - val_loss: 391879200.0000 - val_rmse: 19795.9375\n",
      "Epoch 147/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504576000.0000 - rmse: 22462.7676 - val_loss: 555045632.0000 - val_rmse: 23559.4043\n",
      "Epoch 148/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514086592.0000 - rmse: 22673.4746 - val_loss: 397791104.0000 - val_rmse: 19944.6992\n",
      "Epoch 149/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547123968.0000 - rmse: 23390.6777 - val_loss: 427762272.0000 - val_rmse: 20682.4121\n",
      "Epoch 150/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513709632.0000 - rmse: 22665.1582 - val_loss: 417583840.0000 - val_rmse: 20434.8633\n",
      "Epoch 151/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523241056.0000 - rmse: 22874.4609 - val_loss: 475364960.0000 - val_rmse: 21802.8633\n",
      "Epoch 152/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513142400.0000 - rmse: 22652.6426 - val_loss: 447085568.0000 - val_rmse: 21144.3965\n",
      "Epoch 153/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582810624.0000 - rmse: 24141.4668 - val_loss: 442933152.0000 - val_rmse: 21045.9766\n",
      "Epoch 154/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520057248.0000 - rmse: 22804.7617 - val_loss: 393166688.0000 - val_rmse: 19828.4258\n",
      "Epoch 155/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468317600.0000 - rmse: 21640.6426 - val_loss: 451829024.0000 - val_rmse: 21256.2656\n",
      "Epoch 156/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 543808512.0000 - rmse: 23319.7031 - val_loss: 391849088.0000 - val_rmse: 19795.1758\n",
      "Epoch 157/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534516800.0000 - rmse: 23119.6172 - val_loss: 392646848.0000 - val_rmse: 19815.3164\n",
      "Epoch 158/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500972480.0000 - rmse: 22382.4121 - val_loss: 411819328.0000 - val_rmse: 20293.3301\n",
      "Epoch 159/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529032320.0000 - rmse: 23000.7012 - val_loss: 411316320.0000 - val_rmse: 20280.9316\n",
      "Epoch 160/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458099808.0000 - rmse: 21403.2637 - val_loss: 456137376.0000 - val_rmse: 21357.3711\n",
      "Epoch 161/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549079744.0000 - rmse: 23432.4473 - val_loss: 381718176.0000 - val_rmse: 19537.6074\n",
      "Epoch 162/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508150688.0000 - rmse: 22542.1953 - val_loss: 400513120.0000 - val_rmse: 20012.8203\n",
      "Epoch 163/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475323264.0000 - rmse: 21801.9043 - val_loss: 387428384.0000 - val_rmse: 19683.1953\n",
      "Epoch 164/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514427040.0000 - rmse: 22680.9824 - val_loss: 504581568.0000 - val_rmse: 22462.8906\n",
      "Epoch 165/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512847488.0000 - rmse: 22646.1328 - val_loss: 376276064.0000 - val_rmse: 19397.8340\n",
      "Epoch 166/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454809728.0000 - rmse: 21326.2656 - val_loss: 430556672.0000 - val_rmse: 20749.8574\n",
      "Epoch 167/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502164928.0000 - rmse: 22409.0332 - val_loss: 388951488.0000 - val_rmse: 19721.8496\n",
      "Epoch 168/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469650624.0000 - rmse: 21671.4219 - val_loss: 387627520.0000 - val_rmse: 19688.2539\n",
      "Epoch 169/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508872672.0000 - rmse: 22558.2031 - val_loss: 473770592.0000 - val_rmse: 21766.2676\n",
      "Epoch 170/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561224640.0000 - rmse: 23690.1777 - val_loss: 403839552.0000 - val_rmse: 20095.7559\n",
      "Epoch 171/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481491616.0000 - rmse: 21942.9141 - val_loss: 356483936.0000 - val_rmse: 18880.7793\n",
      "Epoch 172/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479712864.0000 - rmse: 21902.3438 - val_loss: 363323712.0000 - val_rmse: 19061.0469\n",
      "Epoch 173/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466172224.0000 - rmse: 21591.0195 - val_loss: 402750048.0000 - val_rmse: 20068.6289\n",
      "Epoch 174/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477950144.0000 - rmse: 21862.0664 - val_loss: 366930656.0000 - val_rmse: 19155.4316\n",
      "Epoch 175/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514475232.0000 - rmse: 22682.0430 - val_loss: 395588640.0000 - val_rmse: 19889.4043\n",
      "Epoch 176/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492413952.0000 - rmse: 22190.3984 - val_loss: 668211776.0000 - val_rmse: 25849.7910\n",
      "Epoch 177/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512728960.0000 - rmse: 22643.5156 - val_loss: 497205280.0000 - val_rmse: 22298.0957\n",
      "Epoch 178/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479157152.0000 - rmse: 21889.6543 - val_loss: 449006112.0000 - val_rmse: 21189.7578\n",
      "Epoch 179/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536261920.0000 - rmse: 23157.3281 - val_loss: 441033920.0000 - val_rmse: 21000.8066\n",
      "Epoch 180/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439328352.0000 - rmse: 20960.1582 - val_loss: 401268128.0000 - val_rmse: 20031.6738\n",
      "Epoch 181/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479729248.0000 - rmse: 21902.7207 - val_loss: 418368384.0000 - val_rmse: 20454.0527\n",
      "Epoch 182/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535395040.0000 - rmse: 23138.6035 - val_loss: 365010688.0000 - val_rmse: 19105.2480\n",
      "Epoch 183/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480686464.0000 - rmse: 21924.5586 - val_loss: 400356224.0000 - val_rmse: 20008.9004\n",
      "Epoch 184/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524813664.0000 - rmse: 22908.8086 - val_loss: 370644608.0000 - val_rmse: 19252.1309\n",
      "Epoch 185/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478173088.0000 - rmse: 21867.1641 - val_loss: 355217792.0000 - val_rmse: 18847.2188\n",
      "Epoch 186/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505392160.0000 - rmse: 22480.9258 - val_loss: 387333792.0000 - val_rmse: 19680.7949\n",
      "Epoch 187/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484042528.0000 - rmse: 22000.9648 - val_loss: 392248448.0000 - val_rmse: 19805.2598\n",
      "Epoch 188/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517490240.0000 - rmse: 22748.4082 - val_loss: 340598976.0000 - val_rmse: 18455.3203\n",
      "Epoch 189/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459948832.0000 - rmse: 21446.4141 - val_loss: 364154592.0000 - val_rmse: 19082.8301\n",
      "Epoch 190/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468266432.0000 - rmse: 21639.4590 - val_loss: 363745792.0000 - val_rmse: 19072.1172\n",
      "Epoch 191/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429835808.0000 - rmse: 20732.4785 - val_loss: 505267104.0000 - val_rmse: 22478.1445\n",
      "Epoch 192/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487475968.0000 - rmse: 22078.8535 - val_loss: 354265760.0000 - val_rmse: 18821.9453\n",
      "Epoch 193/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489184896.0000 - rmse: 22117.5234 - val_loss: 369070176.0000 - val_rmse: 19211.1973\n",
      "Epoch 194/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452139136.0000 - rmse: 21263.5605 - val_loss: 337771648.0000 - val_rmse: 18378.5605\n",
      "Epoch 195/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466721952.0000 - rmse: 21603.7461 - val_loss: 352213888.0000 - val_rmse: 18767.3594\n",
      "Epoch 196/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487284768.0000 - rmse: 22074.5254 - val_loss: 360698304.0000 - val_rmse: 18992.0547\n",
      "Epoch 197/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512216224.0000 - rmse: 22632.1895 - val_loss: 357456128.0000 - val_rmse: 18906.5059\n",
      "Epoch 198/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486275104.0000 - rmse: 22051.6426 - val_loss: 430384352.0000 - val_rmse: 20745.7031\n",
      "Epoch 199/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470408800.0000 - rmse: 21688.9043 - val_loss: 448719232.0000 - val_rmse: 21182.9902\n",
      "Epoch 200/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472081056.0000 - rmse: 21727.4219 - val_loss: 373620800.0000 - val_rmse: 19329.2695\n",
      "104/104 [==============================] - 0s 686us/step - loss: 951038592.0000 - rmse: 30838.9082\n",
      "[951038592.0, 30838.908203125]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 17833893888.0000 - rmse: 133543.6094 - val_loss: 3314131200.0000 - val_rmse: 57568.4922\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2785675520.0000 - rmse: 52779.5000 - val_loss: 1403454848.0000 - val_rmse: 37462.7109\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1915249792.0000 - rmse: 43763.5664 - val_loss: 1161118592.0000 - val_rmse: 34075.1914\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1755553024.0000 - rmse: 41899.3203 - val_loss: 1093075456.0000 - val_rmse: 33061.6914\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1666581504.0000 - rmse: 40823.7852 - val_loss: 1079483008.0000 - val_rmse: 32855.4844\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1628363904.0000 - rmse: 40352.9922 - val_loss: 1074977664.0000 - val_rmse: 32786.8516\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1585824896.0000 - rmse: 39822.4180 - val_loss: 1103451776.0000 - val_rmse: 33218.2461\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1615245568.0000 - rmse: 40190.1172 - val_loss: 1119329408.0000 - val_rmse: 33456.3789\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1482771712.0000 - rmse: 38506.7734 - val_loss: 1017771968.0000 - val_rmse: 31902.5391\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1489369344.0000 - rmse: 38592.3477 - val_loss: 998539840.0000 - val_rmse: 31599.6816\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1439097344.0000 - rmse: 37935.4375 - val_loss: 978800384.0000 - val_rmse: 31285.7832\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1456416896.0000 - rmse: 38163.0312 - val_loss: 988965952.0000 - val_rmse: 31447.8281\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1423763712.0000 - rmse: 37732.7930 - val_loss: 1016277696.0000 - val_rmse: 31879.1113\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1354595712.0000 - rmse: 36804.8320 - val_loss: 958248064.0000 - val_rmse: 30955.5820\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1325589120.0000 - rmse: 36408.6406 - val_loss: 956642560.0000 - val_rmse: 30929.6387\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1274256896.0000 - rmse: 35696.7344 - val_loss: 933545728.0000 - val_rmse: 30553.9805\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1278593664.0000 - rmse: 35757.4297 - val_loss: 992946496.0000 - val_rmse: 31511.0527\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1237627904.0000 - rmse: 35179.9336 - val_loss: 932336128.0000 - val_rmse: 30534.1777\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1229774080.0000 - rmse: 35068.1367 - val_loss: 941246912.0000 - val_rmse: 30679.7461\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1184828800.0000 - rmse: 34421.3438 - val_loss: 923660096.0000 - val_rmse: 30391.7754\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1180335232.0000 - rmse: 34356.0078 - val_loss: 911394048.0000 - val_rmse: 30189.3008\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1157328640.0000 - rmse: 34019.5352 - val_loss: 918160704.0000 - val_rmse: 30301.1641\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1131124864.0000 - rmse: 33632.1992 - val_loss: 892411328.0000 - val_rmse: 29873.2539\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1152594816.0000 - rmse: 33949.8867 - val_loss: 879552192.0000 - val_rmse: 29657.2461\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1080452352.0000 - rmse: 32870.2344 - val_loss: 1077497344.0000 - val_rmse: 32825.2539\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1080013568.0000 - rmse: 32863.5586 - val_loss: 874800384.0000 - val_rmse: 29577.0254\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1090888192.0000 - rmse: 33028.5977 - val_loss: 1104972288.0000 - val_rmse: 33241.1250\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1059132672.0000 - rmse: 32544.3184 - val_loss: 864362112.0000 - val_rmse: 29400.0352\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1145396480.0000 - rmse: 33843.7031 - val_loss: 901211776.0000 - val_rmse: 30020.1895\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1053492928.0000 - rmse: 32457.5547 - val_loss: 778734272.0000 - val_rmse: 27905.8105\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1045602944.0000 - rmse: 32335.7832 - val_loss: 849568000.0000 - val_rmse: 29147.3496\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1051878848.0000 - rmse: 32432.6816 - val_loss: 787499456.0000 - val_rmse: 28062.4180\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1080020480.0000 - rmse: 32863.6641 - val_loss: 747067904.0000 - val_rmse: 27332.5410\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1072032832.0000 - rmse: 32741.9121 - val_loss: 766123840.0000 - val_rmse: 27678.9414\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 992963264.0000 - rmse: 31511.3164 - val_loss: 1085129472.0000 - val_rmse: 32941.3047\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1026514240.0000 - rmse: 32039.2617 - val_loss: 852041088.0000 - val_rmse: 29189.7422\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 996432384.0000 - rmse: 31566.3145 - val_loss: 800806976.0000 - val_rmse: 28298.5332\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 915898368.0000 - rmse: 30263.8105 - val_loss: 817572928.0000 - val_rmse: 28593.2305\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 947199680.0000 - rmse: 30776.6094 - val_loss: 720386304.0000 - val_rmse: 26840.0117\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 905349952.0000 - rmse: 30089.0332 - val_loss: 838646720.0000 - val_rmse: 28959.3945\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 997831680.0000 - rmse: 31588.4746 - val_loss: 703546048.0000 - val_rmse: 26524.4375\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 895649536.0000 - rmse: 29927.4043 - val_loss: 670914688.0000 - val_rmse: 25902.0195\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 957993472.0000 - rmse: 30951.4688 - val_loss: 822229696.0000 - val_rmse: 28674.5469\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 901645760.0000 - rmse: 30027.4160 - val_loss: 699291008.0000 - val_rmse: 26444.1094\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 968839808.0000 - rmse: 31126.1895 - val_loss: 671402496.0000 - val_rmse: 25911.4336\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 870698176.0000 - rmse: 29507.5938 - val_loss: 870891904.0000 - val_rmse: 29510.8770\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 933900160.0000 - rmse: 30559.7773 - val_loss: 756866752.0000 - val_rmse: 27511.2090\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 960333504.0000 - rmse: 30989.2480 - val_loss: 728428864.0000 - val_rmse: 26989.4219\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 913522560.0000 - rmse: 30224.5332 - val_loss: 647286848.0000 - val_rmse: 25441.8320\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 904717120.0000 - rmse: 30078.5156 - val_loss: 714917824.0000 - val_rmse: 26737.9453\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 832865792.0000 - rmse: 28859.4121 - val_loss: 684842048.0000 - val_rmse: 26169.4824\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 850855104.0000 - rmse: 29169.4199 - val_loss: 627414784.0000 - val_rmse: 25048.2500\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 898868160.0000 - rmse: 29981.1270 - val_loss: 633035264.0000 - val_rmse: 25160.1895\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 918654592.0000 - rmse: 30309.3145 - val_loss: 642000448.0000 - val_rmse: 25337.7246\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 896607936.0000 - rmse: 29943.4121 - val_loss: 647942976.0000 - val_rmse: 25454.7207\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 802431168.0000 - rmse: 28327.2129 - val_loss: 631051328.0000 - val_rmse: 25120.7324\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 875039936.0000 - rmse: 29581.0703 - val_loss: 647010880.0000 - val_rmse: 25436.4062\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 833231296.0000 - rmse: 28865.7461 - val_loss: 651827584.0000 - val_rmse: 25530.9121\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 825268608.0000 - rmse: 28727.4863 - val_loss: 645346624.0000 - val_rmse: 25403.6738\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 798266560.0000 - rmse: 28253.6113 - val_loss: 599557248.0000 - val_rmse: 24485.8555\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 779447680.0000 - rmse: 27918.5898 - val_loss: 654628736.0000 - val_rmse: 25585.7109\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 774979840.0000 - rmse: 27838.4570 - val_loss: 546907904.0000 - val_rmse: 23386.0605\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 790528576.0000 - rmse: 28116.3379 - val_loss: 582831552.0000 - val_rmse: 24141.9023\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 771039360.0000 - rmse: 27767.5938 - val_loss: 605628032.0000 - val_rmse: 24609.5078\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 762392000.0000 - rmse: 27611.4434 - val_loss: 601668544.0000 - val_rmse: 24528.9316\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 850042304.0000 - rmse: 29155.4844 - val_loss: 628931264.0000 - val_rmse: 25078.5000\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 759148608.0000 - rmse: 27552.6504 - val_loss: 543671872.0000 - val_rmse: 23316.7676\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 799602176.0000 - rmse: 28277.2344 - val_loss: 594627584.0000 - val_rmse: 24384.9844\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 754241920.0000 - rmse: 27463.4629 - val_loss: 632858880.0000 - val_rmse: 25156.6875\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 762634176.0000 - rmse: 27615.8301 - val_loss: 569739136.0000 - val_rmse: 23869.2070\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 768430784.0000 - rmse: 27720.5820 - val_loss: 554389952.0000 - val_rmse: 23545.4844\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 780050112.0000 - rmse: 27929.3770 - val_loss: 546109632.0000 - val_rmse: 23368.9863\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 762337536.0000 - rmse: 27610.4570 - val_loss: 625002560.0000 - val_rmse: 25000.0508\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 746502656.0000 - rmse: 27322.1992 - val_loss: 529045088.0000 - val_rmse: 23000.9766\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 738273792.0000 - rmse: 27171.1934 - val_loss: 533774688.0000 - val_rmse: 23103.5625\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 761089472.0000 - rmse: 27587.8477 - val_loss: 558721536.0000 - val_rmse: 23637.2891\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 735072448.0000 - rmse: 27112.2168 - val_loss: 554276928.0000 - val_rmse: 23543.0840\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 653813056.0000 - rmse: 25569.7676 - val_loss: 576986816.0000 - val_rmse: 24020.5449\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 686948992.0000 - rmse: 26209.7090 - val_loss: 532420032.0000 - val_rmse: 23074.2266\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 682026304.0000 - rmse: 26115.6328 - val_loss: 513850784.0000 - val_rmse: 22668.2754\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704260416.0000 - rmse: 26537.9023 - val_loss: 666320128.0000 - val_rmse: 25813.1758\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 682437056.0000 - rmse: 26123.4941 - val_loss: 586064384.0000 - val_rmse: 24208.7656\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742444544.0000 - rmse: 27247.8340 - val_loss: 576084672.0000 - val_rmse: 24001.7637\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646140544.0000 - rmse: 25419.2949 - val_loss: 870775936.0000 - val_rmse: 29508.9102\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 723979840.0000 - rmse: 26906.8730 - val_loss: 511329248.0000 - val_rmse: 22612.5879\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 704892608.0000 - rmse: 26549.8125 - val_loss: 527709152.0000 - val_rmse: 22971.9180\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677432960.0000 - rmse: 26027.5391 - val_loss: 532549152.0000 - val_rmse: 23077.0254\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 673879744.0000 - rmse: 25959.1934 - val_loss: 492852384.0000 - val_rmse: 22200.2773\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666006848.0000 - rmse: 25807.1074 - val_loss: 643190208.0000 - val_rmse: 25361.1934\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718890624.0000 - rmse: 26812.1328 - val_loss: 501432192.0000 - val_rmse: 22392.6797\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 674901056.0000 - rmse: 25978.8535 - val_loss: 533077984.0000 - val_rmse: 23088.4785\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 700027968.0000 - rmse: 26458.0410 - val_loss: 561975808.0000 - val_rmse: 23706.0273\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661175040.0000 - rmse: 25713.3223 - val_loss: 497729888.0000 - val_rmse: 22309.8594\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 660520512.0000 - rmse: 25700.5918 - val_loss: 525499776.0000 - val_rmse: 22923.7812\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 681471424.0000 - rmse: 26105.0059 - val_loss: 492596160.0000 - val_rmse: 22194.5039\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 660278720.0000 - rmse: 25695.8887 - val_loss: 480398144.0000 - val_rmse: 21917.9844\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 658458752.0000 - rmse: 25660.4492 - val_loss: 527993408.0000 - val_rmse: 22978.1035\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649327552.0000 - rmse: 25481.9043 - val_loss: 505840224.0000 - val_rmse: 22490.8906\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598758464.0000 - rmse: 24469.5410 - val_loss: 555280832.0000 - val_rmse: 23564.3926\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 679815232.0000 - rmse: 26073.2637 - val_loss: 477920448.0000 - val_rmse: 21861.3887\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630636032.0000 - rmse: 25112.4668 - val_loss: 490762464.0000 - val_rmse: 22153.1562\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 644006720.0000 - rmse: 25377.2852 - val_loss: 530806848.0000 - val_rmse: 23039.2461\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632223424.0000 - rmse: 25144.0508 - val_loss: 505410272.0000 - val_rmse: 22481.3301\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622541376.0000 - rmse: 24950.7773 - val_loss: 491724736.0000 - val_rmse: 22174.8652\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609290560.0000 - rmse: 24683.8105 - val_loss: 474944064.0000 - val_rmse: 21793.2070\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590593600.0000 - rmse: 24302.1270 - val_loss: 512597472.0000 - val_rmse: 22640.6113\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 603828992.0000 - rmse: 24572.9316 - val_loss: 460110816.0000 - val_rmse: 21450.1914\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628510272.0000 - rmse: 25070.1055 - val_loss: 479651744.0000 - val_rmse: 21900.9512\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597169536.0000 - rmse: 24437.0508 - val_loss: 498834560.0000 - val_rmse: 22334.6016\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648730368.0000 - rmse: 25470.1816 - val_loss: 461813216.0000 - val_rmse: 21489.8379\n",
      "Epoch 111/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591174144.0000 - rmse: 24314.0723 - val_loss: 506388320.0000 - val_rmse: 22503.0703\n",
      "Epoch 112/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577314368.0000 - rmse: 24027.3652 - val_loss: 476386400.0000 - val_rmse: 21826.2734\n",
      "Epoch 113/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571773632.0000 - rmse: 23911.7871 - val_loss: 484509152.0000 - val_rmse: 22011.5645\n",
      "Epoch 114/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552387712.0000 - rmse: 23502.9297 - val_loss: 508310976.0000 - val_rmse: 22545.7520\n",
      "Epoch 115/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568540032.0000 - rmse: 23844.0742 - val_loss: 478578176.0000 - val_rmse: 21876.4277\n",
      "Epoch 116/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591043840.0000 - rmse: 24311.3926 - val_loss: 438503360.0000 - val_rmse: 20940.4688\n",
      "Epoch 117/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 645559808.0000 - rmse: 25407.8672 - val_loss: 479847232.0000 - val_rmse: 21905.4141\n",
      "Epoch 118/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597074944.0000 - rmse: 24435.1152 - val_loss: 473185632.0000 - val_rmse: 21752.8262\n",
      "Epoch 119/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607458624.0000 - rmse: 24646.6738 - val_loss: 451467744.0000 - val_rmse: 21247.7676\n",
      "Epoch 120/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547250432.0000 - rmse: 23393.3828 - val_loss: 478871040.0000 - val_rmse: 21883.1191\n",
      "Epoch 121/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586289920.0000 - rmse: 24213.4219 - val_loss: 453566304.0000 - val_rmse: 21297.0938\n",
      "Epoch 122/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551419136.0000 - rmse: 23482.3125 - val_loss: 461539552.0000 - val_rmse: 21483.4688\n",
      "Epoch 123/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554812480.0000 - rmse: 23554.4570 - val_loss: 449698208.0000 - val_rmse: 21206.0879\n",
      "Epoch 124/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609941312.0000 - rmse: 24696.9902 - val_loss: 438745184.0000 - val_rmse: 20946.2422\n",
      "Epoch 125/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561284544.0000 - rmse: 23691.4414 - val_loss: 429871200.0000 - val_rmse: 20733.3320\n",
      "Epoch 126/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488837824.0000 - rmse: 22109.6738 - val_loss: 470355264.0000 - val_rmse: 21687.6738\n",
      "Epoch 127/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530152000.0000 - rmse: 23025.0273 - val_loss: 481891136.0000 - val_rmse: 21952.0156\n",
      "Epoch 128/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525415808.0000 - rmse: 22921.9473 - val_loss: 445108224.0000 - val_rmse: 21097.5859\n",
      "Epoch 129/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497749824.0000 - rmse: 22310.3047 - val_loss: 616308160.0000 - val_rmse: 24825.5527\n",
      "Epoch 130/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 528420288.0000 - rmse: 22987.3906 - val_loss: 422895488.0000 - val_rmse: 20564.4199\n",
      "Epoch 131/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573418816.0000 - rmse: 23946.1621 - val_loss: 436188800.0000 - val_rmse: 20885.1309\n",
      "Epoch 132/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530430208.0000 - rmse: 23031.0625 - val_loss: 427118144.0000 - val_rmse: 20666.8320\n",
      "Epoch 133/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488266080.0000 - rmse: 22096.7402 - val_loss: 418985184.0000 - val_rmse: 20469.1250\n",
      "Epoch 134/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519864096.0000 - rmse: 22800.5254 - val_loss: 419238944.0000 - val_rmse: 20475.3223\n",
      "Epoch 135/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588459136.0000 - rmse: 24258.1758 - val_loss: 430794816.0000 - val_rmse: 20755.5957\n",
      "Epoch 136/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454560960.0000 - rmse: 21320.4316 - val_loss: 609160704.0000 - val_rmse: 24681.1777\n",
      "Epoch 137/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565941376.0000 - rmse: 23789.5215 - val_loss: 417789216.0000 - val_rmse: 20439.8906\n",
      "Epoch 138/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491029248.0000 - rmse: 22159.1777 - val_loss: 432686592.0000 - val_rmse: 20801.1172\n",
      "Epoch 139/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512631424.0000 - rmse: 22641.3633 - val_loss: 511426272.0000 - val_rmse: 22614.7344\n",
      "Epoch 140/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569643264.0000 - rmse: 23867.1973 - val_loss: 426323616.0000 - val_rmse: 20647.6016\n",
      "Epoch 141/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496056704.0000 - rmse: 22272.3281 - val_loss: 434389504.0000 - val_rmse: 20842.0098\n",
      "Epoch 142/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461964800.0000 - rmse: 21493.3633 - val_loss: 405550240.0000 - val_rmse: 20138.2754\n",
      "Epoch 143/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482453024.0000 - rmse: 21964.8066 - val_loss: 421144096.0000 - val_rmse: 20521.7930\n",
      "Epoch 144/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503038592.0000 - rmse: 22428.5195 - val_loss: 405545312.0000 - val_rmse: 20138.1523\n",
      "Epoch 145/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463139200.0000 - rmse: 21520.6680 - val_loss: 433652320.0000 - val_rmse: 20824.3203\n",
      "Epoch 146/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448033472.0000 - rmse: 21166.7949 - val_loss: 408671104.0000 - val_rmse: 20215.6113\n",
      "Epoch 147/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479712064.0000 - rmse: 21902.3262 - val_loss: 406318048.0000 - val_rmse: 20157.3301\n",
      "Epoch 148/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496990688.0000 - rmse: 22293.2832 - val_loss: 495218112.0000 - val_rmse: 22253.4941\n",
      "Epoch 149/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474488192.0000 - rmse: 21782.7500 - val_loss: 426156896.0000 - val_rmse: 20643.5664\n",
      "Epoch 150/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514688320.0000 - rmse: 22686.7402 - val_loss: 443000320.0000 - val_rmse: 21047.5703\n",
      "Epoch 151/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517776416.0000 - rmse: 22754.6973 - val_loss: 414018080.0000 - val_rmse: 20347.4316\n",
      "Epoch 152/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493942016.0000 - rmse: 22224.8047 - val_loss: 420694720.0000 - val_rmse: 20510.8418\n",
      "Epoch 153/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506940672.0000 - rmse: 22515.3398 - val_loss: 466877472.0000 - val_rmse: 21607.3457\n",
      "Epoch 154/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463680576.0000 - rmse: 21533.2383 - val_loss: 393653600.0000 - val_rmse: 19840.7012\n",
      "Epoch 155/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456517408.0000 - rmse: 21366.2656 - val_loss: 457614752.0000 - val_rmse: 21391.9277\n",
      "Epoch 156/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475991616.0000 - rmse: 21817.2305 - val_loss: 405999936.0000 - val_rmse: 20149.4375\n",
      "Epoch 157/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441260608.0000 - rmse: 21006.2012 - val_loss: 411872704.0000 - val_rmse: 20294.6426\n",
      "Epoch 158/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518744448.0000 - rmse: 22775.9590 - val_loss: 471167520.0000 - val_rmse: 21706.3906\n",
      "Epoch 159/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422828096.0000 - rmse: 20562.7812 - val_loss: 483405888.0000 - val_rmse: 21986.4941\n",
      "Epoch 160/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417532832.0000 - rmse: 20433.6172 - val_loss: 446207360.0000 - val_rmse: 21123.6172\n",
      "Epoch 161/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428390304.0000 - rmse: 20697.5879 - val_loss: 446595520.0000 - val_rmse: 21132.8027\n",
      "Epoch 162/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416547072.0000 - rmse: 20409.4805 - val_loss: 398241856.0000 - val_rmse: 19955.9941\n",
      "Epoch 163/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496570176.0000 - rmse: 22283.8516 - val_loss: 387730176.0000 - val_rmse: 19690.8613\n",
      "Epoch 164/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433660096.0000 - rmse: 20824.5039 - val_loss: 432575616.0000 - val_rmse: 20798.4492\n",
      "Epoch 165/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461659104.0000 - rmse: 21486.2461 - val_loss: 436697536.0000 - val_rmse: 20897.3066\n",
      "Epoch 166/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433754976.0000 - rmse: 20826.7793 - val_loss: 419930016.0000 - val_rmse: 20492.1914\n",
      "Epoch 167/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516868256.0000 - rmse: 22734.7363 - val_loss: 480703872.0000 - val_rmse: 21924.9570\n",
      "Epoch 168/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432163360.0000 - rmse: 20788.5332 - val_loss: 403815968.0000 - val_rmse: 20095.1699\n",
      "Epoch 169/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459750112.0000 - rmse: 21441.7793 - val_loss: 420771680.0000 - val_rmse: 20512.7168\n",
      "Epoch 170/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432321376.0000 - rmse: 20792.3359 - val_loss: 433364224.0000 - val_rmse: 20817.3984\n",
      "Epoch 171/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485296992.0000 - rmse: 22029.4531 - val_loss: 421267968.0000 - val_rmse: 20524.8105\n",
      "Epoch 172/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404215072.0000 - rmse: 20105.0977 - val_loss: 508791552.0000 - val_rmse: 22556.4043\n",
      "Epoch 173/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400805440.0000 - rmse: 20020.1250 - val_loss: 420795072.0000 - val_rmse: 20513.2871\n",
      "Epoch 174/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459775424.0000 - rmse: 21442.3711 - val_loss: 412185760.0000 - val_rmse: 20302.3535\n",
      "Epoch 175/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404163584.0000 - rmse: 20103.8184 - val_loss: 403646208.0000 - val_rmse: 20090.9453\n",
      "Epoch 176/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413761792.0000 - rmse: 20341.1309 - val_loss: 401460512.0000 - val_rmse: 20036.4785\n",
      "Epoch 177/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406192480.0000 - rmse: 20154.2129 - val_loss: 425793088.0000 - val_rmse: 20634.7539\n",
      "Epoch 178/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368862496.0000 - rmse: 19205.7891 - val_loss: 500212160.0000 - val_rmse: 22365.4199\n",
      "Epoch 179/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510356384.0000 - rmse: 22591.0684 - val_loss: 410187776.0000 - val_rmse: 20253.0898\n",
      "Epoch 180/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395422528.0000 - rmse: 19885.2305 - val_loss: 405015232.0000 - val_rmse: 20124.9883\n",
      "Epoch 181/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417325600.0000 - rmse: 20428.5469 - val_loss: 418355264.0000 - val_rmse: 20453.7305\n",
      "Epoch 182/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389732768.0000 - rmse: 19741.6484 - val_loss: 401413024.0000 - val_rmse: 20035.2910\n",
      "Epoch 183/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415934880.0000 - rmse: 20394.4785 - val_loss: 491349888.0000 - val_rmse: 22166.4102\n",
      "Epoch 184/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407722048.0000 - rmse: 20192.1270 - val_loss: 398019808.0000 - val_rmse: 19950.4316\n",
      "Epoch 185/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372215040.0000 - rmse: 19292.8691 - val_loss: 426322208.0000 - val_rmse: 20647.5684\n",
      "Epoch 186/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426146272.0000 - rmse: 20643.3086 - val_loss: 466402368.0000 - val_rmse: 21596.3477\n",
      "Epoch 187/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417820800.0000 - rmse: 20440.6621 - val_loss: 400168416.0000 - val_rmse: 20004.2051\n",
      "Epoch 188/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452358080.0000 - rmse: 21268.7070 - val_loss: 390289280.0000 - val_rmse: 19755.7363\n",
      "Epoch 189/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441254080.0000 - rmse: 21006.0488 - val_loss: 514186368.0000 - val_rmse: 22675.6777\n",
      "Epoch 190/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367898752.0000 - rmse: 19180.6855 - val_loss: 405309664.0000 - val_rmse: 20132.3008\n",
      "Epoch 191/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393560064.0000 - rmse: 19838.3418 - val_loss: 570939520.0000 - val_rmse: 23894.3379\n",
      "Epoch 192/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407760544.0000 - rmse: 20193.0781 - val_loss: 415959680.0000 - val_rmse: 20395.0859\n",
      "Epoch 193/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405472896.0000 - rmse: 20136.3535 - val_loss: 405195072.0000 - val_rmse: 20129.4551\n",
      "Epoch 194/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417185728.0000 - rmse: 20425.1191 - val_loss: 377104224.0000 - val_rmse: 19419.1680\n",
      "Epoch 195/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408820768.0000 - rmse: 20219.3145 - val_loss: 411026688.0000 - val_rmse: 20273.7891\n",
      "Epoch 196/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383707040.0000 - rmse: 19588.4395 - val_loss: 413908192.0000 - val_rmse: 20344.7305\n",
      "Epoch 197/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355714560.0000 - rmse: 18860.3926 - val_loss: 410110816.0000 - val_rmse: 20251.1875\n",
      "Epoch 198/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417177696.0000 - rmse: 20424.9219 - val_loss: 421231040.0000 - val_rmse: 20523.9121\n",
      "Epoch 199/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388573760.0000 - rmse: 19712.2695 - val_loss: 500781984.0000 - val_rmse: 22378.1543\n",
      "Epoch 200/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407286720.0000 - rmse: 20181.3438 - val_loss: 383766560.0000 - val_rmse: 19589.9570\n",
      "104/104 [==============================] - 0s 674us/step - loss: 469202880.0000 - rmse: 21661.0879\n",
      "[469202880.0, 21661.087890625]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,113\n",
      "Trainable params: 26,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 19393689600.0000 - rmse: 139261.2344 - val_loss: 4552744448.0000 - val_rmse: 67474.0312\n",
      "Epoch 2/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 3405253888.0000 - rmse: 58354.5547 - val_loss: 1470468992.0000 - val_rmse: 38346.6953\n",
      "Epoch 3/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2047304320.0000 - rmse: 45247.1484 - val_loss: 1238425088.0000 - val_rmse: 35191.2656\n",
      "Epoch 4/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1855843840.0000 - rmse: 43079.5078 - val_loss: 1265536000.0000 - val_rmse: 35574.3711\n",
      "Epoch 5/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1798360704.0000 - rmse: 42407.0820 - val_loss: 1289482240.0000 - val_rmse: 35909.3633\n",
      "Epoch 6/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1744060160.0000 - rmse: 41761.9453 - val_loss: 1049015104.0000 - val_rmse: 32388.5000\n",
      "Epoch 7/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1705620608.0000 - rmse: 41299.1602 - val_loss: 1081583360.0000 - val_rmse: 32887.4336\n",
      "Epoch 8/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1632869888.0000 - rmse: 40408.7852 - val_loss: 1120437760.0000 - val_rmse: 33472.9414\n",
      "Epoch 9/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1507731840.0000 - rmse: 38829.5234 - val_loss: 1048241472.0000 - val_rmse: 32376.5566\n",
      "Epoch 10/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1560542080.0000 - rmse: 39503.6953 - val_loss: 974335808.0000 - val_rmse: 31214.3535\n",
      "Epoch 11/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1422715904.0000 - rmse: 37718.9062 - val_loss: 914448448.0000 - val_rmse: 30239.8457\n",
      "Epoch 12/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1441908096.0000 - rmse: 37972.4648 - val_loss: 926425984.0000 - val_rmse: 30437.2461\n",
      "Epoch 13/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1408419200.0000 - rmse: 37528.9102 - val_loss: 1001423552.0000 - val_rmse: 31645.2773\n",
      "Epoch 14/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1312030848.0000 - rmse: 36221.9648 - val_loss: 856840128.0000 - val_rmse: 29271.8320\n",
      "Epoch 15/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1309479168.0000 - rmse: 36186.7266 - val_loss: 833633216.0000 - val_rmse: 28872.7070\n",
      "Epoch 16/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1282002688.0000 - rmse: 35805.0664 - val_loss: 869689920.0000 - val_rmse: 29490.5059\n",
      "Epoch 17/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1223478656.0000 - rmse: 34978.2578 - val_loss: 847215424.0000 - val_rmse: 29106.9648\n",
      "Epoch 18/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1345849856.0000 - rmse: 36685.8281 - val_loss: 815345664.0000 - val_rmse: 28554.2578\n",
      "Epoch 19/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1209932416.0000 - rmse: 34784.0820 - val_loss: 770837440.0000 - val_rmse: 27763.9590\n",
      "Epoch 20/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1277368704.0000 - rmse: 35740.2930 - val_loss: 840816256.0000 - val_rmse: 28996.8320\n",
      "Epoch 21/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1183508352.0000 - rmse: 34402.1562 - val_loss: 744722944.0000 - val_rmse: 27289.6094\n",
      "Epoch 22/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1200346880.0000 - rmse: 34646.0234 - val_loss: 895938304.0000 - val_rmse: 29932.2266\n",
      "Epoch 23/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1121783936.0000 - rmse: 33493.0430 - val_loss: 759744192.0000 - val_rmse: 27563.4570\n",
      "Epoch 24/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1163607296.0000 - rmse: 34111.6875 - val_loss: 771374272.0000 - val_rmse: 27773.6250\n",
      "Epoch 25/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1141012864.0000 - rmse: 33778.8789 - val_loss: 743443264.0000 - val_rmse: 27266.1543\n",
      "Epoch 26/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1141926528.0000 - rmse: 33792.4023 - val_loss: 827512256.0000 - val_rmse: 28766.5117\n",
      "Epoch 27/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1136598656.0000 - rmse: 33713.4805 - val_loss: 781934848.0000 - val_rmse: 27963.0977\n",
      "Epoch 28/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1030403520.0000 - rmse: 32099.8965 - val_loss: 849949952.0000 - val_rmse: 29153.9004\n",
      "Epoch 29/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1076576896.0000 - rmse: 32811.2305 - val_loss: 709373376.0000 - val_rmse: 26634.0625\n",
      "Epoch 30/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1021724672.0000 - rmse: 31964.4277 - val_loss: 762027200.0000 - val_rmse: 27604.8398\n",
      "Epoch 31/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1017658368.0000 - rmse: 31900.7559 - val_loss: 652434880.0000 - val_rmse: 25542.8027\n",
      "Epoch 32/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1068149696.0000 - rmse: 32682.5586 - val_loss: 670638784.0000 - val_rmse: 25896.6914\n",
      "Epoch 33/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1011901888.0000 - rmse: 31810.4043 - val_loss: 746693952.0000 - val_rmse: 27325.6992\n",
      "Epoch 34/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 965780672.0000 - rmse: 31077.0098 - val_loss: 779165120.0000 - val_rmse: 27913.5273\n",
      "Epoch 35/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 974577792.0000 - rmse: 31218.2266 - val_loss: 698199424.0000 - val_rmse: 26423.4629\n",
      "Epoch 36/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 975672960.0000 - rmse: 31235.7598 - val_loss: 631286336.0000 - val_rmse: 25125.4102\n",
      "Epoch 37/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 939870464.0000 - rmse: 30657.3047 - val_loss: 706524416.0000 - val_rmse: 26580.5254\n",
      "Epoch 38/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1020159936.0000 - rmse: 31939.9414 - val_loss: 625294976.0000 - val_rmse: 25005.8984\n",
      "Epoch 39/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 932557184.0000 - rmse: 30537.7988 - val_loss: 606657280.0000 - val_rmse: 24630.4141\n",
      "Epoch 40/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 959342912.0000 - rmse: 30973.2578 - val_loss: 641588800.0000 - val_rmse: 25329.5996\n",
      "Epoch 41/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 922240640.0000 - rmse: 30368.4121 - val_loss: 577011392.0000 - val_rmse: 24021.0566\n",
      "Epoch 42/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 946531904.0000 - rmse: 30765.7559 - val_loss: 613908800.0000 - val_rmse: 24777.1816\n",
      "Epoch 43/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 873159744.0000 - rmse: 29549.2773 - val_loss: 632112896.0000 - val_rmse: 25141.8535\n",
      "Epoch 44/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 932828800.0000 - rmse: 30542.2461 - val_loss: 609550336.0000 - val_rmse: 24689.0723\n",
      "Epoch 45/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 895651328.0000 - rmse: 29927.4316 - val_loss: 623165440.0000 - val_rmse: 24963.2812\n",
      "Epoch 46/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 948693312.0000 - rmse: 30800.8652 - val_loss: 630216320.0000 - val_rmse: 25104.1055\n",
      "Epoch 47/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 887985280.0000 - rmse: 29799.0801 - val_loss: 663952128.0000 - val_rmse: 25767.2637\n",
      "Epoch 48/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 915454848.0000 - rmse: 30256.4824 - val_loss: 750441920.0000 - val_rmse: 27394.1934\n",
      "Epoch 49/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 898913408.0000 - rmse: 29981.8809 - val_loss: 571823424.0000 - val_rmse: 23912.8262\n",
      "Epoch 50/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 920544448.0000 - rmse: 30340.4727 - val_loss: 601668864.0000 - val_rmse: 24528.9375\n",
      "Epoch 51/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 852067328.0000 - rmse: 29190.1895 - val_loss: 566225600.0000 - val_rmse: 23795.4941\n",
      "Epoch 52/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 905468416.0000 - rmse: 30091.0020 - val_loss: 573314048.0000 - val_rmse: 23943.9766\n",
      "Epoch 53/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 942551232.0000 - rmse: 30700.9961 - val_loss: 559233920.0000 - val_rmse: 23648.1230\n",
      "Epoch 54/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 773780672.0000 - rmse: 27816.9102 - val_loss: 570054912.0000 - val_rmse: 23875.8184\n",
      "Epoch 55/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 818918720.0000 - rmse: 28616.7539 - val_loss: 600234112.0000 - val_rmse: 24499.6738\n",
      "Epoch 56/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 805127616.0000 - rmse: 28374.7676 - val_loss: 618245632.0000 - val_rmse: 24864.5449\n",
      "Epoch 57/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 845361024.0000 - rmse: 29075.0918 - val_loss: 570585920.0000 - val_rmse: 23886.9375\n",
      "Epoch 58/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 818284800.0000 - rmse: 28605.6777 - val_loss: 605717696.0000 - val_rmse: 24611.3281\n",
      "Epoch 59/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 879392576.0000 - rmse: 29654.5527 - val_loss: 565697344.0000 - val_rmse: 23784.3906\n",
      "Epoch 60/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 797438848.0000 - rmse: 28238.9570 - val_loss: 608083840.0000 - val_rmse: 24659.3535\n",
      "Epoch 61/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 778517504.0000 - rmse: 27901.9258 - val_loss: 616215360.0000 - val_rmse: 24823.6836\n",
      "Epoch 62/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 815806528.0000 - rmse: 28562.3262 - val_loss: 548368640.0000 - val_rmse: 23417.2715\n",
      "Epoch 63/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 756540096.0000 - rmse: 27505.2734 - val_loss: 533012320.0000 - val_rmse: 23087.0547\n",
      "Epoch 64/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 854127104.0000 - rmse: 29225.4512 - val_loss: 615664512.0000 - val_rmse: 24812.5879\n",
      "Epoch 65/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 802158400.0000 - rmse: 28322.3984 - val_loss: 522798720.0000 - val_rmse: 22864.7891\n",
      "Epoch 66/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 776643328.0000 - rmse: 27868.3184 - val_loss: 546758656.0000 - val_rmse: 23382.8691\n",
      "Epoch 67/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 748406848.0000 - rmse: 27357.0215 - val_loss: 536275680.0000 - val_rmse: 23157.6250\n",
      "Epoch 68/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 783939520.0000 - rmse: 27998.9180 - val_loss: 539568192.0000 - val_rmse: 23228.6055\n",
      "Epoch 69/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 768699264.0000 - rmse: 27725.4238 - val_loss: 637704128.0000 - val_rmse: 25252.8008\n",
      "Epoch 70/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 817644480.0000 - rmse: 28594.4824 - val_loss: 565891392.0000 - val_rmse: 23788.4707\n",
      "Epoch 71/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 779477888.0000 - rmse: 27919.1270 - val_loss: 514328640.0000 - val_rmse: 22678.8105\n",
      "Epoch 72/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 674287424.0000 - rmse: 25967.0430 - val_loss: 717515904.0000 - val_rmse: 26786.4844\n",
      "Epoch 73/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 766127936.0000 - rmse: 27679.0156 - val_loss: 509127040.0000 - val_rmse: 22563.8398\n",
      "Epoch 74/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 797842624.0000 - rmse: 28246.1055 - val_loss: 513088864.0000 - val_rmse: 22651.4629\n",
      "Epoch 75/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 799905536.0000 - rmse: 28282.5996 - val_loss: 509962144.0000 - val_rmse: 22582.3379\n",
      "Epoch 76/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 737677696.0000 - rmse: 27160.2188 - val_loss: 504628480.0000 - val_rmse: 22463.9336\n",
      "Epoch 77/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751221952.0000 - rmse: 27408.4258 - val_loss: 613350592.0000 - val_rmse: 24765.9141\n",
      "Epoch 78/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 711300608.0000 - rmse: 26670.2168 - val_loss: 507167072.0000 - val_rmse: 22520.3672\n",
      "Epoch 79/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724358656.0000 - rmse: 26913.9121 - val_loss: 498557888.0000 - val_rmse: 22328.4082\n",
      "Epoch 80/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 749127424.0000 - rmse: 27370.1895 - val_loss: 493451232.0000 - val_rmse: 22213.7578\n",
      "Epoch 81/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 709893184.0000 - rmse: 26643.8203 - val_loss: 512049120.0000 - val_rmse: 22628.5000\n",
      "Epoch 82/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 711154368.0000 - rmse: 26667.4766 - val_loss: 503053248.0000 - val_rmse: 22428.8438\n",
      "Epoch 83/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 722194688.0000 - rmse: 26873.6758 - val_loss: 496061600.0000 - val_rmse: 22272.4395\n",
      "Epoch 84/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 717574976.0000 - rmse: 26787.5879 - val_loss: 490109568.0000 - val_rmse: 22138.4160\n",
      "Epoch 85/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668974016.0000 - rmse: 25864.5273 - val_loss: 542800384.0000 - val_rmse: 23298.0703\n",
      "Epoch 86/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 712991872.0000 - rmse: 26701.9062 - val_loss: 531314048.0000 - val_rmse: 23050.2461\n",
      "Epoch 87/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 744556736.0000 - rmse: 27286.5625 - val_loss: 490892544.0000 - val_rmse: 22156.0918\n",
      "Epoch 88/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668443200.0000 - rmse: 25854.2656 - val_loss: 489349056.0000 - val_rmse: 22121.2344\n",
      "Epoch 89/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661504576.0000 - rmse: 25719.7305 - val_loss: 592367168.0000 - val_rmse: 24338.5918\n",
      "Epoch 90/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630123520.0000 - rmse: 25102.2578 - val_loss: 481738368.0000 - val_rmse: 21948.5371\n",
      "Epoch 91/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656545792.0000 - rmse: 25623.1465 - val_loss: 483004704.0000 - val_rmse: 21977.3633\n",
      "Epoch 92/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733931776.0000 - rmse: 27091.1738 - val_loss: 504798304.0000 - val_rmse: 22467.7148\n",
      "Epoch 93/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652708608.0000 - rmse: 25548.1582 - val_loss: 520224288.0000 - val_rmse: 22808.4180\n",
      "Epoch 94/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 671857792.0000 - rmse: 25920.2168 - val_loss: 467831712.0000 - val_rmse: 21629.4141\n",
      "Epoch 95/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622787200.0000 - rmse: 24955.7012 - val_loss: 505594016.0000 - val_rmse: 22485.4160\n",
      "Epoch 96/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632689536.0000 - rmse: 25153.3184 - val_loss: 455347200.0000 - val_rmse: 21338.8613\n",
      "Epoch 97/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687592640.0000 - rmse: 26221.9824 - val_loss: 453628480.0000 - val_rmse: 21298.5527\n",
      "Epoch 98/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 628981184.0000 - rmse: 25079.4980 - val_loss: 494500800.0000 - val_rmse: 22237.3711\n",
      "Epoch 99/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 644785280.0000 - rmse: 25392.6191 - val_loss: 471932160.0000 - val_rmse: 21723.9961\n",
      "Epoch 100/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 694197632.0000 - rmse: 26347.6270 - val_loss: 460589824.0000 - val_rmse: 21461.3535\n",
      "Epoch 101/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623963072.0000 - rmse: 24979.2500 - val_loss: 479218176.0000 - val_rmse: 21891.0508\n",
      "Epoch 102/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632795520.0000 - rmse: 25155.4238 - val_loss: 445481184.0000 - val_rmse: 21106.4219\n",
      "Epoch 103/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666456064.0000 - rmse: 25815.8086 - val_loss: 474485440.0000 - val_rmse: 21782.6836\n",
      "Epoch 104/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 682659328.0000 - rmse: 26127.7480 - val_loss: 448519168.0000 - val_rmse: 21178.2676\n",
      "Epoch 105/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 602417600.0000 - rmse: 24544.1934 - val_loss: 455285024.0000 - val_rmse: 21337.4062\n",
      "Epoch 106/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598689856.0000 - rmse: 24468.1387 - val_loss: 447884416.0000 - val_rmse: 21163.2754\n",
      "Epoch 107/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580763072.0000 - rmse: 24099.0195 - val_loss: 462136672.0000 - val_rmse: 21497.3613\n",
      "Epoch 108/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588553856.0000 - rmse: 24260.1270 - val_loss: 537836736.0000 - val_rmse: 23191.3066\n",
      "Epoch 109/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577834048.0000 - rmse: 24038.1758 - val_loss: 464375520.0000 - val_rmse: 21549.3711\n",
      "Epoch 110/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 669216704.0000 - rmse: 25869.2227 - val_loss: 443173056.0000 - val_rmse: 21051.6738\n",
      "Epoch 111/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 627205120.0000 - rmse: 25044.0625 - val_loss: 445926304.0000 - val_rmse: 21116.9629\n",
      "Epoch 112/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601879936.0000 - rmse: 24533.2402 - val_loss: 457478304.0000 - val_rmse: 21388.7383\n",
      "Epoch 113/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601788224.0000 - rmse: 24531.3691 - val_loss: 429860384.0000 - val_rmse: 20733.0703\n",
      "Epoch 114/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646370752.0000 - rmse: 25423.8203 - val_loss: 457228192.0000 - val_rmse: 21382.8906\n",
      "Epoch 115/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 645310144.0000 - rmse: 25402.9512 - val_loss: 439014912.0000 - val_rmse: 20952.6777\n",
      "Epoch 116/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580940032.0000 - rmse: 24102.6953 - val_loss: 537909248.0000 - val_rmse: 23192.8652\n",
      "Epoch 117/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592536320.0000 - rmse: 24342.0645 - val_loss: 440733184.0000 - val_rmse: 20993.6406\n",
      "Epoch 118/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616844608.0000 - rmse: 24836.3535 - val_loss: 438693792.0000 - val_rmse: 20945.0137\n",
      "Epoch 119/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 528971840.0000 - rmse: 22999.3828 - val_loss: 459646752.0000 - val_rmse: 21439.3711\n",
      "Epoch 120/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555609408.0000 - rmse: 23571.3652 - val_loss: 451809568.0000 - val_rmse: 21255.8086\n",
      "Epoch 121/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624037760.0000 - rmse: 24980.7441 - val_loss: 441074112.0000 - val_rmse: 21001.7617\n",
      "Epoch 122/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585030528.0000 - rmse: 24187.4023 - val_loss: 473585248.0000 - val_rmse: 21762.0098\n",
      "Epoch 123/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565656192.0000 - rmse: 23783.5215 - val_loss: 469813504.0000 - val_rmse: 21675.1777\n",
      "Epoch 124/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605729984.0000 - rmse: 24611.5781 - val_loss: 427795616.0000 - val_rmse: 20683.2168\n",
      "Epoch 125/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561443776.0000 - rmse: 23694.8008 - val_loss: 427613152.0000 - val_rmse: 20678.8047\n",
      "Epoch 126/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553321152.0000 - rmse: 23522.7773 - val_loss: 434995296.0000 - val_rmse: 20856.5371\n",
      "Epoch 127/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533473824.0000 - rmse: 23097.0488 - val_loss: 480815936.0000 - val_rmse: 21927.5117\n",
      "Epoch 128/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565126528.0000 - rmse: 23772.3867 - val_loss: 450785600.0000 - val_rmse: 21231.7090\n",
      "Epoch 129/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552939776.0000 - rmse: 23514.6680 - val_loss: 441654048.0000 - val_rmse: 21015.5625\n",
      "Epoch 130/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557961664.0000 - rmse: 23621.2090 - val_loss: 433811392.0000 - val_rmse: 20828.1348\n",
      "Epoch 131/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551102016.0000 - rmse: 23475.5586 - val_loss: 441623872.0000 - val_rmse: 21014.8457\n",
      "Epoch 132/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536454336.0000 - rmse: 23161.4824 - val_loss: 431712256.0000 - val_rmse: 20777.6855\n",
      "Epoch 133/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615809664.0000 - rmse: 24815.5098 - val_loss: 482684960.0000 - val_rmse: 21970.0898\n",
      "Epoch 134/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550313216.0000 - rmse: 23458.7520 - val_loss: 499556672.0000 - val_rmse: 22350.7617\n",
      "Epoch 135/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532946784.0000 - rmse: 23085.6367 - val_loss: 477042656.0000 - val_rmse: 21841.3027\n",
      "Epoch 136/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499992480.0000 - rmse: 22360.5059 - val_loss: 437372352.0000 - val_rmse: 20913.4453\n",
      "Epoch 137/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502531072.0000 - rmse: 22417.2012 - val_loss: 440116288.0000 - val_rmse: 20978.9473\n",
      "Epoch 138/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495959200.0000 - rmse: 22270.1387 - val_loss: 434187968.0000 - val_rmse: 20837.1738\n",
      "Epoch 139/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537421056.0000 - rmse: 23182.3398 - val_loss: 445098400.0000 - val_rmse: 21097.3535\n",
      "Epoch 140/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497816032.0000 - rmse: 22311.7852 - val_loss: 473279456.0000 - val_rmse: 21754.9844\n",
      "Epoch 141/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521009536.0000 - rmse: 22825.6289 - val_loss: 466241120.0000 - val_rmse: 21592.6133\n",
      "Epoch 142/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513348384.0000 - rmse: 22657.1875 - val_loss: 479492864.0000 - val_rmse: 21897.3223\n",
      "Epoch 143/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575537152.0000 - rmse: 23990.3535 - val_loss: 432039136.0000 - val_rmse: 20785.5449\n",
      "Epoch 144/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506256640.0000 - rmse: 22500.1406 - val_loss: 428375104.0000 - val_rmse: 20697.2188\n",
      "Epoch 145/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483724704.0000 - rmse: 21993.7363 - val_loss: 436632672.0000 - val_rmse: 20895.7520\n",
      "Epoch 146/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486563264.0000 - rmse: 22058.1777 - val_loss: 459947936.0000 - val_rmse: 21446.3926\n",
      "Epoch 147/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504237888.0000 - rmse: 22455.2383 - val_loss: 424578432.0000 - val_rmse: 20605.2969\n",
      "Epoch 148/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474264192.0000 - rmse: 21777.6035 - val_loss: 450108960.0000 - val_rmse: 21215.7656\n",
      "Epoch 149/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476821536.0000 - rmse: 21836.2422 - val_loss: 430580544.0000 - val_rmse: 20750.4316\n",
      "Epoch 150/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506277312.0000 - rmse: 22500.6035 - val_loss: 443967584.0000 - val_rmse: 21070.5332\n",
      "Epoch 151/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492438976.0000 - rmse: 22190.9609 - val_loss: 433100896.0000 - val_rmse: 20811.0742\n",
      "Epoch 152/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481395744.0000 - rmse: 21940.7305 - val_loss: 437331328.0000 - val_rmse: 20912.4648\n",
      "Epoch 153/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471034816.0000 - rmse: 21703.3320 - val_loss: 443289248.0000 - val_rmse: 21054.4316\n",
      "Epoch 154/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482269888.0000 - rmse: 21960.6406 - val_loss: 450972704.0000 - val_rmse: 21236.1133\n",
      "Epoch 155/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523461056.0000 - rmse: 22879.2637 - val_loss: 472102816.0000 - val_rmse: 21727.9238\n",
      "Epoch 156/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527645440.0000 - rmse: 22970.5293 - val_loss: 447820736.0000 - val_rmse: 21161.7715\n",
      "Epoch 157/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535351136.0000 - rmse: 23137.6504 - val_loss: 448577568.0000 - val_rmse: 21179.6445\n",
      "Epoch 158/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536965056.0000 - rmse: 23172.5020 - val_loss: 455471904.0000 - val_rmse: 21341.7832\n",
      "Epoch 159/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457379392.0000 - rmse: 21386.4258 - val_loss: 448506304.0000 - val_rmse: 21177.9629\n",
      "Epoch 160/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462939424.0000 - rmse: 21516.0215 - val_loss: 423711232.0000 - val_rmse: 20584.2422\n",
      "Epoch 161/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513056736.0000 - rmse: 22650.7520 - val_loss: 424117344.0000 - val_rmse: 20594.1055\n",
      "Epoch 162/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477423744.0000 - rmse: 21850.0215 - val_loss: 550177344.0000 - val_rmse: 23455.8574\n",
      "Epoch 163/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481519232.0000 - rmse: 21943.5410 - val_loss: 436427808.0000 - val_rmse: 20890.8516\n",
      "Epoch 164/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463220576.0000 - rmse: 21522.5566 - val_loss: 486242752.0000 - val_rmse: 22050.9082\n",
      "Epoch 165/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467757440.0000 - rmse: 21627.6973 - val_loss: 524078688.0000 - val_rmse: 22892.7617\n",
      "Epoch 166/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548486208.0000 - rmse: 23419.7773 - val_loss: 420887040.0000 - val_rmse: 20515.5293\n",
      "Epoch 167/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419084320.0000 - rmse: 20471.5469 - val_loss: 439069632.0000 - val_rmse: 20953.9844\n",
      "Epoch 168/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450399456.0000 - rmse: 21222.6133 - val_loss: 425331712.0000 - val_rmse: 20623.5664\n",
      "Epoch 169/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460687520.0000 - rmse: 21463.6289 - val_loss: 464131936.0000 - val_rmse: 21543.7148\n",
      "Epoch 170/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487830304.0000 - rmse: 22086.8770 - val_loss: 419862848.0000 - val_rmse: 20490.5527\n",
      "Epoch 171/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473717984.0000 - rmse: 21765.0586 - val_loss: 486350176.0000 - val_rmse: 22053.3438\n",
      "Epoch 172/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518872224.0000 - rmse: 22778.7637 - val_loss: 457690944.0000 - val_rmse: 21393.7109\n",
      "Epoch 173/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434841280.0000 - rmse: 20852.8438 - val_loss: 429271360.0000 - val_rmse: 20718.8613\n",
      "Epoch 174/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432359776.0000 - rmse: 20793.2578 - val_loss: 491917792.0000 - val_rmse: 22179.2168\n",
      "Epoch 175/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456289408.0000 - rmse: 21360.9297 - val_loss: 483305920.0000 - val_rmse: 21984.2148\n",
      "Epoch 176/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462790048.0000 - rmse: 21512.5508 - val_loss: 448244224.0000 - val_rmse: 21171.7773\n",
      "Epoch 177/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446819264.0000 - rmse: 21138.0957 - val_loss: 438984960.0000 - val_rmse: 20951.9629\n",
      "Epoch 178/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449761568.0000 - rmse: 21207.5820 - val_loss: 467741856.0000 - val_rmse: 21627.3340\n",
      "Epoch 179/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464648608.0000 - rmse: 21555.7031 - val_loss: 473342752.0000 - val_rmse: 21756.4395\n",
      "Epoch 180/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422441152.0000 - rmse: 20553.3691 - val_loss: 503495872.0000 - val_rmse: 22438.7031\n",
      "Epoch 181/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445914144.0000 - rmse: 21116.6758 - val_loss: 458173760.0000 - val_rmse: 21404.9902\n",
      "Epoch 182/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436270240.0000 - rmse: 20887.0801 - val_loss: 448506272.0000 - val_rmse: 21177.9629\n",
      "Epoch 183/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460511040.0000 - rmse: 21459.5176 - val_loss: 481289184.0000 - val_rmse: 21938.2988\n",
      "Epoch 184/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410531136.0000 - rmse: 20261.5605 - val_loss: 440618496.0000 - val_rmse: 20990.9102\n",
      "Epoch 185/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407569536.0000 - rmse: 20188.3477 - val_loss: 453300896.0000 - val_rmse: 21290.8594\n",
      "Epoch 186/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405650752.0000 - rmse: 20140.7695 - val_loss: 574091520.0000 - val_rmse: 23960.2051\n",
      "Epoch 187/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416268736.0000 - rmse: 20402.6602 - val_loss: 420390912.0000 - val_rmse: 20503.4316\n",
      "Epoch 188/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453717568.0000 - rmse: 21300.6465 - val_loss: 502803264.0000 - val_rmse: 22423.2695\n",
      "Epoch 189/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442053408.0000 - rmse: 21025.0625 - val_loss: 466034176.0000 - val_rmse: 21587.8184\n",
      "Epoch 190/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429393984.0000 - rmse: 20721.8184 - val_loss: 471675104.0000 - val_rmse: 21718.0781\n",
      "Epoch 191/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440051648.0000 - rmse: 20977.4043 - val_loss: 487031552.0000 - val_rmse: 22068.7871\n",
      "Epoch 192/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407014976.0000 - rmse: 20174.6074 - val_loss: 463395456.0000 - val_rmse: 21526.6172\n",
      "Epoch 193/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455281632.0000 - rmse: 21337.3281 - val_loss: 525336832.0000 - val_rmse: 22920.2227\n",
      "Epoch 194/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416989120.0000 - rmse: 20420.3047 - val_loss: 411045536.0000 - val_rmse: 20274.2520\n",
      "Epoch 195/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396263776.0000 - rmse: 19906.3711 - val_loss: 426642080.0000 - val_rmse: 20655.3125\n",
      "Epoch 196/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406103584.0000 - rmse: 20152.0078 - val_loss: 418790336.0000 - val_rmse: 20464.3652\n",
      "Epoch 197/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413246976.0000 - rmse: 20328.4707 - val_loss: 432458208.0000 - val_rmse: 20795.6250\n",
      "Epoch 198/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429735808.0000 - rmse: 20730.0605 - val_loss: 427766432.0000 - val_rmse: 20682.5098\n",
      "Epoch 199/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470032800.0000 - rmse: 21680.2324 - val_loss: 437872000.0000 - val_rmse: 20925.3867\n",
      "Epoch 200/200\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428840128.0000 - rmse: 20708.4512 - val_loss: 429118272.0000 - val_rmse: 20715.1660\n",
      "104/104 [==============================] - 0s 672us/step - loss: 383063168.0000 - rmse: 19571.9961\n",
      "[383063168.0, 19571.99609375]\n",
      "[22395.763671875, 29590.62890625, 30838.908203125, 21661.087890625, 19571.99609375]\n",
      "24811.676953125\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "!python train.py kfold baseline\n",
    "# epoch 300 p 25 lr 5e-3"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 21:10:22.383678: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 21:10:22.383728: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 21:10:22.384117: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 21:10:22.597447: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 12828561408.0000 - rmse: 113263.2422 - val_loss: 1893007488.0000 - val_rmse: 43508.7070\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2217238528.0000 - rmse: 47087.5625 - val_loss: 1249118720.0000 - val_rmse: 35342.8750\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1849774592.0000 - rmse: 43009.0078 - val_loss: 1072551168.0000 - val_rmse: 32749.8262\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1680711424.0000 - rmse: 40996.4805 - val_loss: 912736256.0000 - val_rmse: 30211.5254\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1593246720.0000 - rmse: 39915.4961 - val_loss: 856673984.0000 - val_rmse: 29268.9941\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1499197440.0000 - rmse: 38719.4727 - val_loss: 905750464.0000 - val_rmse: 30095.6875\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1470077312.0000 - rmse: 38341.5859 - val_loss: 766702080.0000 - val_rmse: 27689.3848\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1420050688.0000 - rmse: 37683.5586 - val_loss: 732475776.0000 - val_rmse: 27064.2891\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1377919488.0000 - rmse: 37120.3398 - val_loss: 721322944.0000 - val_rmse: 26857.4551\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1371913472.0000 - rmse: 37039.3477 - val_loss: 751471296.0000 - val_rmse: 27412.9766\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1316601984.0000 - rmse: 36285.0117 - val_loss: 706443584.0000 - val_rmse: 26579.0059\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1270083840.0000 - rmse: 35638.2344 - val_loss: 706492928.0000 - val_rmse: 26579.9355\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1308703488.0000 - rmse: 36176.0078 - val_loss: 707580352.0000 - val_rmse: 26600.3828\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1292405504.0000 - rmse: 35950.0430 - val_loss: 736921472.0000 - val_rmse: 27146.2969\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1244606720.0000 - rmse: 35278.9844 - val_loss: 721388096.0000 - val_rmse: 26858.6680\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1166196864.0000 - rmse: 34149.6250 - val_loss: 835470208.0000 - val_rmse: 28904.5020\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1212444800.0000 - rmse: 34820.1797 - val_loss: 721624064.0000 - val_rmse: 26863.0605\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1199089920.0000 - rmse: 34627.8789 - val_loss: 705904960.0000 - val_rmse: 26568.8711\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1169021952.0000 - rmse: 34190.9648 - val_loss: 689135552.0000 - val_rmse: 26251.3906\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1155345920.0000 - rmse: 33990.3789 - val_loss: 667337536.0000 - val_rmse: 25832.8770\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1152067712.0000 - rmse: 33942.1211 - val_loss: 723162688.0000 - val_rmse: 26891.6836\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1162453632.0000 - rmse: 34094.7734 - val_loss: 745001536.0000 - val_rmse: 27294.7168\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1034307136.0000 - rmse: 32160.6465 - val_loss: 1148609024.0000 - val_rmse: 33891.1367\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1129502208.0000 - rmse: 33608.0664 - val_loss: 647699584.0000 - val_rmse: 25449.9434\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1092562176.0000 - rmse: 33053.9297 - val_loss: 689603392.0000 - val_rmse: 26260.3008\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1009812672.0000 - rmse: 31777.5508 - val_loss: 674579776.0000 - val_rmse: 25972.6738\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 968138752.0000 - rmse: 31114.9277 - val_loss: 683832832.0000 - val_rmse: 26150.1973\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 955578368.0000 - rmse: 30912.4297 - val_loss: 668931776.0000 - val_rmse: 25863.7148\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1013077888.0000 - rmse: 31828.8848 - val_loss: 667636928.0000 - val_rmse: 25838.6699\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 936831040.0000 - rmse: 30607.6953 - val_loss: 818758208.0000 - val_rmse: 28613.9512\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 908584320.0000 - rmse: 30142.7324 - val_loss: 658325952.0000 - val_rmse: 25657.8613\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 875937472.0000 - rmse: 29596.2402 - val_loss: 634240832.0000 - val_rmse: 25184.1387\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 904885056.0000 - rmse: 30081.3066 - val_loss: 826126848.0000 - val_rmse: 28742.4219\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 863373632.0000 - rmse: 29383.2207 - val_loss: 705742400.0000 - val_rmse: 26565.8125\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 847855104.0000 - rmse: 29117.9492 - val_loss: 699298176.0000 - val_rmse: 26444.2461\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 805897408.0000 - rmse: 28388.3320 - val_loss: 605857088.0000 - val_rmse: 24614.1641\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 764439936.0000 - rmse: 27648.5078 - val_loss: 774208384.0000 - val_rmse: 27824.5996\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 775512768.0000 - rmse: 27848.0293 - val_loss: 702619328.0000 - val_rmse: 26506.9668\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 844329792.0000 - rmse: 29057.3535 - val_loss: 784956736.0000 - val_rmse: 28017.0801\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 838064576.0000 - rmse: 28949.3457 - val_loss: 585032512.0000 - val_rmse: 24187.4453\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 818849088.0000 - rmse: 28615.5391 - val_loss: 821552704.0000 - val_rmse: 28662.7402\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 696531008.0000 - rmse: 26391.8730 - val_loss: 614734656.0000 - val_rmse: 24793.8438\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 765876992.0000 - rmse: 27674.4824 - val_loss: 661353344.0000 - val_rmse: 25716.7910\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 716332864.0000 - rmse: 26764.3945 - val_loss: 591893760.0000 - val_rmse: 24328.8672\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 725744256.0000 - rmse: 26939.6406 - val_loss: 1144781440.0000 - val_rmse: 33834.6172\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691180928.0000 - rmse: 26290.3203 - val_loss: 762973824.0000 - val_rmse: 27621.9805\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 723762816.0000 - rmse: 26902.8398 - val_loss: 589481920.0000 - val_rmse: 24279.2480\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 750742208.0000 - rmse: 27399.6758 - val_loss: 502226144.0000 - val_rmse: 22410.4023\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 638888256.0000 - rmse: 25276.2383 - val_loss: 740829888.0000 - val_rmse: 27218.1895\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647142144.0000 - rmse: 25438.9883 - val_loss: 644121408.0000 - val_rmse: 25379.5469\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640116672.0000 - rmse: 25300.5273 - val_loss: 631384576.0000 - val_rmse: 25127.3652\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640745856.0000 - rmse: 25312.9590 - val_loss: 518385024.0000 - val_rmse: 22768.0684\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577078336.0000 - rmse: 24022.4531 - val_loss: 601088192.0000 - val_rmse: 24517.0996\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563754816.0000 - rmse: 23743.5215 - val_loss: 495863776.0000 - val_rmse: 22267.9980\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630730944.0000 - rmse: 25114.3574 - val_loss: 448111456.0000 - val_rmse: 21168.6406\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620114944.0000 - rmse: 24902.1074 - val_loss: 514565760.0000 - val_rmse: 22684.0410\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570906368.0000 - rmse: 23893.6465 - val_loss: 530661568.0000 - val_rmse: 23036.0918\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547325888.0000 - rmse: 23394.9980 - val_loss: 1075371520.0000 - val_rmse: 32792.8594\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551679424.0000 - rmse: 23487.8574 - val_loss: 589291392.0000 - val_rmse: 24275.3242\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544694400.0000 - rmse: 23338.6895 - val_loss: 653483584.0000 - val_rmse: 25563.3242\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546702080.0000 - rmse: 23381.6621 - val_loss: 635683712.0000 - val_rmse: 25212.7695\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509313952.0000 - rmse: 22567.9844 - val_loss: 608813184.0000 - val_rmse: 24674.1367\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512746048.0000 - rmse: 22643.8965 - val_loss: 464119872.0000 - val_rmse: 21543.4414\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467394848.0000 - rmse: 21619.3164 - val_loss: 817786880.0000 - val_rmse: 28596.9727\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547822912.0000 - rmse: 23405.6172 - val_loss: 398256064.0000 - val_rmse: 19956.3535\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474341312.0000 - rmse: 21779.3789 - val_loss: 495995456.0000 - val_rmse: 22270.9551\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554267712.0000 - rmse: 23542.8906 - val_loss: 523014048.0000 - val_rmse: 22869.5000\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448802432.0000 - rmse: 21184.9551 - val_loss: 682747008.0000 - val_rmse: 26129.4277\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450350208.0000 - rmse: 21221.4570 - val_loss: 641051648.0000 - val_rmse: 25318.9980\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443499488.0000 - rmse: 21059.4277 - val_loss: 430899232.0000 - val_rmse: 20758.1094\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451610656.0000 - rmse: 21251.1328 - val_loss: 612275008.0000 - val_rmse: 24744.1914\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456795456.0000 - rmse: 21372.7715 - val_loss: 908519808.0000 - val_rmse: 30141.6621\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486966848.0000 - rmse: 22067.3262 - val_loss: 851993792.0000 - val_rmse: 29188.9336\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475379584.0000 - rmse: 21803.1992 - val_loss: 463924288.0000 - val_rmse: 21538.9023\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440384832.0000 - rmse: 20985.3457 - val_loss: 1187957120.0000 - val_rmse: 34466.7539\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443573440.0000 - rmse: 21061.1816 - val_loss: 520886944.0000 - val_rmse: 22822.9473\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457355552.0000 - rmse: 21385.8711 - val_loss: 535336640.0000 - val_rmse: 23137.3418\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419106080.0000 - rmse: 20472.0781 - val_loss: 444681248.0000 - val_rmse: 21087.4668\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440513920.0000 - rmse: 20988.4219 - val_loss: 640466368.0000 - val_rmse: 25307.4375\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433910752.0000 - rmse: 20830.5234 - val_loss: 772163584.0000 - val_rmse: 27787.8320\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470256992.0000 - rmse: 21685.4102 - val_loss: 610876544.0000 - val_rmse: 24715.9160\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425780032.0000 - rmse: 20634.4355 - val_loss: 668075072.0000 - val_rmse: 25847.1465\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431953952.0000 - rmse: 20783.5020 - val_loss: 637520832.0000 - val_rmse: 25249.1738\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390897920.0000 - rmse: 19771.1387 - val_loss: 405920960.0000 - val_rmse: 20147.4805\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402653248.0000 - rmse: 20066.2207 - val_loss: 932409664.0000 - val_rmse: 30535.3848\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367797920.0000 - rmse: 19178.0586 - val_loss: 1062154752.0000 - val_rmse: 32590.7148\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375847712.0000 - rmse: 19386.7930 - val_loss: 739706560.0000 - val_rmse: 27197.5449\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406091648.0000 - rmse: 20151.7148 - val_loss: 524350016.0000 - val_rmse: 22898.6895\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418278368.0000 - rmse: 20451.8535 - val_loss: 547069504.0000 - val_rmse: 23389.5176\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439288800.0000 - rmse: 20959.2168 - val_loss: 618694720.0000 - val_rmse: 24873.5742\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405078624.0000 - rmse: 20126.5645 - val_loss: 762685632.0000 - val_rmse: 27616.7637\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394039584.0000 - rmse: 19850.4297 - val_loss: 621326848.0000 - val_rmse: 24926.4277\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425830848.0000 - rmse: 20635.6680 - val_loss: 858781952.0000 - val_rmse: 29304.9824\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381479680.0000 - rmse: 19531.5039 - val_loss: 750751936.0000 - val_rmse: 27399.8535\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436991776.0000 - rmse: 20904.3457 - val_loss: 544385344.0000 - val_rmse: 23332.0664\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382760576.0000 - rmse: 19564.2676 - val_loss: 794228864.0000 - val_rmse: 28182.0664\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380626432.0000 - rmse: 19509.6504 - val_loss: 1131079296.0000 - val_rmse: 33631.5195\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336908480.0000 - rmse: 18355.0645 - val_loss: 973892096.0000 - val_rmse: 31207.2441\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374868672.0000 - rmse: 19361.5254 - val_loss: 538176320.0000 - val_rmse: 23198.6270\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376594784.0000 - rmse: 19406.0508 - val_loss: 754330688.0000 - val_rmse: 27465.0820\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376197504.0000 - rmse: 19395.8105 - val_loss: 460841600.0000 - val_rmse: 21467.2207\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342760960.0000 - rmse: 18513.8027 - val_loss: 886353408.0000 - val_rmse: 29771.6855\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324266240.0000 - rmse: 18007.3945 - val_loss: 911804672.0000 - val_rmse: 30196.1035\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330763648.0000 - rmse: 18186.9062 - val_loss: 505328864.0000 - val_rmse: 22479.5195\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372362656.0000 - rmse: 19296.7012 - val_loss: 701597760.0000 - val_rmse: 26487.6875\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336990656.0000 - rmse: 18357.3047 - val_loss: 679444544.0000 - val_rmse: 26066.1562\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391187904.0000 - rmse: 19778.4688 - val_loss: 519570848.0000 - val_rmse: 22794.0977\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346059392.0000 - rmse: 18602.6680 - val_loss: 853726784.0000 - val_rmse: 29218.6035\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366415680.0000 - rmse: 19141.9863 - val_loss: 603605120.0000 - val_rmse: 24568.3770\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344282016.0000 - rmse: 18554.8359 - val_loss: 757175104.0000 - val_rmse: 27516.8145\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363918272.0000 - rmse: 19076.6426 - val_loss: 1319467008.0000 - val_rmse: 36324.4648\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304773760.0000 - rmse: 17457.7715 - val_loss: 444840736.0000 - val_rmse: 21091.2480\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351081184.0000 - rmse: 18737.1602 - val_loss: 691410688.0000 - val_rmse: 26294.6895\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316649024.0000 - rmse: 17794.6348 - val_loss: 1183017600.0000 - val_rmse: 34395.0234\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390653056.0000 - rmse: 19764.9434 - val_loss: 366904736.0000 - val_rmse: 19154.7578\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365625216.0000 - rmse: 19121.3281 - val_loss: 362809824.0000 - val_rmse: 19047.5684\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339356992.0000 - rmse: 18421.6426 - val_loss: 571209856.0000 - val_rmse: 23899.9961\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334994688.0000 - rmse: 18302.8594 - val_loss: 1172479616.0000 - val_rmse: 34241.4883\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361503232.0000 - rmse: 19013.2383 - val_loss: 429873984.0000 - val_rmse: 20733.4023\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298619488.0000 - rmse: 17280.6094 - val_loss: 751174784.0000 - val_rmse: 27407.5684\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343528320.0000 - rmse: 18534.5156 - val_loss: 454173856.0000 - val_rmse: 21311.3535\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299473088.0000 - rmse: 17305.2891 - val_loss: 1174653568.0000 - val_rmse: 34273.2188\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329262944.0000 - rmse: 18145.6035 - val_loss: 720355712.0000 - val_rmse: 26839.4434\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383585504.0000 - rmse: 19585.3379 - val_loss: 648434432.0000 - val_rmse: 25464.3750\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289181344.0000 - rmse: 17005.3320 - val_loss: 718605504.0000 - val_rmse: 26806.8184\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390655072.0000 - rmse: 19764.9961 - val_loss: 776331776.0000 - val_rmse: 27862.7305\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312552928.0000 - rmse: 17679.1660 - val_loss: 435635680.0000 - val_rmse: 20871.8867\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304127456.0000 - rmse: 17439.2500 - val_loss: 1410736512.0000 - val_rmse: 37559.7734\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361079776.0000 - rmse: 19002.0977 - val_loss: 699616000.0000 - val_rmse: 26450.2559\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265552000.0000 - rmse: 16295.7656 - val_loss: 548047808.0000 - val_rmse: 23410.4199\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322973152.0000 - rmse: 17971.4531 - val_loss: 784342272.0000 - val_rmse: 28006.1094\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263381280.0000 - rmse: 16229.0254 - val_loss: 344254688.0000 - val_rmse: 18554.1016\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325140704.0000 - rmse: 18031.6562 - val_loss: 955164928.0000 - val_rmse: 30905.7402\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343183968.0000 - rmse: 18525.2246 - val_loss: 623841984.0000 - val_rmse: 24976.8281\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338592288.0000 - rmse: 18400.8770 - val_loss: 678507840.0000 - val_rmse: 26048.1836\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354821696.0000 - rmse: 18836.7109 - val_loss: 631740608.0000 - val_rmse: 25134.4512\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331725472.0000 - rmse: 18213.3320 - val_loss: 674208896.0000 - val_rmse: 25965.5312\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320688672.0000 - rmse: 17907.7812 - val_loss: 658168704.0000 - val_rmse: 25654.7969\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318966784.0000 - rmse: 17859.6406 - val_loss: 584429120.0000 - val_rmse: 24174.9688\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331464608.0000 - rmse: 18206.1680 - val_loss: 671050752.0000 - val_rmse: 25904.6465\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365335488.0000 - rmse: 19113.7500 - val_loss: 986861504.0000 - val_rmse: 31414.3496\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307217024.0000 - rmse: 17527.6074 - val_loss: 368570560.0000 - val_rmse: 19198.1895\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279991136.0000 - rmse: 16732.9355 - val_loss: 510801888.0000 - val_rmse: 22600.9258\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337501472.0000 - rmse: 18371.2109 - val_loss: 848655872.0000 - val_rmse: 29131.6992\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265598448.0000 - rmse: 16297.1904 - val_loss: 1551630080.0000 - val_rmse: 39390.7344\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274674848.0000 - rmse: 16573.3164 - val_loss: 979182016.0000 - val_rmse: 31291.8848\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306350912.0000 - rmse: 17502.8809 - val_loss: 934627392.0000 - val_rmse: 30571.6758\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275994912.0000 - rmse: 16613.0938 - val_loss: 717521920.0000 - val_rmse: 26786.5996\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274682144.0000 - rmse: 16573.5371 - val_loss: 1167176320.0000 - val_rmse: 34163.9609\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307187488.0000 - rmse: 17526.7637 - val_loss: 412728832.0000 - val_rmse: 20315.7285\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289198752.0000 - rmse: 17005.8438 - val_loss: 910692288.0000 - val_rmse: 30177.6777\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300120736.0000 - rmse: 17323.9922 - val_loss: 882336832.0000 - val_rmse: 29704.1523\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287085536.0000 - rmse: 16943.5977 - val_loss: 716183552.0000 - val_rmse: 26761.6016\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266003520.0000 - rmse: 16309.6123 - val_loss: 810986048.0000 - val_rmse: 28477.8164\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301053312.0000 - rmse: 17350.8887 - val_loss: 752015616.0000 - val_rmse: 27422.9023\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298196928.0000 - rmse: 17268.3770 - val_loss: 925492352.0000 - val_rmse: 30421.9062\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295157312.0000 - rmse: 17180.1426 - val_loss: 684203968.0000 - val_rmse: 26157.2930\n",
      "104/104 [==============================] - 0s 718us/step - loss: 382103904.0000 - rmse: 19547.4766\n",
      "[382103904.0, 19547.4765625]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 12011800576.0000 - rmse: 109598.3594 - val_loss: 1717704192.0000 - val_rmse: 41445.1953\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1888574720.0000 - rmse: 43457.7344 - val_loss: 1222020736.0000 - val_rmse: 34957.4141\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1553656320.0000 - rmse: 39416.4492 - val_loss: 1076941696.0000 - val_rmse: 32816.7891\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1376177920.0000 - rmse: 37096.8711 - val_loss: 1059000448.0000 - val_rmse: 32542.2871\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1308166656.0000 - rmse: 36168.5859 - val_loss: 945458944.0000 - val_rmse: 30748.3145\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1216759424.0000 - rmse: 34882.0781 - val_loss: 987489728.0000 - val_rmse: 31424.3496\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1149836160.0000 - rmse: 33909.2344 - val_loss: 952724224.0000 - val_rmse: 30866.2305\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1147890688.0000 - rmse: 33880.5352 - val_loss: 910509952.0000 - val_rmse: 30174.6582\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1077426304.0000 - rmse: 32824.1719 - val_loss: 854463360.0000 - val_rmse: 29231.2051\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1022115648.0000 - rmse: 31970.5430 - val_loss: 853906368.0000 - val_rmse: 29221.6758\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1048596160.0000 - rmse: 32382.0352 - val_loss: 904003264.0000 - val_rmse: 30066.6465\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1050233024.0000 - rmse: 32407.2988 - val_loss: 834446400.0000 - val_rmse: 28886.7852\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 995064640.0000 - rmse: 31544.6445 - val_loss: 821979136.0000 - val_rmse: 28670.1777\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 975008832.0000 - rmse: 31225.1309 - val_loss: 834525312.0000 - val_rmse: 28888.1523\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 956964160.0000 - rmse: 30934.8359 - val_loss: 814106432.0000 - val_rmse: 28532.5508\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 940172736.0000 - rmse: 30662.2363 - val_loss: 796312640.0000 - val_rmse: 28219.0117\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 929766208.0000 - rmse: 30492.0684 - val_loss: 820938240.0000 - val_rmse: 28652.0195\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 859323264.0000 - rmse: 29314.2168 - val_loss: 818902528.0000 - val_rmse: 28616.4727\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 917097280.0000 - rmse: 30283.6133 - val_loss: 782639168.0000 - val_rmse: 27975.6895\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 907878656.0000 - rmse: 30131.0254 - val_loss: 751957568.0000 - val_rmse: 27421.8438\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 884820480.0000 - rmse: 29745.9316 - val_loss: 768044736.0000 - val_rmse: 27713.6191\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 857181120.0000 - rmse: 29277.6562 - val_loss: 732533184.0000 - val_rmse: 27065.3496\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 818508416.0000 - rmse: 28609.5859 - val_loss: 706035968.0000 - val_rmse: 26571.3359\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 824676800.0000 - rmse: 28717.1855 - val_loss: 771898560.0000 - val_rmse: 27783.0625\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 791121344.0000 - rmse: 28126.8789 - val_loss: 691982336.0000 - val_rmse: 26305.5547\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 737438848.0000 - rmse: 27155.8262 - val_loss: 675110720.0000 - val_rmse: 25982.8926\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 715608448.0000 - rmse: 26750.8594 - val_loss: 665507008.0000 - val_rmse: 25797.4219\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 736412096.0000 - rmse: 27136.9141 - val_loss: 655149312.0000 - val_rmse: 25595.8848\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 750211392.0000 - rmse: 27389.9863 - val_loss: 892839168.0000 - val_rmse: 29880.4141\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 709958528.0000 - rmse: 26645.0469 - val_loss: 675054656.0000 - val_rmse: 25981.8145\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685895360.0000 - rmse: 26189.6035 - val_loss: 620722752.0000 - val_rmse: 24914.3086\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684383680.0000 - rmse: 26160.7285 - val_loss: 628257536.0000 - val_rmse: 25065.0664\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634887808.0000 - rmse: 25196.9805 - val_loss: 634137792.0000 - val_rmse: 25182.0918\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651235712.0000 - rmse: 25519.3184 - val_loss: 754526656.0000 - val_rmse: 27468.6465\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592432256.0000 - rmse: 24339.9316 - val_loss: 575848320.0000 - val_rmse: 23996.8398\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588791360.0000 - rmse: 24265.0234 - val_loss: 569129024.0000 - val_rmse: 23856.4258\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612869440.0000 - rmse: 24756.1992 - val_loss: 568341248.0000 - val_rmse: 23839.9082\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572440064.0000 - rmse: 23925.7188 - val_loss: 609526720.0000 - val_rmse: 24688.5957\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551215424.0000 - rmse: 23477.9766 - val_loss: 571228928.0000 - val_rmse: 23900.3945\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562202624.0000 - rmse: 23710.8125 - val_loss: 570642944.0000 - val_rmse: 23888.1348\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582729344.0000 - rmse: 24139.7871 - val_loss: 552359104.0000 - val_rmse: 23502.3203\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557076032.0000 - rmse: 23602.4590 - val_loss: 879753664.0000 - val_rmse: 29660.6426\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507269568.0000 - rmse: 22522.6445 - val_loss: 525335744.0000 - val_rmse: 22920.2031\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515325312.0000 - rmse: 22700.7773 - val_loss: 548189440.0000 - val_rmse: 23413.4453\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463109376.0000 - rmse: 21519.9766 - val_loss: 554656000.0000 - val_rmse: 23551.1367\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506905536.0000 - rmse: 22514.5625 - val_loss: 844757440.0000 - val_rmse: 29064.7090\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479734400.0000 - rmse: 21902.8379 - val_loss: 518596032.0000 - val_rmse: 22772.7031\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433152128.0000 - rmse: 20812.3047 - val_loss: 517688384.0000 - val_rmse: 22752.7656\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472197344.0000 - rmse: 21730.1016 - val_loss: 601800448.0000 - val_rmse: 24531.6191\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485512064.0000 - rmse: 22034.3379 - val_loss: 544985472.0000 - val_rmse: 23344.9219\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430501376.0000 - rmse: 20748.5254 - val_loss: 505426976.0000 - val_rmse: 22481.7031\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431071424.0000 - rmse: 20762.2598 - val_loss: 503736352.0000 - val_rmse: 22444.0723\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424339520.0000 - rmse: 20599.5020 - val_loss: 482782016.0000 - val_rmse: 21972.3008\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404107424.0000 - rmse: 20102.4238 - val_loss: 494230848.0000 - val_rmse: 22231.3027\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425679328.0000 - rmse: 20631.9980 - val_loss: 491221216.0000 - val_rmse: 22163.5117\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385731552.0000 - rmse: 19640.0488 - val_loss: 509229792.0000 - val_rmse: 22566.1211\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393401536.0000 - rmse: 19834.3516 - val_loss: 499569856.0000 - val_rmse: 22351.0586\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397188992.0000 - rmse: 19929.5977 - val_loss: 500088320.0000 - val_rmse: 22362.6543\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403263136.0000 - rmse: 20081.4121 - val_loss: 517276256.0000 - val_rmse: 22743.7090\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399530688.0000 - rmse: 19988.2637 - val_loss: 554174336.0000 - val_rmse: 23540.9082\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397977920.0000 - rmse: 19949.3828 - val_loss: 545314048.0000 - val_rmse: 23351.9609\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443630976.0000 - rmse: 21062.5469 - val_loss: 507003520.0000 - val_rmse: 22516.7383\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357183136.0000 - rmse: 18899.2891 - val_loss: 576118080.0000 - val_rmse: 24002.4590\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379032320.0000 - rmse: 19468.7500 - val_loss: 541709056.0000 - val_rmse: 23274.6445\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365975008.0000 - rmse: 19130.4727 - val_loss: 543139840.0000 - val_rmse: 23305.3613\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374783200.0000 - rmse: 19359.3184 - val_loss: 508780320.0000 - val_rmse: 22556.1562\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344391520.0000 - rmse: 18557.7891 - val_loss: 500827936.0000 - val_rmse: 22379.1855\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370691456.0000 - rmse: 19253.3496 - val_loss: 718858944.0000 - val_rmse: 26811.5449\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365933408.0000 - rmse: 19129.3848 - val_loss: 509400832.0000 - val_rmse: 22569.9102\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367291456.0000 - rmse: 19164.8496 - val_loss: 516384640.0000 - val_rmse: 22724.0977\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369874048.0000 - rmse: 19232.1094 - val_loss: 505075552.0000 - val_rmse: 22473.8867\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321335008.0000 - rmse: 17925.8164 - val_loss: 483680864.0000 - val_rmse: 21992.7441\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341917888.0000 - rmse: 18491.0195 - val_loss: 513419040.0000 - val_rmse: 22658.7520\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316651488.0000 - rmse: 17794.7031 - val_loss: 579794688.0000 - val_rmse: 24078.9258\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310268096.0000 - rmse: 17614.4277 - val_loss: 633482176.0000 - val_rmse: 25169.0723\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317461024.0000 - rmse: 17817.4355 - val_loss: 555748480.0000 - val_rmse: 23574.3184\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344051296.0000 - rmse: 18548.6191 - val_loss: 595271808.0000 - val_rmse: 24398.1934\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326027296.0000 - rmse: 18056.2266 - val_loss: 512933568.0000 - val_rmse: 22648.0352\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337584768.0000 - rmse: 18373.4785 - val_loss: 590655232.0000 - val_rmse: 24303.4004\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325060736.0000 - rmse: 18029.4395 - val_loss: 523821280.0000 - val_rmse: 22887.1406\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304872736.0000 - rmse: 17460.6055 - val_loss: 515673472.0000 - val_rmse: 22708.4434\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319437728.0000 - rmse: 17872.8203 - val_loss: 526184064.0000 - val_rmse: 22938.7031\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314603328.0000 - rmse: 17737.0586 - val_loss: 507164256.0000 - val_rmse: 22520.3047\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309607680.0000 - rmse: 17595.6719 - val_loss: 508761856.0000 - val_rmse: 22555.7480\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294168544.0000 - rmse: 17151.3398 - val_loss: 547007232.0000 - val_rmse: 23388.1836\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283772704.0000 - rmse: 16845.5527 - val_loss: 528426016.0000 - val_rmse: 22987.5195\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320846912.0000 - rmse: 17912.1992 - val_loss: 578239744.0000 - val_rmse: 24046.6152\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288850464.0000 - rmse: 16995.6016 - val_loss: 562908544.0000 - val_rmse: 23725.6934\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319758432.0000 - rmse: 17881.7910 - val_loss: 543308160.0000 - val_rmse: 23308.9707\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271865824.0000 - rmse: 16488.3535 - val_loss: 611822080.0000 - val_rmse: 24735.0371\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308636864.0000 - rmse: 17568.0645 - val_loss: 677432000.0000 - val_rmse: 26027.5234\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287556224.0000 - rmse: 16957.4824 - val_loss: 651757248.0000 - val_rmse: 25529.5371\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272425088.0000 - rmse: 16505.3047 - val_loss: 572870912.0000 - val_rmse: 23934.7227\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298325664.0000 - rmse: 17272.1055 - val_loss: 499615264.0000 - val_rmse: 22352.0742\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340933120.0000 - rmse: 18464.3730 - val_loss: 529186592.0000 - val_rmse: 23004.0547\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315319584.0000 - rmse: 17757.2402 - val_loss: 505583520.0000 - val_rmse: 22485.1836\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306533472.0000 - rmse: 17508.0957 - val_loss: 511046656.0000 - val_rmse: 22606.3418\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272867520.0000 - rmse: 16518.7012 - val_loss: 565999040.0000 - val_rmse: 23790.7344\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292392896.0000 - rmse: 17099.4980 - val_loss: 549044352.0000 - val_rmse: 23431.6953\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246391232.0000 - rmse: 15696.8535 - val_loss: 575515392.0000 - val_rmse: 23989.9004\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277513312.0000 - rmse: 16658.7285 - val_loss: 513240448.0000 - val_rmse: 22654.8105\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286126496.0000 - rmse: 16915.2734 - val_loss: 539936768.0000 - val_rmse: 23236.5391\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329912896.0000 - rmse: 18163.5039 - val_loss: 616289216.0000 - val_rmse: 24825.1738\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312167616.0000 - rmse: 17668.2656 - val_loss: 517681472.0000 - val_rmse: 22752.6133\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274523680.0000 - rmse: 16568.7559 - val_loss: 519219392.0000 - val_rmse: 22786.3867\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295641408.0000 - rmse: 17194.2246 - val_loss: 532274592.0000 - val_rmse: 23071.0762\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306762496.0000 - rmse: 17514.6348 - val_loss: 506720352.0000 - val_rmse: 22510.4492\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304428448.0000 - rmse: 17447.8770 - val_loss: 511392352.0000 - val_rmse: 22613.9863\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300691232.0000 - rmse: 17340.4492 - val_loss: 654793216.0000 - val_rmse: 25588.9258\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271052768.0000 - rmse: 16463.6777 - val_loss: 519446176.0000 - val_rmse: 22791.3613\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274413888.0000 - rmse: 16565.4414 - val_loss: 529519520.0000 - val_rmse: 23011.2910\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267515088.0000 - rmse: 16355.8848 - val_loss: 535592000.0000 - val_rmse: 23142.8594\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283417504.0000 - rmse: 16835.0078 - val_loss: 807665280.0000 - val_rmse: 28419.4531\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274139488.0000 - rmse: 16557.1562 - val_loss: 517622784.0000 - val_rmse: 22751.3242\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262768416.0000 - rmse: 16210.1328 - val_loss: 510890272.0000 - val_rmse: 22602.8809\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253377552.0000 - rmse: 15917.8379 - val_loss: 568536192.0000 - val_rmse: 23843.9961\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262050928.0000 - rmse: 16187.9863 - val_loss: 574747136.0000 - val_rmse: 23973.8828\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258801536.0000 - rmse: 16087.3086 - val_loss: 479326464.0000 - val_rmse: 21893.5254\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262345104.0000 - rmse: 16197.0713 - val_loss: 512873024.0000 - val_rmse: 22646.6992\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271916320.0000 - rmse: 16489.8828 - val_loss: 503041408.0000 - val_rmse: 22428.5840\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275976032.0000 - rmse: 16612.5234 - val_loss: 514839808.0000 - val_rmse: 22690.0820\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241278656.0000 - rmse: 15533.1465 - val_loss: 512498144.0000 - val_rmse: 22638.4219\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245716096.0000 - rmse: 15675.3340 - val_loss: 466373568.0000 - val_rmse: 21595.6836\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268192176.0000 - rmse: 16376.5723 - val_loss: 496946528.0000 - val_rmse: 22292.2969\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247045664.0000 - rmse: 15717.6855 - val_loss: 824150208.0000 - val_rmse: 28708.0156\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285061120.0000 - rmse: 16883.7520 - val_loss: 493176288.0000 - val_rmse: 22207.5723\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239447408.0000 - rmse: 15474.0869 - val_loss: 494879936.0000 - val_rmse: 22245.8965\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260725664.0000 - rmse: 16147.0000 - val_loss: 513747552.0000 - val_rmse: 22666.0000\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269699264.0000 - rmse: 16422.5215 - val_loss: 520392608.0000 - val_rmse: 22812.1152\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255117280.0000 - rmse: 15972.3887 - val_loss: 606051200.0000 - val_rmse: 24618.1055\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230035360.0000 - rmse: 15166.9150 - val_loss: 518929792.0000 - val_rmse: 22780.0312\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264242752.0000 - rmse: 16255.5449 - val_loss: 490758016.0000 - val_rmse: 22153.0586\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215954752.0000 - rmse: 14695.3975 - val_loss: 587562432.0000 - val_rmse: 24239.6875\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286820832.0000 - rmse: 16935.7852 - val_loss: 517334912.0000 - val_rmse: 22744.9961\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324646368.0000 - rmse: 18017.9453 - val_loss: 496887104.0000 - val_rmse: 22290.9648\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249519328.0000 - rmse: 15796.1807 - val_loss: 547212608.0000 - val_rmse: 23392.5742\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285737760.0000 - rmse: 16903.7773 - val_loss: 670335680.0000 - val_rmse: 25890.8418\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237590128.0000 - rmse: 15413.9570 - val_loss: 513905472.0000 - val_rmse: 22669.4824\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246822704.0000 - rmse: 15710.5908 - val_loss: 500075584.0000 - val_rmse: 22362.3691\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259143600.0000 - rmse: 16097.9355 - val_loss: 536371008.0000 - val_rmse: 23159.6855\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268646304.0000 - rmse: 16390.4316 - val_loss: 509230016.0000 - val_rmse: 22566.1250\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236107696.0000 - rmse: 15365.7949 - val_loss: 535075840.0000 - val_rmse: 23131.7051\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270122112.0000 - rmse: 16435.3906 - val_loss: 591279168.0000 - val_rmse: 24316.2305\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291515936.0000 - rmse: 17073.8359 - val_loss: 533628864.0000 - val_rmse: 23100.4062\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222743920.0000 - rmse: 14924.6074 - val_loss: 612378624.0000 - val_rmse: 24746.2852\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254091248.0000 - rmse: 15940.2373 - val_loss: 605533504.0000 - val_rmse: 24607.5898\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242414560.0000 - rmse: 15569.6670 - val_loss: 540701696.0000 - val_rmse: 23252.9922\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 225149648.0000 - rmse: 15004.9844 - val_loss: 482627040.0000 - val_rmse: 21968.7715\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239132288.0000 - rmse: 15463.9023 - val_loss: 698748800.0000 - val_rmse: 26433.8555\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236706656.0000 - rmse: 15385.2744 - val_loss: 506604480.0000 - val_rmse: 22507.8730\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263843152.0000 - rmse: 16243.2480 - val_loss: 518997440.0000 - val_rmse: 22781.5137\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222020368.0000 - rmse: 14900.3467 - val_loss: 559601984.0000 - val_rmse: 23655.9062\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238655472.0000 - rmse: 15448.4766 - val_loss: 530697984.0000 - val_rmse: 23036.8828\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229443664.0000 - rmse: 15147.3965 - val_loss: 556148480.0000 - val_rmse: 23582.8008\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268271168.0000 - rmse: 16378.9834 - val_loss: 559106816.0000 - val_rmse: 23645.4375\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255045488.0000 - rmse: 15970.1436 - val_loss: 566825856.0000 - val_rmse: 23808.1035\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235021632.0000 - rmse: 15330.4141 - val_loss: 605944768.0000 - val_rmse: 24615.9434\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263950784.0000 - rmse: 16246.5605 - val_loss: 563826560.0000 - val_rmse: 23745.0312\n",
      "104/104 [==============================] - 0s 656us/step - loss: 1010806464.0000 - rmse: 31793.1836\n",
      "[1010806464.0, 31793.18359375]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 12382068736.0000 - rmse: 111274.7422 - val_loss: 2190169344.0000 - val_rmse: 46799.2461\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1969759744.0000 - rmse: 44381.9766 - val_loss: 1473421824.0000 - val_rmse: 38385.1758\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1568604160.0000 - rmse: 39605.6094 - val_loss: 1276070144.0000 - val_rmse: 35722.1250\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1389406336.0000 - rmse: 37274.7422 - val_loss: 1177141504.0000 - val_rmse: 34309.4961\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1327491072.0000 - rmse: 36434.7500 - val_loss: 1096852224.0000 - val_rmse: 33118.7578\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1224823552.0000 - rmse: 34997.4805 - val_loss: 1080359936.0000 - val_rmse: 32868.8281\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1168912384.0000 - rmse: 34189.3594 - val_loss: 993455488.0000 - val_rmse: 31519.1289\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1065109632.0000 - rmse: 32636.0156 - val_loss: 954938688.0000 - val_rmse: 30902.0820\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1069779520.0000 - rmse: 32707.4844 - val_loss: 1093442688.0000 - val_rmse: 33067.2461\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1057409216.0000 - rmse: 32517.8301 - val_loss: 937797184.0000 - val_rmse: 30623.4746\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1031462080.0000 - rmse: 32116.3828 - val_loss: 1073703808.0000 - val_rmse: 32767.4199\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 994003840.0000 - rmse: 31527.8262 - val_loss: 1029784768.0000 - val_rmse: 32090.2578\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1014495872.0000 - rmse: 31851.1523 - val_loss: 974232256.0000 - val_rmse: 31212.6934\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 983552640.0000 - rmse: 31361.6426 - val_loss: 908034304.0000 - val_rmse: 30133.6074\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 965654336.0000 - rmse: 31074.9785 - val_loss: 942264192.0000 - val_rmse: 30696.3223\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 942787264.0000 - rmse: 30704.8418 - val_loss: 923337216.0000 - val_rmse: 30386.4648\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 968696640.0000 - rmse: 31123.8926 - val_loss: 906285632.0000 - val_rmse: 30104.5781\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 921127424.0000 - rmse: 30350.0781 - val_loss: 900414272.0000 - val_rmse: 30006.9043\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 954019136.0000 - rmse: 30887.2012 - val_loss: 998627072.0000 - val_rmse: 31601.0605\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 916247616.0000 - rmse: 30269.5820 - val_loss: 965611904.0000 - val_rmse: 31074.2969\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 934194688.0000 - rmse: 30564.5977 - val_loss: 995788096.0000 - val_rmse: 31556.1094\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 916127488.0000 - rmse: 30267.5977 - val_loss: 924978112.0000 - val_rmse: 30413.4531\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 919259904.0000 - rmse: 30319.2988 - val_loss: 923494144.0000 - val_rmse: 30389.0469\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 869433280.0000 - rmse: 29486.1543 - val_loss: 861493952.0000 - val_rmse: 29351.2148\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 868942336.0000 - rmse: 29477.8281 - val_loss: 983998720.0000 - val_rmse: 31368.7539\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 864378240.0000 - rmse: 29400.3105 - val_loss: 855990336.0000 - val_rmse: 29257.3125\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 890532800.0000 - rmse: 29841.7969 - val_loss: 935247360.0000 - val_rmse: 30581.8145\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 845268288.0000 - rmse: 29073.4980 - val_loss: 999043200.0000 - val_rmse: 31607.6445\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 847203584.0000 - rmse: 29106.7617 - val_loss: 1003761728.0000 - val_rmse: 31682.1992\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 819381568.0000 - rmse: 28624.8418 - val_loss: 855785408.0000 - val_rmse: 29253.8105\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 762476096.0000 - rmse: 27612.9688 - val_loss: 886944832.0000 - val_rmse: 29781.6191\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 817362304.0000 - rmse: 28589.5488 - val_loss: 916640960.0000 - val_rmse: 30276.0781\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 756499648.0000 - rmse: 27504.5391 - val_loss: 921752192.0000 - val_rmse: 30360.3711\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 777981632.0000 - rmse: 27892.3223 - val_loss: 915080704.0000 - val_rmse: 30250.3008\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 744970688.0000 - rmse: 27294.1504 - val_loss: 866353728.0000 - val_rmse: 29433.8867\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 750836992.0000 - rmse: 27401.4043 - val_loss: 836932864.0000 - val_rmse: 28929.7910\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 766876160.0000 - rmse: 27692.5293 - val_loss: 987521024.0000 - val_rmse: 31424.8477\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 708559040.0000 - rmse: 26618.7715 - val_loss: 940699456.0000 - val_rmse: 30670.8242\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677441024.0000 - rmse: 26027.6973 - val_loss: 798131584.0000 - val_rmse: 28251.2227\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 682376448.0000 - rmse: 26122.3359 - val_loss: 817848256.0000 - val_rmse: 28598.0469\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 639827264.0000 - rmse: 25294.8066 - val_loss: 810872384.0000 - val_rmse: 28475.8203\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621899392.0000 - rmse: 24937.9102 - val_loss: 801853504.0000 - val_rmse: 28317.0176\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641892864.0000 - rmse: 25335.6055 - val_loss: 747709696.0000 - val_rmse: 27344.2812\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 655544832.0000 - rmse: 25603.6094 - val_loss: 690190080.0000 - val_rmse: 26271.4688\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656179456.0000 - rmse: 25615.9980 - val_loss: 709646336.0000 - val_rmse: 26639.1875\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 689246592.0000 - rmse: 26253.5059 - val_loss: 729827136.0000 - val_rmse: 27015.3125\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666470144.0000 - rmse: 25816.0840 - val_loss: 766133440.0000 - val_rmse: 27679.1152\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583404160.0000 - rmse: 24153.7617 - val_loss: 729252416.0000 - val_rmse: 27004.6738\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632400448.0000 - rmse: 25147.5742 - val_loss: 642514176.0000 - val_rmse: 25347.8633\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 595435776.0000 - rmse: 24401.5527 - val_loss: 1187640960.0000 - val_rmse: 34462.1680\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572287360.0000 - rmse: 23922.5273 - val_loss: 696020288.0000 - val_rmse: 26382.1973\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561682752.0000 - rmse: 23699.8477 - val_loss: 850532160.0000 - val_rmse: 29163.8848\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606680576.0000 - rmse: 24630.8848 - val_loss: 771530752.0000 - val_rmse: 27776.4434\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529880544.0000 - rmse: 23019.1328 - val_loss: 766947648.0000 - val_rmse: 27693.8203\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541536000.0000 - rmse: 23270.9238 - val_loss: 775888384.0000 - val_rmse: 27854.7734\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527966560.0000 - rmse: 22977.5234 - val_loss: 685716224.0000 - val_rmse: 26186.1836\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539511488.0000 - rmse: 23227.3867 - val_loss: 602192896.0000 - val_rmse: 24539.6172\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523952160.0000 - rmse: 22890.0020 - val_loss: 595606848.0000 - val_rmse: 24405.0586\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472796320.0000 - rmse: 21743.8809 - val_loss: 804826112.0000 - val_rmse: 28369.4570\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512232352.0000 - rmse: 22632.5508 - val_loss: 988548160.0000 - val_rmse: 31441.1855\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526104928.0000 - rmse: 22936.9766 - val_loss: 876763392.0000 - val_rmse: 29610.1914\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519723584.0000 - rmse: 22797.4453 - val_loss: 717994240.0000 - val_rmse: 26795.4141\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496684512.0000 - rmse: 22286.4199 - val_loss: 560464576.0000 - val_rmse: 23674.1328\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479735808.0000 - rmse: 21902.8730 - val_loss: 827603520.0000 - val_rmse: 28768.0996\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511392320.0000 - rmse: 22613.9844 - val_loss: 859218176.0000 - val_rmse: 29312.4238\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489277408.0000 - rmse: 22119.6152 - val_loss: 661057216.0000 - val_rmse: 25711.0312\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488653664.0000 - rmse: 22105.5098 - val_loss: 640080064.0000 - val_rmse: 25299.8027\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546320320.0000 - rmse: 23373.4961 - val_loss: 986142080.0000 - val_rmse: 31402.8984\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533334976.0000 - rmse: 23094.0469 - val_loss: 644867456.0000 - val_rmse: 25394.2402\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567133056.0000 - rmse: 23814.5547 - val_loss: 703260672.0000 - val_rmse: 26519.0605\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516366400.0000 - rmse: 22723.6973 - val_loss: 768250816.0000 - val_rmse: 27717.3359\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464158016.0000 - rmse: 21544.3262 - val_loss: 700358592.0000 - val_rmse: 26464.2891\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490653312.0000 - rmse: 22150.6953 - val_loss: 643556800.0000 - val_rmse: 25368.4199\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474804032.0000 - rmse: 21789.9980 - val_loss: 622590784.0000 - val_rmse: 24951.7695\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470257632.0000 - rmse: 21685.4238 - val_loss: 1046233024.0000 - val_rmse: 32345.5254\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426474912.0000 - rmse: 20651.2695 - val_loss: 885221760.0000 - val_rmse: 29752.6758\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488926112.0000 - rmse: 22111.6738 - val_loss: 558713024.0000 - val_rmse: 23637.1113\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461057568.0000 - rmse: 21472.2500 - val_loss: 475958272.0000 - val_rmse: 21816.4668\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437764576.0000 - rmse: 20922.8242 - val_loss: 424947872.0000 - val_rmse: 20614.2617\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475695360.0000 - rmse: 21810.4395 - val_loss: 834765440.0000 - val_rmse: 28892.3047\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418421792.0000 - rmse: 20455.3613 - val_loss: 924808704.0000 - val_rmse: 30410.6680\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460443584.0000 - rmse: 21457.9492 - val_loss: 445862528.0000 - val_rmse: 21115.4570\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431530496.0000 - rmse: 20773.3125 - val_loss: 647949440.0000 - val_rmse: 25454.8516\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391218688.0000 - rmse: 19779.2480 - val_loss: 705693312.0000 - val_rmse: 26564.8887\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419413600.0000 - rmse: 20479.5898 - val_loss: 1325657984.0000 - val_rmse: 36409.5859\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407148192.0000 - rmse: 20177.9121 - val_loss: 744849536.0000 - val_rmse: 27291.9316\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489227680.0000 - rmse: 22118.4902 - val_loss: 575379904.0000 - val_rmse: 23987.0781\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480361696.0000 - rmse: 21917.1562 - val_loss: 555441664.0000 - val_rmse: 23567.8086\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388514912.0000 - rmse: 19710.7812 - val_loss: 446813696.0000 - val_rmse: 21137.9668\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413335968.0000 - rmse: 20330.6641 - val_loss: 515563136.0000 - val_rmse: 22706.0156\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449313088.0000 - rmse: 21197.0059 - val_loss: 609093504.0000 - val_rmse: 24679.8184\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437396512.0000 - rmse: 20914.0254 - val_loss: 547163968.0000 - val_rmse: 23391.5332\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425539040.0000 - rmse: 20628.5957 - val_loss: 503280448.0000 - val_rmse: 22433.9121\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409589664.0000 - rmse: 20238.3223 - val_loss: 545174400.0000 - val_rmse: 23348.9707\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402317856.0000 - rmse: 20057.8613 - val_loss: 511026272.0000 - val_rmse: 22605.8887\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432150368.0000 - rmse: 20788.2266 - val_loss: 520831648.0000 - val_rmse: 22821.7363\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415927072.0000 - rmse: 20394.2871 - val_loss: 501961088.0000 - val_rmse: 22404.4863\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422049728.0000 - rmse: 20543.8496 - val_loss: 483675680.0000 - val_rmse: 21992.6270\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426082912.0000 - rmse: 20641.7754 - val_loss: 1015191296.0000 - val_rmse: 31862.0664\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401314496.0000 - rmse: 20032.8340 - val_loss: 507466944.0000 - val_rmse: 22527.0273\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375041792.0000 - rmse: 19365.9941 - val_loss: 1284514048.0000 - val_rmse: 35840.1172\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404390240.0000 - rmse: 20109.4551 - val_loss: 459757472.0000 - val_rmse: 21441.9551\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367036256.0000 - rmse: 19158.1895 - val_loss: 464100736.0000 - val_rmse: 21542.9961\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389961632.0000 - rmse: 19747.4453 - val_loss: 668639040.0000 - val_rmse: 25858.0547\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388721312.0000 - rmse: 19716.0156 - val_loss: 954600704.0000 - val_rmse: 30896.6133\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439404864.0000 - rmse: 20961.9863 - val_loss: 735345216.0000 - val_rmse: 27117.2500\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397316832.0000 - rmse: 19932.8086 - val_loss: 469600896.0000 - val_rmse: 21670.2754\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430725472.0000 - rmse: 20753.9258 - val_loss: 628286144.0000 - val_rmse: 25065.6367\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387639936.0000 - rmse: 19688.5723 - val_loss: 533381088.0000 - val_rmse: 23095.0449\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414526720.0000 - rmse: 20359.9277 - val_loss: 456230432.0000 - val_rmse: 21359.5508\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390955456.0000 - rmse: 19772.5938 - val_loss: 503167936.0000 - val_rmse: 22431.4043\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437844480.0000 - rmse: 20924.7344 - val_loss: 705287232.0000 - val_rmse: 26557.2422\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382554336.0000 - rmse: 19558.9941 - val_loss: 525213952.0000 - val_rmse: 22917.5449\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373492736.0000 - rmse: 19325.9590 - val_loss: 553115264.0000 - val_rmse: 23518.4023\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379869376.0000 - rmse: 19490.2363 - val_loss: 439076032.0000 - val_rmse: 20954.1406\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367858912.0000 - rmse: 19179.6484 - val_loss: 480048160.0000 - val_rmse: 21910.0000\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359594656.0000 - rmse: 18962.9805 - val_loss: 464231296.0000 - val_rmse: 21546.0273\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375802592.0000 - rmse: 19385.6289 - val_loss: 520561216.0000 - val_rmse: 22815.8086\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395287008.0000 - rmse: 19881.8262 - val_loss: 696977920.0000 - val_rmse: 26400.3398\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340909440.0000 - rmse: 18463.7324 - val_loss: 648124992.0000 - val_rmse: 25458.2988\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383145344.0000 - rmse: 19574.0996 - val_loss: 676868224.0000 - val_rmse: 26016.6914\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366898784.0000 - rmse: 19154.5996 - val_loss: 740300160.0000 - val_rmse: 27208.4570\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392615904.0000 - rmse: 19814.5352 - val_loss: 437750720.0000 - val_rmse: 20922.4922\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308152704.0000 - rmse: 17554.2754 - val_loss: 379449280.0000 - val_rmse: 19479.4570\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359789888.0000 - rmse: 18968.1270 - val_loss: 531064608.0000 - val_rmse: 23044.8379\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391447168.0000 - rmse: 19785.0234 - val_loss: 634004352.0000 - val_rmse: 25179.4414\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333241952.0000 - rmse: 18254.9160 - val_loss: 535136608.0000 - val_rmse: 23133.0195\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337938816.0000 - rmse: 18383.1113 - val_loss: 765752768.0000 - val_rmse: 27672.2363\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315202080.0000 - rmse: 17753.9297 - val_loss: 565006912.0000 - val_rmse: 23769.8750\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376156576.0000 - rmse: 19394.7559 - val_loss: 501090912.0000 - val_rmse: 22385.0605\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421942112.0000 - rmse: 20541.2266 - val_loss: 550949504.0000 - val_rmse: 23472.3125\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322411008.0000 - rmse: 17955.8047 - val_loss: 405350368.0000 - val_rmse: 20133.3125\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328337952.0000 - rmse: 18120.0957 - val_loss: 528269568.0000 - val_rmse: 22984.1152\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386872800.0000 - rmse: 19669.0801 - val_loss: 603619008.0000 - val_rmse: 24568.6562\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317405056.0000 - rmse: 17815.8633 - val_loss: 688620800.0000 - val_rmse: 26241.5820\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311437120.0000 - rmse: 17647.5801 - val_loss: 784105856.0000 - val_rmse: 28001.8906\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304703296.0000 - rmse: 17455.7500 - val_loss: 680319744.0000 - val_rmse: 26082.9395\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341447488.0000 - rmse: 18478.2969 - val_loss: 499499200.0000 - val_rmse: 22349.4785\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332016064.0000 - rmse: 18221.3086 - val_loss: 574247680.0000 - val_rmse: 23963.4648\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343091168.0000 - rmse: 18522.7188 - val_loss: 565322112.0000 - val_rmse: 23776.5020\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336898080.0000 - rmse: 18354.7812 - val_loss: 578104960.0000 - val_rmse: 24043.8125\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289640832.0000 - rmse: 17018.8340 - val_loss: 530985344.0000 - val_rmse: 23043.1191\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292569856.0000 - rmse: 17104.6719 - val_loss: 591825216.0000 - val_rmse: 24327.4570\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328418400.0000 - rmse: 18122.3164 - val_loss: 522622176.0000 - val_rmse: 22860.9316\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327035200.0000 - rmse: 18084.1133 - val_loss: 529506304.0000 - val_rmse: 23011.0020\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324438496.0000 - rmse: 18012.1738 - val_loss: 839196480.0000 - val_rmse: 28968.8887\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354842112.0000 - rmse: 18837.2520 - val_loss: 550506496.0000 - val_rmse: 23462.8730\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309108032.0000 - rmse: 17581.4668 - val_loss: 732221952.0000 - val_rmse: 27059.5996\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282110144.0000 - rmse: 16796.1348 - val_loss: 914440512.0000 - val_rmse: 30239.7168\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282394144.0000 - rmse: 16804.5840 - val_loss: 544193344.0000 - val_rmse: 23327.9512\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276796000.0000 - rmse: 16637.1855 - val_loss: 689033408.0000 - val_rmse: 26249.4453\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312322496.0000 - rmse: 17672.6465 - val_loss: 586258496.0000 - val_rmse: 24212.7734\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313198944.0000 - rmse: 17697.4258 - val_loss: 576676544.0000 - val_rmse: 24014.0898\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331930880.0000 - rmse: 18218.9688 - val_loss: 863896896.0000 - val_rmse: 29392.1211\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312207168.0000 - rmse: 17669.3848 - val_loss: 404570944.0000 - val_rmse: 20113.9492\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299417376.0000 - rmse: 17303.6816 - val_loss: 571489472.0000 - val_rmse: 23905.8438\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281933312.0000 - rmse: 16790.8691 - val_loss: 350228224.0000 - val_rmse: 18714.3828\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266355952.0000 - rmse: 16320.4131 - val_loss: 350633024.0000 - val_rmse: 18725.1973\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253429072.0000 - rmse: 15919.4541 - val_loss: 435267744.0000 - val_rmse: 20863.0723\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313205504.0000 - rmse: 17697.6113 - val_loss: 386874368.0000 - val_rmse: 19669.1211\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282950432.0000 - rmse: 16821.1309 - val_loss: 394665440.0000 - val_rmse: 19866.1875\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316336576.0000 - rmse: 17785.8516 - val_loss: 420059040.0000 - val_rmse: 20495.3418\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269724480.0000 - rmse: 16423.2891 - val_loss: 750793728.0000 - val_rmse: 27400.6152\n",
      "Epoch 164/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271553888.0000 - rmse: 16478.8887 - val_loss: 390943008.0000 - val_rmse: 19772.2773\n",
      "Epoch 165/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294252896.0000 - rmse: 17153.7988 - val_loss: 431265600.0000 - val_rmse: 20766.9355\n",
      "Epoch 166/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252813520.0000 - rmse: 15900.1104 - val_loss: 422003008.0000 - val_rmse: 20542.7090\n",
      "Epoch 167/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288800416.0000 - rmse: 16994.1270 - val_loss: 338098304.0000 - val_rmse: 18387.4473\n",
      "Epoch 168/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290554496.0000 - rmse: 17045.6582 - val_loss: 516308512.0000 - val_rmse: 22722.4219\n",
      "Epoch 169/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279012544.0000 - rmse: 16703.6660 - val_loss: 510321888.0000 - val_rmse: 22590.3027\n",
      "Epoch 170/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240124624.0000 - rmse: 15495.9531 - val_loss: 480040800.0000 - val_rmse: 21909.8320\n",
      "Epoch 171/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267689568.0000 - rmse: 16361.2207 - val_loss: 597681088.0000 - val_rmse: 24447.5176\n",
      "Epoch 172/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234091728.0000 - rmse: 15300.0557 - val_loss: 660500480.0000 - val_rmse: 25700.2012\n",
      "Epoch 173/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270990944.0000 - rmse: 16461.8008 - val_loss: 491539072.0000 - val_rmse: 22170.6797\n",
      "Epoch 174/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258255072.0000 - rmse: 16070.3154 - val_loss: 534310304.0000 - val_rmse: 23115.1523\n",
      "Epoch 175/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260721936.0000 - rmse: 16146.8857 - val_loss: 753160896.0000 - val_rmse: 27443.7754\n",
      "Epoch 176/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239208624.0000 - rmse: 15466.3691 - val_loss: 661918848.0000 - val_rmse: 25727.7832\n",
      "Epoch 177/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236915904.0000 - rmse: 15392.0703 - val_loss: 470866592.0000 - val_rmse: 21699.4609\n",
      "Epoch 178/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238868480.0000 - rmse: 15455.3672 - val_loss: 628129664.0000 - val_rmse: 25062.5156\n",
      "Epoch 179/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253144432.0000 - rmse: 15910.5117 - val_loss: 378292832.0000 - val_rmse: 19449.7500\n",
      "Epoch 180/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278229920.0000 - rmse: 16680.2227 - val_loss: 536308576.0000 - val_rmse: 23158.3379\n",
      "Epoch 181/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240125216.0000 - rmse: 15495.9727 - val_loss: 512063200.0000 - val_rmse: 22628.8145\n",
      "Epoch 182/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286068032.0000 - rmse: 16913.5449 - val_loss: 524848480.0000 - val_rmse: 22909.5703\n",
      "Epoch 183/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271090528.0000 - rmse: 16464.8242 - val_loss: 456808576.0000 - val_rmse: 21373.0801\n",
      "Epoch 184/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248018000.0000 - rmse: 15748.5869 - val_loss: 413154272.0000 - val_rmse: 20326.1953\n",
      "Epoch 185/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277157472.0000 - rmse: 16648.0449 - val_loss: 604822528.0000 - val_rmse: 24593.1387\n",
      "Epoch 186/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244513216.0000 - rmse: 15636.9160 - val_loss: 445794624.0000 - val_rmse: 21113.8496\n",
      "Epoch 187/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226897728.0000 - rmse: 15063.1240 - val_loss: 514427296.0000 - val_rmse: 22680.9883\n",
      "Epoch 188/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271772768.0000 - rmse: 16485.5312 - val_loss: 506903424.0000 - val_rmse: 22514.5137\n",
      "Epoch 189/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234377456.0000 - rmse: 15309.3896 - val_loss: 373471008.0000 - val_rmse: 19325.3965\n",
      "Epoch 190/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 203529136.0000 - rmse: 14266.3623 - val_loss: 446686720.0000 - val_rmse: 21134.9648\n",
      "Epoch 191/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226642944.0000 - rmse: 15054.6611 - val_loss: 543758720.0000 - val_rmse: 23318.6328\n",
      "Epoch 192/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231017120.0000 - rmse: 15199.2451 - val_loss: 425415424.0000 - val_rmse: 20625.5996\n",
      "Epoch 193/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238014976.0000 - rmse: 15427.7314 - val_loss: 546088000.0000 - val_rmse: 23368.5254\n",
      "Epoch 194/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206662816.0000 - rmse: 14375.7705 - val_loss: 469779584.0000 - val_rmse: 21674.3984\n",
      "Epoch 195/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227180048.0000 - rmse: 15072.4912 - val_loss: 434174784.0000 - val_rmse: 20836.8613\n",
      "Epoch 196/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254714976.0000 - rmse: 15959.7891 - val_loss: 404620288.0000 - val_rmse: 20115.1738\n",
      "Epoch 197/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 194624976.0000 - rmse: 13950.8037 - val_loss: 396043744.0000 - val_rmse: 19900.8477\n",
      "Epoch 198/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 208020768.0000 - rmse: 14422.9229 - val_loss: 408747968.0000 - val_rmse: 20217.5156\n",
      "Epoch 199/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230283200.0000 - rmse: 15175.0840 - val_loss: 431562720.0000 - val_rmse: 20774.0879\n",
      "Epoch 200/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243750112.0000 - rmse: 15612.4951 - val_loss: 477520032.0000 - val_rmse: 21852.2305\n",
      "Epoch 201/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264954512.0000 - rmse: 16277.4219 - val_loss: 379616960.0000 - val_rmse: 19483.7598\n",
      "Epoch 202/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 213906832.0000 - rmse: 14625.5537 - val_loss: 427679648.0000 - val_rmse: 20680.4160\n",
      "Epoch 203/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 204125344.0000 - rmse: 14287.2422 - val_loss: 407635040.0000 - val_rmse: 20189.9727\n",
      "Epoch 204/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 188741104.0000 - rmse: 13738.3066 - val_loss: 719011648.0000 - val_rmse: 26814.3926\n",
      "Epoch 205/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242771520.0000 - rmse: 15581.1240 - val_loss: 427504480.0000 - val_rmse: 20676.1816\n",
      "Epoch 206/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 194038960.0000 - rmse: 13929.7852 - val_loss: 466259200.0000 - val_rmse: 21593.0352\n",
      "Epoch 207/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275055072.0000 - rmse: 16584.7852 - val_loss: 398539680.0000 - val_rmse: 19963.4570\n",
      "Epoch 208/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 213764288.0000 - rmse: 14620.6777 - val_loss: 427350560.0000 - val_rmse: 20672.4590\n",
      "Epoch 209/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242160624.0000 - rmse: 15561.5098 - val_loss: 497709280.0000 - val_rmse: 22309.3965\n",
      "Epoch 210/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206524704.0000 - rmse: 14370.9658 - val_loss: 428276992.0000 - val_rmse: 20694.8535\n",
      "Epoch 211/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 213333120.0000 - rmse: 14605.9268 - val_loss: 441861728.0000 - val_rmse: 21020.5059\n",
      "Epoch 212/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206153888.0000 - rmse: 14358.0586 - val_loss: 405239520.0000 - val_rmse: 20130.5625\n",
      "Epoch 213/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269936064.0000 - rmse: 16429.7285 - val_loss: 806934208.0000 - val_rmse: 28406.5879\n",
      "Epoch 214/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222583344.0000 - rmse: 14919.2246 - val_loss: 616951424.0000 - val_rmse: 24838.5039\n",
      "Epoch 215/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262802096.0000 - rmse: 16211.1699 - val_loss: 504715456.0000 - val_rmse: 22465.8730\n",
      "Epoch 216/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 198415440.0000 - rmse: 14086.0000 - val_loss: 445515456.0000 - val_rmse: 21107.2344\n",
      "Epoch 217/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 191169952.0000 - rmse: 13826.4209 - val_loss: 494381568.0000 - val_rmse: 22234.6914\n",
      "Epoch 218/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 193090992.0000 - rmse: 13895.7168 - val_loss: 476031968.0000 - val_rmse: 21818.1543\n",
      "Epoch 219/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215175360.0000 - rmse: 14668.8555 - val_loss: 456488672.0000 - val_rmse: 21365.5938\n",
      "Epoch 220/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 201480064.0000 - rmse: 14194.3643 - val_loss: 476825632.0000 - val_rmse: 21836.3359\n",
      "Epoch 221/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 200796704.0000 - rmse: 14170.2734 - val_loss: 414408704.0000 - val_rmse: 20357.0273\n",
      "Epoch 222/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 196132592.0000 - rmse: 14004.7324 - val_loss: 425936896.0000 - val_rmse: 20638.2383\n",
      "Epoch 223/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 198456880.0000 - rmse: 14087.4697 - val_loss: 380783104.0000 - val_rmse: 19513.6621\n",
      "Epoch 224/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 203184976.0000 - rmse: 14254.2949 - val_loss: 395050784.0000 - val_rmse: 19875.8828\n",
      "Epoch 225/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 225223952.0000 - rmse: 15007.4619 - val_loss: 412809184.0000 - val_rmse: 20317.7051\n",
      "Epoch 226/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210641568.0000 - rmse: 14513.4941 - val_loss: 433517760.0000 - val_rmse: 20821.0879\n",
      "Epoch 227/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 194584128.0000 - rmse: 13949.3408 - val_loss: 544077056.0000 - val_rmse: 23325.4570\n",
      "Epoch 228/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 160082016.0000 - rmse: 12652.3506 - val_loss: 567820480.0000 - val_rmse: 23828.9824\n",
      "Epoch 229/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202414432.0000 - rmse: 14227.2402 - val_loss: 443071552.0000 - val_rmse: 21049.2637\n",
      "Epoch 230/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 209444928.0000 - rmse: 14472.2119 - val_loss: 507130688.0000 - val_rmse: 22519.5625\n",
      "Epoch 231/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 194701440.0000 - rmse: 13953.5449 - val_loss: 406458528.0000 - val_rmse: 20160.8145\n",
      "Epoch 232/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 184416688.0000 - rmse: 13580.0088 - val_loss: 577918848.0000 - val_rmse: 24039.9414\n",
      "Epoch 233/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 194508928.0000 - rmse: 13946.6436 - val_loss: 538463552.0000 - val_rmse: 23204.8145\n",
      "Epoch 234/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202512352.0000 - rmse: 14230.6816 - val_loss: 399849632.0000 - val_rmse: 19996.2383\n",
      "Epoch 235/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 223795024.0000 - rmse: 14959.7773 - val_loss: 397422496.0000 - val_rmse: 19935.4590\n",
      "Epoch 236/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216586112.0000 - rmse: 14716.8633 - val_loss: 407833824.0000 - val_rmse: 20194.8945\n",
      "Epoch 237/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218159904.0000 - rmse: 14770.2354 - val_loss: 584587712.0000 - val_rmse: 24178.2480\n",
      "Epoch 238/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 183157376.0000 - rmse: 13533.5635 - val_loss: 671965184.0000 - val_rmse: 25922.2910\n",
      "Epoch 239/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206855600.0000 - rmse: 14382.4746 - val_loss: 412321376.0000 - val_rmse: 20305.6973\n",
      "Epoch 240/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 191881280.0000 - rmse: 13852.1191 - val_loss: 462153568.0000 - val_rmse: 21497.7539\n",
      "Epoch 241/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 200718464.0000 - rmse: 14167.5127 - val_loss: 495878112.0000 - val_rmse: 22268.3203\n",
      "Epoch 242/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 194047040.0000 - rmse: 13930.0742 - val_loss: 728550976.0000 - val_rmse: 26991.6816\n",
      "Epoch 243/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207653872.0000 - rmse: 14410.2002 - val_loss: 576308736.0000 - val_rmse: 24006.4316\n",
      "Epoch 244/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 190466448.0000 - rmse: 13800.9561 - val_loss: 417677920.0000 - val_rmse: 20437.1699\n",
      "Epoch 245/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238933120.0000 - rmse: 15457.4609 - val_loss: 604489088.0000 - val_rmse: 24586.3594\n",
      "Epoch 246/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 209202944.0000 - rmse: 14463.8486 - val_loss: 462062976.0000 - val_rmse: 21495.6484\n",
      "Epoch 247/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 191674624.0000 - rmse: 13844.6592 - val_loss: 570453952.0000 - val_rmse: 23884.1777\n",
      "Epoch 248/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 178610464.0000 - rmse: 13364.5186 - val_loss: 456669152.0000 - val_rmse: 21369.8184\n",
      "Epoch 249/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215515696.0000 - rmse: 14680.4521 - val_loss: 597377984.0000 - val_rmse: 24441.3164\n",
      "Epoch 250/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 165999744.0000 - rmse: 12884.0859 - val_loss: 450152896.0000 - val_rmse: 21216.8047\n",
      "Epoch 251/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 182254800.0000 - rmse: 13500.1758 - val_loss: 435546176.0000 - val_rmse: 20869.7422\n",
      "Epoch 252/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 197678960.0000 - rmse: 14059.8320 - val_loss: 411776384.0000 - val_rmse: 20292.2734\n",
      "Epoch 253/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 197391744.0000 - rmse: 14049.6152 - val_loss: 544441536.0000 - val_rmse: 23333.2695\n",
      "104/104 [==============================] - 0s 676us/step - loss: 528321888.0000 - rmse: 22985.2539\n",
      "[528321888.0, 22985.25390625]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 13007684608.0000 - rmse: 114051.2344 - val_loss: 2089609600.0000 - val_rmse: 45712.2461\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2162889984.0000 - rmse: 46506.8828 - val_loss: 1388936704.0000 - val_rmse: 37268.4414\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1829827584.0000 - rmse: 42776.4844 - val_loss: 1168208640.0000 - val_rmse: 34179.0664\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1660352896.0000 - rmse: 40747.4297 - val_loss: 1065555328.0000 - val_rmse: 32642.8457\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1547164160.0000 - rmse: 39334.0078 - val_loss: 981228480.0000 - val_rmse: 31324.5664\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1457917568.0000 - rmse: 38182.6836 - val_loss: 932870656.0000 - val_rmse: 30542.9316\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1356087936.0000 - rmse: 36825.0977 - val_loss: 920851712.0000 - val_rmse: 30345.5391\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1310902912.0000 - rmse: 36206.3945 - val_loss: 896584512.0000 - val_rmse: 29943.0215\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1306032768.0000 - rmse: 36139.0742 - val_loss: 848772800.0000 - val_rmse: 29133.7051\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1243418880.0000 - rmse: 35262.1445 - val_loss: 849001408.0000 - val_rmse: 29137.6289\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1239999104.0000 - rmse: 35213.6211 - val_loss: 850770944.0000 - val_rmse: 29167.9785\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1198053760.0000 - rmse: 34612.9141 - val_loss: 1134552064.0000 - val_rmse: 33683.1133\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1191025920.0000 - rmse: 34511.2422 - val_loss: 910680128.0000 - val_rmse: 30177.4766\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1172491648.0000 - rmse: 34241.6641 - val_loss: 874517632.0000 - val_rmse: 29572.2441\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1161712000.0000 - rmse: 34083.8984 - val_loss: 872154944.0000 - val_rmse: 29532.2695\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1100032640.0000 - rmse: 33166.7383 - val_loss: 876789632.0000 - val_rmse: 29610.6328\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1108043648.0000 - rmse: 33287.2891 - val_loss: 855612736.0000 - val_rmse: 29250.8594\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1046451712.0000 - rmse: 32348.9043 - val_loss: 869743360.0000 - val_rmse: 29491.4121\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1126535936.0000 - rmse: 33563.9102 - val_loss: 896546816.0000 - val_rmse: 29942.3926\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1118052608.0000 - rmse: 33437.2930 - val_loss: 917414656.0000 - val_rmse: 30288.8535\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1071467264.0000 - rmse: 32733.2754 - val_loss: 1127792128.0000 - val_rmse: 33582.6172\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1082184576.0000 - rmse: 32896.5703 - val_loss: 942592832.0000 - val_rmse: 30701.6738\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1051050752.0000 - rmse: 32419.9121 - val_loss: 904830656.0000 - val_rmse: 30080.4004\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1045632640.0000 - rmse: 32336.2441 - val_loss: 875476352.0000 - val_rmse: 29588.4492\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 982060160.0000 - rmse: 31337.8398 - val_loss: 918234560.0000 - val_rmse: 30302.3828\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1001557504.0000 - rmse: 31647.3926 - val_loss: 935780544.0000 - val_rmse: 30590.5312\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 972258880.0000 - rmse: 31181.0664 - val_loss: 895725824.0000 - val_rmse: 29928.6777\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 956910912.0000 - rmse: 30933.9766 - val_loss: 902572736.0000 - val_rmse: 30042.8477\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 932431488.0000 - rmse: 30535.7402 - val_loss: 919748288.0000 - val_rmse: 30327.3516\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 944651712.0000 - rmse: 30735.1875 - val_loss: 864587648.0000 - val_rmse: 29403.8711\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 878989696.0000 - rmse: 29647.7578 - val_loss: 861797568.0000 - val_rmse: 29356.3867\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 922534400.0000 - rmse: 30373.2520 - val_loss: 823642176.0000 - val_rmse: 28699.1660\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 881126336.0000 - rmse: 29683.7715 - val_loss: 890665984.0000 - val_rmse: 29844.0273\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 849098560.0000 - rmse: 29139.2949 - val_loss: 901253824.0000 - val_rmse: 30020.8906\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 787493248.0000 - rmse: 28062.3105 - val_loss: 929313536.0000 - val_rmse: 30484.6445\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 819810176.0000 - rmse: 28632.3262 - val_loss: 847485888.0000 - val_rmse: 29111.6094\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 726264128.0000 - rmse: 26949.2871 - val_loss: 947819968.0000 - val_rmse: 30786.6836\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 771190080.0000 - rmse: 27770.3086 - val_loss: 777741312.0000 - val_rmse: 27888.0137\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729486208.0000 - rmse: 27009.0020 - val_loss: 840792640.0000 - val_rmse: 28996.4238\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 768400576.0000 - rmse: 27720.0391 - val_loss: 817879296.0000 - val_rmse: 28598.5898\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 699845184.0000 - rmse: 26454.5879 - val_loss: 1065225664.0000 - val_rmse: 32637.7949\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 767472832.0000 - rmse: 27703.2988 - val_loss: 697856896.0000 - val_rmse: 26416.9805\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 669307200.0000 - rmse: 25870.9707 - val_loss: 796855360.0000 - val_rmse: 28228.6270\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 669083008.0000 - rmse: 25866.6387 - val_loss: 633915264.0000 - val_rmse: 25177.6719\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 700243072.0000 - rmse: 26462.1055 - val_loss: 646738944.0000 - val_rmse: 25431.0605\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621359360.0000 - rmse: 24927.0801 - val_loss: 661757632.0000 - val_rmse: 25724.6504\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598216000.0000 - rmse: 24458.4551 - val_loss: 737018368.0000 - val_rmse: 27148.0820\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614777472.0000 - rmse: 24794.7070 - val_loss: 667196928.0000 - val_rmse: 25830.1562\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551278016.0000 - rmse: 23479.3105 - val_loss: 723473088.0000 - val_rmse: 26897.4551\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566171584.0000 - rmse: 23794.3594 - val_loss: 764676224.0000 - val_rmse: 27652.7793\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502762176.0000 - rmse: 22422.3594 - val_loss: 699979328.0000 - val_rmse: 26457.1211\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590253568.0000 - rmse: 24295.1348 - val_loss: 652177216.0000 - val_rmse: 25537.7598\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523075328.0000 - rmse: 22870.8398 - val_loss: 607584768.0000 - val_rmse: 24649.2324\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486881920.0000 - rmse: 22065.4004 - val_loss: 1089479296.0000 - val_rmse: 33007.2617\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545979776.0000 - rmse: 23366.2109 - val_loss: 652958208.0000 - val_rmse: 25553.0469\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477091456.0000 - rmse: 21842.4238 - val_loss: 1503738112.0000 - val_rmse: 38778.0625\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515382752.0000 - rmse: 22702.0430 - val_loss: 554933376.0000 - val_rmse: 23557.0234\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496236768.0000 - rmse: 22276.3730 - val_loss: 688767872.0000 - val_rmse: 26244.3867\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468279200.0000 - rmse: 21639.7598 - val_loss: 615189248.0000 - val_rmse: 24803.0078\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433250112.0000 - rmse: 20814.6602 - val_loss: 548568576.0000 - val_rmse: 23421.5410\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448944896.0000 - rmse: 21188.3203 - val_loss: 635567616.0000 - val_rmse: 25210.4668\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436397760.0000 - rmse: 20890.1348 - val_loss: 986640832.0000 - val_rmse: 31410.8359\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449581792.0000 - rmse: 21203.3438 - val_loss: 753619264.0000 - val_rmse: 27452.1250\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477205248.0000 - rmse: 21845.0273 - val_loss: 901364800.0000 - val_rmse: 30022.7383\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451548064.0000 - rmse: 21249.6602 - val_loss: 803084864.0000 - val_rmse: 28338.7500\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402188000.0000 - rmse: 20054.6230 - val_loss: 709451328.0000 - val_rmse: 26635.5273\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445638048.0000 - rmse: 21110.1387 - val_loss: 824213184.0000 - val_rmse: 28709.1113\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437396448.0000 - rmse: 20914.0234 - val_loss: 718665536.0000 - val_rmse: 26807.9375\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380987936.0000 - rmse: 19518.9121 - val_loss: 1179690496.0000 - val_rmse: 34346.6211\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367303808.0000 - rmse: 19165.1699 - val_loss: 794496064.0000 - val_rmse: 28186.8066\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435772864.0000 - rmse: 20875.1719 - val_loss: 540660672.0000 - val_rmse: 23252.1113\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363639904.0000 - rmse: 19069.3438 - val_loss: 586843648.0000 - val_rmse: 24224.8555\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390313632.0000 - rmse: 19756.3535 - val_loss: 1207951616.0000 - val_rmse: 34755.5977\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357529984.0000 - rmse: 18908.4629 - val_loss: 946766848.0000 - val_rmse: 30769.5762\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341787552.0000 - rmse: 18487.4961 - val_loss: 764772864.0000 - val_rmse: 27654.5254\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431066272.0000 - rmse: 20762.1348 - val_loss: 763482816.0000 - val_rmse: 27631.1934\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344995904.0000 - rmse: 18574.0645 - val_loss: 1897201408.0000 - val_rmse: 43556.8711\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344973952.0000 - rmse: 18573.4746 - val_loss: 1143591424.0000 - val_rmse: 33817.0273\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364359488.0000 - rmse: 19088.2012 - val_loss: 758179456.0000 - val_rmse: 27535.0586\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394517504.0000 - rmse: 19862.4648 - val_loss: 931859328.0000 - val_rmse: 30526.3691\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352286016.0000 - rmse: 18769.2812 - val_loss: 733772672.0000 - val_rmse: 27088.2383\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386339648.0000 - rmse: 19655.5234 - val_loss: 1308056448.0000 - val_rmse: 36167.0625\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397597120.0000 - rmse: 19939.8379 - val_loss: 1467074048.0000 - val_rmse: 38302.4023\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357631872.0000 - rmse: 18911.1543 - val_loss: 787231168.0000 - val_rmse: 28057.6387\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342004544.0000 - rmse: 18493.3652 - val_loss: 868364288.0000 - val_rmse: 29468.0195\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331077152.0000 - rmse: 18195.5254 - val_loss: 813929152.0000 - val_rmse: 28529.4434\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318582240.0000 - rmse: 17848.8711 - val_loss: 1012457984.0000 - val_rmse: 31819.1445\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295974624.0000 - rmse: 17203.9121 - val_loss: 1007253120.0000 - val_rmse: 31737.2520\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358755424.0000 - rmse: 18940.8398 - val_loss: 1331723520.0000 - val_rmse: 36492.7891\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325847040.0000 - rmse: 18051.2324 - val_loss: 1153546368.0000 - val_rmse: 33963.8984\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291183840.0000 - rmse: 17064.1094 - val_loss: 1113269376.0000 - val_rmse: 33365.6914\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310487680.0000 - rmse: 17620.6602 - val_loss: 1055698048.0000 - val_rmse: 32491.5078\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272697568.0000 - rmse: 16513.5566 - val_loss: 761214336.0000 - val_rmse: 27590.1113\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305203200.0000 - rmse: 17470.0664 - val_loss: 742974720.0000 - val_rmse: 27257.5605\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339655040.0000 - rmse: 18429.7305 - val_loss: 741255936.0000 - val_rmse: 27226.0137\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321483712.0000 - rmse: 17929.9648 - val_loss: 851920896.0000 - val_rmse: 29187.6836\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289220768.0000 - rmse: 17006.4902 - val_loss: 894048192.0000 - val_rmse: 29900.6387\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308105600.0000 - rmse: 17552.9336 - val_loss: 704046144.0000 - val_rmse: 26533.8652\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276565216.0000 - rmse: 16630.2500 - val_loss: 1045489600.0000 - val_rmse: 32334.0312\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327278752.0000 - rmse: 18090.8438 - val_loss: 862467776.0000 - val_rmse: 29367.8008\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300160064.0000 - rmse: 17325.1270 - val_loss: 1095125120.0000 - val_rmse: 33092.6758\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272302272.0000 - rmse: 16501.5820 - val_loss: 1030577984.0000 - val_rmse: 32102.6172\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266239264.0000 - rmse: 16316.8389 - val_loss: 1576807680.0000 - val_rmse: 39709.0391\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264260880.0000 - rmse: 16256.1016 - val_loss: 836579776.0000 - val_rmse: 28923.6875\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293822720.0000 - rmse: 17141.2578 - val_loss: 1472674304.0000 - val_rmse: 38375.4375\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244735296.0000 - rmse: 15644.0166 - val_loss: 1353032832.0000 - val_rmse: 36783.5898\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249941440.0000 - rmse: 15809.5352 - val_loss: 1013108480.0000 - val_rmse: 31829.3652\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244159648.0000 - rmse: 15625.6084 - val_loss: 1447592448.0000 - val_rmse: 38047.2383\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346515520.0000 - rmse: 18614.9258 - val_loss: 953010048.0000 - val_rmse: 30870.8574\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298432320.0000 - rmse: 17275.1934 - val_loss: 893378880.0000 - val_rmse: 29889.4434\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242839696.0000 - rmse: 15583.3145 - val_loss: 831484608.0000 - val_rmse: 28835.4746\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300223680.0000 - rmse: 17326.9648 - val_loss: 647120128.0000 - val_rmse: 25438.5547\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240723856.0000 - rmse: 15515.2773 - val_loss: 941666880.0000 - val_rmse: 30686.5918\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258356416.0000 - rmse: 16073.4678 - val_loss: 1108928000.0000 - val_rmse: 33300.5703\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285551104.0000 - rmse: 16898.2559 - val_loss: 809312064.0000 - val_rmse: 28448.4102\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248479104.0000 - rmse: 15763.2188 - val_loss: 801659456.0000 - val_rmse: 28313.5898\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242925536.0000 - rmse: 15586.0674 - val_loss: 732531840.0000 - val_rmse: 27065.3262\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300264128.0000 - rmse: 17328.1309 - val_loss: 955201792.0000 - val_rmse: 30906.3379\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231857872.0000 - rmse: 15226.8779 - val_loss: 1207458688.0000 - val_rmse: 34748.5078\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258901056.0000 - rmse: 16090.4004 - val_loss: 986402560.0000 - val_rmse: 31407.0469\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238337664.0000 - rmse: 15438.1875 - val_loss: 1475818368.0000 - val_rmse: 38416.3828\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256689888.0000 - rmse: 16021.5439 - val_loss: 993787136.0000 - val_rmse: 31524.3887\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226447072.0000 - rmse: 15048.1582 - val_loss: 1473653376.0000 - val_rmse: 38388.1914\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224313056.0000 - rmse: 14977.0830 - val_loss: 1351171456.0000 - val_rmse: 36758.2812\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271314112.0000 - rmse: 16471.6152 - val_loss: 1199982336.0000 - val_rmse: 34640.7617\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264249392.0000 - rmse: 16255.7480 - val_loss: 730476672.0000 - val_rmse: 27027.3320\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244773520.0000 - rmse: 15645.2383 - val_loss: 840856576.0000 - val_rmse: 28997.5254\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 225700976.0000 - rmse: 15023.3457 - val_loss: 1029873216.0000 - val_rmse: 32091.6367\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249280960.0000 - rmse: 15788.6338 - val_loss: 868322496.0000 - val_rmse: 29467.3125\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232752800.0000 - rmse: 15256.2363 - val_loss: 1160447360.0000 - val_rmse: 34065.3398\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261786880.0000 - rmse: 16179.8281 - val_loss: 989506880.0000 - val_rmse: 31456.4277\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249391536.0000 - rmse: 15792.1328 - val_loss: 785127104.0000 - val_rmse: 28020.1191\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 200378160.0000 - rmse: 14155.4980 - val_loss: 839899904.0000 - val_rmse: 28981.0234\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236205392.0000 - rmse: 15368.9746 - val_loss: 582516928.0000 - val_rmse: 24135.3867\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264607648.0000 - rmse: 16266.7646 - val_loss: 1053841856.0000 - val_rmse: 32462.9297\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229666816.0000 - rmse: 15154.7607 - val_loss: 886746880.0000 - val_rmse: 29778.2949\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238399824.0000 - rmse: 15440.2002 - val_loss: 1424643328.0000 - val_rmse: 37744.4492\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260094112.0000 - rmse: 16127.4326 - val_loss: 979409792.0000 - val_rmse: 31295.5215\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242823024.0000 - rmse: 15582.7793 - val_loss: 712989504.0000 - val_rmse: 26701.8633\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241849616.0000 - rmse: 15551.5127 - val_loss: 1073172608.0000 - val_rmse: 32759.3125\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 204091088.0000 - rmse: 14286.0439 - val_loss: 1472272256.0000 - val_rmse: 38370.1992\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340938976.0000 - rmse: 18464.5312 - val_loss: 1597697920.0000 - val_rmse: 39971.2148\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242166896.0000 - rmse: 15561.7119 - val_loss: 707973120.0000 - val_rmse: 26607.7637\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207282448.0000 - rmse: 14397.3037 - val_loss: 968044800.0000 - val_rmse: 31113.4141\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281409152.0000 - rmse: 16775.2539 - val_loss: 844212800.0000 - val_rmse: 29055.3379\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256964384.0000 - rmse: 16030.1074 - val_loss: 728922432.0000 - val_rmse: 26998.5645\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206434736.0000 - rmse: 14367.8350 - val_loss: 934871488.0000 - val_rmse: 30575.6680\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216889712.0000 - rmse: 14727.1729 - val_loss: 907505280.0000 - val_rmse: 30124.8281\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216894704.0000 - rmse: 14727.3447 - val_loss: 706298560.0000 - val_rmse: 26576.2773\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227391008.0000 - rmse: 15079.4883 - val_loss: 804854272.0000 - val_rmse: 28369.9531\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215901312.0000 - rmse: 14693.5811 - val_loss: 1075244672.0000 - val_rmse: 32790.9219\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248215552.0000 - rmse: 15754.8564 - val_loss: 738736448.0000 - val_rmse: 27179.7070\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 212460848.0000 - rmse: 14576.0352 - val_loss: 940763456.0000 - val_rmse: 30671.8672\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 212746768.0000 - rmse: 14585.8398 - val_loss: 825238976.0000 - val_rmse: 28726.9727\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221748560.0000 - rmse: 14891.2227 - val_loss: 686943488.0000 - val_rmse: 26209.6074\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231995488.0000 - rmse: 15231.3965 - val_loss: 768968384.0000 - val_rmse: 27730.2793\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240119488.0000 - rmse: 15495.7881 - val_loss: 2433015808.0000 - val_rmse: 49325.6094\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236639408.0000 - rmse: 15383.0869 - val_loss: 606622976.0000 - val_rmse: 24629.7168\n",
      "104/104 [==============================] - 0s 651us/step - loss: 460485696.0000 - rmse: 21458.9297\n",
      "[460485696.0, 21458.9296875]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 12150863872.0000 - rmse: 110230.9531 - val_loss: 1848599936.0000 - val_rmse: 42995.3477\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2145832064.0000 - rmse: 46323.1250 - val_loss: 1492240512.0000 - val_rmse: 38629.5273\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1828584832.0000 - rmse: 42761.9570 - val_loss: 1114320640.0000 - val_rmse: 33381.4414\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1655536896.0000 - rmse: 40688.2891 - val_loss: 1012331584.0000 - val_rmse: 31817.1582\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1539934976.0000 - rmse: 39242.0039 - val_loss: 969356672.0000 - val_rmse: 31134.4941\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1409363712.0000 - rmse: 37541.4922 - val_loss: 945818816.0000 - val_rmse: 30754.1680\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1377815808.0000 - rmse: 37118.9414 - val_loss: 1013508544.0000 - val_rmse: 31835.6484\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1367680128.0000 - rmse: 36982.1602 - val_loss: 894796224.0000 - val_rmse: 29913.1445\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1287222016.0000 - rmse: 35877.8750 - val_loss: 941043456.0000 - val_rmse: 30676.4316\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1234568704.0000 - rmse: 35136.4297 - val_loss: 974118208.0000 - val_rmse: 31210.8672\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1210421504.0000 - rmse: 34791.1133 - val_loss: 965748544.0000 - val_rmse: 31076.4941\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1208606208.0000 - rmse: 34765.0156 - val_loss: 889934144.0000 - val_rmse: 29831.7637\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1154151552.0000 - rmse: 33972.8047 - val_loss: 849289472.0000 - val_rmse: 29142.5723\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1101240448.0000 - rmse: 33184.9414 - val_loss: 837270528.0000 - val_rmse: 28935.6270\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1078902656.0000 - rmse: 32846.6523 - val_loss: 832057280.0000 - val_rmse: 28845.4023\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1039166656.0000 - rmse: 32236.1074 - val_loss: 826065280.0000 - val_rmse: 28741.3516\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1026841024.0000 - rmse: 32044.3594 - val_loss: 814553024.0000 - val_rmse: 28540.3750\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1009014080.0000 - rmse: 31764.9824 - val_loss: 814300416.0000 - val_rmse: 28535.9492\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 986292352.0000 - rmse: 31405.2910 - val_loss: 811700992.0000 - val_rmse: 28490.3672\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 888703680.0000 - rmse: 29811.1328 - val_loss: 808361984.0000 - val_rmse: 28431.7070\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 885986880.0000 - rmse: 29765.5312 - val_loss: 862940992.0000 - val_rmse: 29375.8574\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 886424960.0000 - rmse: 29772.8906 - val_loss: 804133504.0000 - val_rmse: 28357.2480\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 841774720.0000 - rmse: 29013.3535 - val_loss: 820954240.0000 - val_rmse: 28652.2988\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 818501184.0000 - rmse: 28609.4590 - val_loss: 816113472.0000 - val_rmse: 28567.6992\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 908922816.0000 - rmse: 30148.3477 - val_loss: 737169088.0000 - val_rmse: 27150.8574\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 810809536.0000 - rmse: 28474.7168 - val_loss: 750120512.0000 - val_rmse: 27388.3281\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 810863680.0000 - rmse: 28475.6680 - val_loss: 735076352.0000 - val_rmse: 27112.2910\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751034112.0000 - rmse: 27405.0020 - val_loss: 923576384.0000 - val_rmse: 30390.3965\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 766939648.0000 - rmse: 27693.6758 - val_loss: 683159680.0000 - val_rmse: 26137.3242\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718975936.0000 - rmse: 26813.7246 - val_loss: 678588608.0000 - val_rmse: 26049.7324\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691533632.0000 - rmse: 26297.0273 - val_loss: 1025159872.0000 - val_rmse: 32018.1172\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 678975232.0000 - rmse: 26057.1523 - val_loss: 700656832.0000 - val_rmse: 26469.9238\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 662947968.0000 - rmse: 25747.7754 - val_loss: 668508288.0000 - val_rmse: 25855.5273\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685775744.0000 - rmse: 26187.3203 - val_loss: 697132608.0000 - val_rmse: 26403.2695\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 671063616.0000 - rmse: 25904.8945 - val_loss: 819232512.0000 - val_rmse: 28622.2383\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 693973376.0000 - rmse: 26343.3750 - val_loss: 684692416.0000 - val_rmse: 26166.6250\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623677632.0000 - rmse: 24973.5371 - val_loss: 675791936.0000 - val_rmse: 25995.9980\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607138112.0000 - rmse: 24640.1719 - val_loss: 550993536.0000 - val_rmse: 23473.2520\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596683712.0000 - rmse: 24427.1094 - val_loss: 753355264.0000 - val_rmse: 27447.3164\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584092416.0000 - rmse: 24168.0039 - val_loss: 774591744.0000 - val_rmse: 27831.4883\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605218048.0000 - rmse: 24601.1797 - val_loss: 790000448.0000 - val_rmse: 28106.9473\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546596992.0000 - rmse: 23379.4141 - val_loss: 573124928.0000 - val_rmse: 23940.0273\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557564224.0000 - rmse: 23612.7988 - val_loss: 609764288.0000 - val_rmse: 24693.4043\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 603324928.0000 - rmse: 24562.6738 - val_loss: 597207104.0000 - val_rmse: 24437.8203\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508554624.0000 - rmse: 22551.1562 - val_loss: 793167104.0000 - val_rmse: 28163.2227\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546618688.0000 - rmse: 23379.8770 - val_loss: 715983808.0000 - val_rmse: 26757.8730\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500122496.0000 - rmse: 22363.4180 - val_loss: 707159040.0000 - val_rmse: 26592.4629\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515802048.0000 - rmse: 22711.2754 - val_loss: 728622848.0000 - val_rmse: 26993.0156\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539205760.0000 - rmse: 23220.8047 - val_loss: 792937024.0000 - val_rmse: 28159.1367\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471639392.0000 - rmse: 21717.2598 - val_loss: 755211136.0000 - val_rmse: 27481.1055\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497321216.0000 - rmse: 22300.6992 - val_loss: 636288960.0000 - val_rmse: 25224.7695\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478938336.0000 - rmse: 21884.6582 - val_loss: 761726080.0000 - val_rmse: 27599.3848\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473489600.0000 - rmse: 21759.8164 - val_loss: 627510912.0000 - val_rmse: 25050.1680\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461151456.0000 - rmse: 21474.4375 - val_loss: 887236608.0000 - val_rmse: 29786.5176\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447680832.0000 - rmse: 21158.4688 - val_loss: 729379584.0000 - val_rmse: 27007.0293\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467491360.0000 - rmse: 21621.5469 - val_loss: 687294272.0000 - val_rmse: 26216.2969\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387715488.0000 - rmse: 19690.4922 - val_loss: 771436352.0000 - val_rmse: 27774.7422\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442906112.0000 - rmse: 21045.3340 - val_loss: 655723712.0000 - val_rmse: 25607.1035\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475877856.0000 - rmse: 21814.6250 - val_loss: 799526912.0000 - val_rmse: 28275.9062\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434805024.0000 - rmse: 20851.9785 - val_loss: 726932672.0000 - val_rmse: 26961.6875\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422051744.0000 - rmse: 20543.8965 - val_loss: 684384448.0000 - val_rmse: 26160.7422\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429556992.0000 - rmse: 20725.7559 - val_loss: 699968000.0000 - val_rmse: 26456.9082\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357577536.0000 - rmse: 18909.7207 - val_loss: 647740224.0000 - val_rmse: 25450.7402\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399393440.0000 - rmse: 19984.8301 - val_loss: 810445248.0000 - val_rmse: 28468.3203\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387467296.0000 - rmse: 19684.1875 - val_loss: 886257920.0000 - val_rmse: 29770.0840\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352435552.0000 - rmse: 18773.2656 - val_loss: 724550464.0000 - val_rmse: 26917.4746\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365545344.0000 - rmse: 19119.2402 - val_loss: 771177408.0000 - val_rmse: 27770.0820\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392613344.0000 - rmse: 19814.4707 - val_loss: 707141312.0000 - val_rmse: 26592.1270\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313980896.0000 - rmse: 17719.5059 - val_loss: 634785024.0000 - val_rmse: 25194.9414\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372312672.0000 - rmse: 19295.4062 - val_loss: 681741888.0000 - val_rmse: 26110.1875\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354758272.0000 - rmse: 18835.0273 - val_loss: 764258688.0000 - val_rmse: 27645.2285\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450845120.0000 - rmse: 21233.1133 - val_loss: 697490560.0000 - val_rmse: 26410.0449\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424344480.0000 - rmse: 20599.6211 - val_loss: 731722880.0000 - val_rmse: 27050.3770\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369272736.0000 - rmse: 19216.4688 - val_loss: 746121984.0000 - val_rmse: 27315.2344\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349018528.0000 - rmse: 18682.0371 - val_loss: 790391616.0000 - val_rmse: 28113.9043\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324570432.0000 - rmse: 18015.8379 - val_loss: 731329216.0000 - val_rmse: 27043.0996\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311958336.0000 - rmse: 17662.3418 - val_loss: 986048064.0000 - val_rmse: 31401.4023\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365977824.0000 - rmse: 19130.5449 - val_loss: 783551744.0000 - val_rmse: 27991.9941\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318805376.0000 - rmse: 17855.1191 - val_loss: 846307200.0000 - val_rmse: 29091.3594\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360420352.0000 - rmse: 18984.7383 - val_loss: 817609280.0000 - val_rmse: 28593.8652\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355944896.0000 - rmse: 18866.5020 - val_loss: 720477056.0000 - val_rmse: 26841.7031\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367679072.0000 - rmse: 19174.9590 - val_loss: 851646144.0000 - val_rmse: 29182.9766\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366401248.0000 - rmse: 19141.6113 - val_loss: 890893568.0000 - val_rmse: 29847.8398\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356933696.0000 - rmse: 18892.6875 - val_loss: 685130304.0000 - val_rmse: 26174.9941\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319156000.0000 - rmse: 17864.9355 - val_loss: 822327040.0000 - val_rmse: 28676.2461\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302350912.0000 - rmse: 17388.2402 - val_loss: 899303424.0000 - val_rmse: 29988.3887\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339477184.0000 - rmse: 18424.9062 - val_loss: 770797376.0000 - val_rmse: 27763.2383\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306526048.0000 - rmse: 17507.8848 - val_loss: 920540608.0000 - val_rmse: 30340.4121\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330964576.0000 - rmse: 18192.4316 - val_loss: 669693568.0000 - val_rmse: 25878.4375\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339743808.0000 - rmse: 18432.1406 - val_loss: 745483648.0000 - val_rmse: 27303.5469\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306241792.0000 - rmse: 17499.7637 - val_loss: 902686720.0000 - val_rmse: 30044.7461\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253204144.0000 - rmse: 15912.3877 - val_loss: 752260864.0000 - val_rmse: 27427.3750\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291256416.0000 - rmse: 17066.2363 - val_loss: 934095424.0000 - val_rmse: 30562.9727\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286505056.0000 - rmse: 16926.4609 - val_loss: 801969088.0000 - val_rmse: 28319.0586\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301098016.0000 - rmse: 17352.1738 - val_loss: 873181632.0000 - val_rmse: 29549.6465\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314933728.0000 - rmse: 17746.3730 - val_loss: 961009408.0000 - val_rmse: 31000.1484\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292868896.0000 - rmse: 17113.4121 - val_loss: 1020686912.0000 - val_rmse: 31948.1895\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262575424.0000 - rmse: 16204.1768 - val_loss: 982749440.0000 - val_rmse: 31348.8340\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272497664.0000 - rmse: 16507.5020 - val_loss: 1096142208.0000 - val_rmse: 33108.0391\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303362432.0000 - rmse: 17417.3008 - val_loss: 750444288.0000 - val_rmse: 27394.2363\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240824880.0000 - rmse: 15518.5322 - val_loss: 744988736.0000 - val_rmse: 27294.4824\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282142880.0000 - rmse: 16797.1094 - val_loss: 1226853120.0000 - val_rmse: 35026.4609\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296402656.0000 - rmse: 17216.3477 - val_loss: 865636096.0000 - val_rmse: 29421.6934\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286883424.0000 - rmse: 16937.6309 - val_loss: 802003456.0000 - val_rmse: 28319.6660\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273679552.0000 - rmse: 16543.2617 - val_loss: 1119211904.0000 - val_rmse: 33454.6250\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277678944.0000 - rmse: 16663.6992 - val_loss: 789520640.0000 - val_rmse: 28098.4102\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300592704.0000 - rmse: 17337.6074 - val_loss: 834807680.0000 - val_rmse: 28893.0371\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257115888.0000 - rmse: 16034.8330 - val_loss: 815694208.0000 - val_rmse: 28560.3613\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252812192.0000 - rmse: 15900.0674 - val_loss: 1079074944.0000 - val_rmse: 32849.2773\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262898848.0000 - rmse: 16214.1543 - val_loss: 896637248.0000 - val_rmse: 29943.9023\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246878832.0000 - rmse: 15712.3779 - val_loss: 1092087424.0000 - val_rmse: 33046.7422\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261368800.0000 - rmse: 16166.9043 - val_loss: 888070784.0000 - val_rmse: 29800.5156\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287576832.0000 - rmse: 16958.0879 - val_loss: 978632512.0000 - val_rmse: 31283.1016\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246942560.0000 - rmse: 15714.4062 - val_loss: 907137792.0000 - val_rmse: 30118.7266\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301016832.0000 - rmse: 17349.8359 - val_loss: 811337280.0000 - val_rmse: 28483.9824\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224158816.0000 - rmse: 14971.9326 - val_loss: 762629632.0000 - val_rmse: 27615.7500\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257785824.0000 - rmse: 16055.7100 - val_loss: 1491054592.0000 - val_rmse: 38614.1758\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263456640.0000 - rmse: 16231.3467 - val_loss: 1050314816.0000 - val_rmse: 32408.5605\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264156480.0000 - rmse: 16252.8896 - val_loss: 980154624.0000 - val_rmse: 31307.4219\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254590560.0000 - rmse: 15955.8936 - val_loss: 756428864.0000 - val_rmse: 27503.2520\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227175472.0000 - rmse: 15072.3389 - val_loss: 1072857152.0000 - val_rmse: 32754.4980\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247603472.0000 - rmse: 15735.4199 - val_loss: 776847680.0000 - val_rmse: 27871.9883\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252564640.0000 - rmse: 15892.2812 - val_loss: 943055360.0000 - val_rmse: 30709.2070\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 233020480.0000 - rmse: 15265.0078 - val_loss: 947006528.0000 - val_rmse: 30773.4668\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276184896.0000 - rmse: 16618.8105 - val_loss: 1026870976.0000 - val_rmse: 32044.8281\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224719760.0000 - rmse: 14990.6543 - val_loss: 939216256.0000 - val_rmse: 30646.6348\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 223246224.0000 - rmse: 14941.4258 - val_loss: 932277952.0000 - val_rmse: 30533.2266\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283800864.0000 - rmse: 16846.3887 - val_loss: 1069349376.0000 - val_rmse: 32700.9082\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229899392.0000 - rmse: 15162.4307 - val_loss: 1049081408.0000 - val_rmse: 32389.5254\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 205883408.0000 - rmse: 14348.6357 - val_loss: 1109398400.0000 - val_rmse: 33307.6328\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246137024.0000 - rmse: 15688.7539 - val_loss: 863480832.0000 - val_rmse: 29385.0449\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274757216.0000 - rmse: 16575.8008 - val_loss: 1200176256.0000 - val_rmse: 34643.5586\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286284800.0000 - rmse: 16919.9531 - val_loss: 902329152.0000 - val_rmse: 30038.7949\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231094608.0000 - rmse: 15201.7949 - val_loss: 1225893376.0000 - val_rmse: 35012.7617\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246420176.0000 - rmse: 15697.7744 - val_loss: 900060288.0000 - val_rmse: 30001.0039\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243492192.0000 - rmse: 15604.2354 - val_loss: 840066688.0000 - val_rmse: 28983.9043\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259256688.0000 - rmse: 16101.4492 - val_loss: 868944192.0000 - val_rmse: 29477.8574\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248854400.0000 - rmse: 15775.1182 - val_loss: 1029077248.0000 - val_rmse: 32079.2344\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239441744.0000 - rmse: 15473.9023 - val_loss: 2583915776.0000 - val_rmse: 50832.2305\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242729712.0000 - rmse: 15579.7852 - val_loss: 1236239104.0000 - val_rmse: 35160.1914\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231831120.0000 - rmse: 15226.0010 - val_loss: 1542753536.0000 - val_rmse: 39277.9023\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239331232.0000 - rmse: 15470.3330 - val_loss: 1011162048.0000 - val_rmse: 31798.7734\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251077072.0000 - rmse: 15845.4111 - val_loss: 923493824.0000 - val_rmse: 30389.0410\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239704944.0000 - rmse: 15482.4062 - val_loss: 1065176448.0000 - val_rmse: 32637.0391\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 196187312.0000 - rmse: 14006.6865 - val_loss: 1248617472.0000 - val_rmse: 35335.7812\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219212384.0000 - rmse: 14805.8223 - val_loss: 999512512.0000 - val_rmse: 31615.0684\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222368896.0000 - rmse: 14912.0381 - val_loss: 1205256192.0000 - val_rmse: 34716.8008\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243722912.0000 - rmse: 15611.6260 - val_loss: 1201472768.0000 - val_rmse: 34662.2656\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227358624.0000 - rmse: 15078.4150 - val_loss: 1340800512.0000 - val_rmse: 36616.9414\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221647648.0000 - rmse: 14887.8350 - val_loss: 1320151296.0000 - val_rmse: 36333.8867\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262935344.0000 - rmse: 16215.2793 - val_loss: 1207545088.0000 - val_rmse: 34749.7500\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 213097744.0000 - rmse: 14597.8643 - val_loss: 1173489024.0000 - val_rmse: 34256.2266\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272669472.0000 - rmse: 16512.7051 - val_loss: 965960960.0000 - val_rmse: 31079.9121\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254889552.0000 - rmse: 15965.2598 - val_loss: 1125347712.0000 - val_rmse: 33546.2031\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 198075456.0000 - rmse: 14073.9268 - val_loss: 1188478208.0000 - val_rmse: 34474.3125\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 197946976.0000 - rmse: 14069.3623 - val_loss: 1151790464.0000 - val_rmse: 33938.0391\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237035664.0000 - rmse: 15395.9629 - val_loss: 1095209984.0000 - val_rmse: 33093.9531\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265515792.0000 - rmse: 16294.6543 - val_loss: 1172192128.0000 - val_rmse: 34237.2930\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226487680.0000 - rmse: 15049.5068 - val_loss: 1200462208.0000 - val_rmse: 34647.6836\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231940704.0000 - rmse: 15229.5967 - val_loss: 962652608.0000 - val_rmse: 31026.6426\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217874624.0000 - rmse: 14760.5752 - val_loss: 1218390912.0000 - val_rmse: 34905.4570\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 223768480.0000 - rmse: 14958.8926 - val_loss: 962373952.0000 - val_rmse: 31022.1523\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 182056384.0000 - rmse: 13492.8252 - val_loss: 1350720256.0000 - val_rmse: 36752.1445\n",
      "Epoch 164/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 233989584.0000 - rmse: 15296.7168 - val_loss: 1403443712.0000 - val_rmse: 37462.5625\n",
      "Epoch 165/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206765168.0000 - rmse: 14379.3301 - val_loss: 829502912.0000 - val_rmse: 28801.0918\n",
      "Epoch 166/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270674720.0000 - rmse: 16452.1934 - val_loss: 959535296.0000 - val_rmse: 30976.3672\n",
      "Epoch 167/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242325104.0000 - rmse: 15566.7939 - val_loss: 1013254016.0000 - val_rmse: 31831.6504\n",
      "Epoch 168/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221907824.0000 - rmse: 14896.5693 - val_loss: 868199680.0000 - val_rmse: 29465.2266\n",
      "Epoch 169/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214707344.0000 - rmse: 14652.8936 - val_loss: 1493377664.0000 - val_rmse: 38644.2422\n",
      "Epoch 170/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 196680240.0000 - rmse: 14024.2715 - val_loss: 1001145920.0000 - val_rmse: 31640.8906\n",
      "Epoch 171/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236372832.0000 - rmse: 15374.4199 - val_loss: 942922816.0000 - val_rmse: 30707.0488\n",
      "Epoch 172/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278594272.0000 - rmse: 16691.1406 - val_loss: 1091575296.0000 - val_rmse: 33038.9961\n",
      "Epoch 173/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214628512.0000 - rmse: 14650.2041 - val_loss: 1063162240.0000 - val_rmse: 32606.1680\n",
      "Epoch 174/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231198928.0000 - rmse: 15205.2246 - val_loss: 1383981440.0000 - val_rmse: 37201.8984\n",
      "Epoch 175/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 182316368.0000 - rmse: 13502.4570 - val_loss: 1375746688.0000 - val_rmse: 37091.0586\n",
      "Epoch 176/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 208687888.0000 - rmse: 14446.0322 - val_loss: 1380373632.0000 - val_rmse: 37153.3789\n",
      "Epoch 177/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259418000.0000 - rmse: 16106.4570 - val_loss: 1221488896.0000 - val_rmse: 34949.8047\n",
      "Epoch 178/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 195652656.0000 - rmse: 13987.5879 - val_loss: 1123152640.0000 - val_rmse: 33513.4688\n",
      "Epoch 179/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 205858096.0000 - rmse: 14347.7539 - val_loss: 1483432576.0000 - val_rmse: 38515.3555\n",
      "Epoch 180/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222160736.0000 - rmse: 14905.0566 - val_loss: 1206249600.0000 - val_rmse: 34731.1055\n",
      "Epoch 181/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 199010768.0000 - rmse: 14107.1162 - val_loss: 1029004864.0000 - val_rmse: 32078.1035\n",
      "Epoch 182/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 180992560.0000 - rmse: 13453.3467 - val_loss: 1233889792.0000 - val_rmse: 35126.7656\n",
      "Epoch 183/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 191847712.0000 - rmse: 13850.9092 - val_loss: 1110249472.0000 - val_rmse: 33320.4062\n",
      "Epoch 184/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243231536.0000 - rmse: 15595.8799 - val_loss: 1141372288.0000 - val_rmse: 33784.2031\n",
      "Epoch 185/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206373872.0000 - rmse: 14365.7168 - val_loss: 896861696.0000 - val_rmse: 29947.6484\n",
      "Epoch 186/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206705392.0000 - rmse: 14377.2510 - val_loss: 1196762240.0000 - val_rmse: 34594.2500\n",
      "Epoch 187/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 199944752.0000 - rmse: 14140.1807 - val_loss: 912567808.0000 - val_rmse: 30208.7363\n",
      "Epoch 188/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211324304.0000 - rmse: 14536.9980 - val_loss: 999585280.0000 - val_rmse: 31616.2188\n",
      "Epoch 189/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 203140112.0000 - rmse: 14252.7217 - val_loss: 959978560.0000 - val_rmse: 30983.5195\n",
      "Epoch 190/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 184595952.0000 - rmse: 13586.6074 - val_loss: 977047488.0000 - val_rmse: 31257.7578\n",
      "Epoch 191/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215260848.0000 - rmse: 14671.7695 - val_loss: 1160660608.0000 - val_rmse: 34068.4688\n",
      "Epoch 192/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 198970800.0000 - rmse: 14105.7002 - val_loss: 986397888.0000 - val_rmse: 31406.9707\n",
      "Epoch 193/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 183327216.0000 - rmse: 13539.8369 - val_loss: 1193559936.0000 - val_rmse: 34547.9375\n",
      "Epoch 194/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217963120.0000 - rmse: 14763.5723 - val_loss: 1477096448.0000 - val_rmse: 38433.0117\n",
      "Epoch 195/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 170434320.0000 - rmse: 13055.0479 - val_loss: 1363823744.0000 - val_rmse: 36929.9844\n",
      "Epoch 196/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 225621856.0000 - rmse: 15020.7129 - val_loss: 1111942016.0000 - val_rmse: 33345.7930\n",
      "Epoch 197/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206556144.0000 - rmse: 14372.0576 - val_loss: 1202214656.0000 - val_rmse: 34672.9648\n",
      "Epoch 198/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 169959184.0000 - rmse: 13036.8379 - val_loss: 1115536384.0000 - val_rmse: 33399.6445\n",
      "Epoch 199/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 200197552.0000 - rmse: 14149.1172 - val_loss: 1671312512.0000 - val_rmse: 40881.6875\n",
      "Epoch 200/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202805392.0000 - rmse: 14240.9756 - val_loss: 1052072128.0000 - val_rmse: 32435.6602\n",
      "Epoch 201/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 190094864.0000 - rmse: 13787.4863 - val_loss: 978287232.0000 - val_rmse: 31277.5840\n",
      "Epoch 202/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 191578720.0000 - rmse: 13841.1953 - val_loss: 1093738368.0000 - val_rmse: 33071.7148\n",
      "Epoch 203/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 205826176.0000 - rmse: 14346.6426 - val_loss: 1087969664.0000 - val_rmse: 32984.3867\n",
      "Epoch 204/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 194869808.0000 - rmse: 13959.5752 - val_loss: 1155727232.0000 - val_rmse: 33995.9883\n",
      "Epoch 205/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207571584.0000 - rmse: 14407.3428 - val_loss: 996091840.0000 - val_rmse: 31560.9219\n",
      "Epoch 206/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 200205984.0000 - rmse: 14149.4131 - val_loss: 921566400.0000 - val_rmse: 30357.3105\n",
      "Epoch 207/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207555968.0000 - rmse: 14406.8008 - val_loss: 971605120.0000 - val_rmse: 31170.5801\n",
      "Epoch 208/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 185776512.0000 - rmse: 13629.9834 - val_loss: 1000472256.0000 - val_rmse: 31630.2422\n",
      "Epoch 209/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202822144.0000 - rmse: 14241.5615 - val_loss: 1026219520.0000 - val_rmse: 32034.6621\n",
      "Epoch 210/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 195393728.0000 - rmse: 13978.3291 - val_loss: 1128956032.0000 - val_rmse: 33599.9375\n",
      "Epoch 211/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 171522192.0000 - rmse: 13096.6465 - val_loss: 967284416.0000 - val_rmse: 31101.1934\n",
      "Epoch 212/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202637776.0000 - rmse: 14235.0869 - val_loss: 1380686080.0000 - val_rmse: 37157.5859\n",
      "Epoch 213/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217451184.0000 - rmse: 14746.2266 - val_loss: 1271862912.0000 - val_rmse: 35663.1875\n",
      "Epoch 214/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 193634016.0000 - rmse: 13915.2432 - val_loss: 1177841920.0000 - val_rmse: 34319.6992\n",
      "Epoch 215/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221035024.0000 - rmse: 14867.2451 - val_loss: 1064664512.0000 - val_rmse: 32629.1973\n",
      "Epoch 216/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207974144.0000 - rmse: 14421.3066 - val_loss: 1215978496.0000 - val_rmse: 34870.8828\n",
      "Epoch 217/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 193220448.0000 - rmse: 13900.3750 - val_loss: 1225927552.0000 - val_rmse: 35013.2500\n",
      "Epoch 218/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 196383760.0000 - rmse: 14013.6973 - val_loss: 1012641920.0000 - val_rmse: 31822.0332\n",
      "Epoch 219/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221100032.0000 - rmse: 14869.4316 - val_loss: 1127785088.0000 - val_rmse: 33582.5117\n",
      "Epoch 220/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 183494928.0000 - rmse: 13546.0293 - val_loss: 1146206848.0000 - val_rmse: 33855.6758\n",
      "Epoch 221/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 203985408.0000 - rmse: 14282.3457 - val_loss: 1206250880.0000 - val_rmse: 34731.1211\n",
      "Epoch 222/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 186856784.0000 - rmse: 13669.5557 - val_loss: 1070749504.0000 - val_rmse: 32722.3066\n",
      "Epoch 223/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 184346864.0000 - rmse: 13577.4385 - val_loss: 1033285184.0000 - val_rmse: 32144.7520\n",
      "104/104 [==============================] - 0s 644us/step - loss: 359306656.0000 - rmse: 18955.3867\n",
      "[359306656.0, 18955.38671875]\n",
      "[19547.4765625, 31793.18359375, 22985.25390625, 21458.9296875, 18955.38671875]\n",
      "22948.04609375\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "!python train.py kfold baseline\n",
    "# epoch 256 p 25 lr 5e-3"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 21:22:32.551923: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 21:22:32.551960: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 21:22:32.552261: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 21:22:32.857077: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/256\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 15170785280.0000 - rmse: 123169.7422 - val_loss: 2163167744.0000 - val_rmse: 46509.8672\n",
      "Epoch 2/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2312769024.0000 - rmse: 48091.2578 - val_loss: 1300583552.0000 - val_rmse: 36063.6055\n",
      "Epoch 3/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1867503360.0000 - rmse: 43214.6211 - val_loss: 1077546880.0000 - val_rmse: 32826.0078\n",
      "Epoch 4/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1700980864.0000 - rmse: 41242.9492 - val_loss: 971333696.0000 - val_rmse: 31166.2246\n",
      "Epoch 5/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1596015104.0000 - rmse: 39950.1562 - val_loss: 858146432.0000 - val_rmse: 29294.1367\n",
      "Epoch 6/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1554604032.0000 - rmse: 39428.4648 - val_loss: 818517760.0000 - val_rmse: 28609.7480\n",
      "Epoch 7/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1476873088.0000 - rmse: 38430.1055 - val_loss: 777640640.0000 - val_rmse: 27886.2090\n",
      "Epoch 8/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1437392128.0000 - rmse: 37912.9531 - val_loss: 747936896.0000 - val_rmse: 27348.4355\n",
      "Epoch 9/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1408550272.0000 - rmse: 37530.6562 - val_loss: 728940480.0000 - val_rmse: 26998.8984\n",
      "Epoch 10/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1350758272.0000 - rmse: 36752.6641 - val_loss: 732002240.0000 - val_rmse: 27055.5391\n",
      "Epoch 11/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1393619968.0000 - rmse: 37331.2188 - val_loss: 738475968.0000 - val_rmse: 27174.9141\n",
      "Epoch 12/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1350426368.0000 - rmse: 36748.1484 - val_loss: 764199488.0000 - val_rmse: 27644.1582\n",
      "Epoch 13/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1348278784.0000 - rmse: 36718.9141 - val_loss: 740577600.0000 - val_rmse: 27213.5547\n",
      "Epoch 14/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1259258240.0000 - rmse: 35486.0273 - val_loss: 703658816.0000 - val_rmse: 26526.5684\n",
      "Epoch 15/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1237285248.0000 - rmse: 35175.0664 - val_loss: 700140160.0000 - val_rmse: 26460.1621\n",
      "Epoch 16/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1244287744.0000 - rmse: 35274.4648 - val_loss: 696243200.0000 - val_rmse: 26386.4199\n",
      "Epoch 17/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1209406336.0000 - rmse: 34776.5195 - val_loss: 731546368.0000 - val_rmse: 27047.1133\n",
      "Epoch 18/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1219994112.0000 - rmse: 34928.4141 - val_loss: 841503488.0000 - val_rmse: 29008.6797\n",
      "Epoch 19/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1214717696.0000 - rmse: 34852.8008 - val_loss: 705217088.0000 - val_rmse: 26555.9238\n",
      "Epoch 20/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1147010304.0000 - rmse: 33867.5391 - val_loss: 719807616.0000 - val_rmse: 26829.2285\n",
      "Epoch 21/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1185239168.0000 - rmse: 34427.3008 - val_loss: 724292288.0000 - val_rmse: 26912.6797\n",
      "Epoch 22/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1143542528.0000 - rmse: 33816.3047 - val_loss: 759953792.0000 - val_rmse: 27567.2578\n",
      "Epoch 23/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1208338688.0000 - rmse: 34761.1680 - val_loss: 695560256.0000 - val_rmse: 26373.4766\n",
      "Epoch 24/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1166237312.0000 - rmse: 34150.2188 - val_loss: 698097280.0000 - val_rmse: 26421.5312\n",
      "Epoch 25/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1098132608.0000 - rmse: 33138.0820 - val_loss: 701654080.0000 - val_rmse: 26488.7539\n",
      "Epoch 26/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1091980672.0000 - rmse: 33045.1289 - val_loss: 686255040.0000 - val_rmse: 26196.4707\n",
      "Epoch 27/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1050087808.0000 - rmse: 32405.0566 - val_loss: 717792832.0000 - val_rmse: 26791.6543\n",
      "Epoch 28/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1101664256.0000 - rmse: 33191.3281 - val_loss: 805806464.0000 - val_rmse: 28386.7305\n",
      "Epoch 29/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1090729088.0000 - rmse: 33026.1875 - val_loss: 681159552.0000 - val_rmse: 26099.0332\n",
      "Epoch 30/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1021883904.0000 - rmse: 31966.9180 - val_loss: 741620288.0000 - val_rmse: 27232.7031\n",
      "Epoch 31/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1037515072.0000 - rmse: 32210.4805 - val_loss: 706039104.0000 - val_rmse: 26571.3965\n",
      "Epoch 32/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 955996608.0000 - rmse: 30919.1953 - val_loss: 803836160.0000 - val_rmse: 28352.0039\n",
      "Epoch 33/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 934842624.0000 - rmse: 30575.1953 - val_loss: 688848064.0000 - val_rmse: 26245.9160\n",
      "Epoch 34/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 972079360.0000 - rmse: 31178.1875 - val_loss: 796232512.0000 - val_rmse: 28217.5918\n",
      "Epoch 35/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 944706816.0000 - rmse: 30736.0820 - val_loss: 681925440.0000 - val_rmse: 26113.7012\n",
      "Epoch 36/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 902948096.0000 - rmse: 30049.0957 - val_loss: 725307840.0000 - val_rmse: 26931.5391\n",
      "Epoch 37/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 906388096.0000 - rmse: 30106.2793 - val_loss: 784764672.0000 - val_rmse: 28013.6504\n",
      "Epoch 38/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 914864384.0000 - rmse: 30246.7246 - val_loss: 750149760.0000 - val_rmse: 27388.8613\n",
      "Epoch 39/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 885719488.0000 - rmse: 29761.0391 - val_loss: 812127936.0000 - val_rmse: 28497.8594\n",
      "Epoch 40/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 806695424.0000 - rmse: 28402.3848 - val_loss: 942951488.0000 - val_rmse: 30707.5156\n",
      "Epoch 41/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 850625472.0000 - rmse: 29165.4824 - val_loss: 852160128.0000 - val_rmse: 29191.7812\n",
      "Epoch 42/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 871026752.0000 - rmse: 29513.1621 - val_loss: 749269888.0000 - val_rmse: 27372.7949\n",
      "Epoch 43/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 889216768.0000 - rmse: 29819.7363 - val_loss: 738273472.0000 - val_rmse: 27171.1875\n",
      "Epoch 44/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 860415296.0000 - rmse: 29332.8359 - val_loss: 726356736.0000 - val_rmse: 26951.0059\n",
      "Epoch 45/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 769795200.0000 - rmse: 27745.1836 - val_loss: 729619008.0000 - val_rmse: 27011.4609\n",
      "Epoch 46/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 809310720.0000 - rmse: 28448.3867 - val_loss: 727981696.0000 - val_rmse: 26981.1367\n",
      "Epoch 47/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 763306432.0000 - rmse: 27627.9980 - val_loss: 654150976.0000 - val_rmse: 25576.3750\n",
      "Epoch 48/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 777634176.0000 - rmse: 27886.0898 - val_loss: 1036799168.0000 - val_rmse: 32199.3652\n",
      "Epoch 49/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 736091456.0000 - rmse: 27131.0059 - val_loss: 607193664.0000 - val_rmse: 24641.3008\n",
      "Epoch 50/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 757313600.0000 - rmse: 27519.3301 - val_loss: 598301632.0000 - val_rmse: 24460.2031\n",
      "Epoch 51/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718402304.0000 - rmse: 26803.0273 - val_loss: 682435520.0000 - val_rmse: 26123.4668\n",
      "Epoch 52/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729063936.0000 - rmse: 27001.1836 - val_loss: 697909120.0000 - val_rmse: 26417.9688\n",
      "Epoch 53/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 719572480.0000 - rmse: 26824.8477 - val_loss: 676514944.0000 - val_rmse: 26009.9004\n",
      "Epoch 54/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 673626816.0000 - rmse: 25954.3223 - val_loss: 639795968.0000 - val_rmse: 25294.1875\n",
      "Epoch 55/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 775658880.0000 - rmse: 27850.6523 - val_loss: 794589312.0000 - val_rmse: 28188.4609\n",
      "Epoch 56/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 660106240.0000 - rmse: 25692.5332 - val_loss: 793669120.0000 - val_rmse: 28172.1328\n",
      "Epoch 57/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665993472.0000 - rmse: 25806.8477 - val_loss: 671924096.0000 - val_rmse: 25921.4980\n",
      "Epoch 58/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651920576.0000 - rmse: 25532.7324 - val_loss: 593996544.0000 - val_rmse: 24372.0449\n",
      "Epoch 59/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687590336.0000 - rmse: 26221.9434 - val_loss: 584234304.0000 - val_rmse: 24170.9375\n",
      "Epoch 60/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649321728.0000 - rmse: 25481.7930 - val_loss: 695510656.0000 - val_rmse: 26372.5332\n",
      "Epoch 61/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 633495744.0000 - rmse: 25169.3418 - val_loss: 609171264.0000 - val_rmse: 24681.3945\n",
      "Epoch 62/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656840640.0000 - rmse: 25628.9004 - val_loss: 641508800.0000 - val_rmse: 25328.0234\n",
      "Epoch 63/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665979520.0000 - rmse: 25806.5781 - val_loss: 698135488.0000 - val_rmse: 26422.2539\n",
      "Epoch 64/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 655260672.0000 - rmse: 25598.0605 - val_loss: 809294144.0000 - val_rmse: 28448.0957\n",
      "Epoch 65/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574101760.0000 - rmse: 23960.4199 - val_loss: 598259776.0000 - val_rmse: 24459.3496\n",
      "Epoch 66/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535445920.0000 - rmse: 23139.7031 - val_loss: 684835392.0000 - val_rmse: 26169.3594\n",
      "Epoch 67/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599316032.0000 - rmse: 24480.9316 - val_loss: 512315584.0000 - val_rmse: 22634.3867\n",
      "Epoch 68/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554696640.0000 - rmse: 23551.9980 - val_loss: 595941952.0000 - val_rmse: 24411.9219\n",
      "Epoch 69/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560424384.0000 - rmse: 23673.2832 - val_loss: 861995520.0000 - val_rmse: 29359.7598\n",
      "Epoch 70/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618590848.0000 - rmse: 24871.4863 - val_loss: 685023872.0000 - val_rmse: 26172.9590\n",
      "Epoch 71/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573843008.0000 - rmse: 23955.0215 - val_loss: 937967040.0000 - val_rmse: 30626.2480\n",
      "Epoch 72/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536581952.0000 - rmse: 23164.2383 - val_loss: 521374048.0000 - val_rmse: 22833.6152\n",
      "Epoch 73/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525661824.0000 - rmse: 22927.3164 - val_loss: 632072640.0000 - val_rmse: 25141.0508\n",
      "Epoch 74/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497307072.0000 - rmse: 22300.3809 - val_loss: 586772096.0000 - val_rmse: 24223.3770\n",
      "Epoch 75/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519492128.0000 - rmse: 22792.3691 - val_loss: 538468032.0000 - val_rmse: 23204.9141\n",
      "Epoch 76/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538449024.0000 - rmse: 23204.5020 - val_loss: 691438592.0000 - val_rmse: 26295.2207\n",
      "Epoch 77/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487488832.0000 - rmse: 22079.1484 - val_loss: 740711040.0000 - val_rmse: 27216.0059\n",
      "Epoch 78/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549770944.0000 - rmse: 23447.1914 - val_loss: 648818816.0000 - val_rmse: 25471.9219\n",
      "Epoch 79/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490871936.0000 - rmse: 22155.6270 - val_loss: 608019264.0000 - val_rmse: 24658.0449\n",
      "Epoch 80/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496145216.0000 - rmse: 22274.3184 - val_loss: 631225792.0000 - val_rmse: 25124.2070\n",
      "Epoch 81/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519718656.0000 - rmse: 22797.3379 - val_loss: 589530560.0000 - val_rmse: 24280.2500\n",
      "Epoch 82/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505665984.0000 - rmse: 22487.0156 - val_loss: 703352448.0000 - val_rmse: 26520.7930\n",
      "Epoch 83/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486420128.0000 - rmse: 22054.9336 - val_loss: 681334016.0000 - val_rmse: 26102.3750\n",
      "Epoch 84/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455293568.0000 - rmse: 21337.6074 - val_loss: 879502208.0000 - val_rmse: 29656.3984\n",
      "Epoch 85/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477813728.0000 - rmse: 21858.9492 - val_loss: 775033408.0000 - val_rmse: 27839.4219\n",
      "Epoch 86/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502409408.0000 - rmse: 22414.4883 - val_loss: 873612032.0000 - val_rmse: 29556.9277\n",
      "Epoch 87/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494127616.0000 - rmse: 22228.9805 - val_loss: 601858752.0000 - val_rmse: 24532.8105\n",
      "Epoch 88/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460430592.0000 - rmse: 21457.6465 - val_loss: 1154227712.0000 - val_rmse: 33973.9258\n",
      "Epoch 89/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517406528.0000 - rmse: 22746.5703 - val_loss: 521985984.0000 - val_rmse: 22847.0117\n",
      "Epoch 90/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440337024.0000 - rmse: 20984.2090 - val_loss: 825243840.0000 - val_rmse: 28727.0586\n",
      "Epoch 91/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462340096.0000 - rmse: 21502.0938 - val_loss: 768501376.0000 - val_rmse: 27721.8574\n",
      "Epoch 92/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513734368.0000 - rmse: 22665.7090 - val_loss: 1039015680.0000 - val_rmse: 32233.7656\n",
      "Epoch 93/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430621984.0000 - rmse: 20751.4336 - val_loss: 729863104.0000 - val_rmse: 27015.9766\n",
      "Epoch 94/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429436672.0000 - rmse: 20722.8535 - val_loss: 543967168.0000 - val_rmse: 23323.1016\n",
      "Epoch 95/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426597344.0000 - rmse: 20654.2324 - val_loss: 1455822848.0000 - val_rmse: 38155.2461\n",
      "Epoch 96/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467730688.0000 - rmse: 21627.0801 - val_loss: 944554816.0000 - val_rmse: 30733.6113\n",
      "Epoch 97/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436983872.0000 - rmse: 20904.1562 - val_loss: 569062720.0000 - val_rmse: 23855.0332\n",
      "Epoch 98/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518425056.0000 - rmse: 22768.9473 - val_loss: 865259456.0000 - val_rmse: 29415.2930\n",
      "Epoch 99/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430182464.0000 - rmse: 20740.8398 - val_loss: 574680640.0000 - val_rmse: 23972.4980\n",
      "Epoch 100/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460993312.0000 - rmse: 21470.7539 - val_loss: 791466880.0000 - val_rmse: 28133.0195\n",
      "Epoch 101/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465660992.0000 - rmse: 21579.1777 - val_loss: 920913344.0000 - val_rmse: 30346.5547\n",
      "Epoch 102/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413909568.0000 - rmse: 20344.7676 - val_loss: 1069737216.0000 - val_rmse: 32706.8359\n",
      "Epoch 103/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457694528.0000 - rmse: 21393.7969 - val_loss: 1001182720.0000 - val_rmse: 31641.4707\n",
      "Epoch 104/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452429024.0000 - rmse: 21270.3789 - val_loss: 679529664.0000 - val_rmse: 26067.7891\n",
      "Epoch 105/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381309376.0000 - rmse: 19527.1426 - val_loss: 561409216.0000 - val_rmse: 23694.0762\n",
      "Epoch 106/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421107456.0000 - rmse: 20520.9023 - val_loss: 1329254528.0000 - val_rmse: 36458.9414\n",
      "Epoch 107/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403102592.0000 - rmse: 20077.4141 - val_loss: 646057792.0000 - val_rmse: 25417.6660\n",
      "Epoch 108/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417818176.0000 - rmse: 20440.5996 - val_loss: 836311808.0000 - val_rmse: 28919.0547\n",
      "Epoch 109/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442432704.0000 - rmse: 21034.0840 - val_loss: 1252428800.0000 - val_rmse: 35389.6719\n",
      "Epoch 110/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398707776.0000 - rmse: 19967.6680 - val_loss: 669683264.0000 - val_rmse: 25878.2383\n",
      "Epoch 111/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447274368.0000 - rmse: 21148.8594 - val_loss: 778003968.0000 - val_rmse: 27892.7227\n",
      "Epoch 112/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480574048.0000 - rmse: 21921.9980 - val_loss: 527320736.0000 - val_rmse: 22963.4648\n",
      "Epoch 113/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403493888.0000 - rmse: 20087.1562 - val_loss: 1079839744.0000 - val_rmse: 32860.9141\n",
      "Epoch 114/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372843936.0000 - rmse: 19309.1660 - val_loss: 563327552.0000 - val_rmse: 23734.5215\n",
      "Epoch 115/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444089568.0000 - rmse: 21073.4316 - val_loss: 563241152.0000 - val_rmse: 23732.7012\n",
      "Epoch 116/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411514752.0000 - rmse: 20285.8262 - val_loss: 528753248.0000 - val_rmse: 22994.6348\n",
      "Epoch 117/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407181408.0000 - rmse: 20178.7363 - val_loss: 1023528384.0000 - val_rmse: 31992.6270\n",
      "Epoch 118/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392581664.0000 - rmse: 19813.6738 - val_loss: 694635392.0000 - val_rmse: 26355.9355\n",
      "Epoch 119/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403146880.0000 - rmse: 20078.5176 - val_loss: 984091072.0000 - val_rmse: 31370.2266\n",
      "Epoch 120/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391416096.0000 - rmse: 19784.2363 - val_loss: 977029440.0000 - val_rmse: 31257.4688\n",
      "Epoch 121/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373119584.0000 - rmse: 19316.3027 - val_loss: 1887579776.0000 - val_rmse: 43446.2852\n",
      "Epoch 122/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390513280.0000 - rmse: 19761.4082 - val_loss: 443836704.0000 - val_rmse: 21067.4316\n",
      "Epoch 123/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396420672.0000 - rmse: 19910.3145 - val_loss: 558500800.0000 - val_rmse: 23632.6211\n",
      "Epoch 124/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454036928.0000 - rmse: 21308.1426 - val_loss: 519055136.0000 - val_rmse: 22782.7812\n",
      "Epoch 125/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427832896.0000 - rmse: 20684.1191 - val_loss: 929517632.0000 - val_rmse: 30487.9922\n",
      "Epoch 126/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376433728.0000 - rmse: 19401.8984 - val_loss: 812634368.0000 - val_rmse: 28506.7422\n",
      "Epoch 127/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400566336.0000 - rmse: 20014.1543 - val_loss: 698565312.0000 - val_rmse: 26430.3867\n",
      "Epoch 128/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373374464.0000 - rmse: 19322.8984 - val_loss: 874325568.0000 - val_rmse: 29568.9961\n",
      "Epoch 129/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354691072.0000 - rmse: 18833.2422 - val_loss: 541759872.0000 - val_rmse: 23275.7363\n",
      "Epoch 130/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341977280.0000 - rmse: 18492.6250 - val_loss: 1140391040.0000 - val_rmse: 33769.6758\n",
      "Epoch 131/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444048672.0000 - rmse: 21072.4629 - val_loss: 678035072.0000 - val_rmse: 26039.1055\n",
      "Epoch 132/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398812672.0000 - rmse: 19970.2949 - val_loss: 1028047680.0000 - val_rmse: 32063.1836\n",
      "Epoch 133/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424807008.0000 - rmse: 20610.8457 - val_loss: 981901056.0000 - val_rmse: 31335.3008\n",
      "Epoch 134/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395521984.0000 - rmse: 19887.7324 - val_loss: 881776192.0000 - val_rmse: 29694.7168\n",
      "Epoch 135/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353464480.0000 - rmse: 18800.6504 - val_loss: 903140416.0000 - val_rmse: 30052.2949\n",
      "Epoch 136/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392800640.0000 - rmse: 19819.1973 - val_loss: 739434048.0000 - val_rmse: 27192.5371\n",
      "Epoch 137/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449045952.0000 - rmse: 21190.7031 - val_loss: 824973888.0000 - val_rmse: 28722.3574\n",
      "Epoch 138/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381975328.0000 - rmse: 19544.1875 - val_loss: 908618368.0000 - val_rmse: 30143.2969\n",
      "Epoch 139/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467492832.0000 - rmse: 21621.5801 - val_loss: 826881920.0000 - val_rmse: 28755.5547\n",
      "Epoch 140/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411104992.0000 - rmse: 20275.7227 - val_loss: 876373376.0000 - val_rmse: 29603.6016\n",
      "Epoch 141/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400791072.0000 - rmse: 20019.7656 - val_loss: 1156864256.0000 - val_rmse: 34012.7070\n",
      "Epoch 142/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373864512.0000 - rmse: 19335.5742 - val_loss: 1402371200.0000 - val_rmse: 37448.2422\n",
      "Epoch 143/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393490912.0000 - rmse: 19836.6035 - val_loss: 1139208832.0000 - val_rmse: 33752.1680\n",
      "Epoch 144/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421404128.0000 - rmse: 20528.1289 - val_loss: 656592256.0000 - val_rmse: 25624.0527\n",
      "Epoch 145/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421501984.0000 - rmse: 20530.5137 - val_loss: 607517376.0000 - val_rmse: 24647.8672\n",
      "Epoch 146/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375212736.0000 - rmse: 19370.4062 - val_loss: 1222017920.0000 - val_rmse: 34957.3711\n",
      "Epoch 147/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326025184.0000 - rmse: 18056.1660 - val_loss: 659327040.0000 - val_rmse: 25677.3652\n",
      "Epoch 148/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336900736.0000 - rmse: 18354.8535 - val_loss: 1029156224.0000 - val_rmse: 32080.4648\n",
      "Epoch 149/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351469440.0000 - rmse: 18747.5176 - val_loss: 539537344.0000 - val_rmse: 23227.9434\n",
      "Epoch 150/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380337472.0000 - rmse: 19502.2402 - val_loss: 1455289728.0000 - val_rmse: 38148.2617\n",
      "Epoch 151/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318336608.0000 - rmse: 17841.9883 - val_loss: 818042048.0000 - val_rmse: 28601.4336\n",
      "Epoch 152/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412337728.0000 - rmse: 20306.0996 - val_loss: 747733696.0000 - val_rmse: 27344.7188\n",
      "Epoch 153/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382161088.0000 - rmse: 19548.9395 - val_loss: 1127192448.0000 - val_rmse: 33573.6836\n",
      "Epoch 154/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333332864.0000 - rmse: 18257.4043 - val_loss: 1631349120.0000 - val_rmse: 40389.9648\n",
      "Epoch 155/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343256032.0000 - rmse: 18527.1680 - val_loss: 1052158208.0000 - val_rmse: 32436.9883\n",
      "Epoch 156/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421656832.0000 - rmse: 20534.2812 - val_loss: 1025729216.0000 - val_rmse: 32027.0078\n",
      "Epoch 157/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299624384.0000 - rmse: 17309.6582 - val_loss: 533514720.0000 - val_rmse: 23097.9375\n",
      "Epoch 158/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363323296.0000 - rmse: 19061.0391 - val_loss: 1259225088.0000 - val_rmse: 35485.5625\n",
      "Epoch 159/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346407968.0000 - rmse: 18612.0352 - val_loss: 738308352.0000 - val_rmse: 27171.8301\n",
      "Epoch 160/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346602880.0000 - rmse: 18617.2734 - val_loss: 731009856.0000 - val_rmse: 27037.1934\n",
      "Epoch 161/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337275680.0000 - rmse: 18365.0645 - val_loss: 700511744.0000 - val_rmse: 26467.1816\n",
      "Epoch 162/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461024032.0000 - rmse: 21471.4688 - val_loss: 1138471680.0000 - val_rmse: 33741.2461\n",
      "Epoch 163/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352525120.0000 - rmse: 18775.6523 - val_loss: 1503976832.0000 - val_rmse: 38781.1406\n",
      "Epoch 164/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349920448.0000 - rmse: 18706.1582 - val_loss: 1012683968.0000 - val_rmse: 31822.6953\n",
      "Epoch 165/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316523776.0000 - rmse: 17791.1133 - val_loss: 1317590016.0000 - val_rmse: 36298.6211\n",
      "Epoch 166/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365221152.0000 - rmse: 19110.7598 - val_loss: 895838976.0000 - val_rmse: 29930.5684\n",
      "Epoch 167/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361371264.0000 - rmse: 19009.7656 - val_loss: 1205174912.0000 - val_rmse: 34715.6289\n",
      "Epoch 168/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371106432.0000 - rmse: 19264.1230 - val_loss: 1496187776.0000 - val_rmse: 38680.5859\n",
      "Epoch 169/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319829472.0000 - rmse: 17883.7754 - val_loss: 706296896.0000 - val_rmse: 26576.2461\n",
      "Epoch 170/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315249312.0000 - rmse: 17755.2578 - val_loss: 744287744.0000 - val_rmse: 27281.6367\n",
      "Epoch 171/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347958976.0000 - rmse: 18653.6582 - val_loss: 694282880.0000 - val_rmse: 26349.2461\n",
      "Epoch 172/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320023200.0000 - rmse: 17889.1914 - val_loss: 1163299968.0000 - val_rmse: 34107.1836\n",
      "Epoch 173/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357643936.0000 - rmse: 18911.4746 - val_loss: 684406848.0000 - val_rmse: 26161.1699\n",
      "Epoch 174/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279425984.0000 - rmse: 16716.0371 - val_loss: 621703616.0000 - val_rmse: 24933.9844\n",
      "Epoch 175/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407148256.0000 - rmse: 20177.9121 - val_loss: 712922368.0000 - val_rmse: 26700.6055\n",
      "Epoch 176/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377900640.0000 - rmse: 19439.6641 - val_loss: 911148864.0000 - val_rmse: 30185.2422\n",
      "Epoch 177/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371673920.0000 - rmse: 19278.8457 - val_loss: 617784128.0000 - val_rmse: 24855.2617\n",
      "Epoch 178/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290135456.0000 - rmse: 17033.3613 - val_loss: 873647552.0000 - val_rmse: 29557.5293\n",
      "Epoch 179/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339588768.0000 - rmse: 18427.9336 - val_loss: 989059840.0000 - val_rmse: 31449.3223\n",
      "Epoch 180/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328166912.0000 - rmse: 18115.3770 - val_loss: 1075782144.0000 - val_rmse: 32799.1172\n",
      "Epoch 181/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367241280.0000 - rmse: 19163.5371 - val_loss: 799617280.0000 - val_rmse: 28277.5039\n",
      "Epoch 182/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327854400.0000 - rmse: 18106.7500 - val_loss: 1033376064.0000 - val_rmse: 32146.1660\n",
      "Epoch 183/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387514848.0000 - rmse: 19685.3965 - val_loss: 1283381504.0000 - val_rmse: 35824.3125\n",
      "Epoch 184/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336142880.0000 - rmse: 18334.1992 - val_loss: 558961536.0000 - val_rmse: 23642.3652\n",
      "Epoch 185/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306632064.0000 - rmse: 17510.9102 - val_loss: 1279924480.0000 - val_rmse: 35776.0312\n",
      "Epoch 186/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305420448.0000 - rmse: 17476.2812 - val_loss: 955486848.0000 - val_rmse: 30910.9473\n",
      "Epoch 187/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319852832.0000 - rmse: 17884.4297 - val_loss: 1808472960.0000 - val_rmse: 42526.1445\n",
      "Epoch 188/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305378560.0000 - rmse: 17475.0820 - val_loss: 890678016.0000 - val_rmse: 29844.2285\n",
      "Epoch 189/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353108320.0000 - rmse: 18791.1758 - val_loss: 692459008.0000 - val_rmse: 26314.6152\n",
      "Epoch 190/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295965600.0000 - rmse: 17203.6465 - val_loss: 851452480.0000 - val_rmse: 29179.6582\n",
      "Epoch 191/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375046144.0000 - rmse: 19366.1074 - val_loss: 1110796416.0000 - val_rmse: 33328.6133\n",
      "Epoch 192/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289450624.0000 - rmse: 17013.2461 - val_loss: 814548352.0000 - val_rmse: 28540.2930\n",
      "Epoch 193/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310765056.0000 - rmse: 17628.5273 - val_loss: 987745472.0000 - val_rmse: 31428.4180\n",
      "Epoch 194/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271349024.0000 - rmse: 16472.6738 - val_loss: 862467968.0000 - val_rmse: 29367.8047\n",
      "Epoch 195/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261921136.0000 - rmse: 16183.9775 - val_loss: 1172047360.0000 - val_rmse: 34235.1758\n",
      "Epoch 196/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284573696.0000 - rmse: 16869.3105 - val_loss: 1127613056.0000 - val_rmse: 33579.9492\n",
      "Epoch 197/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294454368.0000 - rmse: 17159.6719 - val_loss: 1296444800.0000 - val_rmse: 36006.1758\n",
      "Epoch 198/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259500384.0000 - rmse: 16109.0137 - val_loss: 1331947904.0000 - val_rmse: 36495.8633\n",
      "Epoch 199/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338876640.0000 - rmse: 18408.6016 - val_loss: 1367827840.0000 - val_rmse: 36984.1562\n",
      "Epoch 200/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286681472.0000 - rmse: 16931.6680 - val_loss: 557143680.0000 - val_rmse: 23603.8906\n",
      "Epoch 201/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304950848.0000 - rmse: 17462.8418 - val_loss: 865007744.0000 - val_rmse: 29411.0137\n",
      "Epoch 202/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300776704.0000 - rmse: 17342.9141 - val_loss: 1139590656.0000 - val_rmse: 33757.8242\n",
      "Epoch 203/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296970784.0000 - rmse: 17232.8398 - val_loss: 771539584.0000 - val_rmse: 27776.6016\n",
      "Epoch 204/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295084416.0000 - rmse: 17178.0195 - val_loss: 813010304.0000 - val_rmse: 28513.3359\n",
      "Epoch 205/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284850656.0000 - rmse: 16877.5176 - val_loss: 852671168.0000 - val_rmse: 29200.5332\n",
      "Epoch 206/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302761568.0000 - rmse: 17400.0449 - val_loss: 987349632.0000 - val_rmse: 31422.1191\n",
      "Epoch 207/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356946624.0000 - rmse: 18893.0312 - val_loss: 561994880.0000 - val_rmse: 23706.4297\n",
      "Epoch 208/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315736608.0000 - rmse: 17768.9785 - val_loss: 741218176.0000 - val_rmse: 27225.3203\n",
      "Epoch 209/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303374624.0000 - rmse: 17417.6523 - val_loss: 1129493376.0000 - val_rmse: 33607.9336\n",
      "Epoch 210/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252020176.0000 - rmse: 15875.1396 - val_loss: 1106210304.0000 - val_rmse: 33259.7383\n",
      "Epoch 211/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290794336.0000 - rmse: 17052.6914 - val_loss: 1198250496.0000 - val_rmse: 34615.7539\n",
      "Epoch 212/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310011424.0000 - rmse: 17607.1387 - val_loss: 1027527744.0000 - val_rmse: 32055.0742\n",
      "Epoch 213/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295743264.0000 - rmse: 17197.1855 - val_loss: 924449088.0000 - val_rmse: 30404.7539\n",
      "Epoch 214/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343908384.0000 - rmse: 18544.7656 - val_loss: 569808320.0000 - val_rmse: 23870.6582\n",
      "Epoch 215/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294437312.0000 - rmse: 17159.1758 - val_loss: 964023936.0000 - val_rmse: 31048.7344\n",
      "Epoch 216/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353543264.0000 - rmse: 18802.7461 - val_loss: 1114723840.0000 - val_rmse: 33387.4805\n",
      "Epoch 217/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279940352.0000 - rmse: 16731.4180 - val_loss: 1051064640.0000 - val_rmse: 32420.1250\n",
      "Epoch 218/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300551456.0000 - rmse: 17336.4160 - val_loss: 1866702848.0000 - val_rmse: 43205.3555\n",
      "Epoch 219/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263231856.0000 - rmse: 16224.4209 - val_loss: 1138689792.0000 - val_rmse: 33744.4766\n",
      "Epoch 220/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336647680.0000 - rmse: 18347.9570 - val_loss: 1207280128.0000 - val_rmse: 34745.9375\n",
      "Epoch 221/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234826928.0000 - rmse: 15324.0615 - val_loss: 958044416.0000 - val_rmse: 30952.2930\n",
      "Epoch 222/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262087968.0000 - rmse: 16189.1299 - val_loss: 1088176512.0000 - val_rmse: 32987.5195\n",
      "Epoch 223/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304217984.0000 - rmse: 17441.8438 - val_loss: 1432013696.0000 - val_rmse: 37841.9570\n",
      "Epoch 224/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276790368.0000 - rmse: 16637.0176 - val_loss: 1892682240.0000 - val_rmse: 43504.9688\n",
      "Epoch 225/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262451536.0000 - rmse: 16200.3535 - val_loss: 461395296.0000 - val_rmse: 21480.1133\n",
      "Epoch 226/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268345776.0000 - rmse: 16381.2617 - val_loss: 962080640.0000 - val_rmse: 31017.4238\n",
      "Epoch 227/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269976800.0000 - rmse: 16430.9707 - val_loss: 763204672.0000 - val_rmse: 27626.1582\n",
      "Epoch 228/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259863952.0000 - rmse: 16120.2939 - val_loss: 547405632.0000 - val_rmse: 23396.7012\n",
      "Epoch 229/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283008416.0000 - rmse: 16822.8516 - val_loss: 938335424.0000 - val_rmse: 30632.2617\n",
      "Epoch 230/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248819024.0000 - rmse: 15773.9980 - val_loss: 925613888.0000 - val_rmse: 30423.9023\n",
      "Epoch 231/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313546176.0000 - rmse: 17707.2324 - val_loss: 868000064.0000 - val_rmse: 29461.8379\n",
      "Epoch 232/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268815584.0000 - rmse: 16395.5957 - val_loss: 900662016.0000 - val_rmse: 30011.0312\n",
      "Epoch 233/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258479536.0000 - rmse: 16077.2969 - val_loss: 1008926912.0000 - val_rmse: 31763.6094\n",
      "Epoch 234/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273257568.0000 - rmse: 16530.5020 - val_loss: 516817792.0000 - val_rmse: 22733.6250\n",
      "Epoch 235/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270663936.0000 - rmse: 16451.8652 - val_loss: 728272832.0000 - val_rmse: 26986.5293\n",
      "Epoch 236/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307988256.0000 - rmse: 17549.5938 - val_loss: 942315264.0000 - val_rmse: 30697.1543\n",
      "Epoch 237/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263589248.0000 - rmse: 16235.4297 - val_loss: 1347677952.0000 - val_rmse: 36710.7344\n",
      "Epoch 238/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255382624.0000 - rmse: 15980.6934 - val_loss: 1005716288.0000 - val_rmse: 31713.0273\n",
      "Epoch 239/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246076416.0000 - rmse: 15686.8203 - val_loss: 1048975808.0000 - val_rmse: 32387.8926\n",
      "Epoch 240/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266445664.0000 - rmse: 16323.1621 - val_loss: 1230644352.0000 - val_rmse: 35080.5391\n",
      "Epoch 241/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334895072.0000 - rmse: 18300.1387 - val_loss: 768328512.0000 - val_rmse: 27718.7383\n",
      "Epoch 242/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266638112.0000 - rmse: 16329.0547 - val_loss: 1002780864.0000 - val_rmse: 31666.7129\n",
      "Epoch 243/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317476480.0000 - rmse: 17817.8672 - val_loss: 1216180480.0000 - val_rmse: 34873.7773\n",
      "Epoch 244/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238747200.0000 - rmse: 15451.4443 - val_loss: 765576704.0000 - val_rmse: 27669.0566\n",
      "Epoch 245/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259954672.0000 - rmse: 16123.1084 - val_loss: 855119232.0000 - val_rmse: 29242.4199\n",
      "Epoch 246/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260268352.0000 - rmse: 16132.8330 - val_loss: 1380406272.0000 - val_rmse: 37153.8203\n",
      "104/104 [==============================] - 0s 668us/step - loss: 415043360.0000 - rmse: 20372.6113\n",
      "[415043360.0, 20372.611328125]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 15671330816.0000 - rmse: 125185.1875 - val_loss: 2287558400.0000 - val_rmse: 47828.4258\n",
      "Epoch 2/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2293475584.0000 - rmse: 47890.2461 - val_loss: 1383322880.0000 - val_rmse: 37193.0469\n",
      "Epoch 3/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1655921792.0000 - rmse: 40693.0195 - val_loss: 1193991808.0000 - val_rmse: 34554.1875\n",
      "Epoch 4/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1500160128.0000 - rmse: 38731.9023 - val_loss: 1107318016.0000 - val_rmse: 33276.3867\n",
      "Epoch 5/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1379714304.0000 - rmse: 37144.5039 - val_loss: 1027236288.0000 - val_rmse: 32050.5273\n",
      "Epoch 6/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1313449088.0000 - rmse: 36241.5391 - val_loss: 1015757760.0000 - val_rmse: 31870.9551\n",
      "Epoch 7/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1248430080.0000 - rmse: 35333.1289 - val_loss: 954921472.0000 - val_rmse: 30901.8027\n",
      "Epoch 8/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1204217472.0000 - rmse: 34701.8359 - val_loss: 930999552.0000 - val_rmse: 30512.2852\n",
      "Epoch 9/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1147883136.0000 - rmse: 33880.4258 - val_loss: 946509376.0000 - val_rmse: 30765.3906\n",
      "Epoch 10/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1112704640.0000 - rmse: 33357.2266 - val_loss: 894398016.0000 - val_rmse: 29906.4883\n",
      "Epoch 11/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1087826048.0000 - rmse: 32982.2070 - val_loss: 886438784.0000 - val_rmse: 29773.1211\n",
      "Epoch 12/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1108683520.0000 - rmse: 33296.8984 - val_loss: 881414912.0000 - val_rmse: 29688.6328\n",
      "Epoch 13/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1062055296.0000 - rmse: 32589.1895 - val_loss: 864684224.0000 - val_rmse: 29405.5117\n",
      "Epoch 14/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1006903872.0000 - rmse: 31731.7480 - val_loss: 855667136.0000 - val_rmse: 29251.7891\n",
      "Epoch 15/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1064744576.0000 - rmse: 32630.4219 - val_loss: 853777152.0000 - val_rmse: 29219.4648\n",
      "Epoch 16/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 996259904.0000 - rmse: 31563.5859 - val_loss: 841585024.0000 - val_rmse: 29010.0820\n",
      "Epoch 17/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1001187968.0000 - rmse: 31641.5547 - val_loss: 850375616.0000 - val_rmse: 29161.2012\n",
      "Epoch 18/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1017155840.0000 - rmse: 31892.8789 - val_loss: 841386560.0000 - val_rmse: 29006.6641\n",
      "Epoch 19/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 967676928.0000 - rmse: 31107.5020 - val_loss: 860280704.0000 - val_rmse: 29330.5430\n",
      "Epoch 20/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 961740672.0000 - rmse: 31011.9434 - val_loss: 823596608.0000 - val_rmse: 28698.3730\n",
      "Epoch 21/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 922226816.0000 - rmse: 30368.1855 - val_loss: 826779072.0000 - val_rmse: 28753.7656\n",
      "Epoch 22/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 948101184.0000 - rmse: 30791.2520 - val_loss: 855132288.0000 - val_rmse: 29242.6445\n",
      "Epoch 23/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 942976832.0000 - rmse: 30707.9258 - val_loss: 831731456.0000 - val_rmse: 28839.7539\n",
      "Epoch 24/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 914502016.0000 - rmse: 30240.7344 - val_loss: 819575680.0000 - val_rmse: 28628.2324\n",
      "Epoch 25/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 923070144.0000 - rmse: 30382.0703 - val_loss: 836707584.0000 - val_rmse: 28925.8984\n",
      "Epoch 26/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 956117696.0000 - rmse: 30921.1523 - val_loss: 827190528.0000 - val_rmse: 28760.9199\n",
      "Epoch 27/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 877679552.0000 - rmse: 29625.6543 - val_loss: 798302464.0000 - val_rmse: 28254.2461\n",
      "Epoch 28/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 865283840.0000 - rmse: 29415.7070 - val_loss: 773368128.0000 - val_rmse: 27809.4980\n",
      "Epoch 29/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800200000.0000 - rmse: 28287.8066 - val_loss: 884183104.0000 - val_rmse: 29735.2168\n",
      "Epoch 30/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 847169408.0000 - rmse: 29106.1738 - val_loss: 775788992.0000 - val_rmse: 27852.9883\n",
      "Epoch 31/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 810226944.0000 - rmse: 28464.4863 - val_loss: 759235904.0000 - val_rmse: 27554.2363\n",
      "Epoch 32/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 821922752.0000 - rmse: 28669.1953 - val_loss: 789112000.0000 - val_rmse: 28091.1367\n",
      "Epoch 33/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 746851136.0000 - rmse: 27328.5742 - val_loss: 770556608.0000 - val_rmse: 27758.9004\n",
      "Epoch 34/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 728953344.0000 - rmse: 26999.1367 - val_loss: 704440064.0000 - val_rmse: 26541.2891\n",
      "Epoch 35/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 747256832.0000 - rmse: 27335.9961 - val_loss: 721563712.0000 - val_rmse: 26861.9375\n",
      "Epoch 36/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 736032128.0000 - rmse: 27129.9121 - val_loss: 793043392.0000 - val_rmse: 28161.0254\n",
      "Epoch 37/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 760136000.0000 - rmse: 27570.5645 - val_loss: 687388672.0000 - val_rmse: 26218.0957\n",
      "Epoch 38/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706661056.0000 - rmse: 26583.0957 - val_loss: 668377024.0000 - val_rmse: 25852.9883\n",
      "Epoch 39/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729018112.0000 - rmse: 27000.3359 - val_loss: 671538112.0000 - val_rmse: 25914.0527\n",
      "Epoch 40/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 674630080.0000 - rmse: 25973.6426 - val_loss: 654383168.0000 - val_rmse: 25580.9121\n",
      "Epoch 41/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 673741184.0000 - rmse: 25956.5254 - val_loss: 646740928.0000 - val_rmse: 25431.1016\n",
      "Epoch 42/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 689021248.0000 - rmse: 26249.2148 - val_loss: 832095616.0000 - val_rmse: 28846.0684\n",
      "Epoch 43/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641082112.0000 - rmse: 25319.5996 - val_loss: 640652160.0000 - val_rmse: 25311.1074\n",
      "Epoch 44/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641706112.0000 - rmse: 25331.9180 - val_loss: 637162688.0000 - val_rmse: 25242.0820\n",
      "Epoch 45/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 592962688.0000 - rmse: 24350.8262 - val_loss: 609958656.0000 - val_rmse: 24697.3398\n",
      "Epoch 46/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634773888.0000 - rmse: 25194.7188 - val_loss: 607155648.0000 - val_rmse: 24640.5293\n",
      "Epoch 47/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 617200896.0000 - rmse: 24843.5273 - val_loss: 645295872.0000 - val_rmse: 25402.6738\n",
      "Epoch 48/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593636224.0000 - rmse: 24364.6504 - val_loss: 610487680.0000 - val_rmse: 24708.0488\n",
      "Epoch 49/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632101312.0000 - rmse: 25141.6250 - val_loss: 611057280.0000 - val_rmse: 24719.5723\n",
      "Epoch 50/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 602835264.0000 - rmse: 24552.7031 - val_loss: 612503104.0000 - val_rmse: 24748.7988\n",
      "Epoch 51/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 551552704.0000 - rmse: 23485.1602 - val_loss: 689880384.0000 - val_rmse: 26265.5723\n",
      "Epoch 52/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556753536.0000 - rmse: 23595.6230 - val_loss: 615332416.0000 - val_rmse: 24805.8926\n",
      "Epoch 53/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544434496.0000 - rmse: 23333.1211 - val_loss: 637792384.0000 - val_rmse: 25254.5508\n",
      "Epoch 54/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 595512448.0000 - rmse: 24403.1230 - val_loss: 700142976.0000 - val_rmse: 26460.2129\n",
      "Epoch 55/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571233216.0000 - rmse: 23900.4863 - val_loss: 593628672.0000 - val_rmse: 24364.4961\n",
      "Epoch 56/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534717600.0000 - rmse: 23123.9609 - val_loss: 598299968.0000 - val_rmse: 24460.1699\n",
      "Epoch 57/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563019264.0000 - rmse: 23728.0273 - val_loss: 594000704.0000 - val_rmse: 24372.1289\n",
      "Epoch 58/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536859904.0000 - rmse: 23170.2363 - val_loss: 605732416.0000 - val_rmse: 24611.6309\n",
      "Epoch 59/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541575680.0000 - rmse: 23271.7793 - val_loss: 563921984.0000 - val_rmse: 23747.0410\n",
      "Epoch 60/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481036512.0000 - rmse: 21932.5449 - val_loss: 611612480.0000 - val_rmse: 24730.8008\n",
      "Epoch 61/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524288544.0000 - rmse: 22897.3457 - val_loss: 601453760.0000 - val_rmse: 24524.5527\n",
      "Epoch 62/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485154528.0000 - rmse: 22026.2227 - val_loss: 572978944.0000 - val_rmse: 23936.9766\n",
      "Epoch 63/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505965824.0000 - rmse: 22493.6836 - val_loss: 573816256.0000 - val_rmse: 23954.4590\n",
      "Epoch 64/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500915648.0000 - rmse: 22381.1445 - val_loss: 613032832.0000 - val_rmse: 24759.5000\n",
      "Epoch 65/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483157760.0000 - rmse: 21980.8496 - val_loss: 578857984.0000 - val_rmse: 24059.4668\n",
      "Epoch 66/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509846560.0000 - rmse: 22579.7793 - val_loss: 605486784.0000 - val_rmse: 24606.6406\n",
      "Epoch 67/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487780000.0000 - rmse: 22085.7422 - val_loss: 562102016.0000 - val_rmse: 23708.6875\n",
      "Epoch 68/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 474709344.0000 - rmse: 21787.8242 - val_loss: 602188416.0000 - val_rmse: 24539.5273\n",
      "Epoch 69/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458720736.0000 - rmse: 21417.7656 - val_loss: 552152896.0000 - val_rmse: 23497.9336\n",
      "Epoch 70/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440659072.0000 - rmse: 20991.8809 - val_loss: 554135552.0000 - val_rmse: 23540.0820\n",
      "Epoch 71/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441088896.0000 - rmse: 21002.1152 - val_loss: 635199040.0000 - val_rmse: 25203.1543\n",
      "Epoch 72/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434343968.0000 - rmse: 20840.9199 - val_loss: 560441728.0000 - val_rmse: 23673.6484\n",
      "Epoch 73/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445408768.0000 - rmse: 21104.7070 - val_loss: 720833536.0000 - val_rmse: 26848.3438\n",
      "Epoch 74/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459318848.0000 - rmse: 21431.7246 - val_loss: 565989056.0000 - val_rmse: 23790.5254\n",
      "Epoch 75/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394386080.0000 - rmse: 19859.1543 - val_loss: 592313984.0000 - val_rmse: 24337.5020\n",
      "Epoch 76/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426329248.0000 - rmse: 20647.7402 - val_loss: 579508736.0000 - val_rmse: 24072.9883\n",
      "Epoch 77/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422082656.0000 - rmse: 20544.6484 - val_loss: 569919744.0000 - val_rmse: 23872.9883\n",
      "Epoch 78/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421594464.0000 - rmse: 20532.7656 - val_loss: 548628800.0000 - val_rmse: 23422.8262\n",
      "Epoch 79/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468249536.0000 - rmse: 21639.0723 - val_loss: 524982400.0000 - val_rmse: 22912.4922\n",
      "Epoch 80/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408888640.0000 - rmse: 20220.9922 - val_loss: 559552384.0000 - val_rmse: 23654.8574\n",
      "Epoch 81/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400110144.0000 - rmse: 20002.7520 - val_loss: 537078080.0000 - val_rmse: 23174.9453\n",
      "Epoch 82/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395073664.0000 - rmse: 19876.4590 - val_loss: 680713344.0000 - val_rmse: 26090.4844\n",
      "Epoch 83/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433406944.0000 - rmse: 20818.4277 - val_loss: 575363136.0000 - val_rmse: 23986.7285\n",
      "Epoch 84/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413652832.0000 - rmse: 20338.4570 - val_loss: 584415040.0000 - val_rmse: 24174.6758\n",
      "Epoch 85/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433821856.0000 - rmse: 20828.3887 - val_loss: 593279808.0000 - val_rmse: 24357.3340\n",
      "Epoch 86/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387192352.0000 - rmse: 19677.2031 - val_loss: 550145984.0000 - val_rmse: 23455.1914\n",
      "Epoch 87/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397880128.0000 - rmse: 19946.9316 - val_loss: 525899968.0000 - val_rmse: 22932.5078\n",
      "Epoch 88/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402717280.0000 - rmse: 20067.8164 - val_loss: 552184000.0000 - val_rmse: 23498.5957\n",
      "Epoch 89/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373036928.0000 - rmse: 19314.1641 - val_loss: 566179264.0000 - val_rmse: 23794.5195\n",
      "Epoch 90/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349299584.0000 - rmse: 18689.5586 - val_loss: 533212640.0000 - val_rmse: 23091.3945\n",
      "Epoch 91/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386695840.0000 - rmse: 19664.5840 - val_loss: 570212480.0000 - val_rmse: 23879.1230\n",
      "Epoch 92/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417954240.0000 - rmse: 20443.9277 - val_loss: 632944192.0000 - val_rmse: 25158.3809\n",
      "Epoch 93/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364443296.0000 - rmse: 19090.3965 - val_loss: 585685440.0000 - val_rmse: 24200.9355\n",
      "Epoch 94/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368156992.0000 - rmse: 19187.4160 - val_loss: 568684928.0000 - val_rmse: 23847.1152\n",
      "Epoch 95/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361112224.0000 - rmse: 19002.9512 - val_loss: 566305536.0000 - val_rmse: 23797.1758\n",
      "Epoch 96/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395861728.0000 - rmse: 19896.2734 - val_loss: 576248128.0000 - val_rmse: 24005.1660\n",
      "Epoch 97/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371789504.0000 - rmse: 19281.8438 - val_loss: 612600768.0000 - val_rmse: 24750.7715\n",
      "Epoch 98/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393853216.0000 - rmse: 19845.7344 - val_loss: 605184128.0000 - val_rmse: 24600.4902\n",
      "Epoch 99/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330396000.0000 - rmse: 18176.7969 - val_loss: 555273600.0000 - val_rmse: 23564.2441\n",
      "Epoch 100/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374910304.0000 - rmse: 19362.5977 - val_loss: 541538560.0000 - val_rmse: 23270.9805\n",
      "Epoch 101/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338592192.0000 - rmse: 18400.8750 - val_loss: 549012800.0000 - val_rmse: 23431.0215\n",
      "Epoch 102/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357895712.0000 - rmse: 18918.1309 - val_loss: 594983232.0000 - val_rmse: 24392.2773\n",
      "Epoch 103/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327649568.0000 - rmse: 18101.0898 - val_loss: 519185472.0000 - val_rmse: 22785.6406\n",
      "Epoch 104/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327982368.0000 - rmse: 18110.2812 - val_loss: 571578304.0000 - val_rmse: 23907.7031\n",
      "Epoch 105/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356180608.0000 - rmse: 18872.7480 - val_loss: 555964928.0000 - val_rmse: 23578.9062\n",
      "Epoch 106/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346480544.0000 - rmse: 18613.9844 - val_loss: 560698880.0000 - val_rmse: 23679.0801\n",
      "Epoch 107/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376839200.0000 - rmse: 19412.3457 - val_loss: 550785664.0000 - val_rmse: 23468.8223\n",
      "Epoch 108/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346786496.0000 - rmse: 18622.2031 - val_loss: 580584064.0000 - val_rmse: 24095.3105\n",
      "Epoch 109/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341790880.0000 - rmse: 18487.5859 - val_loss: 514997056.0000 - val_rmse: 22693.5449\n",
      "Epoch 110/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306133088.0000 - rmse: 17496.6562 - val_loss: 523747040.0000 - val_rmse: 22885.5176\n",
      "Epoch 111/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382276960.0000 - rmse: 19551.9023 - val_loss: 552406592.0000 - val_rmse: 23503.3301\n",
      "Epoch 112/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362011712.0000 - rmse: 19026.6035 - val_loss: 564719168.0000 - val_rmse: 23763.8184\n",
      "Epoch 113/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315571264.0000 - rmse: 17764.3262 - val_loss: 575004928.0000 - val_rmse: 23979.2578\n",
      "Epoch 114/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313437824.0000 - rmse: 17704.1719 - val_loss: 625381376.0000 - val_rmse: 25007.6250\n",
      "Epoch 115/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336320000.0000 - rmse: 18339.0273 - val_loss: 603132800.0000 - val_rmse: 24558.7617\n",
      "Epoch 116/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313253920.0000 - rmse: 17698.9766 - val_loss: 632557824.0000 - val_rmse: 25150.6992\n",
      "Epoch 117/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324090304.0000 - rmse: 18002.5059 - val_loss: 591000512.0000 - val_rmse: 24310.5000\n",
      "Epoch 118/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302112128.0000 - rmse: 17381.3711 - val_loss: 547525120.0000 - val_rmse: 23399.2539\n",
      "Epoch 119/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317700800.0000 - rmse: 17824.1641 - val_loss: 572738944.0000 - val_rmse: 23931.9648\n",
      "Epoch 120/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333110560.0000 - rmse: 18251.3125 - val_loss: 545757376.0000 - val_rmse: 23361.4512\n",
      "Epoch 121/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301331200.0000 - rmse: 17358.8926 - val_loss: 593379200.0000 - val_rmse: 24359.3730\n",
      "Epoch 122/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312234112.0000 - rmse: 17670.1465 - val_loss: 544482816.0000 - val_rmse: 23334.1523\n",
      "Epoch 123/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306059680.0000 - rmse: 17494.5605 - val_loss: 832332288.0000 - val_rmse: 28850.1699\n",
      "Epoch 124/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336609152.0000 - rmse: 18346.9102 - val_loss: 564833664.0000 - val_rmse: 23766.2285\n",
      "Epoch 125/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309330752.0000 - rmse: 17587.8008 - val_loss: 601888000.0000 - val_rmse: 24533.4043\n",
      "Epoch 126/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334678464.0000 - rmse: 18294.2188 - val_loss: 585988416.0000 - val_rmse: 24207.1934\n",
      "Epoch 127/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290825792.0000 - rmse: 17053.6133 - val_loss: 536466624.0000 - val_rmse: 23161.7480\n",
      "Epoch 128/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333232224.0000 - rmse: 18254.6484 - val_loss: 552244864.0000 - val_rmse: 23499.8906\n",
      "Epoch 129/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292022784.0000 - rmse: 17088.6738 - val_loss: 619127744.0000 - val_rmse: 24882.2773\n",
      "Epoch 130/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348032352.0000 - rmse: 18655.6230 - val_loss: 598969536.0000 - val_rmse: 24473.8535\n",
      "Epoch 131/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333553632.0000 - rmse: 18263.4492 - val_loss: 525859488.0000 - val_rmse: 22931.6230\n",
      "Epoch 132/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331686688.0000 - rmse: 18212.2656 - val_loss: 653322304.0000 - val_rmse: 25560.1699\n",
      "Epoch 133/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283440480.0000 - rmse: 16835.6895 - val_loss: 625511488.0000 - val_rmse: 25010.2246\n",
      "Epoch 134/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318650400.0000 - rmse: 17850.7812 - val_loss: 557530432.0000 - val_rmse: 23612.0820\n",
      "Epoch 135/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332159456.0000 - rmse: 18225.2402 - val_loss: 568204416.0000 - val_rmse: 23837.0391\n",
      "Epoch 136/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309528672.0000 - rmse: 17593.4238 - val_loss: 536178880.0000 - val_rmse: 23155.5352\n",
      "Epoch 137/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325486720.0000 - rmse: 18041.2461 - val_loss: 517860288.0000 - val_rmse: 22756.5410\n",
      "Epoch 138/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302385312.0000 - rmse: 17389.2266 - val_loss: 554147200.0000 - val_rmse: 23540.3281\n",
      "Epoch 139/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307401152.0000 - rmse: 17532.8574 - val_loss: 564763136.0000 - val_rmse: 23764.7422\n",
      "Epoch 140/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285762592.0000 - rmse: 16904.5137 - val_loss: 539227712.0000 - val_rmse: 23221.2773\n",
      "Epoch 141/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262699088.0000 - rmse: 16207.9922 - val_loss: 606908608.0000 - val_rmse: 24635.5137\n",
      "Epoch 142/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312905440.0000 - rmse: 17689.1309 - val_loss: 529208960.0000 - val_rmse: 23004.5430\n",
      "Epoch 143/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305927936.0000 - rmse: 17490.7949 - val_loss: 663736000.0000 - val_rmse: 25763.0723\n",
      "Epoch 144/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314890464.0000 - rmse: 17745.1523 - val_loss: 566184448.0000 - val_rmse: 23794.6309\n",
      "Epoch 145/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287075264.0000 - rmse: 16943.2930 - val_loss: 558335936.0000 - val_rmse: 23629.1328\n",
      "Epoch 146/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302949184.0000 - rmse: 17405.4316 - val_loss: 563796032.0000 - val_rmse: 23744.3887\n",
      "Epoch 147/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308427040.0000 - rmse: 17562.0898 - val_loss: 679210880.0000 - val_rmse: 26061.6738\n",
      "Epoch 148/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322713472.0000 - rmse: 17964.2285 - val_loss: 541918784.0000 - val_rmse: 23279.1484\n",
      "Epoch 149/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295066336.0000 - rmse: 17177.4941 - val_loss: 510495136.0000 - val_rmse: 22594.1406\n",
      "Epoch 150/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286632800.0000 - rmse: 16930.2324 - val_loss: 499367040.0000 - val_rmse: 22346.5176\n",
      "Epoch 151/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322248896.0000 - rmse: 17951.2930 - val_loss: 576368192.0000 - val_rmse: 24007.6680\n",
      "Epoch 152/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293860480.0000 - rmse: 17142.3574 - val_loss: 604860032.0000 - val_rmse: 24593.9023\n",
      "Epoch 153/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292314464.0000 - rmse: 17097.2051 - val_loss: 534308960.0000 - val_rmse: 23115.1230\n",
      "Epoch 154/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296438272.0000 - rmse: 17217.3809 - val_loss: 565701248.0000 - val_rmse: 23784.4727\n",
      "Epoch 155/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313876928.0000 - rmse: 17716.5703 - val_loss: 525129024.0000 - val_rmse: 22915.6934\n",
      "Epoch 156/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282662400.0000 - rmse: 16812.5645 - val_loss: 514666368.0000 - val_rmse: 22686.2578\n",
      "Epoch 157/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278744032.0000 - rmse: 16695.6289 - val_loss: 579952128.0000 - val_rmse: 24082.1934\n",
      "Epoch 158/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301181984.0000 - rmse: 17354.5938 - val_loss: 620567296.0000 - val_rmse: 24911.1855\n",
      "Epoch 159/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285806016.0000 - rmse: 16905.7969 - val_loss: 541864256.0000 - val_rmse: 23277.9766\n",
      "Epoch 160/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260722064.0000 - rmse: 16146.8887 - val_loss: 624326656.0000 - val_rmse: 24986.5273\n",
      "Epoch 161/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284204864.0000 - rmse: 16858.3750 - val_loss: 530726976.0000 - val_rmse: 23037.5117\n",
      "Epoch 162/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285689696.0000 - rmse: 16902.3574 - val_loss: 544926720.0000 - val_rmse: 23343.6641\n",
      "Epoch 163/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273357344.0000 - rmse: 16533.5195 - val_loss: 505994848.0000 - val_rmse: 22494.3262\n",
      "Epoch 164/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267665696.0000 - rmse: 16360.4893 - val_loss: 705635200.0000 - val_rmse: 26563.7949\n",
      "Epoch 165/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307693952.0000 - rmse: 17541.2051 - val_loss: 600099456.0000 - val_rmse: 24496.9258\n",
      "Epoch 166/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283736128.0000 - rmse: 16844.4688 - val_loss: 544948352.0000 - val_rmse: 23344.1270\n",
      "Epoch 167/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297618944.0000 - rmse: 17251.6348 - val_loss: 540157184.0000 - val_rmse: 23241.2812\n",
      "Epoch 168/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297401440.0000 - rmse: 17245.3301 - val_loss: 572966976.0000 - val_rmse: 23936.7266\n",
      "Epoch 169/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243150160.0000 - rmse: 15593.2695 - val_loss: 526838464.0000 - val_rmse: 22952.9609\n",
      "Epoch 170/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266775296.0000 - rmse: 16333.2559 - val_loss: 537785088.0000 - val_rmse: 23190.1934\n",
      "Epoch 171/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258502656.0000 - rmse: 16078.0166 - val_loss: 509908384.0000 - val_rmse: 22581.1484\n",
      "Epoch 172/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243388784.0000 - rmse: 15600.9199 - val_loss: 513111968.0000 - val_rmse: 22651.9727\n",
      "Epoch 173/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265017936.0000 - rmse: 16279.3691 - val_loss: 536362272.0000 - val_rmse: 23159.4961\n",
      "Epoch 174/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288087008.0000 - rmse: 16973.1270 - val_loss: 571388992.0000 - val_rmse: 23903.7422\n",
      "Epoch 175/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265347664.0000 - rmse: 16289.4941 - val_loss: 582335296.0000 - val_rmse: 24131.6230\n",
      "Epoch 176/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256819584.0000 - rmse: 16025.5898 - val_loss: 565749376.0000 - val_rmse: 23785.4844\n",
      "Epoch 177/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272719200.0000 - rmse: 16514.2109 - val_loss: 534026176.0000 - val_rmse: 23109.0059\n",
      "Epoch 178/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262749424.0000 - rmse: 16209.5449 - val_loss: 540379392.0000 - val_rmse: 23246.0625\n",
      "Epoch 179/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283134432.0000 - rmse: 16826.5977 - val_loss: 571065664.0000 - val_rmse: 23896.9785\n",
      "Epoch 180/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320586752.0000 - rmse: 17904.9355 - val_loss: 516025632.0000 - val_rmse: 22716.1953\n",
      "Epoch 181/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282872128.0000 - rmse: 16818.8027 - val_loss: 555528960.0000 - val_rmse: 23569.6621\n",
      "Epoch 182/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251646480.0000 - rmse: 15863.3672 - val_loss: 557889408.0000 - val_rmse: 23619.6797\n",
      "Epoch 183/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248130800.0000 - rmse: 15752.1660 - val_loss: 671518016.0000 - val_rmse: 25913.6641\n",
      "Epoch 184/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265220464.0000 - rmse: 16285.5879 - val_loss: 534651424.0000 - val_rmse: 23122.5293\n",
      "Epoch 185/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254679408.0000 - rmse: 15958.6748 - val_loss: 536165248.0000 - val_rmse: 23155.2422\n",
      "Epoch 186/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237466000.0000 - rmse: 15409.9307 - val_loss: 586966784.0000 - val_rmse: 24227.3945\n",
      "Epoch 187/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242387216.0000 - rmse: 15568.7881 - val_loss: 529073088.0000 - val_rmse: 23001.5879\n",
      "Epoch 188/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252240528.0000 - rmse: 15882.0801 - val_loss: 579853376.0000 - val_rmse: 24080.1426\n",
      "Epoch 189/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231516240.0000 - rmse: 15215.6562 - val_loss: 604048512.0000 - val_rmse: 24577.3965\n",
      "Epoch 190/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240860624.0000 - rmse: 15519.6846 - val_loss: 586227328.0000 - val_rmse: 24212.1309\n",
      "Epoch 191/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262938288.0000 - rmse: 16215.3701 - val_loss: 561301440.0000 - val_rmse: 23691.7988\n",
      "Epoch 192/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279611776.0000 - rmse: 16721.5957 - val_loss: 524730976.0000 - val_rmse: 22907.0039\n",
      "Epoch 193/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277387424.0000 - rmse: 16654.9492 - val_loss: 558356608.0000 - val_rmse: 23629.5684\n",
      "Epoch 194/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272445344.0000 - rmse: 16505.9160 - val_loss: 510601952.0000 - val_rmse: 22596.5000\n",
      "Epoch 195/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218214432.0000 - rmse: 14772.0801 - val_loss: 542519936.0000 - val_rmse: 23292.0566\n",
      "Epoch 196/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264198192.0000 - rmse: 16254.1729 - val_loss: 782263168.0000 - val_rmse: 27968.9668\n",
      "Epoch 197/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289065504.0000 - rmse: 17001.9238 - val_loss: 615562816.0000 - val_rmse: 24810.5391\n",
      "Epoch 198/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249791744.0000 - rmse: 15804.7988 - val_loss: 560362496.0000 - val_rmse: 23671.9766\n",
      "Epoch 199/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245650880.0000 - rmse: 15673.2510 - val_loss: 533375904.0000 - val_rmse: 23094.9316\n",
      "Epoch 200/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232229232.0000 - rmse: 15239.0674 - val_loss: 557929856.0000 - val_rmse: 23620.5371\n",
      "Epoch 201/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252498112.0000 - rmse: 15890.1885 - val_loss: 676268928.0000 - val_rmse: 26005.1719\n",
      "Epoch 202/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242622784.0000 - rmse: 15576.3525 - val_loss: 549718528.0000 - val_rmse: 23446.0762\n",
      "Epoch 203/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227260480.0000 - rmse: 15075.1582 - val_loss: 544737536.0000 - val_rmse: 23339.6113\n",
      "Epoch 204/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231431760.0000 - rmse: 15212.8789 - val_loss: 513780416.0000 - val_rmse: 22666.7227\n",
      "Epoch 205/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241366496.0000 - rmse: 15535.9736 - val_loss: 525526400.0000 - val_rmse: 22924.3613\n",
      "Epoch 206/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255228576.0000 - rmse: 15975.8730 - val_loss: 536715936.0000 - val_rmse: 23167.1289\n",
      "Epoch 207/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210859264.0000 - rmse: 14520.9941 - val_loss: 707995776.0000 - val_rmse: 26608.1875\n",
      "Epoch 208/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256553024.0000 - rmse: 16017.2695 - val_loss: 540492928.0000 - val_rmse: 23248.5000\n",
      "Epoch 209/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224482896.0000 - rmse: 14982.7529 - val_loss: 522355296.0000 - val_rmse: 22855.0918\n",
      "Epoch 210/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243058032.0000 - rmse: 15590.3154 - val_loss: 674826752.0000 - val_rmse: 25977.4277\n",
      "Epoch 211/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235729728.0000 - rmse: 15353.4922 - val_loss: 572864576.0000 - val_rmse: 23934.5859\n",
      "Epoch 212/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245754816.0000 - rmse: 15676.5674 - val_loss: 551651776.0000 - val_rmse: 23487.2676\n",
      "Epoch 213/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238350848.0000 - rmse: 15438.6133 - val_loss: 504502592.0000 - val_rmse: 22461.1348\n",
      "Epoch 214/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219506976.0000 - rmse: 14815.7646 - val_loss: 513581376.0000 - val_rmse: 22662.3320\n",
      "Epoch 215/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247466656.0000 - rmse: 15731.0703 - val_loss: 534043360.0000 - val_rmse: 23109.3750\n",
      "Epoch 216/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214987680.0000 - rmse: 14662.4570 - val_loss: 489016288.0000 - val_rmse: 22113.7129\n",
      "Epoch 217/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242102944.0000 - rmse: 15559.6562 - val_loss: 528304544.0000 - val_rmse: 22984.8750\n",
      "Epoch 218/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 223551536.0000 - rmse: 14951.6367 - val_loss: 576816576.0000 - val_rmse: 24017.0039\n",
      "Epoch 219/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246528528.0000 - rmse: 15701.2256 - val_loss: 562067520.0000 - val_rmse: 23707.9629\n",
      "Epoch 220/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257606288.0000 - rmse: 16050.1172 - val_loss: 488090464.0000 - val_rmse: 22092.7676\n",
      "Epoch 221/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214984400.0000 - rmse: 14662.3457 - val_loss: 514479584.0000 - val_rmse: 22682.1426\n",
      "Epoch 222/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252625504.0000 - rmse: 15894.1963 - val_loss: 526176032.0000 - val_rmse: 22938.5234\n",
      "Epoch 223/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232166768.0000 - rmse: 15237.0166 - val_loss: 494410496.0000 - val_rmse: 22235.3438\n",
      "Epoch 224/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 209528736.0000 - rmse: 14475.1055 - val_loss: 508431296.0000 - val_rmse: 22548.4199\n",
      "Epoch 225/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267116160.0000 - rmse: 16343.6855 - val_loss: 549228608.0000 - val_rmse: 23435.6230\n",
      "Epoch 226/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221199040.0000 - rmse: 14872.7607 - val_loss: 587431040.0000 - val_rmse: 24236.9746\n",
      "Epoch 227/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229985456.0000 - rmse: 15165.2686 - val_loss: 575270400.0000 - val_rmse: 23984.7930\n",
      "Epoch 228/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235211216.0000 - rmse: 15336.5957 - val_loss: 502297312.0000 - val_rmse: 22411.9883\n",
      "Epoch 229/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 213991136.0000 - rmse: 14628.4336 - val_loss: 496301248.0000 - val_rmse: 22277.8184\n",
      "Epoch 230/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215337120.0000 - rmse: 14674.3662 - val_loss: 532665984.0000 - val_rmse: 23079.5547\n",
      "Epoch 231/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222849904.0000 - rmse: 14928.1562 - val_loss: 584519168.0000 - val_rmse: 24176.8281\n",
      "Epoch 232/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240674144.0000 - rmse: 15513.6748 - val_loss: 563153216.0000 - val_rmse: 23730.8496\n",
      "Epoch 233/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238502816.0000 - rmse: 15443.5342 - val_loss: 533428288.0000 - val_rmse: 23096.0645\n",
      "Epoch 234/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239674160.0000 - rmse: 15481.4111 - val_loss: 541122688.0000 - val_rmse: 23262.0430\n",
      "Epoch 235/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215121952.0000 - rmse: 14667.0332 - val_loss: 514001792.0000 - val_rmse: 22671.6055\n",
      "Epoch 236/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219883472.0000 - rmse: 14828.4648 - val_loss: 672020736.0000 - val_rmse: 25923.3633\n",
      "Epoch 237/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226632352.0000 - rmse: 15054.3105 - val_loss: 600735232.0000 - val_rmse: 24509.8984\n",
      "Epoch 238/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230081808.0000 - rmse: 15168.4443 - val_loss: 515916064.0000 - val_rmse: 22713.7832\n",
      "Epoch 239/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234812080.0000 - rmse: 15323.5781 - val_loss: 541800448.0000 - val_rmse: 23276.6055\n",
      "Epoch 240/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210028288.0000 - rmse: 14492.3516 - val_loss: 641135040.0000 - val_rmse: 25320.6445\n",
      "Epoch 241/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258923200.0000 - rmse: 16091.0869 - val_loss: 550749376.0000 - val_rmse: 23468.0469\n",
      "Epoch 242/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220209968.0000 - rmse: 14839.4707 - val_loss: 550313088.0000 - val_rmse: 23458.7500\n",
      "Epoch 243/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 204963712.0000 - rmse: 14316.5518 - val_loss: 581976832.0000 - val_rmse: 24124.1953\n",
      "Epoch 244/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236623040.0000 - rmse: 15382.5547 - val_loss: 534019968.0000 - val_rmse: 23108.8691\n",
      "Epoch 245/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243379104.0000 - rmse: 15600.6094 - val_loss: 509771136.0000 - val_rmse: 22578.1113\n",
      "Epoch 246/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 209262000.0000 - rmse: 14465.8877 - val_loss: 579014720.0000 - val_rmse: 24062.7227\n",
      "Epoch 247/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211867376.0000 - rmse: 14555.6602 - val_loss: 539474176.0000 - val_rmse: 23226.5820\n",
      "Epoch 248/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219161824.0000 - rmse: 14804.1113 - val_loss: 551070080.0000 - val_rmse: 23474.8809\n",
      "Epoch 249/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238554768.0000 - rmse: 15445.2148 - val_loss: 557807232.0000 - val_rmse: 23617.9395\n",
      "Epoch 250/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272084160.0000 - rmse: 16494.9707 - val_loss: 552906944.0000 - val_rmse: 23513.9727\n",
      "Epoch 251/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239271744.0000 - rmse: 15468.4082 - val_loss: 501965280.0000 - val_rmse: 22404.5801\n",
      "Epoch 252/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215004032.0000 - rmse: 14663.0137 - val_loss: 516649088.0000 - val_rmse: 22729.9160\n",
      "Epoch 253/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 199848064.0000 - rmse: 14136.7607 - val_loss: 519146816.0000 - val_rmse: 22784.7910\n",
      "Epoch 254/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202528768.0000 - rmse: 14231.2578 - val_loss: 530904896.0000 - val_rmse: 23041.3730\n",
      "Epoch 255/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 196799136.0000 - rmse: 14028.5078 - val_loss: 573104640.0000 - val_rmse: 23939.6035\n",
      "Epoch 256/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246386816.0000 - rmse: 15696.7119 - val_loss: 492614624.0000 - val_rmse: 22194.9219\n",
      "104/104 [==============================] - 0s 652us/step - loss: 920290880.0000 - rmse: 30336.2949\n",
      "[920290880.0, 30336.294921875]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 9993612288.0000 - rmse: 99968.0547 - val_loss: 1785796352.0000 - val_rmse: 42258.6836\n",
      "Epoch 2/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1672792448.0000 - rmse: 40899.7852 - val_loss: 1312373888.0000 - val_rmse: 36226.7031\n",
      "Epoch 3/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1434693760.0000 - rmse: 37877.3516 - val_loss: 1199150976.0000 - val_rmse: 34628.7578\n",
      "Epoch 4/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1362351744.0000 - rmse: 36910.0508 - val_loss: 1036490944.0000 - val_rmse: 32194.5801\n",
      "Epoch 5/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1142596864.0000 - rmse: 33802.3203 - val_loss: 1068709056.0000 - val_rmse: 32691.1152\n",
      "Epoch 6/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1153435136.0000 - rmse: 33962.2617 - val_loss: 1159957632.0000 - val_rmse: 34058.1523\n",
      "Epoch 7/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1113080448.0000 - rmse: 33362.8594 - val_loss: 1006936576.0000 - val_rmse: 31732.2637\n",
      "Epoch 8/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1035683712.0000 - rmse: 32182.0410 - val_loss: 915091264.0000 - val_rmse: 30250.4746\n",
      "Epoch 9/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1041793728.0000 - rmse: 32276.8301 - val_loss: 888198976.0000 - val_rmse: 29802.6680\n",
      "Epoch 10/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1047854656.0000 - rmse: 32370.5840 - val_loss: 973099840.0000 - val_rmse: 31194.5488\n",
      "Epoch 11/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 987470400.0000 - rmse: 31424.0410 - val_loss: 912519488.0000 - val_rmse: 30207.9375\n",
      "Epoch 12/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 984890240.0000 - rmse: 31382.9609 - val_loss: 935030528.0000 - val_rmse: 30578.2695\n",
      "Epoch 13/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 977693632.0000 - rmse: 31268.0938 - val_loss: 1027909184.0000 - val_rmse: 32061.0234\n",
      "Epoch 14/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 928710784.0000 - rmse: 30474.7559 - val_loss: 942957952.0000 - val_rmse: 30707.6211\n",
      "Epoch 15/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 924856896.0000 - rmse: 30411.4590 - val_loss: 847625472.0000 - val_rmse: 29114.0078\n",
      "Epoch 16/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 901753344.0000 - rmse: 30029.2090 - val_loss: 864091712.0000 - val_rmse: 29395.4375\n",
      "Epoch 17/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 880461760.0000 - rmse: 29672.5762 - val_loss: 889291584.0000 - val_rmse: 29820.9922\n",
      "Epoch 18/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 883792512.0000 - rmse: 29728.6484 - val_loss: 861959616.0000 - val_rmse: 29359.1465\n",
      "Epoch 19/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 866750784.0000 - rmse: 29440.6309 - val_loss: 1003427200.0000 - val_rmse: 31676.9199\n",
      "Epoch 20/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 844813568.0000 - rmse: 29065.6777 - val_loss: 868636800.0000 - val_rmse: 29472.6445\n",
      "Epoch 21/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 862311744.0000 - rmse: 29365.1445 - val_loss: 908695488.0000 - val_rmse: 30144.5762\n",
      "Epoch 22/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 856350272.0000 - rmse: 29263.4629 - val_loss: 864387392.0000 - val_rmse: 29400.4648\n",
      "Epoch 23/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 788457664.0000 - rmse: 28079.4863 - val_loss: 913403328.0000 - val_rmse: 30222.5625\n",
      "Epoch 24/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 839652736.0000 - rmse: 28976.7617 - val_loss: 869129472.0000 - val_rmse: 29481.0020\n",
      "Epoch 25/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 797020032.0000 - rmse: 28231.5430 - val_loss: 1105047296.0000 - val_rmse: 33242.2500\n",
      "Epoch 26/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 783551360.0000 - rmse: 27991.9883 - val_loss: 1040527424.0000 - val_rmse: 32257.2070\n",
      "Epoch 27/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 769774656.0000 - rmse: 27744.8125 - val_loss: 959827776.0000 - val_rmse: 30981.0859\n",
      "Epoch 28/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 772128128.0000 - rmse: 27787.1934 - val_loss: 849105024.0000 - val_rmse: 29139.4062\n",
      "Epoch 29/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718454720.0000 - rmse: 26804.0059 - val_loss: 860114496.0000 - val_rmse: 29327.7090\n",
      "Epoch 30/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 744028288.0000 - rmse: 27276.8789 - val_loss: 896954112.0000 - val_rmse: 29949.1914\n",
      "Epoch 31/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 745446912.0000 - rmse: 27302.8730 - val_loss: 1028855296.0000 - val_rmse: 32075.7734\n",
      "Epoch 32/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 682869312.0000 - rmse: 26131.7676 - val_loss: 963177216.0000 - val_rmse: 31035.0957\n",
      "Epoch 33/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 755470848.0000 - rmse: 27485.8301 - val_loss: 908267264.0000 - val_rmse: 30137.4727\n",
      "Epoch 34/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733743680.0000 - rmse: 27087.7031 - val_loss: 902742272.0000 - val_rmse: 30045.6699\n",
      "Epoch 35/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 664292160.0000 - rmse: 25773.8652 - val_loss: 1023574784.0000 - val_rmse: 31993.3555\n",
      "Epoch 36/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676478464.0000 - rmse: 26009.1992 - val_loss: 1368125056.0000 - val_rmse: 36988.1758\n",
      "Epoch 37/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 696612480.0000 - rmse: 26393.4180 - val_loss: 1040543808.0000 - val_rmse: 32257.4609\n",
      "Epoch 38/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 699726720.0000 - rmse: 26452.3477 - val_loss: 945234176.0000 - val_rmse: 30744.6602\n",
      "Epoch 39/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687425024.0000 - rmse: 26218.7910 - val_loss: 1030011648.0000 - val_rmse: 32093.7910\n",
      "Epoch 40/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 658469056.0000 - rmse: 25660.6504 - val_loss: 977683840.0000 - val_rmse: 31267.9355\n",
      "Epoch 41/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 672506304.0000 - rmse: 25932.7246 - val_loss: 876341760.0000 - val_rmse: 29603.0703\n",
      "Epoch 42/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632242624.0000 - rmse: 25144.4355 - val_loss: 915449920.0000 - val_rmse: 30256.4004\n",
      "Epoch 43/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646083072.0000 - rmse: 25418.1641 - val_loss: 838522560.0000 - val_rmse: 28957.2539\n",
      "Epoch 44/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 673479616.0000 - rmse: 25951.4844 - val_loss: 827336320.0000 - val_rmse: 28763.4551\n",
      "Epoch 45/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 636926336.0000 - rmse: 25237.4004 - val_loss: 827163968.0000 - val_rmse: 28760.4590\n",
      "Epoch 46/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606379264.0000 - rmse: 24624.7695 - val_loss: 1192289408.0000 - val_rmse: 34529.5430\n",
      "Epoch 47/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555072832.0000 - rmse: 23559.9844 - val_loss: 695035968.0000 - val_rmse: 26363.5352\n",
      "Epoch 48/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598851392.0000 - rmse: 24471.4395 - val_loss: 741817664.0000 - val_rmse: 27236.3281\n",
      "Epoch 49/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565425280.0000 - rmse: 23778.6719 - val_loss: 1105583232.0000 - val_rmse: 33250.3125\n",
      "Epoch 50/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541826880.0000 - rmse: 23277.1758 - val_loss: 1124222464.0000 - val_rmse: 33529.4258\n",
      "Epoch 51/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590404544.0000 - rmse: 24298.2402 - val_loss: 956093184.0000 - val_rmse: 30920.7559\n",
      "Epoch 52/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545046976.0000 - rmse: 23346.2402 - val_loss: 1051738112.0000 - val_rmse: 32430.5098\n",
      "Epoch 53/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504648576.0000 - rmse: 22464.3848 - val_loss: 952025536.0000 - val_rmse: 30854.9102\n",
      "Epoch 54/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566948288.0000 - rmse: 23810.6758 - val_loss: 958955648.0000 - val_rmse: 30967.0098\n",
      "Epoch 55/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587679616.0000 - rmse: 24242.1035 - val_loss: 1460237824.0000 - val_rmse: 38213.0586\n",
      "Epoch 56/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536752224.0000 - rmse: 23167.9121 - val_loss: 884431616.0000 - val_rmse: 29739.3945\n",
      "Epoch 57/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546446720.0000 - rmse: 23376.1992 - val_loss: 683969664.0000 - val_rmse: 26152.8145\n",
      "Epoch 58/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563340160.0000 - rmse: 23734.7871 - val_loss: 851265472.0000 - val_rmse: 29176.4531\n",
      "Epoch 59/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521380320.0000 - rmse: 22833.7539 - val_loss: 853387712.0000 - val_rmse: 29212.8008\n",
      "Epoch 60/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553964480.0000 - rmse: 23536.4473 - val_loss: 734942976.0000 - val_rmse: 27109.8320\n",
      "Epoch 61/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481742880.0000 - rmse: 21948.6426 - val_loss: 719128128.0000 - val_rmse: 26816.5645\n",
      "Epoch 62/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487513952.0000 - rmse: 22079.7168 - val_loss: 1196446976.0000 - val_rmse: 34589.6953\n",
      "Epoch 63/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514664320.0000 - rmse: 22686.2129 - val_loss: 839211328.0000 - val_rmse: 28969.1445\n",
      "Epoch 64/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502778368.0000 - rmse: 22422.7207 - val_loss: 706465216.0000 - val_rmse: 26579.4141\n",
      "Epoch 65/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484859616.0000 - rmse: 22019.5273 - val_loss: 916226752.0000 - val_rmse: 30269.2383\n",
      "Epoch 66/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533059008.0000 - rmse: 23088.0684 - val_loss: 680503232.0000 - val_rmse: 26086.4570\n",
      "Epoch 67/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503123808.0000 - rmse: 22430.4219 - val_loss: 958752064.0000 - val_rmse: 30963.7227\n",
      "Epoch 68/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496622272.0000 - rmse: 22285.0234 - val_loss: 651246976.0000 - val_rmse: 25519.5391\n",
      "Epoch 69/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443778336.0000 - rmse: 21066.0449 - val_loss: 1009514560.0000 - val_rmse: 31772.8594\n",
      "Epoch 70/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464949408.0000 - rmse: 21562.6855 - val_loss: 658244992.0000 - val_rmse: 25656.2852\n",
      "Epoch 71/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496075168.0000 - rmse: 22272.7441 - val_loss: 1082062848.0000 - val_rmse: 32894.7227\n",
      "Epoch 72/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467258144.0000 - rmse: 21616.1543 - val_loss: 892255424.0000 - val_rmse: 29870.6445\n",
      "Epoch 73/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461353600.0000 - rmse: 21479.1426 - val_loss: 823691200.0000 - val_rmse: 28700.0215\n",
      "Epoch 74/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469492608.0000 - rmse: 21667.7754 - val_loss: 813587712.0000 - val_rmse: 28523.4570\n",
      "Epoch 75/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439228992.0000 - rmse: 20957.7910 - val_loss: 925408704.0000 - val_rmse: 30420.5312\n",
      "Epoch 76/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412254688.0000 - rmse: 20304.0527 - val_loss: 859157568.0000 - val_rmse: 29311.3906\n",
      "Epoch 77/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439634496.0000 - rmse: 20967.4629 - val_loss: 971627904.0000 - val_rmse: 31170.9473\n",
      "Epoch 78/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447246752.0000 - rmse: 21148.2090 - val_loss: 719982592.0000 - val_rmse: 26832.4883\n",
      "Epoch 79/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470821440.0000 - rmse: 21698.4199 - val_loss: 652426304.0000 - val_rmse: 25542.6367\n",
      "Epoch 80/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430134880.0000 - rmse: 20739.6934 - val_loss: 827601280.0000 - val_rmse: 28768.0605\n",
      "Epoch 81/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388157376.0000 - rmse: 19701.7109 - val_loss: 694193920.0000 - val_rmse: 26347.5605\n",
      "Epoch 82/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485386144.0000 - rmse: 22031.4785 - val_loss: 802797312.0000 - val_rmse: 28333.6777\n",
      "Epoch 83/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453528416.0000 - rmse: 21296.2051 - val_loss: 792545792.0000 - val_rmse: 28152.1895\n",
      "Epoch 84/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443840352.0000 - rmse: 21067.5176 - val_loss: 744939520.0000 - val_rmse: 27293.5801\n",
      "Epoch 85/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433478112.0000 - rmse: 20820.1367 - val_loss: 794931840.0000 - val_rmse: 28194.5352\n",
      "Epoch 86/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491173952.0000 - rmse: 22162.4453 - val_loss: 712579200.0000 - val_rmse: 26694.1797\n",
      "Epoch 87/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365621504.0000 - rmse: 19121.2324 - val_loss: 814206208.0000 - val_rmse: 28534.2988\n",
      "Epoch 88/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413871008.0000 - rmse: 20343.8184 - val_loss: 716661504.0000 - val_rmse: 26770.5332\n",
      "Epoch 89/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403550720.0000 - rmse: 20088.5703 - val_loss: 1034363264.0000 - val_rmse: 32161.5176\n",
      "Epoch 90/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394408224.0000 - rmse: 19859.7109 - val_loss: 837275712.0000 - val_rmse: 28935.7168\n",
      "Epoch 91/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388020512.0000 - rmse: 19698.2344 - val_loss: 736456768.0000 - val_rmse: 27137.7363\n",
      "Epoch 92/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391703200.0000 - rmse: 19791.4922 - val_loss: 725688768.0000 - val_rmse: 26938.6094\n",
      "Epoch 93/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453502112.0000 - rmse: 21295.5898 - val_loss: 926159488.0000 - val_rmse: 30432.8672\n",
      "Epoch 94/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381045824.0000 - rmse: 19520.3945 - val_loss: 1267354112.0000 - val_rmse: 35599.9180\n",
      "Epoch 95/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362889024.0000 - rmse: 19049.6445 - val_loss: 1265248384.0000 - val_rmse: 35570.3320\n",
      "Epoch 96/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419226912.0000 - rmse: 20475.0312 - val_loss: 751861888.0000 - val_rmse: 27420.0977\n",
      "Epoch 97/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375612608.0000 - rmse: 19380.7266 - val_loss: 609743616.0000 - val_rmse: 24692.9863\n",
      "Epoch 98/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345976288.0000 - rmse: 18600.4355 - val_loss: 2657639680.0000 - val_rmse: 51552.3008\n",
      "Epoch 99/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392647616.0000 - rmse: 19815.3379 - val_loss: 1293225216.0000 - val_rmse: 35961.4414\n",
      "Epoch 100/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409707104.0000 - rmse: 20241.2227 - val_loss: 893168640.0000 - val_rmse: 29885.9277\n",
      "Epoch 101/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367131008.0000 - rmse: 19160.6621 - val_loss: 1628602496.0000 - val_rmse: 40355.9453\n",
      "Epoch 102/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392557696.0000 - rmse: 19813.0684 - val_loss: 941904448.0000 - val_rmse: 30690.4609\n",
      "Epoch 103/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462543840.0000 - rmse: 21506.8320 - val_loss: 709987968.0000 - val_rmse: 26645.5977\n",
      "Epoch 104/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409231424.0000 - rmse: 20229.4688 - val_loss: 718331008.0000 - val_rmse: 26801.6973\n",
      "Epoch 105/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328256896.0000 - rmse: 18117.8613 - val_loss: 2800561920.0000 - val_rmse: 52920.3320\n",
      "Epoch 106/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429756256.0000 - rmse: 20730.5625 - val_loss: 626225600.0000 - val_rmse: 25024.5000\n",
      "Epoch 107/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372377728.0000 - rmse: 19297.0898 - val_loss: 999907136.0000 - val_rmse: 31621.3066\n",
      "Epoch 108/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366972224.0000 - rmse: 19156.5176 - val_loss: 818659648.0000 - val_rmse: 28612.2266\n",
      "Epoch 109/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322822816.0000 - rmse: 17967.2695 - val_loss: 571825600.0000 - val_rmse: 23912.8750\n",
      "Epoch 110/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448773824.0000 - rmse: 21184.2812 - val_loss: 692623616.0000 - val_rmse: 26317.7441\n",
      "Epoch 111/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403551872.0000 - rmse: 20088.5996 - val_loss: 679012224.0000 - val_rmse: 26057.8633\n",
      "Epoch 112/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382261920.0000 - rmse: 19551.5195 - val_loss: 1229979008.0000 - val_rmse: 35071.0547\n",
      "Epoch 113/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337366336.0000 - rmse: 18367.5332 - val_loss: 595973760.0000 - val_rmse: 24412.5723\n",
      "Epoch 114/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366168704.0000 - rmse: 19135.5332 - val_loss: 818271104.0000 - val_rmse: 28605.4375\n",
      "Epoch 115/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366499584.0000 - rmse: 19144.1738 - val_loss: 807266688.0000 - val_rmse: 28412.4395\n",
      "Epoch 116/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422104096.0000 - rmse: 20545.1719 - val_loss: 818555136.0000 - val_rmse: 28610.4023\n",
      "Epoch 117/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351012544.0000 - rmse: 18735.3262 - val_loss: 931033216.0000 - val_rmse: 30512.8359\n",
      "Epoch 118/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355946240.0000 - rmse: 18866.5352 - val_loss: 746075328.0000 - val_rmse: 27314.3789\n",
      "Epoch 119/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344572000.0000 - rmse: 18562.6484 - val_loss: 572206464.0000 - val_rmse: 23920.8379\n",
      "Epoch 120/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367310528.0000 - rmse: 19165.3477 - val_loss: 582784320.0000 - val_rmse: 24140.9258\n",
      "Epoch 121/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431299168.0000 - rmse: 20767.7422 - val_loss: 807293568.0000 - val_rmse: 28412.9102\n",
      "Epoch 122/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324781952.0000 - rmse: 18021.7051 - val_loss: 1053128192.0000 - val_rmse: 32451.9375\n",
      "Epoch 123/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309405120.0000 - rmse: 17589.9141 - val_loss: 807419840.0000 - val_rmse: 28415.1328\n",
      "Epoch 124/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344222720.0000 - rmse: 18553.2383 - val_loss: 677635712.0000 - val_rmse: 26031.4355\n",
      "Epoch 125/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305047232.0000 - rmse: 17465.5996 - val_loss: 704742272.0000 - val_rmse: 26546.9805\n",
      "Epoch 126/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307593312.0000 - rmse: 17538.3379 - val_loss: 855293056.0000 - val_rmse: 29245.3906\n",
      "Epoch 127/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303140256.0000 - rmse: 17410.9238 - val_loss: 789064640.0000 - val_rmse: 28090.2930\n",
      "Epoch 128/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300066464.0000 - rmse: 17322.4258 - val_loss: 1008768640.0000 - val_rmse: 31761.1172\n",
      "Epoch 129/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332837920.0000 - rmse: 18243.8438 - val_loss: 749285696.0000 - val_rmse: 27373.0840\n",
      "Epoch 130/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386768768.0000 - rmse: 19666.4355 - val_loss: 714141056.0000 - val_rmse: 26723.4180\n",
      "Epoch 131/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366925792.0000 - rmse: 19155.3066 - val_loss: 678488960.0000 - val_rmse: 26047.8184\n",
      "Epoch 132/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337906656.0000 - rmse: 18382.2383 - val_loss: 1861152128.0000 - val_rmse: 43141.0742\n",
      "Epoch 133/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363856448.0000 - rmse: 19075.0215 - val_loss: 935051072.0000 - val_rmse: 30578.6035\n",
      "Epoch 134/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285024704.0000 - rmse: 16882.6738 - val_loss: 1091594880.0000 - val_rmse: 33039.2930\n",
      "Epoch 135/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322050016.0000 - rmse: 17945.7520 - val_loss: 869131392.0000 - val_rmse: 29481.0332\n",
      "Epoch 136/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314818976.0000 - rmse: 17743.1387 - val_loss: 1094800000.0000 - val_rmse: 33087.7617\n",
      "Epoch 137/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313768768.0000 - rmse: 17713.5176 - val_loss: 1028692032.0000 - val_rmse: 32073.2285\n",
      "Epoch 138/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347586656.0000 - rmse: 18643.6758 - val_loss: 1030878848.0000 - val_rmse: 32107.3008\n",
      "Epoch 139/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273756128.0000 - rmse: 16545.5762 - val_loss: 957566016.0000 - val_rmse: 30944.5625\n",
      "Epoch 140/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284180608.0000 - rmse: 16857.6562 - val_loss: 891486976.0000 - val_rmse: 29857.7773\n",
      "Epoch 141/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299822752.0000 - rmse: 17315.3906 - val_loss: 1043835200.0000 - val_rmse: 32308.4395\n",
      "Epoch 142/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322843264.0000 - rmse: 17967.8398 - val_loss: 1053572800.0000 - val_rmse: 32458.7871\n",
      "Epoch 143/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273304448.0000 - rmse: 16531.9219 - val_loss: 925547136.0000 - val_rmse: 30422.8047\n",
      "Epoch 144/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277748512.0000 - rmse: 16665.7852 - val_loss: 1425751168.0000 - val_rmse: 37759.1211\n",
      "Epoch 145/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295975648.0000 - rmse: 17203.9414 - val_loss: 968821248.0000 - val_rmse: 31125.8906\n",
      "Epoch 146/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296791104.0000 - rmse: 17227.6250 - val_loss: 884109696.0000 - val_rmse: 29733.9805\n",
      "Epoch 147/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293200256.0000 - rmse: 17123.0898 - val_loss: 1023762560.0000 - val_rmse: 31996.2891\n",
      "Epoch 148/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264243520.0000 - rmse: 16255.5674 - val_loss: 833226112.0000 - val_rmse: 28865.6562\n",
      "Epoch 149/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269156192.0000 - rmse: 16405.9785 - val_loss: 897906816.0000 - val_rmse: 29965.0938\n",
      "Epoch 150/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326736864.0000 - rmse: 18075.8633 - val_loss: 730805824.0000 - val_rmse: 27033.4199\n",
      "Epoch 151/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256708544.0000 - rmse: 16022.1260 - val_loss: 1099149440.0000 - val_rmse: 33153.4219\n",
      "Epoch 152/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287591296.0000 - rmse: 16958.5156 - val_loss: 1331773184.0000 - val_rmse: 36493.4648\n",
      "Epoch 153/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263190272.0000 - rmse: 16223.1377 - val_loss: 756755584.0000 - val_rmse: 27509.1914\n",
      "Epoch 154/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313425280.0000 - rmse: 17703.8203 - val_loss: 656046976.0000 - val_rmse: 25613.4121\n",
      "Epoch 155/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266019392.0000 - rmse: 16310.0986 - val_loss: 1331380352.0000 - val_rmse: 36488.0859\n",
      "Epoch 156/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294670816.0000 - rmse: 17165.9766 - val_loss: 681161664.0000 - val_rmse: 26099.0742\n",
      "Epoch 157/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 233436784.0000 - rmse: 15278.6357 - val_loss: 855248192.0000 - val_rmse: 29244.6250\n",
      "Epoch 158/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302852384.0000 - rmse: 17402.6523 - val_loss: 1052058048.0000 - val_rmse: 32435.4434\n",
      "Epoch 159/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269086368.0000 - rmse: 16403.8516 - val_loss: 1126261504.0000 - val_rmse: 33559.8203\n",
      "Epoch 160/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257563568.0000 - rmse: 16048.7861 - val_loss: 633076352.0000 - val_rmse: 25161.0078\n",
      "Epoch 161/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239561200.0000 - rmse: 15477.7637 - val_loss: 768005824.0000 - val_rmse: 27712.9180\n",
      "Epoch 162/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288884032.0000 - rmse: 16996.5879 - val_loss: 647076032.0000 - val_rmse: 25437.6875\n",
      "Epoch 163/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253174064.0000 - rmse: 15911.4434 - val_loss: 871215296.0000 - val_rmse: 29516.3535\n",
      "Epoch 164/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220316448.0000 - rmse: 14843.0586 - val_loss: 668361984.0000 - val_rmse: 25852.6973\n",
      "Epoch 165/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287367872.0000 - rmse: 16951.9277 - val_loss: 748793216.0000 - val_rmse: 27364.0859\n",
      "Epoch 166/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293857216.0000 - rmse: 17142.2637 - val_loss: 1012972800.0000 - val_rmse: 31827.2344\n",
      "Epoch 167/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238928352.0000 - rmse: 15457.3066 - val_loss: 871936064.0000 - val_rmse: 29528.5605\n",
      "Epoch 168/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264488992.0000 - rmse: 16263.1152 - val_loss: 966753600.0000 - val_rmse: 31092.6621\n",
      "Epoch 169/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218217552.0000 - rmse: 14772.1885 - val_loss: 963652800.0000 - val_rmse: 31042.7578\n",
      "Epoch 170/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268812544.0000 - rmse: 16395.5020 - val_loss: 1290826880.0000 - val_rmse: 35928.0781\n",
      "Epoch 171/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273972064.0000 - rmse: 16552.0996 - val_loss: 992660736.0000 - val_rmse: 31506.5176\n",
      "Epoch 172/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259303216.0000 - rmse: 16102.8916 - val_loss: 776845824.0000 - val_rmse: 27871.9531\n",
      "Epoch 173/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232503680.0000 - rmse: 15248.0693 - val_loss: 656497536.0000 - val_rmse: 25622.2051\n",
      "Epoch 174/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245211440.0000 - rmse: 15659.2275 - val_loss: 679394432.0000 - val_rmse: 26065.1953\n",
      "Epoch 175/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267591568.0000 - rmse: 16358.2256 - val_loss: 802830592.0000 - val_rmse: 28334.2637\n",
      "Epoch 176/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 199275728.0000 - rmse: 14116.5029 - val_loss: 658127744.0000 - val_rmse: 25654.0000\n",
      "Epoch 177/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269165984.0000 - rmse: 16406.2773 - val_loss: 725105792.0000 - val_rmse: 26927.7891\n",
      "Epoch 178/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264363648.0000 - rmse: 16259.2617 - val_loss: 793359040.0000 - val_rmse: 28166.6309\n",
      "Epoch 179/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235257072.0000 - rmse: 15338.0908 - val_loss: 968457024.0000 - val_rmse: 31120.0430\n",
      "Epoch 180/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217677920.0000 - rmse: 14753.9102 - val_loss: 871282304.0000 - val_rmse: 29517.4922\n",
      "Epoch 181/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308313632.0000 - rmse: 17558.8574 - val_loss: 807613312.0000 - val_rmse: 28418.5371\n",
      "Epoch 182/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 208934528.0000 - rmse: 14454.5664 - val_loss: 978027200.0000 - val_rmse: 31273.4238\n",
      "Epoch 183/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 213289312.0000 - rmse: 14604.4248 - val_loss: 773846400.0000 - val_rmse: 27818.0957\n",
      "Epoch 184/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262544800.0000 - rmse: 16203.2334 - val_loss: 1075222784.0000 - val_rmse: 32790.5898\n",
      "Epoch 185/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237135232.0000 - rmse: 15399.1934 - val_loss: 894796480.0000 - val_rmse: 29913.1484\n",
      "Epoch 186/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258273840.0000 - rmse: 16070.8984 - val_loss: 876275136.0000 - val_rmse: 29601.9453\n",
      "Epoch 187/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264857744.0000 - rmse: 16274.4473 - val_loss: 832494592.0000 - val_rmse: 28852.9805\n",
      "Epoch 188/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230587920.0000 - rmse: 15185.1211 - val_loss: 682548480.0000 - val_rmse: 26125.6270\n",
      "Epoch 189/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236832960.0000 - rmse: 15389.3740 - val_loss: 710001088.0000 - val_rmse: 26645.8457\n",
      "Epoch 190/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294350688.0000 - rmse: 17156.6484 - val_loss: 754193408.0000 - val_rmse: 27462.5781\n",
      "Epoch 191/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243826720.0000 - rmse: 15614.9502 - val_loss: 1522129920.0000 - val_rmse: 39014.4844\n",
      "Epoch 192/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216200096.0000 - rmse: 14703.7441 - val_loss: 1221984512.0000 - val_rmse: 34956.8945\n",
      "Epoch 193/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235022816.0000 - rmse: 15330.4521 - val_loss: 1219152512.0000 - val_rmse: 34916.3633\n",
      "Epoch 194/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 212106128.0000 - rmse: 14563.8623 - val_loss: 976482624.0000 - val_rmse: 31248.7207\n",
      "Epoch 195/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222020560.0000 - rmse: 14900.3535 - val_loss: 820672576.0000 - val_rmse: 28647.3828\n",
      "Epoch 196/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 225490432.0000 - rmse: 15016.3379 - val_loss: 939121664.0000 - val_rmse: 30645.0898\n",
      "Epoch 197/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269107648.0000 - rmse: 16404.5000 - val_loss: 789052288.0000 - val_rmse: 28090.0742\n",
      "Epoch 198/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254915136.0000 - rmse: 15966.0596 - val_loss: 851237056.0000 - val_rmse: 29175.9648\n",
      "Epoch 199/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 205105632.0000 - rmse: 14321.5078 - val_loss: 1205092480.0000 - val_rmse: 34714.4414\n",
      "Epoch 200/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210109472.0000 - rmse: 14495.1533 - val_loss: 1400870400.0000 - val_rmse: 37428.1992\n",
      "Epoch 201/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218858112.0000 - rmse: 14793.8535 - val_loss: 807555712.0000 - val_rmse: 28417.5234\n",
      "104/104 [==============================] - 0s 682us/step - loss: 825623616.0000 - rmse: 28733.6641\n",
      "[825623616.0, 28733.6640625]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 12977240064.0000 - rmse: 113917.6875 - val_loss: 1971859968.0000 - val_rmse: 44405.6289\n",
      "Epoch 2/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2162976512.0000 - rmse: 46507.8125 - val_loss: 1345019264.0000 - val_rmse: 36674.5039\n",
      "Epoch 3/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1824706944.0000 - rmse: 42716.5898 - val_loss: 1149575808.0000 - val_rmse: 33905.3945\n",
      "Epoch 4/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1640099072.0000 - rmse: 40498.1367 - val_loss: 1050106432.0000 - val_rmse: 32405.3457\n",
      "Epoch 5/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1530197504.0000 - rmse: 39117.7383 - val_loss: 990799744.0000 - val_rmse: 31476.9707\n",
      "Epoch 6/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1498807680.0000 - rmse: 38714.4375 - val_loss: 957623808.0000 - val_rmse: 30945.4980\n",
      "Epoch 7/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1390677120.0000 - rmse: 37291.7852 - val_loss: 903564352.0000 - val_rmse: 30059.3477\n",
      "Epoch 8/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1346802176.0000 - rmse: 36698.8047 - val_loss: 895188096.0000 - val_rmse: 29919.6934\n",
      "Epoch 9/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1278313344.0000 - rmse: 35753.5078 - val_loss: 869135808.0000 - val_rmse: 29481.1094\n",
      "Epoch 10/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1266948736.0000 - rmse: 35594.2227 - val_loss: 902243968.0000 - val_rmse: 30037.3770\n",
      "Epoch 11/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1218746112.0000 - rmse: 34910.5430 - val_loss: 868225664.0000 - val_rmse: 29465.6699\n",
      "Epoch 12/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1186777088.0000 - rmse: 34449.6328 - val_loss: 859605504.0000 - val_rmse: 29319.0293\n",
      "Epoch 13/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1155382656.0000 - rmse: 33990.9219 - val_loss: 844524992.0000 - val_rmse: 29060.7129\n",
      "Epoch 14/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1124061056.0000 - rmse: 33527.0195 - val_loss: 851752384.0000 - val_rmse: 29184.7969\n",
      "Epoch 15/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1125142784.0000 - rmse: 33543.1484 - val_loss: 848817152.0000 - val_rmse: 29134.4668\n",
      "Epoch 16/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1118243456.0000 - rmse: 33440.1484 - val_loss: 872601344.0000 - val_rmse: 29539.8262\n",
      "Epoch 17/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1033291520.0000 - rmse: 32144.8516 - val_loss: 858095680.0000 - val_rmse: 29293.2695\n",
      "Epoch 18/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1001277184.0000 - rmse: 31642.9648 - val_loss: 952214912.0000 - val_rmse: 30857.9805\n",
      "Epoch 19/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 989625792.0000 - rmse: 31458.3184 - val_loss: 853390976.0000 - val_rmse: 29212.8555\n",
      "Epoch 20/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 971434560.0000 - rmse: 31167.8457 - val_loss: 901553408.0000 - val_rmse: 30025.8789\n",
      "Epoch 21/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 976354752.0000 - rmse: 31246.6758 - val_loss: 880407424.0000 - val_rmse: 29671.6582\n",
      "Epoch 22/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 956717440.0000 - rmse: 30930.8496 - val_loss: 861167808.0000 - val_rmse: 29345.6602\n",
      "Epoch 23/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 939624832.0000 - rmse: 30653.3008 - val_loss: 940179328.0000 - val_rmse: 30662.3438\n",
      "Epoch 24/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 869048576.0000 - rmse: 29479.6289 - val_loss: 823191104.0000 - val_rmse: 28691.3066\n",
      "Epoch 25/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 892004224.0000 - rmse: 29866.4375 - val_loss: 879801344.0000 - val_rmse: 29661.4453\n",
      "Epoch 26/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 863579200.0000 - rmse: 29386.7188 - val_loss: 1084923136.0000 - val_rmse: 32938.1719\n",
      "Epoch 27/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 836776768.0000 - rmse: 28927.0938 - val_loss: 848868800.0000 - val_rmse: 29135.3535\n",
      "Epoch 28/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 860897984.0000 - rmse: 29341.0625 - val_loss: 853741696.0000 - val_rmse: 29218.8594\n",
      "Epoch 29/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 870512448.0000 - rmse: 29504.4473 - val_loss: 800371968.0000 - val_rmse: 28290.8457\n",
      "Epoch 30/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 820996288.0000 - rmse: 28653.0332 - val_loss: 865972608.0000 - val_rmse: 29427.4121\n",
      "Epoch 31/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 765487680.0000 - rmse: 27667.4473 - val_loss: 965375936.0000 - val_rmse: 31070.5000\n",
      "Epoch 32/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 765614144.0000 - rmse: 27669.7324 - val_loss: 818820800.0000 - val_rmse: 28615.0449\n",
      "Epoch 33/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 798210176.0000 - rmse: 28252.6133 - val_loss: 830223744.0000 - val_rmse: 28813.6035\n",
      "Epoch 34/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742602752.0000 - rmse: 27250.7383 - val_loss: 805028544.0000 - val_rmse: 28373.0254\n",
      "Epoch 35/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742826688.0000 - rmse: 27254.8477 - val_loss: 817424384.0000 - val_rmse: 28590.6348\n",
      "Epoch 36/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 720092160.0000 - rmse: 26834.5332 - val_loss: 1015450496.0000 - val_rmse: 31866.1348\n",
      "Epoch 37/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 770148480.0000 - rmse: 27751.5488 - val_loss: 750932544.0000 - val_rmse: 27403.1484\n",
      "Epoch 38/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 683817216.0000 - rmse: 26149.8984 - val_loss: 1096325760.0000 - val_rmse: 33110.8086\n",
      "Epoch 39/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 715119680.0000 - rmse: 26741.7227 - val_loss: 773536512.0000 - val_rmse: 27812.5234\n",
      "Epoch 40/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 722564416.0000 - rmse: 26880.5586 - val_loss: 841238336.0000 - val_rmse: 29004.1094\n",
      "Epoch 41/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637799936.0000 - rmse: 25254.6992 - val_loss: 971278592.0000 - val_rmse: 31165.3438\n",
      "Epoch 42/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 627520000.0000 - rmse: 25050.3496 - val_loss: 865679168.0000 - val_rmse: 29422.4258\n",
      "Epoch 43/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580763904.0000 - rmse: 24099.0430 - val_loss: 1666424064.0000 - val_rmse: 40821.8594\n",
      "Epoch 44/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587286784.0000 - rmse: 24234.0000 - val_loss: 1047125696.0000 - val_rmse: 32359.3223\n",
      "Epoch 45/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629353728.0000 - rmse: 25086.9238 - val_loss: 832856832.0000 - val_rmse: 28859.2598\n",
      "Epoch 46/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622387776.0000 - rmse: 24947.7012 - val_loss: 803152576.0000 - val_rmse: 28339.9473\n",
      "Epoch 47/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 613772224.0000 - rmse: 24774.4258 - val_loss: 1140332672.0000 - val_rmse: 33768.8086\n",
      "Epoch 48/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618052224.0000 - rmse: 24860.6562 - val_loss: 751702784.0000 - val_rmse: 27417.1992\n",
      "Epoch 49/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596596352.0000 - rmse: 24425.3223 - val_loss: 820530880.0000 - val_rmse: 28644.9102\n",
      "Epoch 50/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596398144.0000 - rmse: 24421.2637 - val_loss: 794381696.0000 - val_rmse: 28184.7773\n",
      "Epoch 51/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533099648.0000 - rmse: 23088.9512 - val_loss: 740029184.0000 - val_rmse: 27203.4766\n",
      "Epoch 52/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601620992.0000 - rmse: 24527.9629 - val_loss: 749048704.0000 - val_rmse: 27368.7539\n",
      "Epoch 53/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614115968.0000 - rmse: 24781.3633 - val_loss: 994087680.0000 - val_rmse: 31529.1562\n",
      "Epoch 54/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549044608.0000 - rmse: 23431.6992 - val_loss: 1065187648.0000 - val_rmse: 32637.2129\n",
      "Epoch 55/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571129408.0000 - rmse: 23898.3145 - val_loss: 701723328.0000 - val_rmse: 26490.0605\n",
      "Epoch 56/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557349824.0000 - rmse: 23608.2578 - val_loss: 885410496.0000 - val_rmse: 29755.8477\n",
      "Epoch 57/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493457984.0000 - rmse: 22213.9141 - val_loss: 978040320.0000 - val_rmse: 31273.6348\n",
      "Epoch 58/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548426304.0000 - rmse: 23418.5020 - val_loss: 656656384.0000 - val_rmse: 25625.3066\n",
      "Epoch 59/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512926464.0000 - rmse: 22647.8809 - val_loss: 1283730560.0000 - val_rmse: 35829.1875\n",
      "Epoch 60/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511039296.0000 - rmse: 22606.1777 - val_loss: 677239296.0000 - val_rmse: 26023.8223\n",
      "Epoch 61/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 556440000.0000 - rmse: 23588.9785 - val_loss: 584320640.0000 - val_rmse: 24172.7246\n",
      "Epoch 62/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 581547776.0000 - rmse: 24115.3008 - val_loss: 730437632.0000 - val_rmse: 27026.6094\n",
      "Epoch 63/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540287168.0000 - rmse: 23244.0781 - val_loss: 1256126208.0000 - val_rmse: 35441.8672\n",
      "Epoch 64/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547780224.0000 - rmse: 23404.7051 - val_loss: 641016320.0000 - val_rmse: 25318.3008\n",
      "Epoch 65/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 524991552.0000 - rmse: 22912.6934 - val_loss: 1467158400.0000 - val_rmse: 38303.5039\n",
      "Epoch 66/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510161920.0000 - rmse: 22586.7637 - val_loss: 691732800.0000 - val_rmse: 26300.8125\n",
      "Epoch 67/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527350560.0000 - rmse: 22964.1152 - val_loss: 751299840.0000 - val_rmse: 27409.8496\n",
      "Epoch 68/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436034656.0000 - rmse: 20881.4434 - val_loss: 804161728.0000 - val_rmse: 28357.7461\n",
      "Epoch 69/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509361984.0000 - rmse: 22569.0488 - val_loss: 932702656.0000 - val_rmse: 30540.1816\n",
      "Epoch 70/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476483744.0000 - rmse: 21828.5078 - val_loss: 736728640.0000 - val_rmse: 27142.7461\n",
      "Epoch 71/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457723808.0000 - rmse: 21394.4805 - val_loss: 920050880.0000 - val_rmse: 30332.3398\n",
      "Epoch 72/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480520160.0000 - rmse: 21920.7695 - val_loss: 912544320.0000 - val_rmse: 30208.3477\n",
      "Epoch 73/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443499296.0000 - rmse: 21059.4219 - val_loss: 1179335040.0000 - val_rmse: 34341.4492\n",
      "Epoch 74/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447706880.0000 - rmse: 21159.0840 - val_loss: 982114368.0000 - val_rmse: 31338.7012\n",
      "Epoch 75/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499136992.0000 - rmse: 22341.3730 - val_loss: 1353601920.0000 - val_rmse: 36791.3281\n",
      "Epoch 76/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428357568.0000 - rmse: 20696.8008 - val_loss: 1304698880.0000 - val_rmse: 36120.6172\n",
      "Epoch 77/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431143776.0000 - rmse: 20764.0020 - val_loss: 1211172480.0000 - val_rmse: 34801.9023\n",
      "Epoch 78/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459901856.0000 - rmse: 21445.3223 - val_loss: 828225280.0000 - val_rmse: 28778.9023\n",
      "Epoch 79/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467966112.0000 - rmse: 21632.5234 - val_loss: 854392832.0000 - val_rmse: 29229.9980\n",
      "Epoch 80/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444513344.0000 - rmse: 21083.4844 - val_loss: 770189056.0000 - val_rmse: 27752.2793\n",
      "Epoch 81/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440585088.0000 - rmse: 20990.1191 - val_loss: 781376576.0000 - val_rmse: 27953.1133\n",
      "Epoch 82/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462304672.0000 - rmse: 21501.2715 - val_loss: 614883264.0000 - val_rmse: 24796.8398\n",
      "Epoch 83/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367478368.0000 - rmse: 19169.7227 - val_loss: 859362752.0000 - val_rmse: 29314.8887\n",
      "Epoch 84/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411972384.0000 - rmse: 20297.1016 - val_loss: 782903872.0000 - val_rmse: 27980.4180\n",
      "Epoch 85/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495614496.0000 - rmse: 22262.4004 - val_loss: 992253952.0000 - val_rmse: 31500.0605\n",
      "Epoch 86/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 458732992.0000 - rmse: 21418.0508 - val_loss: 1141258240.0000 - val_rmse: 33782.5156\n",
      "Epoch 87/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407451008.0000 - rmse: 20185.4141 - val_loss: 1013504960.0000 - val_rmse: 31835.5918\n",
      "Epoch 88/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392249792.0000 - rmse: 19805.2969 - val_loss: 779470720.0000 - val_rmse: 27919.0020\n",
      "Epoch 89/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413990336.0000 - rmse: 20346.7520 - val_loss: 926905920.0000 - val_rmse: 30445.1289\n",
      "Epoch 90/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476810944.0000 - rmse: 21836.0020 - val_loss: 876551680.0000 - val_rmse: 29606.6152\n",
      "Epoch 91/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348738112.0000 - rmse: 18674.5293 - val_loss: 1491656320.0000 - val_rmse: 38621.9688\n",
      "Epoch 92/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495200896.0000 - rmse: 22253.1094 - val_loss: 730780928.0000 - val_rmse: 27032.9609\n",
      "Epoch 93/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363392864.0000 - rmse: 19062.8652 - val_loss: 1253591936.0000 - val_rmse: 35406.1016\n",
      "Epoch 94/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396128960.0000 - rmse: 19902.9883 - val_loss: 1209599488.0000 - val_rmse: 34779.2969\n",
      "Epoch 95/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392083968.0000 - rmse: 19801.1094 - val_loss: 2110040832.0000 - val_rmse: 45935.1797\n",
      "Epoch 96/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334513824.0000 - rmse: 18289.7188 - val_loss: 1394813696.0000 - val_rmse: 37347.2031\n",
      "Epoch 97/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386313472.0000 - rmse: 19654.8574 - val_loss: 1595494656.0000 - val_rmse: 39943.6445\n",
      "Epoch 98/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344917952.0000 - rmse: 18571.9668 - val_loss: 1625657088.0000 - val_rmse: 40319.4375\n",
      "Epoch 99/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378918976.0000 - rmse: 19465.8398 - val_loss: 945642496.0000 - val_rmse: 30751.3008\n",
      "Epoch 100/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404791168.0000 - rmse: 20119.4219 - val_loss: 1179907584.0000 - val_rmse: 34349.7812\n",
      "Epoch 101/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365861920.0000 - rmse: 19127.5156 - val_loss: 800591296.0000 - val_rmse: 28294.7207\n",
      "Epoch 102/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351868160.0000 - rmse: 18758.1484 - val_loss: 1636679808.0000 - val_rmse: 40455.8984\n",
      "Epoch 103/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385669248.0000 - rmse: 19638.4629 - val_loss: 1032577728.0000 - val_rmse: 32133.7480\n",
      "Epoch 104/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383253920.0000 - rmse: 19576.8711 - val_loss: 1029111360.0000 - val_rmse: 32079.7656\n",
      "Epoch 105/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356400544.0000 - rmse: 18878.5723 - val_loss: 860025280.0000 - val_rmse: 29326.1875\n",
      "Epoch 106/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339636832.0000 - rmse: 18429.2383 - val_loss: 772166720.0000 - val_rmse: 27787.8887\n",
      "Epoch 107/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361863776.0000 - rmse: 19022.7148 - val_loss: 881680320.0000 - val_rmse: 29693.1016\n",
      "Epoch 108/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404140864.0000 - rmse: 20103.2559 - val_loss: 898078272.0000 - val_rmse: 29967.9531\n",
      "Epoch 109/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363988608.0000 - rmse: 19078.4844 - val_loss: 1246654336.0000 - val_rmse: 35307.9922\n",
      "Epoch 110/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421734368.0000 - rmse: 20536.1719 - val_loss: 1572795776.0000 - val_rmse: 39658.4883\n",
      "Epoch 111/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332950912.0000 - rmse: 18246.9414 - val_loss: 798445312.0000 - val_rmse: 28256.7754\n",
      "Epoch 112/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315938400.0000 - rmse: 17774.6543 - val_loss: 1223597952.0000 - val_rmse: 34979.9648\n",
      "Epoch 113/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377750400.0000 - rmse: 19435.8008 - val_loss: 1167326976.0000 - val_rmse: 34166.1680\n",
      "Epoch 114/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527392160.0000 - rmse: 22965.0176 - val_loss: 1059630208.0000 - val_rmse: 32551.9609\n",
      "Epoch 115/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309108832.0000 - rmse: 17581.4902 - val_loss: 2108798336.0000 - val_rmse: 45921.6562\n",
      "Epoch 116/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353651136.0000 - rmse: 18805.6152 - val_loss: 1322557952.0000 - val_rmse: 36366.9883\n",
      "Epoch 117/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335467776.0000 - rmse: 18315.7773 - val_loss: 1341537536.0000 - val_rmse: 36627.0039\n",
      "Epoch 118/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329794336.0000 - rmse: 18160.2402 - val_loss: 1446368640.0000 - val_rmse: 38031.1484\n",
      "Epoch 119/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382525728.0000 - rmse: 19558.2656 - val_loss: 839747456.0000 - val_rmse: 28978.3965\n",
      "Epoch 120/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304873280.0000 - rmse: 17460.6191 - val_loss: 1856794624.0000 - val_rmse: 43090.5391\n",
      "Epoch 121/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379720512.0000 - rmse: 19486.4180 - val_loss: 1044190848.0000 - val_rmse: 32313.9395\n",
      "Epoch 122/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295094976.0000 - rmse: 17178.3281 - val_loss: 1442376192.0000 - val_rmse: 37978.6250\n",
      "Epoch 123/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329308544.0000 - rmse: 18146.8594 - val_loss: 1711797760.0000 - val_rmse: 41373.8789\n",
      "Epoch 124/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323631360.0000 - rmse: 17989.7559 - val_loss: 2227210496.0000 - val_rmse: 47193.3320\n",
      "Epoch 125/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358461952.0000 - rmse: 18933.0898 - val_loss: 1516203520.0000 - val_rmse: 38938.4531\n",
      "Epoch 126/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334374880.0000 - rmse: 18285.9180 - val_loss: 1533952512.0000 - val_rmse: 39165.7070\n",
      "Epoch 127/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396834560.0000 - rmse: 19920.7051 - val_loss: 2542593792.0000 - val_rmse: 50424.1406\n",
      "Epoch 128/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355359712.0000 - rmse: 18850.9863 - val_loss: 1122413824.0000 - val_rmse: 33502.4414\n",
      "Epoch 129/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325787936.0000 - rmse: 18049.5957 - val_loss: 1178998144.0000 - val_rmse: 34336.5391\n",
      "Epoch 130/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317166688.0000 - rmse: 17809.1738 - val_loss: 1335602688.0000 - val_rmse: 36545.8984\n",
      "Epoch 131/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357331968.0000 - rmse: 18903.2266 - val_loss: 1928331648.0000 - val_rmse: 43912.7734\n",
      "Epoch 132/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351443008.0000 - rmse: 18746.8125 - val_loss: 1162033664.0000 - val_rmse: 34088.6133\n",
      "Epoch 133/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367912864.0000 - rmse: 19181.0527 - val_loss: 2767516928.0000 - val_rmse: 52607.1953\n",
      "Epoch 134/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301592736.0000 - rmse: 17366.4238 - val_loss: 1489349888.0000 - val_rmse: 38592.0938\n",
      "Epoch 135/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304234848.0000 - rmse: 17442.3281 - val_loss: 1435870720.0000 - val_rmse: 37892.8828\n",
      "Epoch 136/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309005888.0000 - rmse: 17578.5605 - val_loss: 1765435648.0000 - val_rmse: 42017.0859\n",
      "Epoch 137/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305661376.0000 - rmse: 17483.1738 - val_loss: 1357102208.0000 - val_rmse: 36838.8672\n",
      "Epoch 138/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288808448.0000 - rmse: 16994.3633 - val_loss: 1057691776.0000 - val_rmse: 32522.1719\n",
      "Epoch 139/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360653056.0000 - rmse: 18990.8652 - val_loss: 1305067392.0000 - val_rmse: 36125.7148\n",
      "Epoch 140/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291269920.0000 - rmse: 17066.6309 - val_loss: 1312719360.0000 - val_rmse: 36231.4688\n",
      "Epoch 141/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388722048.0000 - rmse: 19716.0332 - val_loss: 1529130752.0000 - val_rmse: 39104.1016\n",
      "Epoch 142/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256615584.0000 - rmse: 16019.2236 - val_loss: 1507724544.0000 - val_rmse: 38829.4297\n",
      "Epoch 143/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263074368.0000 - rmse: 16219.5664 - val_loss: 1359088640.0000 - val_rmse: 36865.8203\n",
      "Epoch 144/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279942560.0000 - rmse: 16731.4824 - val_loss: 1371019008.0000 - val_rmse: 37027.2734\n",
      "Epoch 145/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285077984.0000 - rmse: 16884.2500 - val_loss: 1082295040.0000 - val_rmse: 32898.2539\n",
      "Epoch 146/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284889920.0000 - rmse: 16878.6797 - val_loss: 839142272.0000 - val_rmse: 28967.9531\n",
      "Epoch 147/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287621760.0000 - rmse: 16959.4160 - val_loss: 2506442496.0000 - val_rmse: 50064.3828\n",
      "Epoch 148/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276960736.0000 - rmse: 16642.1367 - val_loss: 1388705664.0000 - val_rmse: 37265.3398\n",
      "Epoch 149/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377426720.0000 - rmse: 19427.4727 - val_loss: 1356624384.0000 - val_rmse: 36832.3828\n",
      "Epoch 150/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343293280.0000 - rmse: 18528.1719 - val_loss: 2112414976.0000 - val_rmse: 45961.0156\n",
      "Epoch 151/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346203456.0000 - rmse: 18606.5410 - val_loss: 2696523264.0000 - val_rmse: 51928.0586\n",
      "Epoch 152/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296845312.0000 - rmse: 17229.1992 - val_loss: 2308071936.0000 - val_rmse: 48042.3984\n",
      "Epoch 153/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278375104.0000 - rmse: 16684.5762 - val_loss: 1338017920.0000 - val_rmse: 36578.9258\n",
      "Epoch 154/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293638048.0000 - rmse: 17135.8691 - val_loss: 1150833536.0000 - val_rmse: 33923.9375\n",
      "Epoch 155/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299287136.0000 - rmse: 17299.9180 - val_loss: 1455055104.0000 - val_rmse: 38145.1836\n",
      "Epoch 156/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294922656.0000 - rmse: 17173.3125 - val_loss: 1529072256.0000 - val_rmse: 39103.3516\n",
      "Epoch 157/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297413312.0000 - rmse: 17245.6738 - val_loss: 2352274432.0000 - val_rmse: 48500.2500\n",
      "Epoch 158/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261875808.0000 - rmse: 16182.5762 - val_loss: 1057831808.0000 - val_rmse: 32524.3262\n",
      "Epoch 159/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277881888.0000 - rmse: 16669.7891 - val_loss: 1407658240.0000 - val_rmse: 37518.7734\n",
      "Epoch 160/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257544848.0000 - rmse: 16048.2021 - val_loss: 1619185152.0000 - val_rmse: 40239.1016\n",
      "Epoch 161/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301938080.0000 - rmse: 17376.3652 - val_loss: 1050123712.0000 - val_rmse: 32405.6133\n",
      "Epoch 162/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249012576.0000 - rmse: 15780.1309 - val_loss: 1992602624.0000 - val_rmse: 44638.5781\n",
      "Epoch 163/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230589152.0000 - rmse: 15185.1621 - val_loss: 784613056.0000 - val_rmse: 28010.9453\n",
      "Epoch 164/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260522864.0000 - rmse: 16140.7188 - val_loss: 1156351488.0000 - val_rmse: 34005.1680\n",
      "Epoch 165/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270944640.0000 - rmse: 16460.3945 - val_loss: 843469184.0000 - val_rmse: 29042.5410\n",
      "Epoch 166/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263811472.0000 - rmse: 16242.2734 - val_loss: 1130120320.0000 - val_rmse: 33617.2617\n",
      "Epoch 167/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261598944.0000 - rmse: 16174.0195 - val_loss: 1780745856.0000 - val_rmse: 42198.8828\n",
      "Epoch 168/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257571840.0000 - rmse: 16049.0439 - val_loss: 1782714496.0000 - val_rmse: 42222.2031\n",
      "Epoch 169/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303406720.0000 - rmse: 17418.5723 - val_loss: 2722090496.0000 - val_rmse: 52173.6562\n",
      "Epoch 170/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230424432.0000 - rmse: 15179.7363 - val_loss: 1474969856.0000 - val_rmse: 38405.3359\n",
      "Epoch 171/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252574064.0000 - rmse: 15892.5771 - val_loss: 1693179264.0000 - val_rmse: 41148.2578\n",
      "Epoch 172/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297463424.0000 - rmse: 17247.1289 - val_loss: 1328389120.0000 - val_rmse: 36447.0703\n",
      "Epoch 173/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246015152.0000 - rmse: 15684.8701 - val_loss: 1896995456.0000 - val_rmse: 43554.5117\n",
      "Epoch 174/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246455616.0000 - rmse: 15698.9033 - val_loss: 2610994432.0000 - val_rmse: 51097.8906\n",
      "Epoch 175/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277063392.0000 - rmse: 16645.2188 - val_loss: 2046043136.0000 - val_rmse: 45233.2070\n",
      "Epoch 176/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273069536.0000 - rmse: 16524.8125 - val_loss: 1673668736.0000 - val_rmse: 40910.4961\n",
      "Epoch 177/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250594672.0000 - rmse: 15830.1816 - val_loss: 1699744896.0000 - val_rmse: 41227.9609\n",
      "Epoch 178/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239905904.0000 - rmse: 15488.8955 - val_loss: 1614608896.0000 - val_rmse: 40182.1953\n",
      "Epoch 179/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261563120.0000 - rmse: 16172.9121 - val_loss: 1219339904.0000 - val_rmse: 34919.0469\n",
      "Epoch 180/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309888768.0000 - rmse: 17603.6562 - val_loss: 1658037376.0000 - val_rmse: 40719.0039\n",
      "Epoch 181/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242079440.0000 - rmse: 15558.9014 - val_loss: 1751737856.0000 - val_rmse: 41853.7656\n",
      "Epoch 182/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272289024.0000 - rmse: 16501.1816 - val_loss: 1539189248.0000 - val_rmse: 39232.5000\n",
      "Epoch 183/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251801312.0000 - rmse: 15868.2471 - val_loss: 2279250432.0000 - val_rmse: 47741.4961\n",
      "Epoch 184/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258541760.0000 - rmse: 16079.2334 - val_loss: 1776576256.0000 - val_rmse: 42149.4531\n",
      "Epoch 185/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214652848.0000 - rmse: 14651.0322 - val_loss: 1614296320.0000 - val_rmse: 40178.3047\n",
      "Epoch 186/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270866912.0000 - rmse: 16458.0332 - val_loss: 1873096448.0000 - val_rmse: 43279.2852\n",
      "Epoch 187/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301571648.0000 - rmse: 17365.8184 - val_loss: 1684892160.0000 - val_rmse: 41047.4375\n",
      "Epoch 188/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236902832.0000 - rmse: 15391.6475 - val_loss: 1753403648.0000 - val_rmse: 41873.6641\n",
      "Epoch 189/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402611872.0000 - rmse: 20065.1895 - val_loss: 1971260288.0000 - val_rmse: 44398.8789\n",
      "Epoch 190/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270928000.0000 - rmse: 16459.8906 - val_loss: 1522942720.0000 - val_rmse: 39024.8984\n",
      "Epoch 191/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261989568.0000 - rmse: 16186.0908 - val_loss: 1454780544.0000 - val_rmse: 38141.5859\n",
      "Epoch 192/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246240880.0000 - rmse: 15692.0625 - val_loss: 1722943616.0000 - val_rmse: 41508.3555\n",
      "Epoch 193/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296528928.0000 - rmse: 17220.0117 - val_loss: 1504103680.0000 - val_rmse: 38782.7734\n",
      "Epoch 194/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226373440.0000 - rmse: 15045.7100 - val_loss: 2447557888.0000 - val_rmse: 49472.8008\n",
      "Epoch 195/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229761616.0000 - rmse: 15157.8887 - val_loss: 1418938496.0000 - val_rmse: 37668.8008\n",
      "Epoch 196/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270703680.0000 - rmse: 16453.0723 - val_loss: 1185948160.0000 - val_rmse: 34437.5977\n",
      "Epoch 197/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248127376.0000 - rmse: 15752.0576 - val_loss: 1350403072.0000 - val_rmse: 36747.8281\n",
      "Epoch 198/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260001344.0000 - rmse: 16124.5566 - val_loss: 1965959552.0000 - val_rmse: 44339.1406\n",
      "Epoch 199/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228012064.0000 - rmse: 15100.0664 - val_loss: 819312448.0000 - val_rmse: 28623.6328\n",
      "Epoch 200/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 233786400.0000 - rmse: 15290.0732 - val_loss: 1563518976.0000 - val_rmse: 39541.3594\n",
      "Epoch 201/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260328224.0000 - rmse: 16134.6885 - val_loss: 2054300416.0000 - val_rmse: 45324.3906\n",
      "Epoch 202/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 233351968.0000 - rmse: 15275.8613 - val_loss: 2057921024.0000 - val_rmse: 45364.3125\n",
      "Epoch 203/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275781376.0000 - rmse: 16606.6660 - val_loss: 2133826432.0000 - val_rmse: 46193.3594\n",
      "Epoch 204/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214504736.0000 - rmse: 14645.9795 - val_loss: 2964701440.0000 - val_rmse: 54449.0703\n",
      "Epoch 205/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231126704.0000 - rmse: 15202.8516 - val_loss: 1835642880.0000 - val_rmse: 42844.3984\n",
      "Epoch 206/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260590528.0000 - rmse: 16142.8164 - val_loss: 2111701888.0000 - val_rmse: 45953.2578\n",
      "Epoch 207/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229212336.0000 - rmse: 15139.7598 - val_loss: 2296645376.0000 - val_rmse: 47923.3281\n",
      "Epoch 208/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227370304.0000 - rmse: 15078.8027 - val_loss: 2121607936.0000 - val_rmse: 46060.9141\n",
      "Epoch 209/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 225530464.0000 - rmse: 15017.6699 - val_loss: 1076089472.0000 - val_rmse: 32803.8008\n",
      "Epoch 210/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237932128.0000 - rmse: 15425.0469 - val_loss: 2289214208.0000 - val_rmse: 47845.7344\n",
      "Epoch 211/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244345264.0000 - rmse: 15631.5469 - val_loss: 2686660352.0000 - val_rmse: 51833.0039\n",
      "Epoch 212/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258397184.0000 - rmse: 16074.7363 - val_loss: 2892521984.0000 - val_rmse: 53782.1680\n",
      "Epoch 213/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218388960.0000 - rmse: 14777.9883 - val_loss: 3214080768.0000 - val_rmse: 56692.8633\n",
      "Epoch 214/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247357456.0000 - rmse: 15727.5996 - val_loss: 1818687744.0000 - val_rmse: 42646.0742\n",
      "Epoch 215/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257800848.0000 - rmse: 16056.1768 - val_loss: 1393277056.0000 - val_rmse: 37326.6250\n",
      "Epoch 216/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232190880.0000 - rmse: 15237.8096 - val_loss: 2298753024.0000 - val_rmse: 47945.3125\n",
      "Epoch 217/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265010368.0000 - rmse: 16279.1357 - val_loss: 1634848640.0000 - val_rmse: 40433.2617\n",
      "Epoch 218/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232116560.0000 - rmse: 15235.3711 - val_loss: 1956619648.0000 - val_rmse: 44233.6914\n",
      "Epoch 219/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 212608768.0000 - rmse: 14581.1074 - val_loss: 1945706240.0000 - val_rmse: 44110.1562\n",
      "Epoch 220/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299476768.0000 - rmse: 17305.3945 - val_loss: 2566340352.0000 - val_rmse: 50659.0586\n",
      "Epoch 221/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275552896.0000 - rmse: 16599.7832 - val_loss: 1808444288.0000 - val_rmse: 42525.8086\n",
      "Epoch 222/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222746512.0000 - rmse: 14924.6934 - val_loss: 2136471808.0000 - val_rmse: 46221.9844\n",
      "Epoch 223/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219779696.0000 - rmse: 14824.9668 - val_loss: 2222884096.0000 - val_rmse: 47147.4688\n",
      "Epoch 224/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207021712.0000 - rmse: 14388.2471 - val_loss: 1397166080.0000 - val_rmse: 37378.6836\n",
      "Epoch 225/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224049712.0000 - rmse: 14968.2891 - val_loss: 866797824.0000 - val_rmse: 29441.4277\n",
      "Epoch 226/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215489184.0000 - rmse: 14679.5469 - val_loss: 2583555328.0000 - val_rmse: 50828.6875\n",
      "Epoch 227/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236937968.0000 - rmse: 15392.7881 - val_loss: 3154814720.0000 - val_rmse: 56167.7383\n",
      "Epoch 228/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217114032.0000 - rmse: 14734.7900 - val_loss: 3190332672.0000 - val_rmse: 56483.0312\n",
      "Epoch 229/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232764384.0000 - rmse: 15256.6162 - val_loss: 2508227072.0000 - val_rmse: 50082.2031\n",
      "Epoch 230/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217340288.0000 - rmse: 14742.4648 - val_loss: 3082044672.0000 - val_rmse: 55516.1641\n",
      "Epoch 231/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265876736.0000 - rmse: 16305.7256 - val_loss: 1893687168.0000 - val_rmse: 43516.5078\n",
      "Epoch 232/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215868000.0000 - rmse: 14692.4463 - val_loss: 2732476672.0000 - val_rmse: 52273.0977\n",
      "Epoch 233/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226425056.0000 - rmse: 15047.4258 - val_loss: 3215572992.0000 - val_rmse: 56706.0234\n",
      "Epoch 234/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 191813520.0000 - rmse: 13849.6738 - val_loss: 1536363904.0000 - val_rmse: 39196.4766\n",
      "Epoch 235/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 203921296.0000 - rmse: 14280.0996 - val_loss: 2373537024.0000 - val_rmse: 48718.9570\n",
      "Epoch 236/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220219600.0000 - rmse: 14839.7959 - val_loss: 1561949184.0000 - val_rmse: 39521.5039\n",
      "Epoch 237/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268488384.0000 - rmse: 16385.6133 - val_loss: 1766905856.0000 - val_rmse: 42034.5781\n",
      "Epoch 238/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260279632.0000 - rmse: 16133.1826 - val_loss: 1218207744.0000 - val_rmse: 34902.8320\n",
      "Epoch 239/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 189003936.0000 - rmse: 13747.8691 - val_loss: 2895349504.0000 - val_rmse: 53808.4531\n",
      "Epoch 240/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218182784.0000 - rmse: 14771.0088 - val_loss: 1424238080.0000 - val_rmse: 37739.0742\n",
      "Epoch 241/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249611856.0000 - rmse: 15799.1064 - val_loss: 999179008.0000 - val_rmse: 31609.7910\n",
      "Epoch 242/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230273728.0000 - rmse: 15174.7715 - val_loss: 1825359872.0000 - val_rmse: 42724.2305\n",
      "Epoch 243/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 185684560.0000 - rmse: 13626.6094 - val_loss: 2316766976.0000 - val_rmse: 48132.8047\n",
      "Epoch 244/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 201524304.0000 - rmse: 14195.9238 - val_loss: 1149273600.0000 - val_rmse: 33900.9375\n",
      "Epoch 245/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 204037920.0000 - rmse: 14284.1826 - val_loss: 2552293376.0000 - val_rmse: 50520.2266\n",
      "Epoch 246/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237549008.0000 - rmse: 15412.6230 - val_loss: 2216246016.0000 - val_rmse: 47077.0234\n",
      "Epoch 247/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219522272.0000 - rmse: 14816.2812 - val_loss: 1503549312.0000 - val_rmse: 38775.6289\n",
      "Epoch 248/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 195634608.0000 - rmse: 13986.9414 - val_loss: 1478928640.0000 - val_rmse: 38456.8398\n",
      "Epoch 249/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206509712.0000 - rmse: 14370.4443 - val_loss: 2525727232.0000 - val_rmse: 50256.6133\n",
      "Epoch 250/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211881808.0000 - rmse: 14556.1602 - val_loss: 1865183104.0000 - val_rmse: 43187.7656\n",
      "Epoch 251/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 191045568.0000 - rmse: 13821.9209 - val_loss: 2216996864.0000 - val_rmse: 47084.9961\n",
      "Epoch 252/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235406000.0000 - rmse: 15342.9453 - val_loss: 2267661312.0000 - val_rmse: 47619.9688\n",
      "Epoch 253/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231502064.0000 - rmse: 15215.1904 - val_loss: 1132553216.0000 - val_rmse: 33653.4258\n",
      "Epoch 254/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 205316256.0000 - rmse: 14328.8594 - val_loss: 1976500992.0000 - val_rmse: 44457.8555\n",
      "Epoch 255/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224331264.0000 - rmse: 14977.6904 - val_loss: 1671682304.0000 - val_rmse: 40886.2109\n",
      "Epoch 256/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 199713536.0000 - rmse: 14132.0020 - val_loss: 3123921664.0000 - val_rmse: 55892.0547\n",
      "104/104 [==============================] - 0s 700us/step - loss: 768043520.0000 - rmse: 27713.5957\n",
      "[768043520.0, 27713.595703125]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 13379465216.0000 - rmse: 115669.6406 - val_loss: 1886946688.0000 - val_rmse: 43439.0000\n",
      "Epoch 2/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2244169984.0000 - rmse: 47372.6719 - val_loss: 1295917696.0000 - val_rmse: 35998.8555\n",
      "Epoch 3/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1802883072.0000 - rmse: 42460.3711 - val_loss: 1177761024.0000 - val_rmse: 34318.5234\n",
      "Epoch 4/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1648631936.0000 - rmse: 40603.3477 - val_loss: 1153646976.0000 - val_rmse: 33965.3789\n",
      "Epoch 5/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1516406784.0000 - rmse: 38941.0664 - val_loss: 1018710464.0000 - val_rmse: 31917.2441\n",
      "Epoch 6/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1423018112.0000 - rmse: 37722.9141 - val_loss: 968432000.0000 - val_rmse: 31119.6406\n",
      "Epoch 7/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1347268992.0000 - rmse: 36705.1641 - val_loss: 904147392.0000 - val_rmse: 30069.0430\n",
      "Epoch 8/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1344583296.0000 - rmse: 36668.5586 - val_loss: 1002716416.0000 - val_rmse: 31665.6953\n",
      "Epoch 9/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1284006528.0000 - rmse: 35833.0352 - val_loss: 911705728.0000 - val_rmse: 30194.4648\n",
      "Epoch 10/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1298395008.0000 - rmse: 36033.2500 - val_loss: 863272960.0000 - val_rmse: 29381.5078\n",
      "Epoch 11/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1208793472.0000 - rmse: 34767.7070 - val_loss: 886957440.0000 - val_rmse: 29781.8301\n",
      "Epoch 12/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1168237824.0000 - rmse: 34179.4961 - val_loss: 877708672.0000 - val_rmse: 29626.1484\n",
      "Epoch 13/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1147926272.0000 - rmse: 33881.0625 - val_loss: 833524288.0000 - val_rmse: 28870.8203\n",
      "Epoch 14/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1119902336.0000 - rmse: 33464.9414 - val_loss: 939549056.0000 - val_rmse: 30652.0645\n",
      "Epoch 15/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1118830336.0000 - rmse: 33448.9219 - val_loss: 850187200.0000 - val_rmse: 29157.9707\n",
      "Epoch 16/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1033277376.0000 - rmse: 32144.6328 - val_loss: 830893824.0000 - val_rmse: 28825.2285\n",
      "Epoch 17/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1023067776.0000 - rmse: 31985.4316 - val_loss: 806853568.0000 - val_rmse: 28405.1680\n",
      "Epoch 18/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1031685120.0000 - rmse: 32119.8555 - val_loss: 904333184.0000 - val_rmse: 30072.1328\n",
      "Epoch 19/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 987640960.0000 - rmse: 31426.7559 - val_loss: 795710976.0000 - val_rmse: 28208.3496\n",
      "Epoch 20/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 934965248.0000 - rmse: 30577.2012 - val_loss: 835455488.0000 - val_rmse: 28904.2461\n",
      "Epoch 21/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 931380544.0000 - rmse: 30518.5273 - val_loss: 838935424.0000 - val_rmse: 28964.3828\n",
      "Epoch 22/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 958139328.0000 - rmse: 30953.8262 - val_loss: 819160128.0000 - val_rmse: 28620.9727\n",
      "Epoch 23/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 915018432.0000 - rmse: 30249.2715 - val_loss: 783322432.0000 - val_rmse: 27987.8984\n",
      "Epoch 24/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 899638208.0000 - rmse: 29993.9668 - val_loss: 813985088.0000 - val_rmse: 28530.4219\n",
      "Epoch 25/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 911046720.0000 - rmse: 30183.5508 - val_loss: 763566080.0000 - val_rmse: 27632.6992\n",
      "Epoch 26/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 878693824.0000 - rmse: 29642.7695 - val_loss: 939165184.0000 - val_rmse: 30645.8008\n",
      "Epoch 27/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 848784512.0000 - rmse: 29133.9062 - val_loss: 1015685568.0000 - val_rmse: 31869.8223\n",
      "Epoch 28/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 827912000.0000 - rmse: 28773.4609 - val_loss: 839286528.0000 - val_rmse: 28970.4414\n",
      "Epoch 29/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 850154944.0000 - rmse: 29157.4160 - val_loss: 769179648.0000 - val_rmse: 27734.0879\n",
      "Epoch 30/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 837036736.0000 - rmse: 28931.5879 - val_loss: 769226432.0000 - val_rmse: 27734.9316\n",
      "Epoch 31/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 780115776.0000 - rmse: 27930.5527 - val_loss: 918469312.0000 - val_rmse: 30306.2578\n",
      "Epoch 32/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 763953088.0000 - rmse: 27639.6992 - val_loss: 699358336.0000 - val_rmse: 26445.3848\n",
      "Epoch 33/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782763520.0000 - rmse: 27977.9121 - val_loss: 720118656.0000 - val_rmse: 26835.0273\n",
      "Epoch 34/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 793205312.0000 - rmse: 28163.9004 - val_loss: 776983040.0000 - val_rmse: 27874.4141\n",
      "Epoch 35/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 748213120.0000 - rmse: 27353.4844 - val_loss: 812875968.0000 - val_rmse: 28510.9785\n",
      "Epoch 36/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 758577856.0000 - rmse: 27542.2910 - val_loss: 703736704.0000 - val_rmse: 26528.0371\n",
      "Epoch 37/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 732828992.0000 - rmse: 27070.8125 - val_loss: 651232064.0000 - val_rmse: 25519.2480\n",
      "Epoch 38/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 715779648.0000 - rmse: 26754.0586 - val_loss: 718812608.0000 - val_rmse: 26810.6797\n",
      "Epoch 39/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685621120.0000 - rmse: 26184.3672 - val_loss: 830312640.0000 - val_rmse: 28815.1465\n",
      "Epoch 40/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 716400000.0000 - rmse: 26765.6504 - val_loss: 694863296.0000 - val_rmse: 26360.2578\n",
      "Epoch 41/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646787904.0000 - rmse: 25432.0234 - val_loss: 734294720.0000 - val_rmse: 27097.8711\n",
      "Epoch 42/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 627682176.0000 - rmse: 25053.5840 - val_loss: 691638784.0000 - val_rmse: 26299.0254\n",
      "Epoch 43/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606200896.0000 - rmse: 24621.1465 - val_loss: 693176320.0000 - val_rmse: 26328.2422\n",
      "Epoch 44/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718466304.0000 - rmse: 26804.2227 - val_loss: 797538624.0000 - val_rmse: 28240.7246\n",
      "Epoch 45/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593521920.0000 - rmse: 24362.3047 - val_loss: 678466496.0000 - val_rmse: 26047.3887\n",
      "Epoch 46/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572740288.0000 - rmse: 23931.9922 - val_loss: 1035021248.0000 - val_rmse: 32171.7461\n",
      "Epoch 47/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591522368.0000 - rmse: 24321.2324 - val_loss: 787543296.0000 - val_rmse: 28063.2012\n",
      "Epoch 48/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547108800.0000 - rmse: 23390.3574 - val_loss: 648410880.0000 - val_rmse: 25463.9141\n",
      "Epoch 49/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532986144.0000 - rmse: 23086.4922 - val_loss: 607839680.0000 - val_rmse: 24654.4043\n",
      "Epoch 50/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560434560.0000 - rmse: 23673.5000 - val_loss: 697666048.0000 - val_rmse: 26413.3691\n",
      "Epoch 51/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502189792.0000 - rmse: 22409.5898 - val_loss: 719339072.0000 - val_rmse: 26820.4941\n",
      "Epoch 52/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561855232.0000 - rmse: 23703.4863 - val_loss: 668837824.0000 - val_rmse: 25861.8984\n",
      "Epoch 53/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522481696.0000 - rmse: 22857.8594 - val_loss: 741832896.0000 - val_rmse: 27236.6094\n",
      "Epoch 54/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541953280.0000 - rmse: 23279.8906 - val_loss: 1075286656.0000 - val_rmse: 32791.5625\n",
      "Epoch 55/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466506944.0000 - rmse: 21598.7715 - val_loss: 689897216.0000 - val_rmse: 26265.8945\n",
      "Epoch 56/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500253216.0000 - rmse: 22366.3418 - val_loss: 626939648.0000 - val_rmse: 25038.7617\n",
      "Epoch 57/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488993888.0000 - rmse: 22113.2070 - val_loss: 617127040.0000 - val_rmse: 24842.0410\n",
      "Epoch 58/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478325120.0000 - rmse: 21870.6445 - val_loss: 889424000.0000 - val_rmse: 29823.2109\n",
      "Epoch 59/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508672096.0000 - rmse: 22553.7598 - val_loss: 938396928.0000 - val_rmse: 30633.2656\n",
      "Epoch 60/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472893408.0000 - rmse: 21746.1133 - val_loss: 673150976.0000 - val_rmse: 25945.1523\n",
      "Epoch 61/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537292416.0000 - rmse: 23179.5684 - val_loss: 622564736.0000 - val_rmse: 24951.2480\n",
      "Epoch 62/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464489792.0000 - rmse: 21552.0254 - val_loss: 700927232.0000 - val_rmse: 26475.0312\n",
      "Epoch 63/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470370816.0000 - rmse: 21688.0332 - val_loss: 868104704.0000 - val_rmse: 29463.6172\n",
      "Epoch 64/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406211392.0000 - rmse: 20154.6855 - val_loss: 618805952.0000 - val_rmse: 24875.8105\n",
      "Epoch 65/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460208032.0000 - rmse: 21452.4590 - val_loss: 611064064.0000 - val_rmse: 24719.7090\n",
      "Epoch 66/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399610560.0000 - rmse: 19990.2617 - val_loss: 716075456.0000 - val_rmse: 26759.5859\n",
      "Epoch 67/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456544768.0000 - rmse: 21366.9082 - val_loss: 666444544.0000 - val_rmse: 25815.5879\n",
      "Epoch 68/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423569184.0000 - rmse: 20580.7949 - val_loss: 630175680.0000 - val_rmse: 25103.2988\n",
      "Epoch 69/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388685568.0000 - rmse: 19715.1094 - val_loss: 1079598208.0000 - val_rmse: 32857.2383\n",
      "Epoch 70/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455290880.0000 - rmse: 21337.5469 - val_loss: 857015872.0000 - val_rmse: 29274.8340\n",
      "Epoch 71/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443592352.0000 - rmse: 21061.6328 - val_loss: 755087424.0000 - val_rmse: 27478.8535\n",
      "Epoch 72/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 422639648.0000 - rmse: 20558.2012 - val_loss: 731753600.0000 - val_rmse: 27050.9434\n",
      "Epoch 73/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430841728.0000 - rmse: 20756.7266 - val_loss: 574937664.0000 - val_rmse: 23977.8574\n",
      "Epoch 74/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405920416.0000 - rmse: 20147.4668 - val_loss: 1048654080.0000 - val_rmse: 32382.9297\n",
      "Epoch 75/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396472544.0000 - rmse: 19911.6172 - val_loss: 697321472.0000 - val_rmse: 26406.8457\n",
      "Epoch 76/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370012544.0000 - rmse: 19235.7090 - val_loss: 606296576.0000 - val_rmse: 24623.0898\n",
      "Epoch 77/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390050656.0000 - rmse: 19749.6992 - val_loss: 957203648.0000 - val_rmse: 30938.7070\n",
      "Epoch 78/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427356704.0000 - rmse: 20672.6055 - val_loss: 589982592.0000 - val_rmse: 24289.5566\n",
      "Epoch 79/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360844768.0000 - rmse: 18995.9141 - val_loss: 595158528.0000 - val_rmse: 24395.8691\n",
      "Epoch 80/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374786976.0000 - rmse: 19359.4141 - val_loss: 1055678208.0000 - val_rmse: 32491.2012\n",
      "Epoch 81/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379018784.0000 - rmse: 19468.4043 - val_loss: 704697664.0000 - val_rmse: 26546.1406\n",
      "Epoch 82/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383028352.0000 - rmse: 19571.1094 - val_loss: 1005487744.0000 - val_rmse: 31709.4258\n",
      "Epoch 83/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411108576.0000 - rmse: 20275.8125 - val_loss: 772459968.0000 - val_rmse: 27793.1641\n",
      "Epoch 84/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419785664.0000 - rmse: 20488.6699 - val_loss: 763091584.0000 - val_rmse: 27624.1133\n",
      "Epoch 85/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329433760.0000 - rmse: 18150.3086 - val_loss: 803477760.0000 - val_rmse: 28345.6816\n",
      "Epoch 86/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377401120.0000 - rmse: 19426.8145 - val_loss: 917038272.0000 - val_rmse: 30282.6367\n",
      "Epoch 87/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366288384.0000 - rmse: 19138.6621 - val_loss: 735641408.0000 - val_rmse: 27122.7090\n",
      "Epoch 88/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378796480.0000 - rmse: 19462.6953 - val_loss: 627426432.0000 - val_rmse: 25048.4824\n",
      "Epoch 89/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368208928.0000 - rmse: 19188.7715 - val_loss: 666539200.0000 - val_rmse: 25817.4199\n",
      "Epoch 90/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393263104.0000 - rmse: 19830.8613 - val_loss: 687860224.0000 - val_rmse: 26227.0879\n",
      "Epoch 91/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331639552.0000 - rmse: 18210.9727 - val_loss: 776863232.0000 - val_rmse: 27872.2637\n",
      "Epoch 92/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373886176.0000 - rmse: 19336.1348 - val_loss: 767798720.0000 - val_rmse: 27709.1797\n",
      "Epoch 93/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348459584.0000 - rmse: 18667.0723 - val_loss: 596343808.0000 - val_rmse: 24420.1523\n",
      "Epoch 94/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320761472.0000 - rmse: 17909.8145 - val_loss: 582387776.0000 - val_rmse: 24132.7109\n",
      "Epoch 95/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346518336.0000 - rmse: 18615.0020 - val_loss: 682245632.0000 - val_rmse: 26119.8320\n",
      "Epoch 96/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352703360.0000 - rmse: 18780.3965 - val_loss: 638220800.0000 - val_rmse: 25263.0332\n",
      "Epoch 97/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350078400.0000 - rmse: 18710.3809 - val_loss: 726071936.0000 - val_rmse: 26945.7207\n",
      "Epoch 98/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348792896.0000 - rmse: 18675.9961 - val_loss: 651647744.0000 - val_rmse: 25527.3906\n",
      "Epoch 99/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376813728.0000 - rmse: 19411.6914 - val_loss: 646434688.0000 - val_rmse: 25425.0781\n",
      "Epoch 100/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289405440.0000 - rmse: 17011.9199 - val_loss: 627862208.0000 - val_rmse: 25057.1777\n",
      "Epoch 101/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313376672.0000 - rmse: 17702.4473 - val_loss: 746492416.0000 - val_rmse: 27322.0117\n",
      "Epoch 102/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314052480.0000 - rmse: 17721.5254 - val_loss: 1088242560.0000 - val_rmse: 32988.5234\n",
      "Epoch 103/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306766176.0000 - rmse: 17514.7402 - val_loss: 555120448.0000 - val_rmse: 23560.9922\n",
      "Epoch 104/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344198400.0000 - rmse: 18552.5840 - val_loss: 684041984.0000 - val_rmse: 26154.1973\n",
      "Epoch 105/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351525696.0000 - rmse: 18749.0176 - val_loss: 652247360.0000 - val_rmse: 25539.1328\n",
      "Epoch 106/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276951264.0000 - rmse: 16641.8535 - val_loss: 945379456.0000 - val_rmse: 30747.0215\n",
      "Epoch 107/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340301312.0000 - rmse: 18447.2559 - val_loss: 1083453184.0000 - val_rmse: 32915.8516\n",
      "Epoch 108/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336105120.0000 - rmse: 18333.1699 - val_loss: 649544384.0000 - val_rmse: 25486.1602\n",
      "Epoch 109/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319616896.0000 - rmse: 17877.8320 - val_loss: 886501824.0000 - val_rmse: 29774.1777\n",
      "Epoch 110/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325173696.0000 - rmse: 18032.5723 - val_loss: 728457280.0000 - val_rmse: 26989.9453\n",
      "Epoch 111/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351235232.0000 - rmse: 18741.2715 - val_loss: 566999040.0000 - val_rmse: 23811.7422\n",
      "Epoch 112/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297819104.0000 - rmse: 17257.4355 - val_loss: 655814400.0000 - val_rmse: 25608.8730\n",
      "Epoch 113/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360189184.0000 - rmse: 18978.6504 - val_loss: 766914944.0000 - val_rmse: 27693.2285\n",
      "Epoch 114/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282684032.0000 - rmse: 16813.2090 - val_loss: 611850304.0000 - val_rmse: 24735.6074\n",
      "Epoch 115/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335005792.0000 - rmse: 18303.1621 - val_loss: 689805952.0000 - val_rmse: 26264.1562\n",
      "Epoch 116/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288059840.0000 - rmse: 16972.3242 - val_loss: 856382976.0000 - val_rmse: 29264.0215\n",
      "Epoch 117/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302815904.0000 - rmse: 17401.6055 - val_loss: 600396672.0000 - val_rmse: 24502.9902\n",
      "Epoch 118/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331066464.0000 - rmse: 18195.2324 - val_loss: 700583680.0000 - val_rmse: 26468.5410\n",
      "Epoch 119/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293663264.0000 - rmse: 17136.6055 - val_loss: 712546368.0000 - val_rmse: 26693.5645\n",
      "Epoch 120/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301782656.0000 - rmse: 17371.8926 - val_loss: 811589824.0000 - val_rmse: 28488.4160\n",
      "Epoch 121/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299800288.0000 - rmse: 17314.7402 - val_loss: 691800000.0000 - val_rmse: 26302.0918\n",
      "Epoch 122/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326670528.0000 - rmse: 18074.0293 - val_loss: 705947136.0000 - val_rmse: 26569.6641\n",
      "Epoch 123/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321198624.0000 - rmse: 17922.0137 - val_loss: 631577664.0000 - val_rmse: 25131.2070\n",
      "Epoch 124/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314039296.0000 - rmse: 17721.1523 - val_loss: 764095232.0000 - val_rmse: 27642.2734\n",
      "Epoch 125/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251761920.0000 - rmse: 15867.0049 - val_loss: 1112467712.0000 - val_rmse: 33353.6758\n",
      "Epoch 126/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283065632.0000 - rmse: 16824.5547 - val_loss: 1087234048.0000 - val_rmse: 32973.2305\n",
      "Epoch 127/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317045472.0000 - rmse: 17805.7695 - val_loss: 816231616.0000 - val_rmse: 28569.7656\n",
      "Epoch 128/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301332928.0000 - rmse: 17358.9434 - val_loss: 612120256.0000 - val_rmse: 24741.0645\n",
      "Epoch 129/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283469184.0000 - rmse: 16836.5430 - val_loss: 574954560.0000 - val_rmse: 23978.2109\n",
      "Epoch 130/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295438304.0000 - rmse: 17188.3184 - val_loss: 713562496.0000 - val_rmse: 26712.5898\n",
      "Epoch 131/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272023712.0000 - rmse: 16493.1406 - val_loss: 808795072.0000 - val_rmse: 28439.3223\n",
      "Epoch 132/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289852768.0000 - rmse: 17025.0625 - val_loss: 793256704.0000 - val_rmse: 28164.8125\n",
      "Epoch 133/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298369280.0000 - rmse: 17273.3672 - val_loss: 1034887744.0000 - val_rmse: 32169.6719\n",
      "Epoch 134/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302985216.0000 - rmse: 17406.4707 - val_loss: 797097536.0000 - val_rmse: 28232.9160\n",
      "Epoch 135/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252258208.0000 - rmse: 15882.6377 - val_loss: 767616768.0000 - val_rmse: 27705.8984\n",
      "Epoch 136/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274566752.0000 - rmse: 16570.0547 - val_loss: 876777280.0000 - val_rmse: 29610.4238\n",
      "Epoch 137/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300805696.0000 - rmse: 17343.7500 - val_loss: 734033344.0000 - val_rmse: 27093.0488\n",
      "Epoch 138/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289056896.0000 - rmse: 17001.6719 - val_loss: 778782848.0000 - val_rmse: 27906.6816\n",
      "Epoch 139/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293537184.0000 - rmse: 17132.9258 - val_loss: 882244864.0000 - val_rmse: 29702.6074\n",
      "Epoch 140/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275809728.0000 - rmse: 16607.5195 - val_loss: 978114368.0000 - val_rmse: 31274.8203\n",
      "Epoch 141/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301104544.0000 - rmse: 17352.3633 - val_loss: 1057733888.0000 - val_rmse: 32522.8184\n",
      "Epoch 142/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317395968.0000 - rmse: 17815.6094 - val_loss: 734795776.0000 - val_rmse: 27107.1172\n",
      "Epoch 143/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285729888.0000 - rmse: 16903.5430 - val_loss: 818605696.0000 - val_rmse: 28611.2871\n",
      "Epoch 144/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328544640.0000 - rmse: 18125.8008 - val_loss: 664510336.0000 - val_rmse: 25778.0977\n",
      "Epoch 145/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285356448.0000 - rmse: 16892.4961 - val_loss: 774948416.0000 - val_rmse: 27837.8945\n",
      "Epoch 146/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299710368.0000 - rmse: 17312.1426 - val_loss: 929178944.0000 - val_rmse: 30482.4375\n",
      "Epoch 147/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289165792.0000 - rmse: 17004.8730 - val_loss: 622824576.0000 - val_rmse: 24956.4531\n",
      "Epoch 148/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251491680.0000 - rmse: 15858.4873 - val_loss: 1047774656.0000 - val_rmse: 32369.3477\n",
      "Epoch 149/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273435936.0000 - rmse: 16535.8965 - val_loss: 714020736.0000 - val_rmse: 26721.1660\n",
      "Epoch 150/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266655856.0000 - rmse: 16329.5986 - val_loss: 1060778944.0000 - val_rmse: 32569.6016\n",
      "Epoch 151/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291715040.0000 - rmse: 17079.6660 - val_loss: 702676416.0000 - val_rmse: 26508.0449\n",
      "Epoch 152/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249725376.0000 - rmse: 15802.7012 - val_loss: 710303872.0000 - val_rmse: 26651.5273\n",
      "Epoch 153/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295779232.0000 - rmse: 17198.2324 - val_loss: 1089823360.0000 - val_rmse: 33012.4727\n",
      "Epoch 154/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271454400.0000 - rmse: 16475.8711 - val_loss: 713124864.0000 - val_rmse: 26704.3965\n",
      "Epoch 155/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253403056.0000 - rmse: 15918.6367 - val_loss: 937766080.0000 - val_rmse: 30622.9668\n",
      "Epoch 156/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255900144.0000 - rmse: 15996.8770 - val_loss: 670657216.0000 - val_rmse: 25897.0488\n",
      "Epoch 157/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249581872.0000 - rmse: 15798.1582 - val_loss: 1018315968.0000 - val_rmse: 31911.0625\n",
      "Epoch 158/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277190464.0000 - rmse: 16649.0371 - val_loss: 690831232.0000 - val_rmse: 26283.6680\n",
      "Epoch 159/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295402048.0000 - rmse: 17187.2637 - val_loss: 1254923648.0000 - val_rmse: 35424.8984\n",
      "Epoch 160/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264712112.0000 - rmse: 16269.9746 - val_loss: 769664192.0000 - val_rmse: 27742.8223\n",
      "Epoch 161/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275459296.0000 - rmse: 16596.9648 - val_loss: 682786560.0000 - val_rmse: 26130.1855\n",
      "Epoch 162/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274144768.0000 - rmse: 16557.3164 - val_loss: 977361472.0000 - val_rmse: 31262.7793\n",
      "Epoch 163/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259248320.0000 - rmse: 16101.1885 - val_loss: 806494336.0000 - val_rmse: 28398.8438\n",
      "Epoch 164/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286697408.0000 - rmse: 16932.1406 - val_loss: 699694592.0000 - val_rmse: 26451.7402\n",
      "Epoch 165/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241557616.0000 - rmse: 15542.1230 - val_loss: 989780032.0000 - val_rmse: 31460.7695\n",
      "Epoch 166/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277745504.0000 - rmse: 16665.6973 - val_loss: 888782784.0000 - val_rmse: 29812.4609\n",
      "Epoch 167/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259418240.0000 - rmse: 16106.4648 - val_loss: 690124736.0000 - val_rmse: 26270.2246\n",
      "Epoch 168/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328496928.0000 - rmse: 18124.4844 - val_loss: 926399424.0000 - val_rmse: 30436.8105\n",
      "Epoch 169/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254013728.0000 - rmse: 15937.8066 - val_loss: 760130496.0000 - val_rmse: 27570.4648\n",
      "Epoch 170/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245084016.0000 - rmse: 15655.1592 - val_loss: 777794496.0000 - val_rmse: 27888.9668\n",
      "Epoch 171/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344313728.0000 - rmse: 18555.6934 - val_loss: 760846144.0000 - val_rmse: 27583.4395\n",
      "Epoch 172/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263456448.0000 - rmse: 16231.3398 - val_loss: 878422336.0000 - val_rmse: 29638.1914\n",
      "Epoch 173/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253310048.0000 - rmse: 15915.7148 - val_loss: 739249856.0000 - val_rmse: 27189.1504\n",
      "Epoch 174/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245527776.0000 - rmse: 15669.3252 - val_loss: 754051712.0000 - val_rmse: 27460.0020\n",
      "Epoch 175/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253865568.0000 - rmse: 15933.1582 - val_loss: 1194730368.0000 - val_rmse: 34564.8711\n",
      "Epoch 176/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248019824.0000 - rmse: 15748.6436 - val_loss: 956761984.0000 - val_rmse: 30931.5703\n",
      "Epoch 177/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293176416.0000 - rmse: 17122.3945 - val_loss: 887252288.0000 - val_rmse: 29786.7773\n",
      "Epoch 178/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266375616.0000 - rmse: 16321.0156 - val_loss: 1448310272.0000 - val_rmse: 38056.6719\n",
      "Epoch 179/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266243200.0000 - rmse: 16316.9600 - val_loss: 747422080.0000 - val_rmse: 27339.0215\n",
      "Epoch 180/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258867552.0000 - rmse: 16089.3613 - val_loss: 907385728.0000 - val_rmse: 30122.8438\n",
      "Epoch 181/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257425232.0000 - rmse: 16044.4766 - val_loss: 824663296.0000 - val_rmse: 28716.9512\n",
      "Epoch 182/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234057488.0000 - rmse: 15298.9355 - val_loss: 1009368320.0000 - val_rmse: 31770.5566\n",
      "Epoch 183/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289718368.0000 - rmse: 17021.1152 - val_loss: 816031808.0000 - val_rmse: 28566.2676\n",
      "Epoch 184/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258496352.0000 - rmse: 16077.8193 - val_loss: 856459968.0000 - val_rmse: 29265.3379\n",
      "Epoch 185/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216125296.0000 - rmse: 14701.1992 - val_loss: 998011712.0000 - val_rmse: 31591.3242\n",
      "Epoch 186/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256136864.0000 - rmse: 16004.2754 - val_loss: 870343168.0000 - val_rmse: 29501.5781\n",
      "Epoch 187/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220561552.0000 - rmse: 14851.3135 - val_loss: 815308864.0000 - val_rmse: 28553.6133\n",
      "Epoch 188/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239121248.0000 - rmse: 15463.5449 - val_loss: 988689664.0000 - val_rmse: 31443.4355\n",
      "Epoch 189/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252250992.0000 - rmse: 15882.4111 - val_loss: 701253824.0000 - val_rmse: 26481.1973\n",
      "Epoch 190/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253544080.0000 - rmse: 15923.0674 - val_loss: 905736128.0000 - val_rmse: 30095.4492\n",
      "Epoch 191/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255515584.0000 - rmse: 15984.8525 - val_loss: 1192984576.0000 - val_rmse: 34539.6055\n",
      "Epoch 192/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241255744.0000 - rmse: 15532.4072 - val_loss: 734877504.0000 - val_rmse: 27108.6250\n",
      "Epoch 193/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228192192.0000 - rmse: 15106.0312 - val_loss: 1043821056.0000 - val_rmse: 32308.2188\n",
      "Epoch 194/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248584368.0000 - rmse: 15766.5576 - val_loss: 1039804480.0000 - val_rmse: 32245.9980\n",
      "Epoch 195/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286349312.0000 - rmse: 16921.8574 - val_loss: 929033216.0000 - val_rmse: 30480.0469\n",
      "Epoch 196/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238357408.0000 - rmse: 15438.8271 - val_loss: 752950848.0000 - val_rmse: 27439.9492\n",
      "Epoch 197/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309312576.0000 - rmse: 17587.2832 - val_loss: 972201664.0000 - val_rmse: 31180.1484\n",
      "Epoch 198/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234934480.0000 - rmse: 15327.5713 - val_loss: 861652608.0000 - val_rmse: 29353.9180\n",
      "Epoch 199/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229943296.0000 - rmse: 15163.8799 - val_loss: 1036788416.0000 - val_rmse: 32199.1973\n",
      "Epoch 200/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 233213344.0000 - rmse: 15271.3223 - val_loss: 861192832.0000 - val_rmse: 29346.0859\n",
      "Epoch 201/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263301792.0000 - rmse: 16226.5752 - val_loss: 852213440.0000 - val_rmse: 29192.6953\n",
      "Epoch 202/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272592864.0000 - rmse: 16510.3828 - val_loss: 796082944.0000 - val_rmse: 28214.9414\n",
      "Epoch 203/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220879984.0000 - rmse: 14862.0303 - val_loss: 838757632.0000 - val_rmse: 28961.3105\n",
      "Epoch 204/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214240608.0000 - rmse: 14636.9590 - val_loss: 976654272.0000 - val_rmse: 31251.4668\n",
      "Epoch 205/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 223531744.0000 - rmse: 14950.9766 - val_loss: 1011091200.0000 - val_rmse: 31797.6582\n",
      "Epoch 206/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269212416.0000 - rmse: 16407.6914 - val_loss: 897418368.0000 - val_rmse: 29956.9414\n",
      "Epoch 207/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258442576.0000 - rmse: 16076.1494 - val_loss: 1245778944.0000 - val_rmse: 35295.5938\n",
      "Epoch 208/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220747440.0000 - rmse: 14857.5703 - val_loss: 761878400.0000 - val_rmse: 27602.1445\n",
      "Epoch 209/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284639072.0000 - rmse: 16871.2500 - val_loss: 1029594112.0000 - val_rmse: 32087.2852\n",
      "Epoch 210/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231891760.0000 - rmse: 15227.9922 - val_loss: 718154304.0000 - val_rmse: 26798.4004\n",
      "Epoch 211/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226315552.0000 - rmse: 15043.7881 - val_loss: 959517760.0000 - val_rmse: 30976.0840\n",
      "Epoch 212/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242326832.0000 - rmse: 15566.8486 - val_loss: 943119680.0000 - val_rmse: 30710.2520\n",
      "Epoch 213/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240102128.0000 - rmse: 15495.2285 - val_loss: 790859200.0000 - val_rmse: 28122.2188\n",
      "Epoch 214/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 201959360.0000 - rmse: 14211.2402 - val_loss: 1233639040.0000 - val_rmse: 35123.1992\n",
      "Epoch 215/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221481504.0000 - rmse: 14882.2539 - val_loss: 756291136.0000 - val_rmse: 27500.7480\n",
      "Epoch 216/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 223507600.0000 - rmse: 14950.1699 - val_loss: 761897088.0000 - val_rmse: 27602.4824\n",
      "Epoch 217/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 208210208.0000 - rmse: 14429.4883 - val_loss: 1392612736.0000 - val_rmse: 37317.7266\n",
      "Epoch 218/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 201761600.0000 - rmse: 14204.2803 - val_loss: 775362880.0000 - val_rmse: 27845.3379\n",
      "Epoch 219/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221643600.0000 - rmse: 14887.6982 - val_loss: 924492608.0000 - val_rmse: 30405.4688\n",
      "Epoch 220/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214698080.0000 - rmse: 14652.5781 - val_loss: 811054528.0000 - val_rmse: 28479.0176\n",
      "Epoch 221/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229702000.0000 - rmse: 15155.9219 - val_loss: 926885376.0000 - val_rmse: 30444.7930\n",
      "Epoch 222/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202483568.0000 - rmse: 14229.6689 - val_loss: 1408121216.0000 - val_rmse: 37524.9414\n",
      "Epoch 223/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266248480.0000 - rmse: 16317.1211 - val_loss: 815892672.0000 - val_rmse: 28563.8340\n",
      "Epoch 224/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254605920.0000 - rmse: 15956.3750 - val_loss: 863910784.0000 - val_rmse: 29392.3574\n",
      "Epoch 225/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218758464.0000 - rmse: 14790.4844 - val_loss: 910248896.0000 - val_rmse: 30170.3320\n",
      "Epoch 226/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 195838384.0000 - rmse: 13994.2246 - val_loss: 1316891008.0000 - val_rmse: 36288.9922\n",
      "Epoch 227/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 225610848.0000 - rmse: 15020.3447 - val_loss: 793889024.0000 - val_rmse: 28176.0352\n",
      "Epoch 228/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224386000.0000 - rmse: 14979.5176 - val_loss: 787119488.0000 - val_rmse: 28055.6504\n",
      "Epoch 229/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241968560.0000 - rmse: 15555.3359 - val_loss: 910212672.0000 - val_rmse: 30169.7305\n",
      "Epoch 230/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221438912.0000 - rmse: 14880.8203 - val_loss: 1029534272.0000 - val_rmse: 32086.3574\n",
      "Epoch 231/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 225367392.0000 - rmse: 15012.2402 - val_loss: 783666880.0000 - val_rmse: 27994.0508\n",
      "Epoch 232/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247559088.0000 - rmse: 15734.0098 - val_loss: 796302912.0000 - val_rmse: 28218.8398\n",
      "Epoch 233/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222494992.0000 - rmse: 14916.2637 - val_loss: 1235585664.0000 - val_rmse: 35150.8984\n",
      "Epoch 234/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215112672.0000 - rmse: 14666.7188 - val_loss: 804823232.0000 - val_rmse: 28369.4062\n",
      "Epoch 235/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244277424.0000 - rmse: 15629.3740 - val_loss: 820868160.0000 - val_rmse: 28650.7969\n",
      "Epoch 236/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211918480.0000 - rmse: 14557.4189 - val_loss: 904894720.0000 - val_rmse: 30081.4688\n",
      "Epoch 237/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 209275456.0000 - rmse: 14466.3545 - val_loss: 1027432576.0000 - val_rmse: 32053.5879\n",
      "Epoch 238/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 223539472.0000 - rmse: 14951.2354 - val_loss: 744418688.0000 - val_rmse: 27284.0352\n",
      "Epoch 239/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 193994768.0000 - rmse: 13928.1973 - val_loss: 694774208.0000 - val_rmse: 26358.5684\n",
      "Epoch 240/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221515824.0000 - rmse: 14883.4072 - val_loss: 672125632.0000 - val_rmse: 25925.3848\n",
      "Epoch 241/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 209777984.0000 - rmse: 14483.7139 - val_loss: 1056378880.0000 - val_rmse: 32501.9824\n",
      "Epoch 242/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 213469232.0000 - rmse: 14610.5850 - val_loss: 1136325248.0000 - val_rmse: 33709.4219\n",
      "Epoch 243/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 198853104.0000 - rmse: 14101.5264 - val_loss: 1624267264.0000 - val_rmse: 40302.1992\n",
      "Epoch 244/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259600976.0000 - rmse: 16112.1357 - val_loss: 884806912.0000 - val_rmse: 29745.7031\n",
      "Epoch 245/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218214096.0000 - rmse: 14772.0713 - val_loss: 1186472576.0000 - val_rmse: 34445.2109\n",
      "Epoch 246/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244999008.0000 - rmse: 15652.4434 - val_loss: 820534720.0000 - val_rmse: 28644.9766\n",
      "Epoch 247/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 199509824.0000 - rmse: 14124.7920 - val_loss: 985220800.0000 - val_rmse: 31388.2246\n",
      "Epoch 248/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 225963632.0000 - rmse: 15032.0850 - val_loss: 700522880.0000 - val_rmse: 26467.3906\n",
      "Epoch 249/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235878768.0000 - rmse: 15358.3428 - val_loss: 770651904.0000 - val_rmse: 27760.6172\n",
      "Epoch 250/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257294000.0000 - rmse: 16040.3828 - val_loss: 843791104.0000 - val_rmse: 29048.0820\n",
      "Epoch 251/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243017344.0000 - rmse: 15589.0107 - val_loss: 930845696.0000 - val_rmse: 30509.7637\n",
      "Epoch 252/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 203074064.0000 - rmse: 14250.4043 - val_loss: 721870336.0000 - val_rmse: 26867.6426\n",
      "Epoch 253/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 201608544.0000 - rmse: 14198.8906 - val_loss: 930792256.0000 - val_rmse: 30508.8867\n",
      "Epoch 254/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234792560.0000 - rmse: 15322.9404 - val_loss: 746944512.0000 - val_rmse: 27330.2852\n",
      "Epoch 255/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207761056.0000 - rmse: 14413.9170 - val_loss: 997568384.0000 - val_rmse: 31584.3066\n",
      "Epoch 256/256\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 186478320.0000 - rmse: 13655.7051 - val_loss: 892941760.0000 - val_rmse: 29882.1289\n",
      "104/104 [==============================] - 0s 676us/step - loss: 417662080.0000 - rmse: 20436.7812\n",
      "[417662080.0, 20436.78125]\n",
      "[20372.611328125, 30336.294921875, 28733.6640625, 27713.595703125, 20436.78125]\n",
      "25518.589453125\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "!python train.py kfold baseline\n",
    "# epoch 300 p 25 lr 5e-3"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 21:40:41.851488: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 21:40:41.851522: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 21:40:41.851834: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 21:40:42.764330: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 13149789184.0000 - rmse: 114672.5312 - val_loss: 1992526848.0000 - val_rmse: 44637.7305\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2270147072.0000 - rmse: 47646.0586 - val_loss: 1270546304.0000 - val_rmse: 35644.7227\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1830226176.0000 - rmse: 42781.1445 - val_loss: 1000837632.0000 - val_rmse: 31636.0176\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1681380352.0000 - rmse: 41004.6367 - val_loss: 893113280.0000 - val_rmse: 29885.0000\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1613101696.0000 - rmse: 40163.4375 - val_loss: 838116032.0000 - val_rmse: 28950.2344\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1504914176.0000 - rmse: 38793.2227 - val_loss: 780230912.0000 - val_rmse: 27932.6133\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1500814080.0000 - rmse: 38740.3398 - val_loss: 781886784.0000 - val_rmse: 27962.2383\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1377097088.0000 - rmse: 37109.2578 - val_loss: 736187328.0000 - val_rmse: 27132.7715\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1325353088.0000 - rmse: 36405.3984 - val_loss: 728482496.0000 - val_rmse: 26990.4141\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1379590528.0000 - rmse: 37142.8398 - val_loss: 748084800.0000 - val_rmse: 27351.1387\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1319943680.0000 - rmse: 36331.0273 - val_loss: 686410560.0000 - val_rmse: 26199.4375\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1332560768.0000 - rmse: 36504.2578 - val_loss: 754539456.0000 - val_rmse: 27468.8809\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1259793280.0000 - rmse: 35493.5664 - val_loss: 690824704.0000 - val_rmse: 26283.5449\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1268219520.0000 - rmse: 35612.0703 - val_loss: 715187264.0000 - val_rmse: 26742.9824\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1258324352.0000 - rmse: 35472.8672 - val_loss: 697518656.0000 - val_rmse: 26410.5781\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1233191296.0000 - rmse: 35116.8242 - val_loss: 764377856.0000 - val_rmse: 27647.3848\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1225612032.0000 - rmse: 35008.7422 - val_loss: 672366336.0000 - val_rmse: 25930.0273\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1215690368.0000 - rmse: 34866.7500 - val_loss: 694916992.0000 - val_rmse: 26361.2773\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1180354176.0000 - rmse: 34356.2812 - val_loss: 681232832.0000 - val_rmse: 26100.4375\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1167977728.0000 - rmse: 34175.6875 - val_loss: 699597440.0000 - val_rmse: 26449.9043\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1117620224.0000 - rmse: 33430.8281 - val_loss: 693450048.0000 - val_rmse: 26333.4395\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1120383744.0000 - rmse: 33472.1328 - val_loss: 825132864.0000 - val_rmse: 28725.1270\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1071777408.0000 - rmse: 32738.0117 - val_loss: 672918976.0000 - val_rmse: 25940.6797\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1052699008.0000 - rmse: 32445.3242 - val_loss: 635135552.0000 - val_rmse: 25201.8945\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1063833088.0000 - rmse: 32616.4531 - val_loss: 649150784.0000 - val_rmse: 25478.4375\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1004784512.0000 - rmse: 31698.3359 - val_loss: 656300160.0000 - val_rmse: 25618.3555\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1006472256.0000 - rmse: 31724.9473 - val_loss: 700591168.0000 - val_rmse: 26468.6836\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1004437824.0000 - rmse: 31692.8672 - val_loss: 720556864.0000 - val_rmse: 26843.1895\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 980207552.0000 - rmse: 31308.2656 - val_loss: 748217344.0000 - val_rmse: 27353.5625\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 991383296.0000 - rmse: 31486.2402 - val_loss: 652330880.0000 - val_rmse: 25540.7695\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 882938624.0000 - rmse: 29714.2832 - val_loss: 754285824.0000 - val_rmse: 27464.2637\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 960424000.0000 - rmse: 30990.7090 - val_loss: 799633600.0000 - val_rmse: 28277.7930\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 878307072.0000 - rmse: 29636.2461 - val_loss: 668787136.0000 - val_rmse: 25860.9199\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 856535552.0000 - rmse: 29266.6289 - val_loss: 655210240.0000 - val_rmse: 25597.0742\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 832570432.0000 - rmse: 28854.2969 - val_loss: 575955072.0000 - val_rmse: 23999.0645\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 804298944.0000 - rmse: 28360.1641 - val_loss: 653273600.0000 - val_rmse: 25559.2168\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 826396672.0000 - rmse: 28747.1152 - val_loss: 615969600.0000 - val_rmse: 24818.7344\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 814555392.0000 - rmse: 28540.4160 - val_loss: 631380736.0000 - val_rmse: 25127.2891\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 785769024.0000 - rmse: 28031.5723 - val_loss: 621662720.0000 - val_rmse: 24933.1660\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 757294336.0000 - rmse: 27518.9805 - val_loss: 631221632.0000 - val_rmse: 25124.1250\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 778070464.0000 - rmse: 27893.9141 - val_loss: 677611456.0000 - val_rmse: 26030.9707\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692715776.0000 - rmse: 26319.4941 - val_loss: 525476352.0000 - val_rmse: 22923.2695\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 752627584.0000 - rmse: 27434.0586 - val_loss: 592934464.0000 - val_rmse: 24350.2461\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656382080.0000 - rmse: 25619.9551 - val_loss: 629895040.0000 - val_rmse: 25097.7090\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 672289408.0000 - rmse: 25928.5449 - val_loss: 754910272.0000 - val_rmse: 27475.6309\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656336832.0000 - rmse: 25619.0723 - val_loss: 541626752.0000 - val_rmse: 23272.8750\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692140864.0000 - rmse: 26308.5703 - val_loss: 790725056.0000 - val_rmse: 28119.8340\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 694264896.0000 - rmse: 26348.9062 - val_loss: 636519424.0000 - val_rmse: 25229.3359\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629552128.0000 - rmse: 25090.8770 - val_loss: 687973056.0000 - val_rmse: 26229.2402\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 686998272.0000 - rmse: 26210.6523 - val_loss: 825016320.0000 - val_rmse: 28723.0977\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616682624.0000 - rmse: 24833.0957 - val_loss: 669956608.0000 - val_rmse: 25883.5195\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587260096.0000 - rmse: 24233.4492 - val_loss: 737914112.0000 - val_rmse: 27164.5742\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590435712.0000 - rmse: 24298.8809 - val_loss: 840575744.0000 - val_rmse: 28992.6816\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611083520.0000 - rmse: 24720.1016 - val_loss: 732614272.0000 - val_rmse: 27066.8477\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570478848.0000 - rmse: 23884.6992 - val_loss: 1611231488.0000 - val_rmse: 40140.1484\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606086528.0000 - rmse: 24618.8242 - val_loss: 852878336.0000 - val_rmse: 29204.0801\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568402752.0000 - rmse: 23841.1992 - val_loss: 550962240.0000 - val_rmse: 23472.5840\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630548224.0000 - rmse: 25110.7188 - val_loss: 602380032.0000 - val_rmse: 24543.4316\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605433344.0000 - rmse: 24605.5547 - val_loss: 672542400.0000 - val_rmse: 25933.4219\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536034368.0000 - rmse: 23152.4141 - val_loss: 577969344.0000 - val_rmse: 24040.9922\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548757248.0000 - rmse: 23425.5684 - val_loss: 749023360.0000 - val_rmse: 27368.2910\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565962496.0000 - rmse: 23789.9629 - val_loss: 746889088.0000 - val_rmse: 27329.2695\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514963712.0000 - rmse: 22692.8105 - val_loss: 685891008.0000 - val_rmse: 26189.5215\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546247040.0000 - rmse: 23371.9277 - val_loss: 989586496.0000 - val_rmse: 31457.6934\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593483968.0000 - rmse: 24361.5273 - val_loss: 669144064.0000 - val_rmse: 25867.8184\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496020416.0000 - rmse: 22271.5137 - val_loss: 865686272.0000 - val_rmse: 29422.5469\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485115392.0000 - rmse: 22025.3340 - val_loss: 901850816.0000 - val_rmse: 30030.8320\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545128768.0000 - rmse: 23347.9922 - val_loss: 1096176000.0000 - val_rmse: 33108.5469\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 648191040.0000 - rmse: 25459.5957 - val_loss: 784731648.0000 - val_rmse: 28013.0605\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554368832.0000 - rmse: 23545.0391 - val_loss: 575213824.0000 - val_rmse: 23983.6152\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485250912.0000 - rmse: 22028.4121 - val_loss: 652314368.0000 - val_rmse: 25540.4453\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482029280.0000 - rmse: 21955.1660 - val_loss: 620490048.0000 - val_rmse: 24909.6367\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462018240.0000 - rmse: 21494.6074 - val_loss: 862119040.0000 - val_rmse: 29361.8633\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488297664.0000 - rmse: 22097.4570 - val_loss: 700892224.0000 - val_rmse: 26474.3691\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502299424.0000 - rmse: 22412.0352 - val_loss: 553278016.0000 - val_rmse: 23521.8633\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461831712.0000 - rmse: 21490.2695 - val_loss: 927721920.0000 - val_rmse: 30458.5254\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586152960.0000 - rmse: 24210.5938 - val_loss: 766788800.0000 - val_rmse: 27690.9512\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512967552.0000 - rmse: 22648.7871 - val_loss: 933829120.0000 - val_rmse: 30558.6172\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503875488.0000 - rmse: 22447.1699 - val_loss: 791256192.0000 - val_rmse: 28129.2773\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427170400.0000 - rmse: 20668.0996 - val_loss: 853716992.0000 - val_rmse: 29218.4355\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444604160.0000 - rmse: 21085.6387 - val_loss: 915866752.0000 - val_rmse: 30263.2910\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445614976.0000 - rmse: 21109.5938 - val_loss: 594394752.0000 - val_rmse: 24380.2129\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509090304.0000 - rmse: 22563.0293 - val_loss: 1290329600.0000 - val_rmse: 35921.1562\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500659392.0000 - rmse: 22375.4180 - val_loss: 871030784.0000 - val_rmse: 29513.2305\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414110880.0000 - rmse: 20349.7148 - val_loss: 1274149504.0000 - val_rmse: 35695.2305\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413898432.0000 - rmse: 20344.4922 - val_loss: 987157376.0000 - val_rmse: 31419.0605\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418367072.0000 - rmse: 20454.0234 - val_loss: 907815552.0000 - val_rmse: 30129.9746\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401904448.0000 - rmse: 20047.5547 - val_loss: 932770368.0000 - val_rmse: 30541.2871\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468286144.0000 - rmse: 21639.9199 - val_loss: 1134099072.0000 - val_rmse: 33676.3867\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384739168.0000 - rmse: 19614.7695 - val_loss: 772985856.0000 - val_rmse: 27802.6230\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479034080.0000 - rmse: 21886.8477 - val_loss: 810781248.0000 - val_rmse: 28474.2207\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425890208.0000 - rmse: 20637.1074 - val_loss: 610478976.0000 - val_rmse: 24707.8730\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405815520.0000 - rmse: 20144.8613 - val_loss: 1126290688.0000 - val_rmse: 33560.2539\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383637184.0000 - rmse: 19586.6582 - val_loss: 1218726912.0000 - val_rmse: 34910.2695\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391537952.0000 - rmse: 19787.3184 - val_loss: 1347947648.0000 - val_rmse: 36714.4062\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433688064.0000 - rmse: 20825.1777 - val_loss: 1933820032.0000 - val_rmse: 43975.2188\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408041728.0000 - rmse: 20200.0410 - val_loss: 1216904960.0000 - val_rmse: 34884.1641\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379759008.0000 - rmse: 19487.4062 - val_loss: 901480128.0000 - val_rmse: 30024.6582\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378698208.0000 - rmse: 19460.1699 - val_loss: 669621888.0000 - val_rmse: 25877.0527\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451474752.0000 - rmse: 21247.9336 - val_loss: 961300608.0000 - val_rmse: 31004.8477\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380565344.0000 - rmse: 19508.0840 - val_loss: 817499968.0000 - val_rmse: 28591.9570\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354313632.0000 - rmse: 18823.2188 - val_loss: 754840960.0000 - val_rmse: 27474.3691\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386938816.0000 - rmse: 19670.7598 - val_loss: 834038016.0000 - val_rmse: 28879.7168\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396989952.0000 - rmse: 19924.6055 - val_loss: 767830464.0000 - val_rmse: 27709.7539\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403062624.0000 - rmse: 20076.4180 - val_loss: 830951232.0000 - val_rmse: 28826.2227\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370452736.0000 - rmse: 19247.1465 - val_loss: 1021307712.0000 - val_rmse: 31957.9062\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364878880.0000 - rmse: 19101.8027 - val_loss: 896221568.0000 - val_rmse: 29936.9570\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397579968.0000 - rmse: 19939.4062 - val_loss: 850370752.0000 - val_rmse: 29161.1172\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392263072.0000 - rmse: 19805.6309 - val_loss: 856001344.0000 - val_rmse: 29257.5000\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366638208.0000 - rmse: 19147.7988 - val_loss: 597858880.0000 - val_rmse: 24451.1523\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390877568.0000 - rmse: 19770.6230 - val_loss: 960577472.0000 - val_rmse: 30993.1836\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403403744.0000 - rmse: 20084.9141 - val_loss: 1322266624.0000 - val_rmse: 36362.9844\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369738592.0000 - rmse: 19228.5859 - val_loss: 1021744512.0000 - val_rmse: 31964.7383\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368355584.0000 - rmse: 19192.5898 - val_loss: 884048000.0000 - val_rmse: 29732.9453\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342979040.0000 - rmse: 18519.6934 - val_loss: 740793664.0000 - val_rmse: 27217.5254\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357314016.0000 - rmse: 18902.7520 - val_loss: 1392982400.0000 - val_rmse: 37322.6758\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360917248.0000 - rmse: 18997.8203 - val_loss: 787754432.0000 - val_rmse: 28066.9629\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325072672.0000 - rmse: 18029.7715 - val_loss: 1147325568.0000 - val_rmse: 33872.1953\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364526080.0000 - rmse: 19092.5645 - val_loss: 948335424.0000 - val_rmse: 30795.0547\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366610912.0000 - rmse: 19147.0859 - val_loss: 1167268864.0000 - val_rmse: 34165.3164\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308920896.0000 - rmse: 17576.1445 - val_loss: 1411630208.0000 - val_rmse: 37571.6641\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379932960.0000 - rmse: 19491.8691 - val_loss: 1294645248.0000 - val_rmse: 35981.1797\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380105408.0000 - rmse: 19496.2930 - val_loss: 1212385152.0000 - val_rmse: 34819.3203\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337627424.0000 - rmse: 18374.6406 - val_loss: 968482560.0000 - val_rmse: 31120.4531\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308203648.0000 - rmse: 17555.7285 - val_loss: 1084745088.0000 - val_rmse: 32935.4648\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321892128.0000 - rmse: 17941.3496 - val_loss: 1764826112.0000 - val_rmse: 42009.8320\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362383744.0000 - rmse: 19036.3789 - val_loss: 1001074496.0000 - val_rmse: 31639.7617\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327770912.0000 - rmse: 18104.4414 - val_loss: 1325297664.0000 - val_rmse: 36404.6367\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376316416.0000 - rmse: 19398.8750 - val_loss: 1010542208.0000 - val_rmse: 31789.0273\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432622880.0000 - rmse: 20799.5879 - val_loss: 891629056.0000 - val_rmse: 29860.1582\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320019168.0000 - rmse: 17889.0801 - val_loss: 1467478016.0000 - val_rmse: 38307.6758\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418356192.0000 - rmse: 20453.7559 - val_loss: 1535039104.0000 - val_rmse: 39179.5742\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343038144.0000 - rmse: 18521.2871 - val_loss: 1434099584.0000 - val_rmse: 37869.5078\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365090368.0000 - rmse: 19107.3379 - val_loss: 1509785600.0000 - val_rmse: 38855.9609\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357811392.0000 - rmse: 18915.9023 - val_loss: 1067777280.0000 - val_rmse: 32676.8613\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341271232.0000 - rmse: 18473.5273 - val_loss: 1182262144.0000 - val_rmse: 34384.0391\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325849248.0000 - rmse: 18051.2949 - val_loss: 1070209152.0000 - val_rmse: 32714.0508\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316952672.0000 - rmse: 17803.1641 - val_loss: 1208249984.0000 - val_rmse: 34759.8906\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373225568.0000 - rmse: 19319.0469 - val_loss: 963976448.0000 - val_rmse: 31047.9707\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306803040.0000 - rmse: 17515.7949 - val_loss: 1969021312.0000 - val_rmse: 44373.6562\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430788736.0000 - rmse: 20755.4492 - val_loss: 1049788864.0000 - val_rmse: 32400.4453\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358926464.0000 - rmse: 18945.3555 - val_loss: 1713006720.0000 - val_rmse: 41388.4844\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347875616.0000 - rmse: 18651.4238 - val_loss: 1063158976.0000 - val_rmse: 32606.1152\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329661920.0000 - rmse: 18156.5938 - val_loss: 889306816.0000 - val_rmse: 29821.2480\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395032832.0000 - rmse: 19875.4316 - val_loss: 1317444608.0000 - val_rmse: 36296.6211\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302819104.0000 - rmse: 17401.6973 - val_loss: 1564836224.0000 - val_rmse: 39558.0117\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297024768.0000 - rmse: 17234.4043 - val_loss: 1103214080.0000 - val_rmse: 33214.6680\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279680800.0000 - rmse: 16723.6582 - val_loss: 1398509056.0000 - val_rmse: 37396.6445\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366562560.0000 - rmse: 19145.8223 - val_loss: 1367594752.0000 - val_rmse: 36981.0039\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387208928.0000 - rmse: 19677.6250 - val_loss: 1320617088.0000 - val_rmse: 36340.2969\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394883776.0000 - rmse: 19871.6816 - val_loss: 950405184.0000 - val_rmse: 30828.6406\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335923424.0000 - rmse: 18328.2148 - val_loss: 1407419264.0000 - val_rmse: 37515.5859\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298441376.0000 - rmse: 17275.4531 - val_loss: 1253575040.0000 - val_rmse: 35405.8594\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328694688.0000 - rmse: 18129.9375 - val_loss: 1345410944.0000 - val_rmse: 36679.8438\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304581600.0000 - rmse: 17452.2656 - val_loss: 1588054016.0000 - val_rmse: 39850.3945\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327417760.0000 - rmse: 18094.6875 - val_loss: 768926528.0000 - val_rmse: 27729.5254\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329150240.0000 - rmse: 18142.4980 - val_loss: 1184060160.0000 - val_rmse: 34410.1719\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298403296.0000 - rmse: 17274.3535 - val_loss: 1358083968.0000 - val_rmse: 36852.1914\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276622016.0000 - rmse: 16631.9570 - val_loss: 1173478912.0000 - val_rmse: 34256.0781\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333790784.0000 - rmse: 18269.9395 - val_loss: 1110315648.0000 - val_rmse: 33321.3984\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312625152.0000 - rmse: 17681.2070 - val_loss: 2095227264.0000 - val_rmse: 45773.6523\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309885216.0000 - rmse: 17603.5547 - val_loss: 2345193984.0000 - val_rmse: 48427.2031\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435407648.0000 - rmse: 20866.4238 - val_loss: 1687908224.0000 - val_rmse: 41084.1562\n",
      "Epoch 164/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306615776.0000 - rmse: 17510.4453 - val_loss: 1157574016.0000 - val_rmse: 34023.1406\n",
      "Epoch 165/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262619920.0000 - rmse: 16205.5508 - val_loss: 2426546176.0000 - val_rmse: 49259.9844\n",
      "Epoch 166/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331581440.0000 - rmse: 18209.3770 - val_loss: 1474611072.0000 - val_rmse: 38400.6641\n",
      "Epoch 167/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282794368.0000 - rmse: 16816.4902 - val_loss: 1469570432.0000 - val_rmse: 38334.9727\n",
      "Epoch 168/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262038208.0000 - rmse: 16187.5938 - val_loss: 1621524480.0000 - val_rmse: 40268.1562\n",
      "Epoch 169/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330784288.0000 - rmse: 18187.4746 - val_loss: 1697657984.0000 - val_rmse: 41202.6445\n",
      "Epoch 170/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308514880.0000 - rmse: 17564.5898 - val_loss: 1585636608.0000 - val_rmse: 39820.0547\n",
      "Epoch 171/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264555520.0000 - rmse: 16265.1611 - val_loss: 1350491392.0000 - val_rmse: 36749.0312\n",
      "Epoch 172/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277485312.0000 - rmse: 16657.8887 - val_loss: 1617525376.0000 - val_rmse: 40218.4688\n",
      "Epoch 173/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320061088.0000 - rmse: 17890.2500 - val_loss: 2049945344.0000 - val_rmse: 45276.3203\n",
      "Epoch 174/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278308864.0000 - rmse: 16682.5898 - val_loss: 1248872704.0000 - val_rmse: 35339.3906\n",
      "Epoch 175/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295562656.0000 - rmse: 17191.9336 - val_loss: 2003236864.0000 - val_rmse: 44757.5352\n",
      "Epoch 176/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285880480.0000 - rmse: 16908.0000 - val_loss: 1119256576.0000 - val_rmse: 33455.2930\n",
      "Epoch 177/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261729184.0000 - rmse: 16178.0449 - val_loss: 1288385408.0000 - val_rmse: 35894.0859\n",
      "Epoch 178/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258779440.0000 - rmse: 16086.6211 - val_loss: 1812991488.0000 - val_rmse: 42579.2383\n",
      "Epoch 179/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280293728.0000 - rmse: 16741.9746 - val_loss: 1696206336.0000 - val_rmse: 41185.0273\n",
      "Epoch 180/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298469472.0000 - rmse: 17276.2676 - val_loss: 1378772736.0000 - val_rmse: 37131.8281\n",
      "Epoch 181/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271683904.0000 - rmse: 16482.8340 - val_loss: 1061138880.0000 - val_rmse: 32575.1250\n",
      "Epoch 182/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257055328.0000 - rmse: 16032.9434 - val_loss: 1412980736.0000 - val_rmse: 37589.6328\n",
      "Epoch 183/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247598560.0000 - rmse: 15735.2646 - val_loss: 1776258688.0000 - val_rmse: 42145.6836\n",
      "Epoch 184/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255252400.0000 - rmse: 15976.6191 - val_loss: 1378876160.0000 - val_rmse: 37133.2227\n",
      "Epoch 185/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307425952.0000 - rmse: 17533.5645 - val_loss: 1244657792.0000 - val_rmse: 35279.7070\n",
      "Epoch 186/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306224544.0000 - rmse: 17499.2715 - val_loss: 1891575552.0000 - val_rmse: 43492.2461\n",
      "Epoch 187/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266855328.0000 - rmse: 16335.7061 - val_loss: 1159675520.0000 - val_rmse: 34054.0078\n",
      "Epoch 188/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276536864.0000 - rmse: 16629.3965 - val_loss: 1527501824.0000 - val_rmse: 39083.2695\n",
      "Epoch 189/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261290752.0000 - rmse: 16164.4893 - val_loss: 2321683200.0000 - val_rmse: 48183.8477\n",
      "Epoch 190/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232562720.0000 - rmse: 15250.0068 - val_loss: 1462262784.0000 - val_rmse: 38239.5430\n",
      "Epoch 191/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283479968.0000 - rmse: 16836.8633 - val_loss: 1181123840.0000 - val_rmse: 34367.4805\n",
      "Epoch 192/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249194368.0000 - rmse: 15785.8906 - val_loss: 1032374912.0000 - val_rmse: 32130.5898\n",
      "Epoch 193/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274276416.0000 - rmse: 16561.2891 - val_loss: 2151324160.0000 - val_rmse: 46382.3672\n",
      "Epoch 194/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242872656.0000 - rmse: 15584.3711 - val_loss: 1708415616.0000 - val_rmse: 41332.9844\n",
      "Epoch 195/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260859232.0000 - rmse: 16151.1357 - val_loss: 852034560.0000 - val_rmse: 29189.6309\n",
      "Epoch 196/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284698336.0000 - rmse: 16873.0059 - val_loss: 1050686784.0000 - val_rmse: 32414.2969\n",
      "Epoch 197/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260265552.0000 - rmse: 16132.7461 - val_loss: 1282171776.0000 - val_rmse: 35807.4258\n",
      "Epoch 198/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295790112.0000 - rmse: 17198.5469 - val_loss: 1936567680.0000 - val_rmse: 44006.4492\n",
      "Epoch 199/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262670432.0000 - rmse: 16207.1094 - val_loss: 1540552320.0000 - val_rmse: 39249.8711\n",
      "Epoch 200/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243955040.0000 - rmse: 15619.0576 - val_loss: 2029753088.0000 - val_rmse: 45052.7812\n",
      "Epoch 201/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255594848.0000 - rmse: 15987.3330 - val_loss: 2120969472.0000 - val_rmse: 46053.9844\n",
      "Epoch 202/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259198752.0000 - rmse: 16099.6494 - val_loss: 1433756544.0000 - val_rmse: 37864.9766\n",
      "Epoch 203/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263953408.0000 - rmse: 16246.6416 - val_loss: 1549516416.0000 - val_rmse: 39363.8984\n",
      "Epoch 204/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325405696.0000 - rmse: 18039.0039 - val_loss: 1839369344.0000 - val_rmse: 42887.8711\n",
      "Epoch 205/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265387616.0000 - rmse: 16290.7197 - val_loss: 1256270976.0000 - val_rmse: 35443.9102\n",
      "Epoch 206/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300462176.0000 - rmse: 17333.8457 - val_loss: 823787008.0000 - val_rmse: 28701.6895\n",
      "Epoch 207/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248259280.0000 - rmse: 15756.2461 - val_loss: 1572600192.0000 - val_rmse: 39656.0234\n",
      "Epoch 208/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 233006192.0000 - rmse: 15264.5391 - val_loss: 897729728.0000 - val_rmse: 29962.1367\n",
      "Epoch 209/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251061408.0000 - rmse: 15844.9160 - val_loss: 1110632320.0000 - val_rmse: 33326.1523\n",
      "Epoch 210/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240718096.0000 - rmse: 15515.0908 - val_loss: 1147006592.0000 - val_rmse: 33867.4844\n",
      "Epoch 211/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251764816.0000 - rmse: 15867.0977 - val_loss: 2626414848.0000 - val_rmse: 51248.5586\n",
      "Epoch 212/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234192720.0000 - rmse: 15303.3545 - val_loss: 1054155840.0000 - val_rmse: 32467.7656\n",
      "Epoch 213/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206025600.0000 - rmse: 14353.5898 - val_loss: 2077902208.0000 - val_rmse: 45584.0117\n",
      "Epoch 214/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265236544.0000 - rmse: 16286.0820 - val_loss: 948447168.0000 - val_rmse: 30796.8691\n",
      "Epoch 215/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232906976.0000 - rmse: 15261.2871 - val_loss: 2200423680.0000 - val_rmse: 46908.6719\n",
      "Epoch 216/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264937904.0000 - rmse: 16276.9082 - val_loss: 1107373312.0000 - val_rmse: 33277.2188\n",
      "Epoch 217/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 196773632.0000 - rmse: 14027.6016 - val_loss: 2515001600.0000 - val_rmse: 50149.7930\n",
      "Epoch 218/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216415920.0000 - rmse: 14711.0801 - val_loss: 2888059392.0000 - val_rmse: 53740.6641\n",
      "Epoch 219/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253648816.0000 - rmse: 15926.3525 - val_loss: 1838782720.0000 - val_rmse: 42881.0312\n",
      "Epoch 220/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274547360.0000 - rmse: 16569.4707 - val_loss: 1754287232.0000 - val_rmse: 41884.2109\n",
      "Epoch 221/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255943712.0000 - rmse: 15998.2402 - val_loss: 1431977984.0000 - val_rmse: 37841.4844\n",
      "Epoch 222/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273630336.0000 - rmse: 16541.7734 - val_loss: 2211964672.0000 - val_rmse: 47031.5273\n",
      "Epoch 223/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235247872.0000 - rmse: 15337.7910 - val_loss: 1498397184.0000 - val_rmse: 38709.1367\n",
      "Epoch 224/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272013824.0000 - rmse: 16492.8398 - val_loss: 2242924288.0000 - val_rmse: 47359.5234\n",
      "Epoch 225/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219249232.0000 - rmse: 14807.0654 - val_loss: 2201725952.0000 - val_rmse: 46922.5508\n",
      "Epoch 226/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266335712.0000 - rmse: 16319.7939 - val_loss: 1301126272.0000 - val_rmse: 36071.1289\n",
      "Epoch 227/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251343792.0000 - rmse: 15853.8242 - val_loss: 2535743488.0000 - val_rmse: 50356.1641\n",
      "Epoch 228/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242485968.0000 - rmse: 15571.9580 - val_loss: 1500543744.0000 - val_rmse: 38736.8516\n",
      "Epoch 229/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219359936.0000 - rmse: 14810.8027 - val_loss: 1448908160.0000 - val_rmse: 38064.5273\n",
      "Epoch 230/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236815472.0000 - rmse: 15388.8066 - val_loss: 2812285952.0000 - val_rmse: 53030.9922\n",
      "Epoch 231/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260384976.0000 - rmse: 16136.4473 - val_loss: 1630075136.0000 - val_rmse: 40374.1875\n",
      "Epoch 232/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 191410880.0000 - rmse: 13835.1309 - val_loss: 1143204736.0000 - val_rmse: 33811.3125\n",
      "Epoch 233/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 205181472.0000 - rmse: 14324.1562 - val_loss: 1274013696.0000 - val_rmse: 35693.3281\n",
      "Epoch 234/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290868192.0000 - rmse: 17054.8574 - val_loss: 2233828864.0000 - val_rmse: 47263.3945\n",
      "Epoch 235/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227028416.0000 - rmse: 15067.4580 - val_loss: 1522447872.0000 - val_rmse: 39018.5547\n",
      "Epoch 236/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253780480.0000 - rmse: 15930.4883 - val_loss: 1777451264.0000 - val_rmse: 42159.8281\n",
      "Epoch 237/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232708960.0000 - rmse: 15254.7988 - val_loss: 1208353024.0000 - val_rmse: 34761.3711\n",
      "Epoch 238/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216801216.0000 - rmse: 14724.1680 - val_loss: 1669229184.0000 - val_rmse: 40856.1992\n",
      "Epoch 239/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243469344.0000 - rmse: 15603.5039 - val_loss: 1844749056.0000 - val_rmse: 42950.5430\n",
      "Epoch 240/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238031744.0000 - rmse: 15428.2744 - val_loss: 1407827584.0000 - val_rmse: 37521.0273\n",
      "Epoch 241/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217939520.0000 - rmse: 14762.7744 - val_loss: 1177004800.0000 - val_rmse: 34307.5039\n",
      "Epoch 242/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261137424.0000 - rmse: 16159.7461 - val_loss: 1292574208.0000 - val_rmse: 35952.3867\n",
      "Epoch 243/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241571008.0000 - rmse: 15542.5537 - val_loss: 1881034368.0000 - val_rmse: 43370.8945\n",
      "Epoch 244/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207982224.0000 - rmse: 14421.5869 - val_loss: 629972864.0000 - val_rmse: 25099.2598\n",
      "Epoch 245/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239898976.0000 - rmse: 15488.6709 - val_loss: 2688252672.0000 - val_rmse: 51848.3633\n",
      "Epoch 246/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221532912.0000 - rmse: 14883.9814 - val_loss: 1327414912.0000 - val_rmse: 36433.7070\n",
      "Epoch 247/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265747696.0000 - rmse: 16301.7686 - val_loss: 2046224384.0000 - val_rmse: 45235.2070\n",
      "Epoch 248/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252309456.0000 - rmse: 15884.2490 - val_loss: 1345843584.0000 - val_rmse: 36685.7422\n",
      "Epoch 249/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235610912.0000 - rmse: 15349.6201 - val_loss: 2227969024.0000 - val_rmse: 47201.3633\n",
      "Epoch 250/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206553248.0000 - rmse: 14371.9590 - val_loss: 1002262144.0000 - val_rmse: 31658.5234\n",
      "Epoch 251/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249257664.0000 - rmse: 15787.8945 - val_loss: 1877326464.0000 - val_rmse: 43328.1250\n",
      "Epoch 252/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237861104.0000 - rmse: 15422.7451 - val_loss: 2123080448.0000 - val_rmse: 46076.8984\n",
      "Epoch 253/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 186718192.0000 - rmse: 13664.4844 - val_loss: 1587783680.0000 - val_rmse: 39847.0000\n",
      "Epoch 254/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238259168.0000 - rmse: 15435.6416 - val_loss: 2171646208.0000 - val_rmse: 46600.9258\n",
      "Epoch 255/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234074144.0000 - rmse: 15299.4814 - val_loss: 3377320704.0000 - val_rmse: 58114.7188\n",
      "Epoch 256/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227756688.0000 - rmse: 15091.6084 - val_loss: 1868883968.0000 - val_rmse: 43230.5898\n",
      "Epoch 257/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228621616.0000 - rmse: 15120.2354 - val_loss: 2215002880.0000 - val_rmse: 47063.8164\n",
      "Epoch 258/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231196224.0000 - rmse: 15205.1357 - val_loss: 3355161600.0000 - val_rmse: 57923.7578\n",
      "Epoch 259/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316071104.0000 - rmse: 17778.3887 - val_loss: 3182677248.0000 - val_rmse: 56415.2227\n",
      "Epoch 260/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 212208208.0000 - rmse: 14567.3662 - val_loss: 2976009472.0000 - val_rmse: 54552.8125\n",
      "Epoch 261/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218819840.0000 - rmse: 14792.5566 - val_loss: 2191963136.0000 - val_rmse: 46818.4062\n",
      "Epoch 262/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 204990208.0000 - rmse: 14317.4756 - val_loss: 1356383232.0000 - val_rmse: 36829.1055\n",
      "Epoch 263/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214445424.0000 - rmse: 14643.9531 - val_loss: 1226957184.0000 - val_rmse: 35027.9492\n",
      "Epoch 264/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202635712.0000 - rmse: 14235.0156 - val_loss: 2974646272.0000 - val_rmse: 54540.3164\n",
      "Epoch 265/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 187747152.0000 - rmse: 13702.0859 - val_loss: 1704789504.0000 - val_rmse: 41289.0977\n",
      "Epoch 266/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255228512.0000 - rmse: 15975.8721 - val_loss: 2043035264.0000 - val_rmse: 45199.9453\n",
      "Epoch 267/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266340032.0000 - rmse: 16319.9258 - val_loss: 1430137856.0000 - val_rmse: 37817.1602\n",
      "Epoch 268/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218588224.0000 - rmse: 14784.7285 - val_loss: 2275463424.0000 - val_rmse: 47701.8164\n",
      "Epoch 269/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238067440.0000 - rmse: 15429.4336 - val_loss: 2382668288.0000 - val_rmse: 48812.5820\n",
      "Epoch 270/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220846640.0000 - rmse: 14860.9082 - val_loss: 1414400896.0000 - val_rmse: 37608.5195\n",
      "Epoch 271/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254620640.0000 - rmse: 15956.8350 - val_loss: 1721495552.0000 - val_rmse: 41490.9102\n",
      "Epoch 272/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 197523808.0000 - rmse: 14054.3145 - val_loss: 1315345152.0000 - val_rmse: 36267.6875\n",
      "Epoch 273/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214255440.0000 - rmse: 14637.4648 - val_loss: 2402878976.0000 - val_rmse: 49019.1680\n",
      "Epoch 274/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263973952.0000 - rmse: 16247.2734 - val_loss: 2385350656.0000 - val_rmse: 48840.0508\n",
      "Epoch 275/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202461968.0000 - rmse: 14228.9111 - val_loss: 2025518720.0000 - val_rmse: 45005.7617\n",
      "Epoch 276/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 204513920.0000 - rmse: 14300.8350 - val_loss: 3295853312.0000 - val_rmse: 57409.5234\n",
      "Epoch 277/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230480352.0000 - rmse: 15181.5791 - val_loss: 2343825408.0000 - val_rmse: 48413.0664\n",
      "Epoch 278/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 193336544.0000 - rmse: 13904.5498 - val_loss: 864834304.0000 - val_rmse: 29408.0645\n",
      "104/104 [==============================] - 0s 723us/step - loss: 385747264.0000 - rmse: 19640.4473\n",
      "[385747264.0, 19640.447265625]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 13732402176.0000 - rmse: 117185.3359 - val_loss: 1928982784.0000 - val_rmse: 43920.1875\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2033532416.0000 - rmse: 45094.7031 - val_loss: 1283437824.0000 - val_rmse: 35825.1016\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1659207168.0000 - rmse: 40733.3672 - val_loss: 1133898624.0000 - val_rmse: 33673.4102\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1461281792.0000 - rmse: 38226.7148 - val_loss: 1045631296.0000 - val_rmse: 32336.2227\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1362513664.0000 - rmse: 36912.2422 - val_loss: 1044832128.0000 - val_rmse: 32323.8633\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1300185856.0000 - rmse: 36058.0898 - val_loss: 969100544.0000 - val_rmse: 31130.3789\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1223080320.0000 - rmse: 34972.5664 - val_loss: 949954048.0000 - val_rmse: 30821.3242\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1208412800.0000 - rmse: 34762.2305 - val_loss: 921682304.0000 - val_rmse: 30359.2207\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1136198656.0000 - rmse: 33707.5469 - val_loss: 868769536.0000 - val_rmse: 29474.8965\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1110939008.0000 - rmse: 33330.7500 - val_loss: 858280768.0000 - val_rmse: 29296.4297\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1076393984.0000 - rmse: 32808.4453 - val_loss: 877000640.0000 - val_rmse: 29614.1973\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1038754176.0000 - rmse: 32229.7090 - val_loss: 830287232.0000 - val_rmse: 28814.7051\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 992281920.0000 - rmse: 31500.5059 - val_loss: 796297984.0000 - val_rmse: 28218.7520\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 990316992.0000 - rmse: 31469.3008 - val_loss: 790560000.0000 - val_rmse: 28116.8984\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 926617792.0000 - rmse: 30440.3965 - val_loss: 837899136.0000 - val_rmse: 28946.4844\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 905213568.0000 - rmse: 30086.7676 - val_loss: 762805184.0000 - val_rmse: 27618.9277\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 866139968.0000 - rmse: 29430.2559 - val_loss: 762256576.0000 - val_rmse: 27608.9941\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 852377536.0000 - rmse: 29195.5059 - val_loss: 746645248.0000 - val_rmse: 27324.8105\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 821771136.0000 - rmse: 28666.5488 - val_loss: 794451904.0000 - val_rmse: 28186.0234\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 826582656.0000 - rmse: 28750.3496 - val_loss: 719843840.0000 - val_rmse: 26829.9062\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 785548352.0000 - rmse: 28027.6348 - val_loss: 700410880.0000 - val_rmse: 26465.2773\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 774461760.0000 - rmse: 27829.1523 - val_loss: 708504640.0000 - val_rmse: 26617.7500\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 774622656.0000 - rmse: 27832.0430 - val_loss: 678683712.0000 - val_rmse: 26051.5586\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 766621376.0000 - rmse: 27687.9277 - val_loss: 702529216.0000 - val_rmse: 26505.2676\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 757664128.0000 - rmse: 27525.6992 - val_loss: 682766976.0000 - val_rmse: 26129.8086\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 712952192.0000 - rmse: 26701.1641 - val_loss: 678152896.0000 - val_rmse: 26041.3672\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729931200.0000 - rmse: 27017.2383 - val_loss: 685436864.0000 - val_rmse: 26180.8496\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692000128.0000 - rmse: 26305.8945 - val_loss: 669147264.0000 - val_rmse: 25867.8809\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 722364928.0000 - rmse: 26876.8477 - val_loss: 729207552.0000 - val_rmse: 27003.8438\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 712848256.0000 - rmse: 26699.2188 - val_loss: 660222528.0000 - val_rmse: 25694.7949\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 662492928.0000 - rmse: 25738.9375 - val_loss: 642681152.0000 - val_rmse: 25351.1562\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 653103040.0000 - rmse: 25555.8809 - val_loss: 680892800.0000 - val_rmse: 26093.9219\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615455936.0000 - rmse: 24808.3828 - val_loss: 636582080.0000 - val_rmse: 25230.5762\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 707344064.0000 - rmse: 26595.9414 - val_loss: 613793472.0000 - val_rmse: 24774.8535\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 606333632.0000 - rmse: 24623.8418 - val_loss: 609558336.0000 - val_rmse: 24689.2344\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572641984.0000 - rmse: 23929.9395 - val_loss: 712634560.0000 - val_rmse: 26695.2168\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609811968.0000 - rmse: 24694.3711 - val_loss: 589991424.0000 - val_rmse: 24289.7383\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564389696.0000 - rmse: 23756.8867 - val_loss: 618609472.0000 - val_rmse: 24871.8594\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574606976.0000 - rmse: 23970.9609 - val_loss: 579946624.0000 - val_rmse: 24082.0801\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520276320.0000 - rmse: 22809.5664 - val_loss: 583287168.0000 - val_rmse: 24151.3379\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569613056.0000 - rmse: 23866.5684 - val_loss: 566860160.0000 - val_rmse: 23808.8262\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548080960.0000 - rmse: 23411.1289 - val_loss: 549416576.0000 - val_rmse: 23439.6367\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519305280.0000 - rmse: 22788.2715 - val_loss: 579477440.0000 - val_rmse: 24072.3359\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585759104.0000 - rmse: 24202.4609 - val_loss: 572890112.0000 - val_rmse: 23935.1211\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544205504.0000 - rmse: 23328.2129 - val_loss: 539351424.0000 - val_rmse: 23223.9414\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476636352.0000 - rmse: 21832.0039 - val_loss: 654386560.0000 - val_rmse: 25580.9805\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534541408.0000 - rmse: 23120.1504 - val_loss: 623443200.0000 - val_rmse: 24968.8438\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552934720.0000 - rmse: 23514.5625 - val_loss: 566452736.0000 - val_rmse: 23800.2656\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487218624.0000 - rmse: 22073.0293 - val_loss: 608905920.0000 - val_rmse: 24676.0195\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499305632.0000 - rmse: 22345.1465 - val_loss: 622607296.0000 - val_rmse: 24952.0996\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479831840.0000 - rmse: 21905.0645 - val_loss: 524833728.0000 - val_rmse: 22909.2500\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486722720.0000 - rmse: 22061.7930 - val_loss: 502075264.0000 - val_rmse: 22407.0352\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480788352.0000 - rmse: 21926.8867 - val_loss: 577389952.0000 - val_rmse: 24028.9395\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468327872.0000 - rmse: 21640.8828 - val_loss: 526206752.0000 - val_rmse: 22939.1973\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424515872.0000 - rmse: 20603.7832 - val_loss: 613453760.0000 - val_rmse: 24767.9980\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451026688.0000 - rmse: 21237.3887 - val_loss: 582683712.0000 - val_rmse: 24138.8418\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453450208.0000 - rmse: 21294.3711 - val_loss: 538838784.0000 - val_rmse: 23212.9004\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424283424.0000 - rmse: 20598.1406 - val_loss: 530031328.0000 - val_rmse: 23022.4082\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428776704.0000 - rmse: 20706.9238 - val_loss: 521681280.0000 - val_rmse: 22840.3438\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462647648.0000 - rmse: 21509.2441 - val_loss: 529174432.0000 - val_rmse: 23003.7910\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406556704.0000 - rmse: 20163.2500 - val_loss: 531269664.0000 - val_rmse: 23049.2871\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447685312.0000 - rmse: 21158.5762 - val_loss: 491467712.0000 - val_rmse: 22169.0703\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389144064.0000 - rmse: 19726.7344 - val_loss: 564915648.0000 - val_rmse: 23767.9551\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369602272.0000 - rmse: 19225.0430 - val_loss: 551925056.0000 - val_rmse: 23493.0859\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385301440.0000 - rmse: 19629.0957 - val_loss: 543484736.0000 - val_rmse: 23312.7578\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349407328.0000 - rmse: 18692.4395 - val_loss: 503687616.0000 - val_rmse: 22442.9844\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366896992.0000 - rmse: 19154.5547 - val_loss: 481696160.0000 - val_rmse: 21947.5762\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377810912.0000 - rmse: 19437.3594 - val_loss: 482613312.0000 - val_rmse: 21968.4609\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365838592.0000 - rmse: 19126.9082 - val_loss: 460901440.0000 - val_rmse: 21468.6152\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383146816.0000 - rmse: 19574.1348 - val_loss: 537642752.0000 - val_rmse: 23187.1250\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427095232.0000 - rmse: 20666.2832 - val_loss: 505781408.0000 - val_rmse: 22489.5840\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332186912.0000 - rmse: 18225.9922 - val_loss: 551285184.0000 - val_rmse: 23479.4609\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354336544.0000 - rmse: 18823.8301 - val_loss: 513535936.0000 - val_rmse: 22661.3320\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364354080.0000 - rmse: 19088.0605 - val_loss: 660674048.0000 - val_rmse: 25703.5801\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326667648.0000 - rmse: 18073.9473 - val_loss: 520070048.0000 - val_rmse: 22805.0449\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341935104.0000 - rmse: 18491.4883 - val_loss: 664423232.0000 - val_rmse: 25776.4082\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344609984.0000 - rmse: 18563.6719 - val_loss: 668157504.0000 - val_rmse: 25848.7422\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365555136.0000 - rmse: 19119.4961 - val_loss: 484687072.0000 - val_rmse: 22015.6094\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326009088.0000 - rmse: 18055.7227 - val_loss: 513773120.0000 - val_rmse: 22666.5645\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293262336.0000 - rmse: 17124.9043 - val_loss: 481441664.0000 - val_rmse: 21941.7773\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308122656.0000 - rmse: 17553.4219 - val_loss: 490724192.0000 - val_rmse: 22152.2949\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340087584.0000 - rmse: 18441.4629 - val_loss: 465818400.0000 - val_rmse: 21582.8242\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286861024.0000 - rmse: 16936.9707 - val_loss: 517667360.0000 - val_rmse: 22752.3047\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313837088.0000 - rmse: 17715.4453 - val_loss: 495663104.0000 - val_rmse: 22263.4922\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334762048.0000 - rmse: 18296.5020 - val_loss: 497024736.0000 - val_rmse: 22294.0508\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319833984.0000 - rmse: 17883.9023 - val_loss: 478695136.0000 - val_rmse: 21879.1016\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308555360.0000 - rmse: 17565.7422 - val_loss: 472290048.0000 - val_rmse: 21732.2344\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302223456.0000 - rmse: 17384.5742 - val_loss: 521684032.0000 - val_rmse: 22840.4023\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317276352.0000 - rmse: 17812.2520 - val_loss: 509799872.0000 - val_rmse: 22578.7480\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344420896.0000 - rmse: 18558.5781 - val_loss: 470286816.0000 - val_rmse: 21686.0957\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337231168.0000 - rmse: 18363.8535 - val_loss: 498145024.0000 - val_rmse: 22319.1621\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313402144.0000 - rmse: 17703.1660 - val_loss: 479958496.0000 - val_rmse: 21907.9551\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332435392.0000 - rmse: 18232.8086 - val_loss: 552117632.0000 - val_rmse: 23497.1836\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358285824.0000 - rmse: 18928.4395 - val_loss: 472071776.0000 - val_rmse: 21727.2129\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304543392.0000 - rmse: 17451.1719 - val_loss: 488969312.0000 - val_rmse: 22112.6504\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329320288.0000 - rmse: 18147.1836 - val_loss: 468618304.0000 - val_rmse: 21647.5898\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275057568.0000 - rmse: 16584.8574 - val_loss: 510733088.0000 - val_rmse: 22599.4023\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304173728.0000 - rmse: 17440.5762 - val_loss: 493225184.0000 - val_rmse: 22208.6719\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302929920.0000 - rmse: 17404.8809 - val_loss: 543355712.0000 - val_rmse: 23309.9922\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303309600.0000 - rmse: 17415.7852 - val_loss: 484104352.0000 - val_rmse: 22002.3691\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303480928.0000 - rmse: 17420.7031 - val_loss: 431594784.0000 - val_rmse: 20774.8574\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329878304.0000 - rmse: 18162.5508 - val_loss: 504401824.0000 - val_rmse: 22458.8906\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275636512.0000 - rmse: 16602.3027 - val_loss: 503257632.0000 - val_rmse: 22433.4023\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306450176.0000 - rmse: 17505.7188 - val_loss: 461224704.0000 - val_rmse: 21476.1406\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282396416.0000 - rmse: 16804.6543 - val_loss: 448935328.0000 - val_rmse: 21188.0918\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277362624.0000 - rmse: 16654.2051 - val_loss: 463245920.0000 - val_rmse: 21523.1465\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335738272.0000 - rmse: 18323.1621 - val_loss: 472786912.0000 - val_rmse: 21743.6621\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261140736.0000 - rmse: 16159.8477 - val_loss: 525218848.0000 - val_rmse: 22917.6523\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283929568.0000 - rmse: 16850.2090 - val_loss: 500872800.0000 - val_rmse: 22380.1875\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267157504.0000 - rmse: 16344.9512 - val_loss: 473215136.0000 - val_rmse: 21753.5078\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282623328.0000 - rmse: 16811.4023 - val_loss: 463457056.0000 - val_rmse: 21528.0527\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277196992.0000 - rmse: 16649.2344 - val_loss: 450749792.0000 - val_rmse: 21230.8672\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269585664.0000 - rmse: 16419.0625 - val_loss: 434770112.0000 - val_rmse: 20851.1406\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284677088.0000 - rmse: 16872.3770 - val_loss: 424508896.0000 - val_rmse: 20603.6133\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264335808.0000 - rmse: 16258.4062 - val_loss: 456624288.0000 - val_rmse: 21368.7676\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263578112.0000 - rmse: 16235.0879 - val_loss: 455384704.0000 - val_rmse: 21339.7422\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251953824.0000 - rmse: 15873.0527 - val_loss: 555049536.0000 - val_rmse: 23559.4902\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299115776.0000 - rmse: 17294.9648 - val_loss: 504716096.0000 - val_rmse: 22465.8867\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275973344.0000 - rmse: 16612.4434 - val_loss: 492932064.0000 - val_rmse: 22202.0723\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255502496.0000 - rmse: 15984.4443 - val_loss: 459011712.0000 - val_rmse: 21424.5566\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249604256.0000 - rmse: 15798.8672 - val_loss: 472579776.0000 - val_rmse: 21738.8984\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265937088.0000 - rmse: 16307.5762 - val_loss: 526539040.0000 - val_rmse: 22946.4375\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274322656.0000 - rmse: 16562.6895 - val_loss: 497494944.0000 - val_rmse: 22304.5938\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243434560.0000 - rmse: 15602.3877 - val_loss: 473558752.0000 - val_rmse: 21761.4023\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271035520.0000 - rmse: 16463.1543 - val_loss: 434343296.0000 - val_rmse: 20840.9043\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247973488.0000 - rmse: 15747.1729 - val_loss: 479461824.0000 - val_rmse: 21896.6152\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263573616.0000 - rmse: 16234.9502 - val_loss: 545704896.0000 - val_rmse: 23360.3281\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253394432.0000 - rmse: 15918.3662 - val_loss: 482686880.0000 - val_rmse: 21970.1348\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281562368.0000 - rmse: 16779.8184 - val_loss: 546757312.0000 - val_rmse: 23382.8398\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240410240.0000 - rmse: 15505.1670 - val_loss: 459650848.0000 - val_rmse: 21439.4688\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264361136.0000 - rmse: 16259.1846 - val_loss: 507412832.0000 - val_rmse: 22525.8242\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251779536.0000 - rmse: 15867.5605 - val_loss: 512851712.0000 - val_rmse: 22646.2285\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259712928.0000 - rmse: 16115.6084 - val_loss: 462711360.0000 - val_rmse: 21510.7266\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264698688.0000 - rmse: 16269.5615 - val_loss: 505011616.0000 - val_rmse: 22472.4609\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256776688.0000 - rmse: 16024.2520 - val_loss: 473748896.0000 - val_rmse: 21765.7715\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252612976.0000 - rmse: 15893.8018 - val_loss: 465489600.0000 - val_rmse: 21575.2070\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261631472.0000 - rmse: 16175.0254 - val_loss: 554378368.0000 - val_rmse: 23545.2402\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242328912.0000 - rmse: 15566.9150 - val_loss: 522008000.0000 - val_rmse: 22847.4922\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242372432.0000 - rmse: 15568.3135 - val_loss: 466044160.0000 - val_rmse: 21588.0547\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246986672.0000 - rmse: 15715.8076 - val_loss: 541531392.0000 - val_rmse: 23270.8262\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264880000.0000 - rmse: 16275.1328 - val_loss: 538750592.0000 - val_rmse: 23210.9980\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261557728.0000 - rmse: 16172.7451 - val_loss: 451228032.0000 - val_rmse: 21242.1289\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257322864.0000 - rmse: 16041.2861 - val_loss: 467520704.0000 - val_rmse: 21622.2266\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260845152.0000 - rmse: 16150.7012 - val_loss: 461488064.0000 - val_rmse: 21482.2734\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272394240.0000 - rmse: 16504.3691 - val_loss: 465662624.0000 - val_rmse: 21579.2168\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254260496.0000 - rmse: 15945.5469 - val_loss: 458888544.0000 - val_rmse: 21421.6816\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239895968.0000 - rmse: 15488.5732 - val_loss: 457487008.0000 - val_rmse: 21388.9453\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247230912.0000 - rmse: 15723.5771 - val_loss: 450208864.0000 - val_rmse: 21218.1230\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347347488.0000 - rmse: 18637.2598 - val_loss: 457547424.0000 - val_rmse: 21390.3574\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246857168.0000 - rmse: 15711.6855 - val_loss: 457088512.0000 - val_rmse: 21379.6270\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226651904.0000 - rmse: 15054.9609 - val_loss: 438491968.0000 - val_rmse: 20940.1973\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250358160.0000 - rmse: 15822.7070 - val_loss: 489429056.0000 - val_rmse: 22123.0430\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232174656.0000 - rmse: 15237.2773 - val_loss: 469598400.0000 - val_rmse: 21670.2168\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260072992.0000 - rmse: 16126.7773 - val_loss: 553969984.0000 - val_rmse: 23536.5664\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227946704.0000 - rmse: 15097.9023 - val_loss: 448603264.0000 - val_rmse: 21180.2559\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227039120.0000 - rmse: 15067.8154 - val_loss: 449324192.0000 - val_rmse: 21197.2676\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251071600.0000 - rmse: 15845.2373 - val_loss: 438511456.0000 - val_rmse: 20940.6641\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265199936.0000 - rmse: 16284.9590 - val_loss: 409759136.0000 - val_rmse: 20242.5078\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222947296.0000 - rmse: 14931.4189 - val_loss: 437127456.0000 - val_rmse: 20907.5918\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224776304.0000 - rmse: 14992.5400 - val_loss: 515700128.0000 - val_rmse: 22709.0312\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237437072.0000 - rmse: 15408.9912 - val_loss: 447310176.0000 - val_rmse: 21149.7070\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230144240.0000 - rmse: 15170.5039 - val_loss: 438645568.0000 - val_rmse: 20943.8652\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250843632.0000 - rmse: 15838.0410 - val_loss: 434179296.0000 - val_rmse: 20836.9668\n",
      "Epoch 164/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219566192.0000 - rmse: 14817.7646 - val_loss: 420505792.0000 - val_rmse: 20506.2383\n",
      "Epoch 165/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276998848.0000 - rmse: 16643.2812 - val_loss: 550899584.0000 - val_rmse: 23471.2500\n",
      "Epoch 166/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214613232.0000 - rmse: 14649.6836 - val_loss: 433062080.0000 - val_rmse: 20810.1406\n",
      "Epoch 167/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257195024.0000 - rmse: 16037.2998 - val_loss: 458333248.0000 - val_rmse: 21408.7188\n",
      "Epoch 168/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232761248.0000 - rmse: 15256.5137 - val_loss: 474533376.0000 - val_rmse: 21783.7871\n",
      "Epoch 169/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227304304.0000 - rmse: 15076.6123 - val_loss: 411093248.0000 - val_rmse: 20275.4336\n",
      "Epoch 170/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 223105712.0000 - rmse: 14936.7207 - val_loss: 434172960.0000 - val_rmse: 20836.8164\n",
      "Epoch 171/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217480496.0000 - rmse: 14747.2188 - val_loss: 420104160.0000 - val_rmse: 20496.4414\n",
      "Epoch 172/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248690352.0000 - rmse: 15769.9180 - val_loss: 453895904.0000 - val_rmse: 21304.8320\n",
      "Epoch 173/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234315728.0000 - rmse: 15307.3740 - val_loss: 439200608.0000 - val_rmse: 20957.1113\n",
      "Epoch 174/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258301872.0000 - rmse: 16071.7715 - val_loss: 419517824.0000 - val_rmse: 20482.1328\n",
      "Epoch 175/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221199408.0000 - rmse: 14872.7725 - val_loss: 438975936.0000 - val_rmse: 20951.7520\n",
      "Epoch 176/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253070960.0000 - rmse: 15908.2031 - val_loss: 406326016.0000 - val_rmse: 20157.5293\n",
      "Epoch 177/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235614384.0000 - rmse: 15349.7354 - val_loss: 430725984.0000 - val_rmse: 20753.9375\n",
      "Epoch 178/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220376480.0000 - rmse: 14845.0801 - val_loss: 418544928.0000 - val_rmse: 20458.3691\n",
      "Epoch 179/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228101584.0000 - rmse: 15103.0312 - val_loss: 438717888.0000 - val_rmse: 20945.5918\n",
      "Epoch 180/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216804544.0000 - rmse: 14724.2822 - val_loss: 464300064.0000 - val_rmse: 21547.6211\n",
      "Epoch 181/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211935296.0000 - rmse: 14557.9961 - val_loss: 455693664.0000 - val_rmse: 21346.9824\n",
      "Epoch 182/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256914128.0000 - rmse: 16028.5391 - val_loss: 423382688.0000 - val_rmse: 20576.2656\n",
      "Epoch 183/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244894032.0000 - rmse: 15649.0879 - val_loss: 402814208.0000 - val_rmse: 20070.2305\n",
      "Epoch 184/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239080384.0000 - rmse: 15462.2236 - val_loss: 426128064.0000 - val_rmse: 20642.8672\n",
      "Epoch 185/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269678048.0000 - rmse: 16421.8770 - val_loss: 404113216.0000 - val_rmse: 20102.5684\n",
      "Epoch 186/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234271216.0000 - rmse: 15305.9189 - val_loss: 487883520.0000 - val_rmse: 22088.0840\n",
      "Epoch 187/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215209536.0000 - rmse: 14670.0186 - val_loss: 493336192.0000 - val_rmse: 22211.1719\n",
      "Epoch 188/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214246512.0000 - rmse: 14637.1602 - val_loss: 421180832.0000 - val_rmse: 20522.6895\n",
      "Epoch 189/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214292128.0000 - rmse: 14638.7188 - val_loss: 424386240.0000 - val_rmse: 20600.6367\n",
      "Epoch 190/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220548688.0000 - rmse: 14850.8809 - val_loss: 435624864.0000 - val_rmse: 20871.6270\n",
      "Epoch 191/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219675888.0000 - rmse: 14821.4648 - val_loss: 440456960.0000 - val_rmse: 20987.0664\n",
      "Epoch 192/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 213835568.0000 - rmse: 14623.1152 - val_loss: 433717152.0000 - val_rmse: 20825.8750\n",
      "Epoch 193/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 194982448.0000 - rmse: 13963.6094 - val_loss: 420541536.0000 - val_rmse: 20507.1074\n",
      "Epoch 194/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211452048.0000 - rmse: 14541.3906 - val_loss: 420881472.0000 - val_rmse: 20515.3965\n",
      "Epoch 195/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 223306272.0000 - rmse: 14943.4326 - val_loss: 493210912.0000 - val_rmse: 22208.3496\n",
      "Epoch 196/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215928624.0000 - rmse: 14694.5088 - val_loss: 440024064.0000 - val_rmse: 20976.7480\n",
      "Epoch 197/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254442336.0000 - rmse: 15951.2461 - val_loss: 425791008.0000 - val_rmse: 20634.7031\n",
      "Epoch 198/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210569664.0000 - rmse: 14511.0156 - val_loss: 430865664.0000 - val_rmse: 20757.3027\n",
      "Epoch 199/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 199259696.0000 - rmse: 14115.9365 - val_loss: 630678656.0000 - val_rmse: 25113.3145\n",
      "Epoch 200/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232688752.0000 - rmse: 15254.1377 - val_loss: 430296448.0000 - val_rmse: 20743.5859\n",
      "Epoch 201/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239696640.0000 - rmse: 15482.1367 - val_loss: 419956800.0000 - val_rmse: 20492.8477\n",
      "Epoch 202/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243507088.0000 - rmse: 15604.7109 - val_loss: 422974272.0000 - val_rmse: 20566.3379\n",
      "Epoch 203/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211534160.0000 - rmse: 14544.2109 - val_loss: 431265888.0000 - val_rmse: 20766.9395\n",
      "Epoch 204/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 203971152.0000 - rmse: 14281.8447 - val_loss: 418653248.0000 - val_rmse: 20461.0156\n",
      "Epoch 205/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 198324016.0000 - rmse: 14082.7529 - val_loss: 427132928.0000 - val_rmse: 20667.1934\n",
      "Epoch 206/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226758528.0000 - rmse: 15058.5020 - val_loss: 423359264.0000 - val_rmse: 20575.6953\n",
      "Epoch 207/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 196954176.0000 - rmse: 14034.0342 - val_loss: 419591104.0000 - val_rmse: 20483.9199\n",
      "Epoch 208/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 204458992.0000 - rmse: 14298.9131 - val_loss: 456473408.0000 - val_rmse: 21365.2383\n",
      "Epoch 209/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232945488.0000 - rmse: 15262.5498 - val_loss: 470124960.0000 - val_rmse: 21682.3633\n",
      "Epoch 210/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216107456.0000 - rmse: 14700.5908 - val_loss: 424127328.0000 - val_rmse: 20594.3496\n",
      "Epoch 211/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231534336.0000 - rmse: 15216.2490 - val_loss: 450405984.0000 - val_rmse: 21222.7676\n",
      "Epoch 212/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232069936.0000 - rmse: 15233.8398 - val_loss: 392537088.0000 - val_rmse: 19812.5469\n",
      "Epoch 213/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214262512.0000 - rmse: 14637.7061 - val_loss: 407428832.0000 - val_rmse: 20184.8652\n",
      "Epoch 214/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 213402752.0000 - rmse: 14608.3096 - val_loss: 413173792.0000 - val_rmse: 20326.6758\n",
      "Epoch 215/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 203282096.0000 - rmse: 14257.7002 - val_loss: 430989504.0000 - val_rmse: 20760.2852\n",
      "Epoch 216/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218395760.0000 - rmse: 14778.2178 - val_loss: 425520224.0000 - val_rmse: 20628.1387\n",
      "Epoch 217/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 203668896.0000 - rmse: 14271.2598 - val_loss: 404154528.0000 - val_rmse: 20103.5938\n",
      "Epoch 218/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206781632.0000 - rmse: 14379.9014 - val_loss: 421027488.0000 - val_rmse: 20518.9531\n",
      "104/104 [==============================] - 0s 693us/step - loss: 958121728.0000 - rmse: 30953.5410\n",
      "[958121728.0, 30953.541015625]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 14024437760.0000 - rmse: 118424.8203 - val_loss: 2242342912.0000 - val_rmse: 47353.3828\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1992971136.0000 - rmse: 44642.7070 - val_loss: 1512609408.0000 - val_rmse: 38892.2812\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1611124224.0000 - rmse: 40138.8125 - val_loss: 1381767936.0000 - val_rmse: 37172.1406\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1419869056.0000 - rmse: 37681.1484 - val_loss: 1178126080.0000 - val_rmse: 34323.8398\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1295618176.0000 - rmse: 35994.6953 - val_loss: 1120379264.0000 - val_rmse: 33472.0664\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1248258944.0000 - rmse: 35330.7070 - val_loss: 1025802240.0000 - val_rmse: 32028.1484\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1187933312.0000 - rmse: 34466.4102 - val_loss: 982370944.0000 - val_rmse: 31342.7969\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1128717568.0000 - rmse: 33596.3906 - val_loss: 972002688.0000 - val_rmse: 31176.9570\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1103728512.0000 - rmse: 33222.4102 - val_loss: 928849088.0000 - val_rmse: 30477.0254\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1070474304.0000 - rmse: 32718.1035 - val_loss: 926362176.0000 - val_rmse: 30436.1992\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1041719360.0000 - rmse: 32275.6777 - val_loss: 957634048.0000 - val_rmse: 30945.6621\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1028946240.0000 - rmse: 32077.1914 - val_loss: 939459648.0000 - val_rmse: 30650.6055\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1006363584.0000 - rmse: 31723.2344 - val_loss: 970344448.0000 - val_rmse: 31150.3516\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 995489280.0000 - rmse: 31551.3750 - val_loss: 963608960.0000 - val_rmse: 31042.0508\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 981219520.0000 - rmse: 31324.4238 - val_loss: 900109120.0000 - val_rmse: 30001.8184\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 936401216.0000 - rmse: 30600.6738 - val_loss: 932572608.0000 - val_rmse: 30538.0527\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 919460800.0000 - rmse: 30322.6113 - val_loss: 895511232.0000 - val_rmse: 29925.0938\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 953691840.0000 - rmse: 30881.9004 - val_loss: 904254528.0000 - val_rmse: 30070.8262\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 956328192.0000 - rmse: 30924.5566 - val_loss: 1165997184.0000 - val_rmse: 34146.6992\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 952897600.0000 - rmse: 30869.0391 - val_loss: 896619456.0000 - val_rmse: 29943.6055\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 930768576.0000 - rmse: 30508.5000 - val_loss: 891474624.0000 - val_rmse: 29857.5703\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 956552000.0000 - rmse: 30928.1758 - val_loss: 885615168.0000 - val_rmse: 29759.2871\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 893599936.0000 - rmse: 29893.1406 - val_loss: 894321216.0000 - val_rmse: 29905.2012\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 878949760.0000 - rmse: 29647.0859 - val_loss: 883243200.0000 - val_rmse: 29719.4082\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 892779072.0000 - rmse: 29879.4082 - val_loss: 883775104.0000 - val_rmse: 29728.3535\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 852034304.0000 - rmse: 29189.6270 - val_loss: 1075484032.0000 - val_rmse: 32794.5742\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 908743232.0000 - rmse: 30145.3691 - val_loss: 906880768.0000 - val_rmse: 30114.4609\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 893903616.0000 - rmse: 29898.2207 - val_loss: 905727936.0000 - val_rmse: 30095.3145\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 898247552.0000 - rmse: 29970.7773 - val_loss: 897195776.0000 - val_rmse: 29953.2266\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 878706368.0000 - rmse: 29642.9824 - val_loss: 905585600.0000 - val_rmse: 30092.9492\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 833100032.0000 - rmse: 28863.4727 - val_loss: 882379456.0000 - val_rmse: 29704.8730\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 887115072.0000 - rmse: 29784.4766 - val_loss: 909656000.0000 - val_rmse: 30160.5039\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 836893824.0000 - rmse: 28929.1172 - val_loss: 1028860032.0000 - val_rmse: 32075.8477\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 822198400.0000 - rmse: 28674.0020 - val_loss: 1031486592.0000 - val_rmse: 32116.7656\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 815115712.0000 - rmse: 28550.2305 - val_loss: 895710784.0000 - val_rmse: 29928.4277\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 808728320.0000 - rmse: 28438.1484 - val_loss: 1054243328.0000 - val_rmse: 32469.1133\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 764450048.0000 - rmse: 27648.6895 - val_loss: 1225802752.0000 - val_rmse: 35011.4648\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 785728320.0000 - rmse: 28030.8457 - val_loss: 993473408.0000 - val_rmse: 31519.4121\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 769796352.0000 - rmse: 27745.2051 - val_loss: 911328384.0000 - val_rmse: 30188.2168\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 766509248.0000 - rmse: 27685.9043 - val_loss: 1029248512.0000 - val_rmse: 32081.9023\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 759455296.0000 - rmse: 27558.2168 - val_loss: 1076908672.0000 - val_rmse: 32816.2852\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 783646528.0000 - rmse: 27993.6855 - val_loss: 914750528.0000 - val_rmse: 30244.8438\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730123200.0000 - rmse: 27020.7930 - val_loss: 1075566720.0000 - val_rmse: 32795.8320\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 791768448.0000 - rmse: 28138.3809 - val_loss: 959393792.0000 - val_rmse: 30974.0820\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 710710528.0000 - rmse: 26659.1543 - val_loss: 961945920.0000 - val_rmse: 31015.2539\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 695716608.0000 - rmse: 26376.4395 - val_loss: 926090560.0000 - val_rmse: 30431.7363\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 749758272.0000 - rmse: 27381.7148 - val_loss: 965521152.0000 - val_rmse: 31072.8359\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684712000.0000 - rmse: 26167.0020 - val_loss: 1030776256.0000 - val_rmse: 32105.7051\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 745320256.0000 - rmse: 27300.5527 - val_loss: 997170176.0000 - val_rmse: 31578.0020\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 705525184.0000 - rmse: 26561.7227 - val_loss: 1057080640.0000 - val_rmse: 32512.7773\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629672384.0000 - rmse: 25093.2734 - val_loss: 980758144.0000 - val_rmse: 31317.0586\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668044608.0000 - rmse: 25846.5586 - val_loss: 1100977024.0000 - val_rmse: 33180.9727\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 707920128.0000 - rmse: 26606.7676 - val_loss: 1222064128.0000 - val_rmse: 34958.0312\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 670763200.0000 - rmse: 25899.0957 - val_loss: 964750528.0000 - val_rmse: 31060.4336\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 688098048.0000 - rmse: 26231.6211 - val_loss: 1083009536.0000 - val_rmse: 32909.1094\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668537216.0000 - rmse: 25856.0859 - val_loss: 898420736.0000 - val_rmse: 29973.6680\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 641246784.0000 - rmse: 25322.8516 - val_loss: 915226560.0000 - val_rmse: 30252.7090\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642067328.0000 - rmse: 25339.0469 - val_loss: 931633728.0000 - val_rmse: 30522.6738\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618475648.0000 - rmse: 24869.1699 - val_loss: 953819776.0000 - val_rmse: 30883.9727\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609718080.0000 - rmse: 24692.4707 - val_loss: 1186670336.0000 - val_rmse: 34448.0781\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 659717952.0000 - rmse: 25684.9746 - val_loss: 973310976.0000 - val_rmse: 31197.9316\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575071552.0000 - rmse: 23980.6465 - val_loss: 1440506624.0000 - val_rmse: 37954.0078\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585650112.0000 - rmse: 24200.2090 - val_loss: 930209664.0000 - val_rmse: 30499.3359\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605152640.0000 - rmse: 24599.8496 - val_loss: 1074788096.0000 - val_rmse: 32783.9609\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 575254592.0000 - rmse: 23984.4648 - val_loss: 983836480.0000 - val_rmse: 31366.1660\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585714240.0000 - rmse: 24201.5332 - val_loss: 1417075200.0000 - val_rmse: 37644.0586\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560439296.0000 - rmse: 23673.5957 - val_loss: 914690816.0000 - val_rmse: 30243.8555\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538851584.0000 - rmse: 23213.1758 - val_loss: 935130624.0000 - val_rmse: 30579.9043\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598658112.0000 - rmse: 24467.4902 - val_loss: 1113528704.0000 - val_rmse: 33369.5781\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544877824.0000 - rmse: 23342.6172 - val_loss: 941516224.0000 - val_rmse: 30684.1367\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564311808.0000 - rmse: 23755.2480 - val_loss: 1000555968.0000 - val_rmse: 31631.5664\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565168192.0000 - rmse: 23773.2656 - val_loss: 954121600.0000 - val_rmse: 30888.8594\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520953408.0000 - rmse: 22824.4023 - val_loss: 950317504.0000 - val_rmse: 30827.2207\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487895552.0000 - rmse: 22088.3555 - val_loss: 1647925888.0000 - val_rmse: 40594.6523\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 547408896.0000 - rmse: 23396.7695 - val_loss: 890567936.0000 - val_rmse: 29842.3848\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476916032.0000 - rmse: 21838.4082 - val_loss: 830162624.0000 - val_rmse: 28812.5430\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434809280.0000 - rmse: 20852.0801 - val_loss: 1013635136.0000 - val_rmse: 31837.6348\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501193792.0000 - rmse: 22387.3574 - val_loss: 1145415296.0000 - val_rmse: 33843.9844\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501561632.0000 - rmse: 22395.5703 - val_loss: 931253056.0000 - val_rmse: 30516.4375\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504242592.0000 - rmse: 22455.3457 - val_loss: 933226304.0000 - val_rmse: 30548.7520\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467510560.0000 - rmse: 21621.9922 - val_loss: 1055042624.0000 - val_rmse: 32481.4160\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462310688.0000 - rmse: 21501.4121 - val_loss: 898109888.0000 - val_rmse: 29968.4824\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 431445472.0000 - rmse: 20771.2637 - val_loss: 944060032.0000 - val_rmse: 30725.5605\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469769152.0000 - rmse: 21674.1582 - val_loss: 761931136.0000 - val_rmse: 27603.0996\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438458592.0000 - rmse: 20939.4023 - val_loss: 815473408.0000 - val_rmse: 28556.4941\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482907968.0000 - rmse: 21975.1660 - val_loss: 1040460032.0000 - val_rmse: 32256.1621\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 454286624.0000 - rmse: 21313.9980 - val_loss: 615346496.0000 - val_rmse: 24806.1777\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410672864.0000 - rmse: 20265.0645 - val_loss: 1266007552.0000 - val_rmse: 35581.0000\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403649152.0000 - rmse: 20091.0215 - val_loss: 983891328.0000 - val_rmse: 31367.0430\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468934080.0000 - rmse: 21654.8867 - val_loss: 701726528.0000 - val_rmse: 26490.1211\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437377824.0000 - rmse: 20913.5801 - val_loss: 909200512.0000 - val_rmse: 30152.9512\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437516800.0000 - rmse: 20916.9023 - val_loss: 526504768.0000 - val_rmse: 22945.6914\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416366656.0000 - rmse: 20405.0625 - val_loss: 574469248.0000 - val_rmse: 23968.0879\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434401792.0000 - rmse: 20842.3066 - val_loss: 644149952.0000 - val_rmse: 25380.1074\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443958752.0000 - rmse: 21070.3281 - val_loss: 560350464.0000 - val_rmse: 23671.7207\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410564832.0000 - rmse: 20262.4004 - val_loss: 461032352.0000 - val_rmse: 21471.6641\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392126560.0000 - rmse: 19802.1836 - val_loss: 535504320.0000 - val_rmse: 23140.9668\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448519232.0000 - rmse: 21178.2715 - val_loss: 1386698368.0000 - val_rmse: 37238.3984\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404721088.0000 - rmse: 20117.6797 - val_loss: 586173504.0000 - val_rmse: 24211.0195\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390762656.0000 - rmse: 19767.7168 - val_loss: 461581568.0000 - val_rmse: 21484.4492\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371573408.0000 - rmse: 19276.2383 - val_loss: 394505568.0000 - val_rmse: 19862.1621\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378398240.0000 - rmse: 19452.4609 - val_loss: 531712288.0000 - val_rmse: 23058.8867\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401959520.0000 - rmse: 20048.9277 - val_loss: 638145408.0000 - val_rmse: 25261.5410\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395430496.0000 - rmse: 19885.4336 - val_loss: 375805184.0000 - val_rmse: 19385.6914\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387927456.0000 - rmse: 19695.8730 - val_loss: 634489984.0000 - val_rmse: 25189.0840\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384978240.0000 - rmse: 19620.8613 - val_loss: 587720896.0000 - val_rmse: 24242.9531\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 384615296.0000 - rmse: 19611.6113 - val_loss: 787418496.0000 - val_rmse: 28060.9785\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393872352.0000 - rmse: 19846.2168 - val_loss: 763019584.0000 - val_rmse: 27622.8066\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393779648.0000 - rmse: 19843.8809 - val_loss: 791906880.0000 - val_rmse: 28140.8398\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313134208.0000 - rmse: 17695.5977 - val_loss: 1139196416.0000 - val_rmse: 33751.9844\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357140640.0000 - rmse: 18898.1621 - val_loss: 630562432.0000 - val_rmse: 25111.0020\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376062848.0000 - rmse: 19392.3398 - val_loss: 602700032.0000 - val_rmse: 24549.9492\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346301248.0000 - rmse: 18609.1699 - val_loss: 588204224.0000 - val_rmse: 24252.9199\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336863040.0000 - rmse: 18353.8281 - val_loss: 531314112.0000 - val_rmse: 23050.2520\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365441056.0000 - rmse: 19116.5117 - val_loss: 627735872.0000 - val_rmse: 25054.6562\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316434912.0000 - rmse: 17788.6172 - val_loss: 619187776.0000 - val_rmse: 24883.4805\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312651392.0000 - rmse: 17681.9492 - val_loss: 568195072.0000 - val_rmse: 23836.8418\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303561088.0000 - rmse: 17423.0039 - val_loss: 567809920.0000 - val_rmse: 23828.7617\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310766496.0000 - rmse: 17628.5703 - val_loss: 728883712.0000 - val_rmse: 26997.8457\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347146240.0000 - rmse: 18631.8594 - val_loss: 620827520.0000 - val_rmse: 24916.4102\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324676992.0000 - rmse: 18018.7949 - val_loss: 999544576.0000 - val_rmse: 31615.5742\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277401280.0000 - rmse: 16655.3672 - val_loss: 728138176.0000 - val_rmse: 26984.0352\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350794720.0000 - rmse: 18729.5137 - val_loss: 495674912.0000 - val_rmse: 22263.7559\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298437952.0000 - rmse: 17275.3555 - val_loss: 427462624.0000 - val_rmse: 20675.1680\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289643968.0000 - rmse: 17018.9277 - val_loss: 583386880.0000 - val_rmse: 24153.4023\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336264608.0000 - rmse: 18337.5176 - val_loss: 695594944.0000 - val_rmse: 26374.1348\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295647776.0000 - rmse: 17194.4102 - val_loss: 558490816.0000 - val_rmse: 23632.4102\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313239744.0000 - rmse: 17698.5781 - val_loss: 736867392.0000 - val_rmse: 27145.3008\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320344992.0000 - rmse: 17898.1836 - val_loss: 434388512.0000 - val_rmse: 20841.9883\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311212960.0000 - rmse: 17641.2285 - val_loss: 574846144.0000 - val_rmse: 23975.9473\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322019712.0000 - rmse: 17944.9043 - val_loss: 601991552.0000 - val_rmse: 24535.5156\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276378368.0000 - rmse: 16624.6309 - val_loss: 475529120.0000 - val_rmse: 21806.6309\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266085168.0000 - rmse: 16312.1152 - val_loss: 751718400.0000 - val_rmse: 27417.4824\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307369632.0000 - rmse: 17531.9609 - val_loss: 601974720.0000 - val_rmse: 24535.1719\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281586016.0000 - rmse: 16780.5234 - val_loss: 605707520.0000 - val_rmse: 24611.1250\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281693760.0000 - rmse: 16783.7324 - val_loss: 534761856.0000 - val_rmse: 23124.9180\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274881920.0000 - rmse: 16579.5605 - val_loss: 568087232.0000 - val_rmse: 23834.5781\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333238304.0000 - rmse: 18254.8145 - val_loss: 469685920.0000 - val_rmse: 21672.2383\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323434208.0000 - rmse: 17984.2754 - val_loss: 400296000.0000 - val_rmse: 20007.3984\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296006368.0000 - rmse: 17204.8340 - val_loss: 490294496.0000 - val_rmse: 22142.5938\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307996384.0000 - rmse: 17549.8262 - val_loss: 953891264.0000 - val_rmse: 30885.1309\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293525216.0000 - rmse: 17132.5762 - val_loss: 440789024.0000 - val_rmse: 20994.9746\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287225664.0000 - rmse: 16947.7324 - val_loss: 652605120.0000 - val_rmse: 25546.1367\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305756576.0000 - rmse: 17485.8965 - val_loss: 761921024.0000 - val_rmse: 27602.9160\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296560512.0000 - rmse: 17220.9297 - val_loss: 524962304.0000 - val_rmse: 22912.0547\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265517296.0000 - rmse: 16294.6992 - val_loss: 499386880.0000 - val_rmse: 22346.9648\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278186592.0000 - rmse: 16678.9238 - val_loss: 730700864.0000 - val_rmse: 27031.4785\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302132128.0000 - rmse: 17381.9473 - val_loss: 705756928.0000 - val_rmse: 26566.0859\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272490624.0000 - rmse: 16507.2891 - val_loss: 516320960.0000 - val_rmse: 22722.6934\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250352352.0000 - rmse: 15822.5264 - val_loss: 715561920.0000 - val_rmse: 26749.9883\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264866848.0000 - rmse: 16274.7285 - val_loss: 723821696.0000 - val_rmse: 26903.9355\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239657328.0000 - rmse: 15480.8662 - val_loss: 376670272.0000 - val_rmse: 19407.9941\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279988736.0000 - rmse: 16732.8633 - val_loss: 597276736.0000 - val_rmse: 24439.2461\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286843232.0000 - rmse: 16936.4453 - val_loss: 572862592.0000 - val_rmse: 23934.5488\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261875392.0000 - rmse: 16182.5615 - val_loss: 657453888.0000 - val_rmse: 25640.8633\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271969216.0000 - rmse: 16491.4883 - val_loss: 461832768.0000 - val_rmse: 21490.2930\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353428896.0000 - rmse: 18799.7012 - val_loss: 340428064.0000 - val_rmse: 18450.6914\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238303024.0000 - rmse: 15437.0654 - val_loss: 402299648.0000 - val_rmse: 20057.4082\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287246656.0000 - rmse: 16948.3516 - val_loss: 659320704.0000 - val_rmse: 25677.2383\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237881728.0000 - rmse: 15423.4121 - val_loss: 533684416.0000 - val_rmse: 23101.6094\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272604832.0000 - rmse: 16510.7480 - val_loss: 449199872.0000 - val_rmse: 21194.3340\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281097408.0000 - rmse: 16765.9590 - val_loss: 600243840.0000 - val_rmse: 24499.8730\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 233413280.0000 - rmse: 15277.8682 - val_loss: 469301312.0000 - val_rmse: 21663.3613\n",
      "Epoch 164/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236977584.0000 - rmse: 15394.0742 - val_loss: 774066560.0000 - val_rmse: 27822.0488\n",
      "Epoch 165/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348831392.0000 - rmse: 18677.0273 - val_loss: 526204352.0000 - val_rmse: 22939.1445\n",
      "Epoch 166/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287050592.0000 - rmse: 16942.5664 - val_loss: 559496192.0000 - val_rmse: 23653.6719\n",
      "Epoch 167/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274525472.0000 - rmse: 16568.8086 - val_loss: 444109696.0000 - val_rmse: 21073.9102\n",
      "Epoch 168/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237662480.0000 - rmse: 15416.3037 - val_loss: 459268480.0000 - val_rmse: 21430.5488\n",
      "Epoch 169/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246053872.0000 - rmse: 15686.1025 - val_loss: 409690400.0000 - val_rmse: 20240.8105\n",
      "Epoch 170/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273977408.0000 - rmse: 16552.2617 - val_loss: 1039621056.0000 - val_rmse: 32243.1523\n",
      "Epoch 171/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285047200.0000 - rmse: 16883.3398 - val_loss: 496639072.0000 - val_rmse: 22285.3984\n",
      "Epoch 172/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243825136.0000 - rmse: 15614.9014 - val_loss: 571839168.0000 - val_rmse: 23913.1582\n",
      "Epoch 173/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244968064.0000 - rmse: 15651.4541 - val_loss: 425879328.0000 - val_rmse: 20636.8438\n",
      "Epoch 174/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227442496.0000 - rmse: 15081.1934 - val_loss: 489034528.0000 - val_rmse: 22114.1230\n",
      "Epoch 175/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 201429824.0000 - rmse: 14192.5938 - val_loss: 584077952.0000 - val_rmse: 24167.7031\n",
      "Epoch 176/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241363216.0000 - rmse: 15535.8672 - val_loss: 388942848.0000 - val_rmse: 19721.6328\n",
      "Epoch 177/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220622480.0000 - rmse: 14853.3633 - val_loss: 456720224.0000 - val_rmse: 21371.0117\n",
      "Epoch 178/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 205056176.0000 - rmse: 14319.7812 - val_loss: 446541120.0000 - val_rmse: 21131.5195\n",
      "Epoch 179/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229473680.0000 - rmse: 15148.3867 - val_loss: 329233152.0000 - val_rmse: 18144.7812\n",
      "Epoch 180/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271776384.0000 - rmse: 16485.6406 - val_loss: 500358944.0000 - val_rmse: 22368.7051\n",
      "Epoch 181/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235282544.0000 - rmse: 15338.9219 - val_loss: 552030208.0000 - val_rmse: 23495.3223\n",
      "Epoch 182/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270709728.0000 - rmse: 16453.2559 - val_loss: 427722688.0000 - val_rmse: 20681.4570\n",
      "Epoch 183/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 212606096.0000 - rmse: 14581.0156 - val_loss: 594063296.0000 - val_rmse: 24373.4121\n",
      "Epoch 184/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 199687632.0000 - rmse: 14131.0859 - val_loss: 570586112.0000 - val_rmse: 23886.9453\n",
      "Epoch 185/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258290512.0000 - rmse: 16071.4170 - val_loss: 580511488.0000 - val_rmse: 24093.8066\n",
      "Epoch 186/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 204916416.0000 - rmse: 14314.9004 - val_loss: 487129248.0000 - val_rmse: 22071.0039\n",
      "Epoch 187/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319721408.0000 - rmse: 17880.7520 - val_loss: 543412224.0000 - val_rmse: 23311.2012\n",
      "Epoch 188/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240839632.0000 - rmse: 15519.0068 - val_loss: 553330304.0000 - val_rmse: 23522.9746\n",
      "Epoch 189/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207802880.0000 - rmse: 14415.3682 - val_loss: 492575776.0000 - val_rmse: 22194.0488\n",
      "Epoch 190/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 174531264.0000 - rmse: 13211.0273 - val_loss: 405844736.0000 - val_rmse: 20145.5859\n",
      "Epoch 191/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 201226000.0000 - rmse: 14185.4131 - val_loss: 739825280.0000 - val_rmse: 27199.7285\n",
      "Epoch 192/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214372336.0000 - rmse: 14641.4580 - val_loss: 569743552.0000 - val_rmse: 23869.3008\n",
      "Epoch 193/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224391904.0000 - rmse: 14979.7158 - val_loss: 513181728.0000 - val_rmse: 22653.5117\n",
      "Epoch 194/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210788624.0000 - rmse: 14518.5596 - val_loss: 457481760.0000 - val_rmse: 21388.8242\n",
      "Epoch 195/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217293328.0000 - rmse: 14740.8701 - val_loss: 703693888.0000 - val_rmse: 26527.2266\n",
      "Epoch 196/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 204049104.0000 - rmse: 14284.5742 - val_loss: 636117376.0000 - val_rmse: 25221.3672\n",
      "Epoch 197/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292521984.0000 - rmse: 17103.2734 - val_loss: 555507072.0000 - val_rmse: 23569.1953\n",
      "Epoch 198/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 213167520.0000 - rmse: 14600.2549 - val_loss: 696930304.0000 - val_rmse: 26399.4355\n",
      "Epoch 199/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 191004944.0000 - rmse: 13820.4521 - val_loss: 533065632.0000 - val_rmse: 23088.2148\n",
      "Epoch 200/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230556688.0000 - rmse: 15184.0918 - val_loss: 492522912.0000 - val_rmse: 22192.8555\n",
      "Epoch 201/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 208779712.0000 - rmse: 14449.2080 - val_loss: 457381952.0000 - val_rmse: 21386.4883\n",
      "Epoch 202/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 193589312.0000 - rmse: 13913.6367 - val_loss: 488956064.0000 - val_rmse: 22112.3496\n",
      "Epoch 203/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 167218240.0000 - rmse: 12931.2861 - val_loss: 508700544.0000 - val_rmse: 22554.3887\n",
      "Epoch 204/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239335248.0000 - rmse: 15470.4619 - val_loss: 527347584.0000 - val_rmse: 22964.0469\n",
      "Epoch 205/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 193563248.0000 - rmse: 13912.6992 - val_loss: 926188992.0000 - val_rmse: 30433.3516\n",
      "Epoch 206/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 197826976.0000 - rmse: 14065.0947 - val_loss: 364269824.0000 - val_rmse: 19085.8516\n",
      "Epoch 207/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 193334688.0000 - rmse: 13904.4834 - val_loss: 582312000.0000 - val_rmse: 24131.1406\n",
      "Epoch 208/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219625824.0000 - rmse: 14819.7764 - val_loss: 505510752.0000 - val_rmse: 22483.5664\n",
      "Epoch 209/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 195431968.0000 - rmse: 13979.6963 - val_loss: 539266816.0000 - val_rmse: 23222.1191\n",
      "Epoch 210/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 199647360.0000 - rmse: 14129.6611 - val_loss: 360217312.0000 - val_rmse: 18979.3887\n",
      "Epoch 211/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 204352032.0000 - rmse: 14295.1738 - val_loss: 441100576.0000 - val_rmse: 21002.3926\n",
      "Epoch 212/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 205630304.0000 - rmse: 14339.8135 - val_loss: 484661344.0000 - val_rmse: 22015.0254\n",
      "Epoch 213/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 177802976.0000 - rmse: 13334.2754 - val_loss: 515630880.0000 - val_rmse: 22707.5059\n",
      "Epoch 214/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 186176384.0000 - rmse: 13644.6445 - val_loss: 406146432.0000 - val_rmse: 20153.0723\n",
      "Epoch 215/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234318160.0000 - rmse: 15307.4531 - val_loss: 620629056.0000 - val_rmse: 24912.4277\n",
      "Epoch 216/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216336608.0000 - rmse: 14708.3838 - val_loss: 510918752.0000 - val_rmse: 22603.5117\n",
      "Epoch 217/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242407952.0000 - rmse: 15569.4541 - val_loss: 913669760.0000 - val_rmse: 30226.9707\n",
      "Epoch 218/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207218912.0000 - rmse: 14395.0977 - val_loss: 462794848.0000 - val_rmse: 21512.6680\n",
      "Epoch 219/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 200751616.0000 - rmse: 14168.6816 - val_loss: 467046016.0000 - val_rmse: 21611.2480\n",
      "Epoch 220/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 208523312.0000 - rmse: 14440.3320 - val_loss: 445524160.0000 - val_rmse: 21107.4414\n",
      "Epoch 221/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 188160144.0000 - rmse: 13717.1465 - val_loss: 473657984.0000 - val_rmse: 21763.6836\n",
      "Epoch 222/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 184615776.0000 - rmse: 13587.3369 - val_loss: 541756416.0000 - val_rmse: 23275.6602\n",
      "Epoch 223/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 171343184.0000 - rmse: 13089.8086 - val_loss: 411044224.0000 - val_rmse: 20274.2266\n",
      "Epoch 224/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 177325296.0000 - rmse: 13316.3525 - val_loss: 509132384.0000 - val_rmse: 22563.9609\n",
      "Epoch 225/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 212412224.0000 - rmse: 14574.3662 - val_loss: 507720928.0000 - val_rmse: 22532.6602\n",
      "Epoch 226/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 212422352.0000 - rmse: 14574.7148 - val_loss: 963965760.0000 - val_rmse: 31047.7988\n",
      "Epoch 227/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271690688.0000 - rmse: 16483.0410 - val_loss: 584864192.0000 - val_rmse: 24183.9648\n",
      "Epoch 228/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 183393024.0000 - rmse: 13542.2656 - val_loss: 618910528.0000 - val_rmse: 24877.9121\n",
      "104/104 [==============================] - 0s 648us/step - loss: 749790784.0000 - rmse: 27382.3086\n",
      "[749790784.0, 27382.30859375]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 11855794176.0000 - rmse: 108884.3125 - val_loss: 1773954048.0000 - val_rmse: 42118.3320\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1998012032.0000 - rmse: 44699.1289 - val_loss: 1303213696.0000 - val_rmse: 36100.0508\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1775510016.0000 - rmse: 42136.8008 - val_loss: 1206389760.0000 - val_rmse: 34733.1211\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1601687296.0000 - rmse: 40021.0859 - val_loss: 1026390528.0000 - val_rmse: 32037.3301\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1507195008.0000 - rmse: 38822.6094 - val_loss: 971954048.0000 - val_rmse: 31176.1777\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1382696320.0000 - rmse: 37184.6250 - val_loss: 895664960.0000 - val_rmse: 29927.6621\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1344926208.0000 - rmse: 36673.2344 - val_loss: 893226688.0000 - val_rmse: 29886.8984\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1272609024.0000 - rmse: 35673.6445 - val_loss: 851254016.0000 - val_rmse: 29176.2578\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1237167104.0000 - rmse: 35173.3867 - val_loss: 903917632.0000 - val_rmse: 30065.2227\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1252760192.0000 - rmse: 35394.3516 - val_loss: 854311104.0000 - val_rmse: 29228.6016\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1207198720.0000 - rmse: 34744.7656 - val_loss: 827717632.0000 - val_rmse: 28770.0820\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1113662336.0000 - rmse: 33371.5781 - val_loss: 813179904.0000 - val_rmse: 28516.3086\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1125624320.0000 - rmse: 33550.3242 - val_loss: 827995328.0000 - val_rmse: 28774.9082\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1123069824.0000 - rmse: 33512.2344 - val_loss: 832914112.0000 - val_rmse: 28860.2520\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1092615552.0000 - rmse: 33054.7344 - val_loss: 813357376.0000 - val_rmse: 28519.4219\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1063907008.0000 - rmse: 32617.5879 - val_loss: 837541888.0000 - val_rmse: 28940.3164\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1060656448.0000 - rmse: 32567.7207 - val_loss: 833067776.0000 - val_rmse: 28862.9141\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 994692352.0000 - rmse: 31538.7422 - val_loss: 876522048.0000 - val_rmse: 29606.1152\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 983667712.0000 - rmse: 31363.4766 - val_loss: 946415552.0000 - val_rmse: 30763.8672\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 970203520.0000 - rmse: 31148.0898 - val_loss: 857841600.0000 - val_rmse: 29288.9336\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 917422912.0000 - rmse: 30288.9902 - val_loss: 877587520.0000 - val_rmse: 29624.1035\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 925257408.0000 - rmse: 30418.0449 - val_loss: 1040097024.0000 - val_rmse: 32250.5352\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 850830336.0000 - rmse: 29168.9941 - val_loss: 1214528256.0000 - val_rmse: 34850.0820\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 864651968.0000 - rmse: 29404.9648 - val_loss: 974735424.0000 - val_rmse: 31220.7539\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 915846208.0000 - rmse: 30262.9512 - val_loss: 963691904.0000 - val_rmse: 31043.3867\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 876063936.0000 - rmse: 29598.3750 - val_loss: 863330944.0000 - val_rmse: 29382.4941\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 852980608.0000 - rmse: 29205.8320 - val_loss: 965031744.0000 - val_rmse: 31064.9609\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 778282048.0000 - rmse: 27897.7070 - val_loss: 967758144.0000 - val_rmse: 31108.8105\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 823656960.0000 - rmse: 28699.4238 - val_loss: 834417856.0000 - val_rmse: 28886.2910\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 811743936.0000 - rmse: 28491.1211 - val_loss: 1112254720.0000 - val_rmse: 33350.4844\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800816192.0000 - rmse: 28298.6953 - val_loss: 1432018304.0000 - val_rmse: 37842.0195\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 755283264.0000 - rmse: 27482.4180 - val_loss: 932232256.0000 - val_rmse: 30532.4766\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 739517760.0000 - rmse: 27194.0742 - val_loss: 849656384.0000 - val_rmse: 29148.8652\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 759797824.0000 - rmse: 27564.4297 - val_loss: 1073143104.0000 - val_rmse: 32758.8633\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 757795264.0000 - rmse: 27528.0801 - val_loss: 874344512.0000 - val_rmse: 29569.3164\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 754568384.0000 - rmse: 27469.4062 - val_loss: 969083968.0000 - val_rmse: 31130.1133\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718952768.0000 - rmse: 26813.2949 - val_loss: 1164570624.0000 - val_rmse: 34125.8047\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 689384768.0000 - rmse: 26256.1387 - val_loss: 902150336.0000 - val_rmse: 30035.8184\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 660336768.0000 - rmse: 25697.0195 - val_loss: 898732800.0000 - val_rmse: 29978.8730\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 635901312.0000 - rmse: 25217.0840 - val_loss: 923702720.0000 - val_rmse: 30392.4785\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680776384.0000 - rmse: 26091.6914 - val_loss: 908697472.0000 - val_rmse: 30144.6074\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 696615936.0000 - rmse: 26393.4824 - val_loss: 815538304.0000 - val_rmse: 28557.6309\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590524288.0000 - rmse: 24300.7051 - val_loss: 803565568.0000 - val_rmse: 28347.2324\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624973632.0000 - rmse: 24999.4727 - val_loss: 813015104.0000 - val_rmse: 28513.4180\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564186432.0000 - rmse: 23752.6074 - val_loss: 832989184.0000 - val_rmse: 28861.5527\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 612682048.0000 - rmse: 24752.4141 - val_loss: 1642114816.0000 - val_rmse: 40523.0156\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632469568.0000 - rmse: 25148.9473 - val_loss: 786966784.0000 - val_rmse: 28052.9277\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 619472320.0000 - rmse: 24889.2012 - val_loss: 1695568128.0000 - val_rmse: 41177.2773\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534392096.0000 - rmse: 23116.9219 - val_loss: 1313011328.0000 - val_rmse: 36235.5000\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 557185792.0000 - rmse: 23604.7832 - val_loss: 980434880.0000 - val_rmse: 31311.8965\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611998208.0000 - rmse: 24738.5977 - val_loss: 1009995328.0000 - val_rmse: 31780.4238\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507478592.0000 - rmse: 22527.2832 - val_loss: 902426432.0000 - val_rmse: 30040.4141\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486078784.0000 - rmse: 22047.1934 - val_loss: 737131200.0000 - val_rmse: 27150.1602\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550307648.0000 - rmse: 23458.6367 - val_loss: 1583435520.0000 - val_rmse: 39792.4062\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582266048.0000 - rmse: 24130.1875 - val_loss: 1120627328.0000 - val_rmse: 33475.7734\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517258112.0000 - rmse: 22743.3086 - val_loss: 1307619456.0000 - val_rmse: 36161.0195\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510420192.0000 - rmse: 22592.4805 - val_loss: 903510720.0000 - val_rmse: 30058.4551\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481206752.0000 - rmse: 21936.4258 - val_loss: 721647552.0000 - val_rmse: 26863.4980\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475282240.0000 - rmse: 21800.9668 - val_loss: 1061856768.0000 - val_rmse: 32586.1445\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463348960.0000 - rmse: 21525.5430 - val_loss: 1068871680.0000 - val_rmse: 32693.6035\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501212032.0000 - rmse: 22387.7656 - val_loss: 1857551744.0000 - val_rmse: 43099.3242\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443862336.0000 - rmse: 21068.0410 - val_loss: 1937494144.0000 - val_rmse: 44016.9727\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459116576.0000 - rmse: 21427.0059 - val_loss: 1187437952.0000 - val_rmse: 34459.2188\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414164576.0000 - rmse: 20351.0332 - val_loss: 1469090560.0000 - val_rmse: 38328.7188\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400569184.0000 - rmse: 20014.2227 - val_loss: 1079069184.0000 - val_rmse: 32849.1875\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410812416.0000 - rmse: 20268.5059 - val_loss: 846221760.0000 - val_rmse: 29089.8906\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446137888.0000 - rmse: 21121.9766 - val_loss: 1129492480.0000 - val_rmse: 33607.9219\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489252160.0000 - rmse: 22119.0449 - val_loss: 929975552.0000 - val_rmse: 30495.4980\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493093600.0000 - rmse: 22205.7109 - val_loss: 1070490240.0000 - val_rmse: 32718.3477\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449844736.0000 - rmse: 21209.5430 - val_loss: 1170627328.0000 - val_rmse: 34214.4297\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373434688.0000 - rmse: 19324.4590 - val_loss: 1292525184.0000 - val_rmse: 35951.7031\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457975680.0000 - rmse: 21400.3652 - val_loss: 810279232.0000 - val_rmse: 28465.4043\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 442728224.0000 - rmse: 21041.1074 - val_loss: 1129334272.0000 - val_rmse: 33605.5703\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411921344.0000 - rmse: 20295.8457 - val_loss: 1727880704.0000 - val_rmse: 41567.7852\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393677088.0000 - rmse: 19841.2969 - val_loss: 722115584.0000 - val_rmse: 26872.2090\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504946400.0000 - rmse: 22471.0117 - val_loss: 837519744.0000 - val_rmse: 28939.9336\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436212416.0000 - rmse: 20885.6992 - val_loss: 880363776.0000 - val_rmse: 29670.9238\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440462656.0000 - rmse: 20987.2031 - val_loss: 628106432.0000 - val_rmse: 25062.0508\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409292416.0000 - rmse: 20230.9766 - val_loss: 746561920.0000 - val_rmse: 27323.2852\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382897152.0000 - rmse: 19567.7578 - val_loss: 1225857920.0000 - val_rmse: 35012.2539\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351113952.0000 - rmse: 18738.0332 - val_loss: 816084224.0000 - val_rmse: 28567.1875\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413403200.0000 - rmse: 20332.3184 - val_loss: 1064480576.0000 - val_rmse: 32626.3789\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412820192.0000 - rmse: 20317.9746 - val_loss: 1203429504.0000 - val_rmse: 34690.4805\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409401824.0000 - rmse: 20233.6777 - val_loss: 866438144.0000 - val_rmse: 29435.3203\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393890240.0000 - rmse: 19846.6680 - val_loss: 1340206080.0000 - val_rmse: 36608.8242\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403266912.0000 - rmse: 20081.5059 - val_loss: 794217792.0000 - val_rmse: 28181.8691\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380691104.0000 - rmse: 19511.3066 - val_loss: 817621120.0000 - val_rmse: 28594.0742\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389495936.0000 - rmse: 19735.6504 - val_loss: 812163968.0000 - val_rmse: 28498.4902\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358458304.0000 - rmse: 18932.9941 - val_loss: 1163181184.0000 - val_rmse: 34105.4414\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369886240.0000 - rmse: 19232.4258 - val_loss: 1050969280.0000 - val_rmse: 32418.6562\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375901888.0000 - rmse: 19388.1875 - val_loss: 825270400.0000 - val_rmse: 28727.5195\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375753632.0000 - rmse: 19384.3633 - val_loss: 1523633024.0000 - val_rmse: 39033.7422\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438094688.0000 - rmse: 20930.7109 - val_loss: 976609344.0000 - val_rmse: 31250.7500\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467272480.0000 - rmse: 21616.4863 - val_loss: 891056960.0000 - val_rmse: 29850.5781\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382390176.0000 - rmse: 19554.7969 - val_loss: 1187296512.0000 - val_rmse: 34457.1680\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381104992.0000 - rmse: 19521.9102 - val_loss: 937548992.0000 - val_rmse: 30619.4219\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379320352.0000 - rmse: 19476.1484 - val_loss: 820296000.0000 - val_rmse: 28640.8086\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379193280.0000 - rmse: 19472.8828 - val_loss: 1062176768.0000 - val_rmse: 32591.0527\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362202272.0000 - rmse: 19031.6133 - val_loss: 878752512.0000 - val_rmse: 29643.7578\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325348032.0000 - rmse: 18037.4062 - val_loss: 1016467968.0000 - val_rmse: 31882.0938\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382505888.0000 - rmse: 19557.7559 - val_loss: 1453283200.0000 - val_rmse: 38121.9492\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356202080.0000 - rmse: 18873.3145 - val_loss: 840652928.0000 - val_rmse: 28994.0137\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327928448.0000 - rmse: 18108.7930 - val_loss: 1419778304.0000 - val_rmse: 37679.9453\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329426272.0000 - rmse: 18150.1035 - val_loss: 1260296448.0000 - val_rmse: 35500.6523\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386692288.0000 - rmse: 19664.4922 - val_loss: 763160320.0000 - val_rmse: 27625.3555\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333735168.0000 - rmse: 18268.4180 - val_loss: 682602752.0000 - val_rmse: 26126.6660\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326564864.0000 - rmse: 18071.1055 - val_loss: 781939776.0000 - val_rmse: 27963.1855\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314141792.0000 - rmse: 17724.0430 - val_loss: 1000948224.0000 - val_rmse: 31637.7656\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327682176.0000 - rmse: 18101.9941 - val_loss: 1089610240.0000 - val_rmse: 33009.2461\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352511776.0000 - rmse: 18775.2949 - val_loss: 1021982720.0000 - val_rmse: 31968.4648\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407122176.0000 - rmse: 20177.2676 - val_loss: 1001796160.0000 - val_rmse: 31651.1641\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351311840.0000 - rmse: 18743.3145 - val_loss: 1479115136.0000 - val_rmse: 38459.2656\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326756736.0000 - rmse: 18076.4121 - val_loss: 1104377984.0000 - val_rmse: 33232.1836\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301594784.0000 - rmse: 17366.4824 - val_loss: 1828540416.0000 - val_rmse: 42761.4375\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328563264.0000 - rmse: 18126.3125 - val_loss: 1301556224.0000 - val_rmse: 36077.0859\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356059200.0000 - rmse: 18869.5293 - val_loss: 1051522112.0000 - val_rmse: 32427.1797\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328537280.0000 - rmse: 18125.5957 - val_loss: 855859776.0000 - val_rmse: 29255.0820\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305305696.0000 - rmse: 17472.9980 - val_loss: 1691000576.0000 - val_rmse: 41121.7773\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302008864.0000 - rmse: 17378.4004 - val_loss: 780590080.0000 - val_rmse: 27939.0430\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284473440.0000 - rmse: 16866.3398 - val_loss: 1645826688.0000 - val_rmse: 40568.7891\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316791968.0000 - rmse: 17798.6484 - val_loss: 1121674752.0000 - val_rmse: 33491.4102\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335647936.0000 - rmse: 18320.6953 - val_loss: 1642685952.0000 - val_rmse: 40530.0625\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318261248.0000 - rmse: 17839.8789 - val_loss: 1377999488.0000 - val_rmse: 37121.4141\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274177056.0000 - rmse: 16558.2930 - val_loss: 1324238080.0000 - val_rmse: 36390.0820\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276300576.0000 - rmse: 16622.2910 - val_loss: 1330654848.0000 - val_rmse: 36478.1406\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280612064.0000 - rmse: 16751.4785 - val_loss: 1164599808.0000 - val_rmse: 34126.2344\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303473024.0000 - rmse: 17420.4766 - val_loss: 1521748352.0000 - val_rmse: 39009.5938\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285129248.0000 - rmse: 16885.7695 - val_loss: 1857385088.0000 - val_rmse: 43097.3906\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522745856.0000 - rmse: 22863.6348 - val_loss: 1320744960.0000 - val_rmse: 36342.0547\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317406688.0000 - rmse: 17815.9082 - val_loss: 1438241792.0000 - val_rmse: 37924.1602\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331845376.0000 - rmse: 18216.6230 - val_loss: 1512862208.0000 - val_rmse: 38895.5312\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268694720.0000 - rmse: 16391.9102 - val_loss: 1256831232.0000 - val_rmse: 35451.8164\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324538304.0000 - rmse: 18014.9453 - val_loss: 1320875648.0000 - val_rmse: 36343.8477\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278986208.0000 - rmse: 16702.8809 - val_loss: 1598337536.0000 - val_rmse: 39979.2148\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263932832.0000 - rmse: 16246.0088 - val_loss: 1176526208.0000 - val_rmse: 34300.5273\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286939648.0000 - rmse: 16939.2930 - val_loss: 898153216.0000 - val_rmse: 29969.2051\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359205888.0000 - rmse: 18952.7285 - val_loss: 1203030528.0000 - val_rmse: 34684.7305\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279991552.0000 - rmse: 16732.9453 - val_loss: 1910342784.0000 - val_rmse: 43707.4648\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261136768.0000 - rmse: 16159.7256 - val_loss: 1273789056.0000 - val_rmse: 35690.1797\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305843712.0000 - rmse: 17488.3867 - val_loss: 1033158592.0000 - val_rmse: 32142.7852\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326756448.0000 - rmse: 18076.4062 - val_loss: 3126141184.0000 - val_rmse: 55911.9062\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359462784.0000 - rmse: 18959.5039 - val_loss: 1601928192.0000 - val_rmse: 40024.0938\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288502304.0000 - rmse: 16985.3555 - val_loss: 1344729344.0000 - val_rmse: 36670.5508\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250351968.0000 - rmse: 15822.5146 - val_loss: 1736892032.0000 - val_rmse: 41676.0352\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296467232.0000 - rmse: 17218.2227 - val_loss: 1458183936.0000 - val_rmse: 38186.1758\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248201008.0000 - rmse: 15754.3936 - val_loss: 1408462848.0000 - val_rmse: 37529.4922\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287064608.0000 - rmse: 16942.9785 - val_loss: 879377920.0000 - val_rmse: 29654.3066\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243534448.0000 - rmse: 15605.5879 - val_loss: 1351274752.0000 - val_rmse: 36759.6875\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285932096.0000 - rmse: 16909.5254 - val_loss: 1449994624.0000 - val_rmse: 38078.7969\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255781920.0000 - rmse: 15993.1816 - val_loss: 2037216128.0000 - val_rmse: 45135.5312\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261650304.0000 - rmse: 16175.6074 - val_loss: 2078752000.0000 - val_rmse: 45593.3320\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332742784.0000 - rmse: 18241.2383 - val_loss: 1930823552.0000 - val_rmse: 43941.1367\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263588560.0000 - rmse: 16235.4111 - val_loss: 2151374080.0000 - val_rmse: 46382.9062\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292093664.0000 - rmse: 17090.7461 - val_loss: 1059085248.0000 - val_rmse: 32543.5898\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262866992.0000 - rmse: 16213.1729 - val_loss: 1560151936.0000 - val_rmse: 39498.7578\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316172320.0000 - rmse: 17781.2344 - val_loss: 888572416.0000 - val_rmse: 29808.9316\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284231968.0000 - rmse: 16859.1777 - val_loss: 1795129472.0000 - val_rmse: 42368.9688\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 233454320.0000 - rmse: 15279.2109 - val_loss: 1368455424.0000 - val_rmse: 36992.6406\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239912528.0000 - rmse: 15489.1094 - val_loss: 1082113152.0000 - val_rmse: 32895.4883\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243205952.0000 - rmse: 15595.0605 - val_loss: 1357433216.0000 - val_rmse: 36843.3594\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218761792.0000 - rmse: 14790.5967 - val_loss: 1425559936.0000 - val_rmse: 37756.5898\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254376704.0000 - rmse: 15949.1904 - val_loss: 1003369856.0000 - val_rmse: 31676.0117\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232725840.0000 - rmse: 15255.3525 - val_loss: 1374987392.0000 - val_rmse: 37080.8242\n",
      "Epoch 164/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279817280.0000 - rmse: 16727.7383 - val_loss: 1687463936.0000 - val_rmse: 41078.7539\n",
      "Epoch 165/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257336416.0000 - rmse: 16041.7080 - val_loss: 1878675328.0000 - val_rmse: 43343.6875\n",
      "Epoch 166/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268132624.0000 - rmse: 16374.7529 - val_loss: 1227186560.0000 - val_rmse: 35031.2227\n",
      "Epoch 167/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258140896.0000 - rmse: 16066.7627 - val_loss: 1494102912.0000 - val_rmse: 38653.6289\n",
      "Epoch 168/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237240368.0000 - rmse: 15402.6084 - val_loss: 1284798976.0000 - val_rmse: 35844.0938\n",
      "Epoch 169/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293939200.0000 - rmse: 17144.6543 - val_loss: 1434267136.0000 - val_rmse: 37871.7148\n",
      "Epoch 170/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238524176.0000 - rmse: 15444.2266 - val_loss: 2190456320.0000 - val_rmse: 46802.3125\n",
      "Epoch 171/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238245168.0000 - rmse: 15435.1914 - val_loss: 1018071808.0000 - val_rmse: 31907.2344\n",
      "Epoch 172/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246131856.0000 - rmse: 15688.5879 - val_loss: 1210284672.0000 - val_rmse: 34789.1445\n",
      "Epoch 173/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258329648.0000 - rmse: 16072.6348 - val_loss: 1266012032.0000 - val_rmse: 35581.0625\n",
      "Epoch 174/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263115168.0000 - rmse: 16220.8242 - val_loss: 1182665344.0000 - val_rmse: 34389.9023\n",
      "Epoch 175/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260920768.0000 - rmse: 16153.0410 - val_loss: 2449143040.0000 - val_rmse: 49488.8164\n",
      "Epoch 176/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234017952.0000 - rmse: 15297.6445 - val_loss: 1019793984.0000 - val_rmse: 31934.2129\n",
      "Epoch 177/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242607904.0000 - rmse: 15575.8740 - val_loss: 1851280640.0000 - val_rmse: 43026.5117\n",
      "Epoch 178/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287968064.0000 - rmse: 16969.6211 - val_loss: 1541203968.0000 - val_rmse: 39258.1719\n",
      "Epoch 179/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278601120.0000 - rmse: 16691.3477 - val_loss: 1502824704.0000 - val_rmse: 38766.2812\n",
      "Epoch 180/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 197497408.0000 - rmse: 14053.3750 - val_loss: 1302653824.0000 - val_rmse: 36092.2969\n",
      "Epoch 181/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252422320.0000 - rmse: 15887.8037 - val_loss: 1346873344.0000 - val_rmse: 36699.7695\n",
      "Epoch 182/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222514176.0000 - rmse: 14916.9082 - val_loss: 1172551808.0000 - val_rmse: 34242.5430\n",
      "Epoch 183/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 223304688.0000 - rmse: 14943.3809 - val_loss: 3068128768.0000 - val_rmse: 55390.6914\n",
      "Epoch 184/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288996736.0000 - rmse: 16999.9004 - val_loss: 2847904000.0000 - val_rmse: 53365.7578\n",
      "Epoch 185/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299019200.0000 - rmse: 17292.1719 - val_loss: 2789585152.0000 - val_rmse: 52816.5234\n",
      "Epoch 186/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 223315600.0000 - rmse: 14943.7461 - val_loss: 1076288384.0000 - val_rmse: 32806.8359\n",
      "Epoch 187/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294018976.0000 - rmse: 17146.9805 - val_loss: 1742729984.0000 - val_rmse: 41746.0195\n",
      "Epoch 188/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330647872.0000 - rmse: 18183.7246 - val_loss: 1146661888.0000 - val_rmse: 33862.3945\n",
      "Epoch 189/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269071104.0000 - rmse: 16403.3867 - val_loss: 1604436096.0000 - val_rmse: 40055.4141\n",
      "Epoch 190/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244018928.0000 - rmse: 15621.1025 - val_loss: 1191020288.0000 - val_rmse: 34511.1602\n",
      "Epoch 191/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237157328.0000 - rmse: 15399.9121 - val_loss: 1132956928.0000 - val_rmse: 33659.4258\n",
      "Epoch 192/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216376736.0000 - rmse: 14709.7490 - val_loss: 1700495104.0000 - val_rmse: 41237.0586\n",
      "Epoch 193/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 225059840.0000 - rmse: 15001.9932 - val_loss: 1550366080.0000 - val_rmse: 39374.6875\n",
      "Epoch 194/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252825520.0000 - rmse: 15900.4883 - val_loss: 794334656.0000 - val_rmse: 28183.9434\n",
      "Epoch 195/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254610736.0000 - rmse: 15956.5264 - val_loss: 1296936448.0000 - val_rmse: 36013.0039\n",
      "Epoch 196/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234973840.0000 - rmse: 15328.8545 - val_loss: 1988222080.0000 - val_rmse: 44589.4844\n",
      "Epoch 197/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 205854512.0000 - rmse: 14347.6289 - val_loss: 1693460480.0000 - val_rmse: 41151.6758\n",
      "Epoch 198/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243586688.0000 - rmse: 15607.2607 - val_loss: 2537074944.0000 - val_rmse: 50369.3867\n",
      "Epoch 199/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 213844240.0000 - rmse: 14623.4121 - val_loss: 1122684160.0000 - val_rmse: 33506.4805\n",
      "Epoch 200/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218577072.0000 - rmse: 14784.3506 - val_loss: 1399381504.0000 - val_rmse: 37408.3047\n",
      "Epoch 201/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239075184.0000 - rmse: 15462.0537 - val_loss: 1757285760.0000 - val_rmse: 41919.9922\n",
      "Epoch 202/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271724608.0000 - rmse: 16484.0684 - val_loss: 1155137536.0000 - val_rmse: 33987.3125\n",
      "Epoch 203/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258945408.0000 - rmse: 16091.7793 - val_loss: 1284670336.0000 - val_rmse: 35842.2969\n",
      "Epoch 204/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 233985008.0000 - rmse: 15296.5674 - val_loss: 1004127424.0000 - val_rmse: 31687.9688\n",
      "Epoch 205/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247105856.0000 - rmse: 15719.6006 - val_loss: 1119413888.0000 - val_rmse: 33457.6445\n",
      "104/104 [==============================] - 0s 646us/step - loss: 476276096.0000 - rmse: 21823.7500\n",
      "[476276096.0, 21823.75]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 13186780160.0000 - rmse: 114833.7031 - val_loss: 1906945280.0000 - val_rmse: 43668.5859\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2304305408.0000 - rmse: 48003.1797 - val_loss: 1351030400.0000 - val_rmse: 36756.3672\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1895806464.0000 - rmse: 43540.8594 - val_loss: 1156553216.0000 - val_rmse: 34008.1328\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1665088128.0000 - rmse: 40805.4922 - val_loss: 1053608768.0000 - val_rmse: 32459.3398\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1561100416.0000 - rmse: 39510.7617 - val_loss: 977695616.0000 - val_rmse: 31268.1250\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1455644672.0000 - rmse: 38152.9102 - val_loss: 1179622912.0000 - val_rmse: 34345.6406\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1385904256.0000 - rmse: 37227.7344 - val_loss: 984511872.0000 - val_rmse: 31376.9316\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1359830272.0000 - rmse: 36875.8750 - val_loss: 961890944.0000 - val_rmse: 31014.3672\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1311561216.0000 - rmse: 36215.4844 - val_loss: 908208704.0000 - val_rmse: 30136.5020\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1253475328.0000 - rmse: 35404.4531 - val_loss: 1131437056.0000 - val_rmse: 33636.8398\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1227017600.0000 - rmse: 35028.8125 - val_loss: 1056431808.0000 - val_rmse: 32502.7969\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1252317952.0000 - rmse: 35388.1055 - val_loss: 907154880.0000 - val_rmse: 30119.0117\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1177107456.0000 - rmse: 34309.0000 - val_loss: 1009507136.0000 - val_rmse: 31772.7422\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1142707584.0000 - rmse: 33803.9570 - val_loss: 840204416.0000 - val_rmse: 28986.2773\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1192499456.0000 - rmse: 34532.5859 - val_loss: 851294016.0000 - val_rmse: 29176.9434\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1110734080.0000 - rmse: 33327.6758 - val_loss: 840615424.0000 - val_rmse: 28993.3691\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1039866624.0000 - rmse: 32246.9629 - val_loss: 1184419072.0000 - val_rmse: 34415.3906\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1084116224.0000 - rmse: 32925.9219 - val_loss: 818700224.0000 - val_rmse: 28612.9375\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1003115200.0000 - rmse: 31671.9941 - val_loss: 811591040.0000 - val_rmse: 28488.4375\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1011905280.0000 - rmse: 31810.4590 - val_loss: 808241600.0000 - val_rmse: 28429.5898\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 959183232.0000 - rmse: 30970.6836 - val_loss: 804467072.0000 - val_rmse: 28363.1289\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 948832320.0000 - rmse: 30803.1211 - val_loss: 850158976.0000 - val_rmse: 29157.4863\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 920453568.0000 - rmse: 30338.9785 - val_loss: 1018145984.0000 - val_rmse: 31908.4004\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 840490624.0000 - rmse: 28991.2168 - val_loss: 931515904.0000 - val_rmse: 30520.7461\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 880702784.0000 - rmse: 29676.6367 - val_loss: 769085952.0000 - val_rmse: 27732.3984\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 878225408.0000 - rmse: 29634.8672 - val_loss: 747542400.0000 - val_rmse: 27341.2207\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 858411712.0000 - rmse: 29298.6641 - val_loss: 749043136.0000 - val_rmse: 27368.6504\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 879095424.0000 - rmse: 29649.5430 - val_loss: 706659456.0000 - val_rmse: 26583.0664\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 859352256.0000 - rmse: 29314.7109 - val_loss: 761047424.0000 - val_rmse: 27587.0879\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 862981696.0000 - rmse: 29376.5508 - val_loss: 711899392.0000 - val_rmse: 26681.4434\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 810797952.0000 - rmse: 28474.5137 - val_loss: 735147648.0000 - val_rmse: 27113.6055\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 830226688.0000 - rmse: 28813.6543 - val_loss: 688550592.0000 - val_rmse: 26240.2480\n",
      "Epoch 33/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 741751680.0000 - rmse: 27235.1191 - val_loss: 695937600.0000 - val_rmse: 26380.6289\n",
      "Epoch 34/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 746158656.0000 - rmse: 27315.9043 - val_loss: 724786176.0000 - val_rmse: 26921.8535\n",
      "Epoch 35/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 789159680.0000 - rmse: 28091.9863 - val_loss: 877555840.0000 - val_rmse: 29623.5684\n",
      "Epoch 36/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742203968.0000 - rmse: 27243.4199 - val_loss: 711593408.0000 - val_rmse: 26675.7090\n",
      "Epoch 37/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724801472.0000 - rmse: 26922.1367 - val_loss: 846719808.0000 - val_rmse: 29098.4512\n",
      "Epoch 38/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 728950464.0000 - rmse: 26999.0820 - val_loss: 681427264.0000 - val_rmse: 26104.1621\n",
      "Epoch 39/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 708616768.0000 - rmse: 26619.8574 - val_loss: 665133504.0000 - val_rmse: 25790.1816\n",
      "Epoch 40/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620740864.0000 - rmse: 24914.6719 - val_loss: 1416524544.0000 - val_rmse: 37636.7461\n",
      "Epoch 41/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 719011904.0000 - rmse: 26814.3965 - val_loss: 691847488.0000 - val_rmse: 26302.9941\n",
      "Epoch 42/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677163584.0000 - rmse: 26022.3652 - val_loss: 707633664.0000 - val_rmse: 26601.3828\n",
      "Epoch 43/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677725504.0000 - rmse: 26033.1621 - val_loss: 693750336.0000 - val_rmse: 26339.1406\n",
      "Epoch 44/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640131776.0000 - rmse: 25300.8262 - val_loss: 672027200.0000 - val_rmse: 25923.4883\n",
      "Epoch 45/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677731968.0000 - rmse: 26033.2852 - val_loss: 721640448.0000 - val_rmse: 26863.3652\n",
      "Epoch 46/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651482304.0000 - rmse: 25524.1523 - val_loss: 630739712.0000 - val_rmse: 25114.5312\n",
      "Epoch 47/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 652124928.0000 - rmse: 25536.7363 - val_loss: 693291584.0000 - val_rmse: 26330.4316\n",
      "Epoch 48/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 602991360.0000 - rmse: 24555.8828 - val_loss: 601395136.0000 - val_rmse: 24523.3594\n",
      "Epoch 49/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610620416.0000 - rmse: 24710.7344 - val_loss: 639961536.0000 - val_rmse: 25297.4609\n",
      "Epoch 50/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 597754496.0000 - rmse: 24449.0176 - val_loss: 685079936.0000 - val_rmse: 26174.0312\n",
      "Epoch 51/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565517696.0000 - rmse: 23780.6152 - val_loss: 629619456.0000 - val_rmse: 25092.2188\n",
      "Epoch 52/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569075904.0000 - rmse: 23855.3105 - val_loss: 690616320.0000 - val_rmse: 26279.5801\n",
      "Epoch 53/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587309184.0000 - rmse: 24234.4609 - val_loss: 651144704.0000 - val_rmse: 25517.5352\n",
      "Epoch 54/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514450880.0000 - rmse: 22681.5098 - val_loss: 710114752.0000 - val_rmse: 26647.9785\n",
      "Epoch 55/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546308608.0000 - rmse: 23373.2461 - val_loss: 662909760.0000 - val_rmse: 25747.0332\n",
      "Epoch 56/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 541392448.0000 - rmse: 23267.8418 - val_loss: 678899264.0000 - val_rmse: 26055.6934\n",
      "Epoch 57/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512355296.0000 - rmse: 22635.2676 - val_loss: 628618624.0000 - val_rmse: 25072.2676\n",
      "Epoch 58/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514025056.0000 - rmse: 22672.1191 - val_loss: 596301248.0000 - val_rmse: 24419.2793\n",
      "Epoch 59/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488231872.0000 - rmse: 22095.9688 - val_loss: 617214912.0000 - val_rmse: 24843.8105\n",
      "Epoch 60/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555262464.0000 - rmse: 23564.0078 - val_loss: 975848768.0000 - val_rmse: 31238.5781\n",
      "Epoch 61/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509938848.0000 - rmse: 22581.8242 - val_loss: 947891584.0000 - val_rmse: 30787.8477\n",
      "Epoch 62/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450770752.0000 - rmse: 21231.3613 - val_loss: 831309760.0000 - val_rmse: 28832.4434\n",
      "Epoch 63/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477543072.0000 - rmse: 21852.7598 - val_loss: 594440832.0000 - val_rmse: 24381.1582\n",
      "Epoch 64/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484052192.0000 - rmse: 22001.1855 - val_loss: 643667136.0000 - val_rmse: 25370.5957\n",
      "Epoch 65/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531207200.0000 - rmse: 23047.9316 - val_loss: 826726272.0000 - val_rmse: 28752.8477\n",
      "Epoch 66/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460280160.0000 - rmse: 21454.1406 - val_loss: 821372864.0000 - val_rmse: 28659.6035\n",
      "Epoch 67/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455316736.0000 - rmse: 21338.1523 - val_loss: 670203648.0000 - val_rmse: 25888.2910\n",
      "Epoch 68/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430553472.0000 - rmse: 20749.7812 - val_loss: 820708096.0000 - val_rmse: 28648.0039\n",
      "Epoch 69/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455823744.0000 - rmse: 21350.0293 - val_loss: 662626944.0000 - val_rmse: 25741.5391\n",
      "Epoch 70/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429418528.0000 - rmse: 20722.4160 - val_loss: 739622784.0000 - val_rmse: 27196.0059\n",
      "Epoch 71/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413517888.0000 - rmse: 20335.1387 - val_loss: 978489280.0000 - val_rmse: 31280.8125\n",
      "Epoch 72/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462670304.0000 - rmse: 21509.7715 - val_loss: 719845504.0000 - val_rmse: 26829.9375\n",
      "Epoch 73/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420438432.0000 - rmse: 20504.5957 - val_loss: 1007248896.0000 - val_rmse: 31737.1855\n",
      "Epoch 74/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493305216.0000 - rmse: 22210.4746 - val_loss: 1017793088.0000 - val_rmse: 31902.8672\n",
      "Epoch 75/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388205568.0000 - rmse: 19702.9336 - val_loss: 608828992.0000 - val_rmse: 24674.4590\n",
      "Epoch 76/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482957888.0000 - rmse: 21976.3027 - val_loss: 1137241984.0000 - val_rmse: 33723.0195\n",
      "Epoch 77/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424580448.0000 - rmse: 20605.3477 - val_loss: 693752064.0000 - val_rmse: 26339.1719\n",
      "Epoch 78/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397634208.0000 - rmse: 19940.7676 - val_loss: 907445440.0000 - val_rmse: 30123.8359\n",
      "Epoch 79/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429456416.0000 - rmse: 20723.3301 - val_loss: 924987712.0000 - val_rmse: 30413.6113\n",
      "Epoch 80/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407368096.0000 - rmse: 20183.3613 - val_loss: 778977664.0000 - val_rmse: 27910.1719\n",
      "Epoch 81/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386661824.0000 - rmse: 19663.7168 - val_loss: 659142464.0000 - val_rmse: 25673.7695\n",
      "Epoch 82/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460046656.0000 - rmse: 21448.6973 - val_loss: 599259584.0000 - val_rmse: 24479.7793\n",
      "Epoch 83/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401881920.0000 - rmse: 20046.9902 - val_loss: 622016192.0000 - val_rmse: 24940.2520\n",
      "Epoch 84/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419297312.0000 - rmse: 20476.7500 - val_loss: 656343616.0000 - val_rmse: 25619.2031\n",
      "Epoch 85/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370825728.0000 - rmse: 19256.8359 - val_loss: 780245760.0000 - val_rmse: 27932.8789\n",
      "Epoch 86/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360057344.0000 - rmse: 18975.1758 - val_loss: 880754688.0000 - val_rmse: 29677.5117\n",
      "Epoch 87/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381932608.0000 - rmse: 19543.0938 - val_loss: 647333952.0000 - val_rmse: 25442.7578\n",
      "Epoch 88/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342682880.0000 - rmse: 18511.6934 - val_loss: 668713920.0000 - val_rmse: 25859.5039\n",
      "Epoch 89/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389399136.0000 - rmse: 19733.1973 - val_loss: 695236800.0000 - val_rmse: 26367.3418\n",
      "Epoch 90/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376232128.0000 - rmse: 19396.7031 - val_loss: 1092817024.0000 - val_rmse: 33057.7812\n",
      "Epoch 91/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393016128.0000 - rmse: 19824.6328 - val_loss: 799999488.0000 - val_rmse: 28284.2617\n",
      "Epoch 92/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405144544.0000 - rmse: 20128.2031 - val_loss: 668263104.0000 - val_rmse: 25850.7852\n",
      "Epoch 93/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417374336.0000 - rmse: 20429.7402 - val_loss: 712354624.0000 - val_rmse: 26689.9727\n",
      "Epoch 94/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350261632.0000 - rmse: 18715.2773 - val_loss: 703552192.0000 - val_rmse: 26524.5586\n",
      "Epoch 95/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342858976.0000 - rmse: 18516.4492 - val_loss: 628312128.0000 - val_rmse: 25066.1543\n",
      "Epoch 96/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396893728.0000 - rmse: 19922.1914 - val_loss: 696776256.0000 - val_rmse: 26396.5195\n",
      "Epoch 97/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374327104.0000 - rmse: 19347.5352 - val_loss: 974720320.0000 - val_rmse: 31220.5117\n",
      "Epoch 98/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396436544.0000 - rmse: 19910.7129 - val_loss: 689563392.0000 - val_rmse: 26259.5371\n",
      "Epoch 99/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362215264.0000 - rmse: 19031.9512 - val_loss: 571629568.0000 - val_rmse: 23908.7754\n",
      "Epoch 100/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351979840.0000 - rmse: 18761.1250 - val_loss: 877312512.0000 - val_rmse: 29619.4609\n",
      "Epoch 101/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373779232.0000 - rmse: 19333.3691 - val_loss: 992156672.0000 - val_rmse: 31498.5176\n",
      "Epoch 102/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401680672.0000 - rmse: 20041.9727 - val_loss: 626780800.0000 - val_rmse: 25035.5879\n",
      "Epoch 103/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352961344.0000 - rmse: 18787.2637 - val_loss: 823853888.0000 - val_rmse: 28702.8555\n",
      "Epoch 104/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326311552.0000 - rmse: 18064.0957 - val_loss: 706860160.0000 - val_rmse: 26586.8418\n",
      "Epoch 105/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402986624.0000 - rmse: 20074.5273 - val_loss: 662652672.0000 - val_rmse: 25742.0391\n",
      "Epoch 106/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378064576.0000 - rmse: 19443.8809 - val_loss: 693656640.0000 - val_rmse: 26337.3613\n",
      "Epoch 107/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410397568.0000 - rmse: 20258.2715 - val_loss: 1029767872.0000 - val_rmse: 32089.9961\n",
      "Epoch 108/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296658304.0000 - rmse: 17223.7695 - val_loss: 636987904.0000 - val_rmse: 25238.6191\n",
      "Epoch 109/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374450432.0000 - rmse: 19350.7207 - val_loss: 662617664.0000 - val_rmse: 25741.3613\n",
      "Epoch 110/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357155104.0000 - rmse: 18898.5449 - val_loss: 857905472.0000 - val_rmse: 29290.0234\n",
      "Epoch 111/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386939936.0000 - rmse: 19670.7871 - val_loss: 1109780992.0000 - val_rmse: 33313.3750\n",
      "Epoch 112/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357828352.0000 - rmse: 18916.3516 - val_loss: 649311680.0000 - val_rmse: 25481.5938\n",
      "Epoch 113/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375882432.0000 - rmse: 19387.6875 - val_loss: 737003328.0000 - val_rmse: 27147.8047\n",
      "Epoch 114/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355475232.0000 - rmse: 18854.0488 - val_loss: 598624576.0000 - val_rmse: 24466.8047\n",
      "Epoch 115/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335316832.0000 - rmse: 18311.6582 - val_loss: 863336384.0000 - val_rmse: 29382.5859\n",
      "Epoch 116/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307661152.0000 - rmse: 17540.2715 - val_loss: 700784896.0000 - val_rmse: 26472.3418\n",
      "Epoch 117/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348019680.0000 - rmse: 18655.2852 - val_loss: 837247552.0000 - val_rmse: 28935.2305\n",
      "Epoch 118/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363953184.0000 - rmse: 19077.5547 - val_loss: 968430336.0000 - val_rmse: 31119.6133\n",
      "Epoch 119/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345697312.0000 - rmse: 18592.9375 - val_loss: 931243712.0000 - val_rmse: 30516.2852\n",
      "Epoch 120/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359739616.0000 - rmse: 18966.8027 - val_loss: 968446016.0000 - val_rmse: 31119.8652\n",
      "Epoch 121/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276627616.0000 - rmse: 16632.1250 - val_loss: 752309760.0000 - val_rmse: 27428.2656\n",
      "Epoch 122/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324687808.0000 - rmse: 18019.0957 - val_loss: 836066752.0000 - val_rmse: 28914.8184\n",
      "Epoch 123/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329307136.0000 - rmse: 18146.8203 - val_loss: 683574464.0000 - val_rmse: 26145.2539\n",
      "Epoch 124/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343575168.0000 - rmse: 18535.7793 - val_loss: 738841280.0000 - val_rmse: 27181.6348\n",
      "Epoch 125/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300929024.0000 - rmse: 17347.3047 - val_loss: 1031406208.0000 - val_rmse: 32115.5137\n",
      "Epoch 126/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350946880.0000 - rmse: 18733.5762 - val_loss: 1115225600.0000 - val_rmse: 33394.9922\n",
      "Epoch 127/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325740576.0000 - rmse: 18048.2832 - val_loss: 950972800.0000 - val_rmse: 30837.8477\n",
      "Epoch 128/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324437920.0000 - rmse: 18012.1602 - val_loss: 761343808.0000 - val_rmse: 27592.4570\n",
      "Epoch 129/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287532992.0000 - rmse: 16956.7969 - val_loss: 1043537088.0000 - val_rmse: 32303.8242\n",
      "Epoch 130/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286310912.0000 - rmse: 16920.7227 - val_loss: 753299840.0000 - val_rmse: 27446.3047\n",
      "Epoch 131/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311029088.0000 - rmse: 17636.0176 - val_loss: 777408896.0000 - val_rmse: 27882.0508\n",
      "Epoch 132/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292998624.0000 - rmse: 17117.2012 - val_loss: 951435712.0000 - val_rmse: 30845.3516\n",
      "Epoch 133/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284523744.0000 - rmse: 16867.8301 - val_loss: 759835520.0000 - val_rmse: 27565.1133\n",
      "Epoch 134/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296283456.0000 - rmse: 17212.8848 - val_loss: 790788352.0000 - val_rmse: 28120.9590\n",
      "Epoch 135/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267322416.0000 - rmse: 16349.9961 - val_loss: 683894720.0000 - val_rmse: 26151.3809\n",
      "Epoch 136/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264545920.0000 - rmse: 16264.8652 - val_loss: 674576768.0000 - val_rmse: 25972.6152\n",
      "Epoch 137/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315751136.0000 - rmse: 17769.3867 - val_loss: 1028609024.0000 - val_rmse: 32071.9355\n",
      "Epoch 138/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333748128.0000 - rmse: 18268.7734 - val_loss: 1117086080.0000 - val_rmse: 33422.8359\n",
      "Epoch 139/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312084416.0000 - rmse: 17665.9102 - val_loss: 806631872.0000 - val_rmse: 28401.2656\n",
      "Epoch 140/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320013664.0000 - rmse: 17888.9238 - val_loss: 753523200.0000 - val_rmse: 27450.3770\n",
      "Epoch 141/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291527328.0000 - rmse: 17074.1699 - val_loss: 828833856.0000 - val_rmse: 28789.4727\n",
      "Epoch 142/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264195280.0000 - rmse: 16254.0840 - val_loss: 947027840.0000 - val_rmse: 30773.8184\n",
      "Epoch 143/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278307008.0000 - rmse: 16682.5332 - val_loss: 892673088.0000 - val_rmse: 29877.6328\n",
      "Epoch 144/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228799248.0000 - rmse: 15126.1104 - val_loss: 804770752.0000 - val_rmse: 28368.4824\n",
      "Epoch 145/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329395232.0000 - rmse: 18149.2461 - val_loss: 1095265024.0000 - val_rmse: 33094.7891\n",
      "Epoch 146/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268803872.0000 - rmse: 16395.2383 - val_loss: 812411136.0000 - val_rmse: 28502.8262\n",
      "Epoch 147/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334895520.0000 - rmse: 18300.1484 - val_loss: 1053620352.0000 - val_rmse: 32459.5195\n",
      "Epoch 148/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264127648.0000 - rmse: 16252.0039 - val_loss: 873855808.0000 - val_rmse: 29561.0508\n",
      "Epoch 149/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286004320.0000 - rmse: 16911.6602 - val_loss: 803602368.0000 - val_rmse: 28347.8809\n",
      "Epoch 150/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271213120.0000 - rmse: 16468.5469 - val_loss: 1229518720.0000 - val_rmse: 35064.4922\n",
      "Epoch 151/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264915904.0000 - rmse: 16276.2354 - val_loss: 1152442880.0000 - val_rmse: 33947.6484\n",
      "Epoch 152/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298372160.0000 - rmse: 17273.4512 - val_loss: 983087872.0000 - val_rmse: 31354.2324\n",
      "Epoch 153/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295328992.0000 - rmse: 17185.1367 - val_loss: 767452416.0000 - val_rmse: 27702.9297\n",
      "Epoch 154/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247064928.0000 - rmse: 15718.2988 - val_loss: 1231828864.0000 - val_rmse: 35097.4180\n",
      "Epoch 155/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297598464.0000 - rmse: 17251.0410 - val_loss: 1209434624.0000 - val_rmse: 34776.9258\n",
      "Epoch 156/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265656384.0000 - rmse: 16298.9668 - val_loss: 752798336.0000 - val_rmse: 27437.1699\n",
      "Epoch 157/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262080848.0000 - rmse: 16188.9092 - val_loss: 1030814080.0000 - val_rmse: 32106.2930\n",
      "Epoch 158/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276833792.0000 - rmse: 16638.3203 - val_loss: 722361024.0000 - val_rmse: 26876.7754\n",
      "Epoch 159/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273432256.0000 - rmse: 16535.7871 - val_loss: 1075944192.0000 - val_rmse: 32801.5859\n",
      "Epoch 160/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241562304.0000 - rmse: 15542.2725 - val_loss: 781365632.0000 - val_rmse: 27952.9160\n",
      "Epoch 161/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271100256.0000 - rmse: 16465.1211 - val_loss: 755740992.0000 - val_rmse: 27490.7422\n",
      "Epoch 162/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253666976.0000 - rmse: 15926.9248 - val_loss: 1113671040.0000 - val_rmse: 33371.7109\n",
      "Epoch 163/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247955856.0000 - rmse: 15746.6113 - val_loss: 869806848.0000 - val_rmse: 29492.4883\n",
      "Epoch 164/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259327360.0000 - rmse: 16103.6416 - val_loss: 889786944.0000 - val_rmse: 29829.2969\n",
      "Epoch 165/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243598512.0000 - rmse: 15607.6396 - val_loss: 835518656.0000 - val_rmse: 28905.3398\n",
      "Epoch 166/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257960064.0000 - rmse: 16061.1348 - val_loss: 1372656256.0000 - val_rmse: 37049.3750\n",
      "Epoch 167/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251050640.0000 - rmse: 15844.5752 - val_loss: 1290240896.0000 - val_rmse: 35919.9219\n",
      "Epoch 168/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248620640.0000 - rmse: 15767.7080 - val_loss: 1046166144.0000 - val_rmse: 32344.4902\n",
      "Epoch 169/300\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272234208.0000 - rmse: 16499.5195 - val_loss: 1003949696.0000 - val_rmse: 31685.1660\n",
      "104/104 [==============================] - 0s 730us/step - loss: 476924608.0000 - rmse: 21838.6035\n",
      "[476924608.0, 21838.603515625]\n",
      "[19640.447265625, 30953.541015625, 27382.30859375, 21823.75, 21838.603515625]\n",
      "24327.730078125\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!python train.py train"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 23:32:13.407522: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 23:32:13.407556: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 23:32:13.407982: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2021-10-05 23:32:15.851771: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 25071489024.0000 - rmse: 158339.7969 - val_loss: 23855239168.0000 - val_rmse: 154451.4062\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 17199902720.0000 - rmse: 131148.4062 - val_loss: 8081795072.0000 - val_rmse: 89898.8047\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 4200166400.0000 - rmse: 64808.6914 - val_loss: 2272960000.0000 - val_rmse: 47675.5703\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2668447232.0000 - rmse: 51657.0156 - val_loss: 1696050688.0000 - val_rmse: 41183.1367\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2198835712.0000 - rmse: 46891.7461 - val_loss: 1435605760.0000 - val_rmse: 37889.3867\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2085879424.0000 - rmse: 45671.4297 - val_loss: 1267137152.0000 - val_rmse: 35596.8711\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1888622336.0000 - rmse: 43458.2812 - val_loss: 1155735424.0000 - val_rmse: 33996.1094\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1818590080.0000 - rmse: 42644.9297 - val_loss: 1115371264.0000 - val_rmse: 33397.1758\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1713986944.0000 - rmse: 41400.3242 - val_loss: 1047356992.0000 - val_rmse: 32362.8945\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1708317952.0000 - rmse: 41331.8047 - val_loss: 1017046848.0000 - val_rmse: 31891.1719\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1597227008.0000 - rmse: 39965.3242 - val_loss: 979963456.0000 - val_rmse: 31304.3672\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1589489536.0000 - rmse: 39868.4023 - val_loss: 952929600.0000 - val_rmse: 30869.5586\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1543941376.0000 - rmse: 39293.0195 - val_loss: 931650816.0000 - val_rmse: 30522.9551\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1539758336.0000 - rmse: 39239.7539 - val_loss: 911005376.0000 - val_rmse: 30182.8652\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1508384512.0000 - rmse: 38837.9258 - val_loss: 899503360.0000 - val_rmse: 29991.7207\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1478641280.0000 - rmse: 38453.1055 - val_loss: 880903552.0000 - val_rmse: 29680.0195\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1441294336.0000 - rmse: 37964.3828 - val_loss: 892594688.0000 - val_rmse: 29876.3223\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1400424448.0000 - rmse: 37422.2461 - val_loss: 874599744.0000 - val_rmse: 29573.6328\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1399616384.0000 - rmse: 37411.4453 - val_loss: 847673088.0000 - val_rmse: 29114.8262\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1396847616.0000 - rmse: 37374.4258 - val_loss: 836436928.0000 - val_rmse: 28921.2188\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1354935040.0000 - rmse: 36809.4414 - val_loss: 826817920.0000 - val_rmse: 28754.4414\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1346892160.0000 - rmse: 36700.0312 - val_loss: 817229824.0000 - val_rmse: 28587.2324\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1375257984.0000 - rmse: 37084.4727 - val_loss: 809321152.0000 - val_rmse: 28448.5703\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1356091392.0000 - rmse: 36825.1445 - val_loss: 819475264.0000 - val_rmse: 28626.4785\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1312138368.0000 - rmse: 36223.4492 - val_loss: 799495488.0000 - val_rmse: 28275.3516\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1334801024.0000 - rmse: 36534.9258 - val_loss: 831511808.0000 - val_rmse: 28835.9473\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1296833920.0000 - rmse: 36011.5820 - val_loss: 783596224.0000 - val_rmse: 27992.7891\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1267872896.0000 - rmse: 35607.1992 - val_loss: 780711168.0000 - val_rmse: 27941.2090\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1332408960.0000 - rmse: 36502.1758 - val_loss: 816899136.0000 - val_rmse: 28581.4473\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1279897984.0000 - rmse: 35775.6602 - val_loss: 781248768.0000 - val_rmse: 27950.8281\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1269653888.0000 - rmse: 35632.2031 - val_loss: 765777152.0000 - val_rmse: 27672.6797\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1249114240.0000 - rmse: 35342.8086 - val_loss: 758296704.0000 - val_rmse: 27537.1875\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1246571520.0000 - rmse: 35306.8203 - val_loss: 754799488.0000 - val_rmse: 27473.6152\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1272992256.0000 - rmse: 35679.0156 - val_loss: 750138304.0000 - val_rmse: 27388.6523\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1244497792.0000 - rmse: 35277.4414 - val_loss: 746095872.0000 - val_rmse: 27314.7539\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1249216256.0000 - rmse: 35344.2539 - val_loss: 764231168.0000 - val_rmse: 27644.7305\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1206399616.0000 - rmse: 34733.2656 - val_loss: 743053504.0000 - val_rmse: 27259.0078\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1213166720.0000 - rmse: 34830.5430 - val_loss: 741799040.0000 - val_rmse: 27235.9863\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1192834304.0000 - rmse: 34537.4336 - val_loss: 741099712.0000 - val_rmse: 27223.1465\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1252964608.0000 - rmse: 35397.2383 - val_loss: 735056640.0000 - val_rmse: 27111.9258\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1167287936.0000 - rmse: 34165.5977 - val_loss: 742655360.0000 - val_rmse: 27251.7031\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1182838016.0000 - rmse: 34392.4141 - val_loss: 781398912.0000 - val_rmse: 27953.5137\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1223102080.0000 - rmse: 34972.8750 - val_loss: 748920576.0000 - val_rmse: 27366.4141\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1177453696.0000 - rmse: 34314.0469 - val_loss: 719420096.0000 - val_rmse: 26822.0078\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1183538816.0000 - rmse: 34402.5977 - val_loss: 721307200.0000 - val_rmse: 26857.1602\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1214518016.0000 - rmse: 34849.9375 - val_loss: 729978240.0000 - val_rmse: 27018.1074\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1148202624.0000 - rmse: 33885.1406 - val_loss: 721948352.0000 - val_rmse: 26869.0957\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1196758912.0000 - rmse: 34594.2031 - val_loss: 711767488.0000 - val_rmse: 26678.9707\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1128255744.0000 - rmse: 33589.5195 - val_loss: 735825344.0000 - val_rmse: 27126.1016\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1104889984.0000 - rmse: 33239.8867 - val_loss: 727835904.0000 - val_rmse: 26978.4336\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1129065728.0000 - rmse: 33601.5742 - val_loss: 714507200.0000 - val_rmse: 26730.2676\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1142069632.0000 - rmse: 33794.5195 - val_loss: 706499968.0000 - val_rmse: 26580.0664\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1134979456.0000 - rmse: 33689.4570 - val_loss: 700579520.0000 - val_rmse: 26468.4629\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1167513856.0000 - rmse: 34168.9023 - val_loss: 697269504.0000 - val_rmse: 26405.8613\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1078194048.0000 - rmse: 32835.8672 - val_loss: 717160512.0000 - val_rmse: 26779.8516\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1090438016.0000 - rmse: 33021.7812 - val_loss: 704135168.0000 - val_rmse: 26535.5449\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1112038400.0000 - rmse: 33347.2383 - val_loss: 696555008.0000 - val_rmse: 26392.3281\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1126497792.0000 - rmse: 33563.3398 - val_loss: 712616256.0000 - val_rmse: 26694.8730\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1142790400.0000 - rmse: 33805.1836 - val_loss: 682308992.0000 - val_rmse: 26121.0449\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1055302976.0000 - rmse: 32485.4277 - val_loss: 668903936.0000 - val_rmse: 25863.1758\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1089653248.0000 - rmse: 33009.8945 - val_loss: 671931968.0000 - val_rmse: 25921.6504\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1035262144.0000 - rmse: 32175.4902 - val_loss: 663488320.0000 - val_rmse: 25758.2676\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1008433216.0000 - rmse: 31755.8359 - val_loss: 666390912.0000 - val_rmse: 25814.5488\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 986154240.0000 - rmse: 31403.0938 - val_loss: 670177472.0000 - val_rmse: 25887.7871\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1023664832.0000 - rmse: 31994.7617 - val_loss: 650929408.0000 - val_rmse: 25513.3184\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1001040000.0000 - rmse: 31639.2168 - val_loss: 668289472.0000 - val_rmse: 25851.2949\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1010537536.0000 - rmse: 31788.9531 - val_loss: 626348352.0000 - val_rmse: 25026.9531\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 985069184.0000 - rmse: 31385.8125 - val_loss: 620699072.0000 - val_rmse: 24913.8320\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 942547456.0000 - rmse: 30700.9355 - val_loss: 695743552.0000 - val_rmse: 26376.9512\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1005332224.0000 - rmse: 31706.9746 - val_loss: 608472896.0000 - val_rmse: 24667.2441\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 996226304.0000 - rmse: 31563.0527 - val_loss: 627853696.0000 - val_rmse: 25057.0078\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 947207936.0000 - rmse: 30776.7441 - val_loss: 612004096.0000 - val_rmse: 24738.7168\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 946773568.0000 - rmse: 30769.6855 - val_loss: 598256832.0000 - val_rmse: 24459.2891\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 908073920.0000 - rmse: 30134.2637 - val_loss: 646838464.0000 - val_rmse: 25433.0195\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 906391040.0000 - rmse: 30106.3281 - val_loss: 580004224.0000 - val_rmse: 24083.2773\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 932927616.0000 - rmse: 30543.8633 - val_loss: 577870976.0000 - val_rmse: 24038.9473\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 911381120.0000 - rmse: 30189.0898 - val_loss: 591009792.0000 - val_rmse: 24310.6934\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 888547968.0000 - rmse: 29808.5215 - val_loss: 585902784.0000 - val_rmse: 24205.4297\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 859125376.0000 - rmse: 29310.8398 - val_loss: 567463680.0000 - val_rmse: 23821.4961\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 872985152.0000 - rmse: 29546.3223 - val_loss: 570798656.0000 - val_rmse: 23891.3926\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 835177344.0000 - rmse: 28899.4355 - val_loss: 619770496.0000 - val_rmse: 24895.1895\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 824509376.0000 - rmse: 28714.2715 - val_loss: 609898624.0000 - val_rmse: 24696.1250\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 829042048.0000 - rmse: 28793.0898 - val_loss: 612307840.0000 - val_rmse: 24744.8555\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 885361408.0000 - rmse: 29755.0234 - val_loss: 553366208.0000 - val_rmse: 23523.7363\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 833698048.0000 - rmse: 28873.8301 - val_loss: 551310208.0000 - val_rmse: 23479.9961\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 839589568.0000 - rmse: 28975.6719 - val_loss: 559097920.0000 - val_rmse: 23645.2520\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 819931456.0000 - rmse: 28634.4453 - val_loss: 551368576.0000 - val_rmse: 23481.2383\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 775771840.0000 - rmse: 27852.6816 - val_loss: 538612288.0000 - val_rmse: 23208.0215\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 771454720.0000 - rmse: 27775.0742 - val_loss: 537470272.0000 - val_rmse: 23183.4043\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 761030976.0000 - rmse: 27586.7891 - val_loss: 592867904.0000 - val_rmse: 24348.8789\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 775877184.0000 - rmse: 27854.5723 - val_loss: 540849088.0000 - val_rmse: 23256.1621\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 778191232.0000 - rmse: 27896.0801 - val_loss: 526310176.0000 - val_rmse: 22941.4512\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 823099520.0000 - rmse: 28689.7109 - val_loss: 562722240.0000 - val_rmse: 23721.7676\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 780898880.0000 - rmse: 27944.5684 - val_loss: 515975136.0000 - val_rmse: 22715.0859\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 760986432.0000 - rmse: 27585.9824 - val_loss: 514621504.0000 - val_rmse: 22685.2715\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 793833856.0000 - rmse: 28175.0566 - val_loss: 514681824.0000 - val_rmse: 22686.5996\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 788390976.0000 - rmse: 28078.2988 - val_loss: 524262464.0000 - val_rmse: 22896.7793\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 762695168.0000 - rmse: 27616.9355 - val_loss: 525770624.0000 - val_rmse: 22929.6895\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 777764736.0000 - rmse: 27888.4336 - val_loss: 509686528.0000 - val_rmse: 22576.2383\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 828988352.0000 - rmse: 28792.1582 - val_loss: 509492992.0000 - val_rmse: 22571.9512\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 713348160.0000 - rmse: 26708.5781 - val_loss: 525744160.0000 - val_rmse: 22929.1094\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 732499264.0000 - rmse: 27064.7227 - val_loss: 528656224.0000 - val_rmse: 22992.5254\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 784912640.0000 - rmse: 28016.2930 - val_loss: 547717504.0000 - val_rmse: 23403.3652\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 763213184.0000 - rmse: 27626.3125 - val_loss: 508011520.0000 - val_rmse: 22539.1113\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 727292736.0000 - rmse: 26968.3652 - val_loss: 642531008.0000 - val_rmse: 25348.1934\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 717629568.0000 - rmse: 26788.6094 - val_loss: 515377120.0000 - val_rmse: 22701.9180\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 750579264.0000 - rmse: 27396.7012 - val_loss: 512454784.0000 - val_rmse: 22637.4629\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 736340032.0000 - rmse: 27135.5859 - val_loss: 508312416.0000 - val_rmse: 22545.7852\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 758792576.0000 - rmse: 27546.1895 - val_loss: 506313216.0000 - val_rmse: 22501.4043\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 708448512.0000 - rmse: 26616.6953 - val_loss: 480614912.0000 - val_rmse: 21922.9316\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 733411392.0000 - rmse: 27081.5684 - val_loss: 606157120.0000 - val_rmse: 24620.2578\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 696090880.0000 - rmse: 26383.5352 - val_loss: 486049184.0000 - val_rmse: 22046.5215\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680858624.0000 - rmse: 26093.2676 - val_loss: 482200864.0000 - val_rmse: 21959.0723\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 697522688.0000 - rmse: 26410.6543 - val_loss: 492261248.0000 - val_rmse: 22186.9609\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 700565760.0000 - rmse: 26468.2031 - val_loss: 465390400.0000 - val_rmse: 21572.9082\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 683845824.0000 - rmse: 26150.4453 - val_loss: 516726464.0000 - val_rmse: 22731.6172\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 679249280.0000 - rmse: 26062.4121 - val_loss: 478085824.0000 - val_rmse: 21865.1738\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 664505664.0000 - rmse: 25778.0059 - val_loss: 489768512.0000 - val_rmse: 22130.7129\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651358784.0000 - rmse: 25521.7324 - val_loss: 464661056.0000 - val_rmse: 21555.9961\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 674216320.0000 - rmse: 25965.6758 - val_loss: 458379872.0000 - val_rmse: 21409.8086\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 680284992.0000 - rmse: 26082.2734 - val_loss: 458648320.0000 - val_rmse: 21416.0762\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620978176.0000 - rmse: 24919.4336 - val_loss: 494786880.0000 - val_rmse: 22243.8047\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 654408448.0000 - rmse: 25581.4082 - val_loss: 458586336.0000 - val_rmse: 21414.6289\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677148544.0000 - rmse: 26022.0781 - val_loss: 452343040.0000 - val_rmse: 21268.3574\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614383424.0000 - rmse: 24786.7578 - val_loss: 478041344.0000 - val_rmse: 21864.1562\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661625344.0000 - rmse: 25722.0781 - val_loss: 460115552.0000 - val_rmse: 21450.3027\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 622096512.0000 - rmse: 24941.8633 - val_loss: 446113120.0000 - val_rmse: 21121.3906\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610830016.0000 - rmse: 24714.9746 - val_loss: 437124768.0000 - val_rmse: 20907.5293\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 598467264.0000 - rmse: 24463.5898 - val_loss: 461240480.0000 - val_rmse: 21476.5098\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 581993728.0000 - rmse: 24124.5469 - val_loss: 455177120.0000 - val_rmse: 21334.8809\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605326912.0000 - rmse: 24603.3926 - val_loss: 456408096.0000 - val_rmse: 21363.7090\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573412608.0000 - rmse: 23946.0352 - val_loss: 430847296.0000 - val_rmse: 20756.8613\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 602716032.0000 - rmse: 24550.2734 - val_loss: 454543776.0000 - val_rmse: 21320.0332\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 687962816.0000 - rmse: 26229.0449 - val_loss: 439019232.0000 - val_rmse: 20952.7852\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564979264.0000 - rmse: 23769.2930 - val_loss: 483786560.0000 - val_rmse: 21995.1484\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 619871616.0000 - rmse: 24897.2207 - val_loss: 431830752.0000 - val_rmse: 20780.5371\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569939200.0000 - rmse: 23873.4004 - val_loss: 436418848.0000 - val_rmse: 20890.6387\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 631824064.0000 - rmse: 25136.1113 - val_loss: 423485920.0000 - val_rmse: 20578.7715\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 591544064.0000 - rmse: 24321.6797 - val_loss: 436408480.0000 - val_rmse: 20890.3926\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564101184.0000 - rmse: 23750.8145 - val_loss: 419836704.0000 - val_rmse: 20489.9180\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 611360064.0000 - rmse: 24725.6973 - val_loss: 415936736.0000 - val_rmse: 20394.5254\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594340544.0000 - rmse: 24379.0996 - val_loss: 432544608.0000 - val_rmse: 20797.7051\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 650977600.0000 - rmse: 25514.2617 - val_loss: 461683808.0000 - val_rmse: 21486.8262\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586583424.0000 - rmse: 24219.4824 - val_loss: 432696576.0000 - val_rmse: 20801.3594\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579907200.0000 - rmse: 24081.2617 - val_loss: 469097472.0000 - val_rmse: 21658.6562\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560888320.0000 - rmse: 23683.0801 - val_loss: 501777152.0000 - val_rmse: 22400.3828\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621955136.0000 - rmse: 24939.0273 - val_loss: 422761440.0000 - val_rmse: 20561.1641\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596814976.0000 - rmse: 24429.7969 - val_loss: 432140448.0000 - val_rmse: 20787.9883\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540738496.0000 - rmse: 23253.7852 - val_loss: 422811328.0000 - val_rmse: 20562.3770\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555330304.0000 - rmse: 23565.4453 - val_loss: 423598752.0000 - val_rmse: 20581.5137\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 523106912.0000 - rmse: 22871.5312 - val_loss: 408989504.0000 - val_rmse: 20223.4883\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553951936.0000 - rmse: 23536.1836 - val_loss: 420395360.0000 - val_rmse: 20503.5430\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545377600.0000 - rmse: 23353.3203 - val_loss: 422339296.0000 - val_rmse: 20550.8945\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587339392.0000 - rmse: 24235.0859 - val_loss: 451490464.0000 - val_rmse: 21248.3027\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519156128.0000 - rmse: 22784.9980 - val_loss: 421505984.0000 - val_rmse: 20530.6094\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500401376.0000 - rmse: 22369.6523 - val_loss: 408789824.0000 - val_rmse: 20218.5508\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534098240.0000 - rmse: 23110.5625 - val_loss: 453219584.0000 - val_rmse: 21288.9551\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494012288.0000 - rmse: 22226.3867 - val_loss: 439040800.0000 - val_rmse: 20953.3008\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504097664.0000 - rmse: 22452.1191 - val_loss: 404444448.0000 - val_rmse: 20110.8047\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531305056.0000 - rmse: 23050.0547 - val_loss: 424342624.0000 - val_rmse: 20599.5781\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542554240.0000 - rmse: 23292.7910 - val_loss: 417322528.0000 - val_rmse: 20428.4727\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 476279936.0000 - rmse: 21823.8379 - val_loss: 448490976.0000 - val_rmse: 21177.6055\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 550971520.0000 - rmse: 23472.7832 - val_loss: 489432384.0000 - val_rmse: 22123.1191\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529871840.0000 - rmse: 23018.9453 - val_loss: 433422976.0000 - val_rmse: 20818.8125\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506537728.0000 - rmse: 22506.3926 - val_loss: 444611072.0000 - val_rmse: 21085.8027\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482523040.0000 - rmse: 21966.4062 - val_loss: 420002688.0000 - val_rmse: 20493.9648\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487116704.0000 - rmse: 22070.7207 - val_loss: 431801920.0000 - val_rmse: 20779.8418\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501205344.0000 - rmse: 22387.6152 - val_loss: 404114944.0000 - val_rmse: 20102.6094\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481371264.0000 - rmse: 21940.1738 - val_loss: 412279776.0000 - val_rmse: 20304.6719\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589078592.0000 - rmse: 24270.9414 - val_loss: 397837184.0000 - val_rmse: 19945.8555\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480306176.0000 - rmse: 21915.8887 - val_loss: 399648448.0000 - val_rmse: 19991.2090\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505321344.0000 - rmse: 22479.3535 - val_loss: 406675712.0000 - val_rmse: 20166.2012\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471978336.0000 - rmse: 21725.0605 - val_loss: 544388736.0000 - val_rmse: 23332.1387\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508457184.0000 - rmse: 22548.9941 - val_loss: 391210144.0000 - val_rmse: 19779.0332\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443502720.0000 - rmse: 21059.5020 - val_loss: 397546496.0000 - val_rmse: 19938.5684\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452455008.0000 - rmse: 21270.9883 - val_loss: 396068128.0000 - val_rmse: 19901.4590\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488918400.0000 - rmse: 22111.5000 - val_loss: 492034080.0000 - val_rmse: 22181.8418\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544615680.0000 - rmse: 23337.0020 - val_loss: 401969408.0000 - val_rmse: 20049.1738\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580026240.0000 - rmse: 24083.7344 - val_loss: 486244768.0000 - val_rmse: 22050.9570\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416509344.0000 - rmse: 20408.5605 - val_loss: 428294624.0000 - val_rmse: 20695.2793\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455786944.0000 - rmse: 21349.1680 - val_loss: 395910720.0000 - val_rmse: 19897.5059\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453231872.0000 - rmse: 21289.2422 - val_loss: 496906624.0000 - val_rmse: 22291.4004\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443303776.0000 - rmse: 21054.7812 - val_loss: 466605056.0000 - val_rmse: 21601.0430\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459441376.0000 - rmse: 21434.5820 - val_loss: 400729280.0000 - val_rmse: 20018.2246\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425384896.0000 - rmse: 20624.8613 - val_loss: 416624608.0000 - val_rmse: 20411.3848\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532058976.0000 - rmse: 23066.4043 - val_loss: 397618528.0000 - val_rmse: 19940.3730\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468813536.0000 - rmse: 21652.1016 - val_loss: 384318912.0000 - val_rmse: 19604.0527\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452401472.0000 - rmse: 21269.7305 - val_loss: 398680736.0000 - val_rmse: 19966.9902\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480272544.0000 - rmse: 21915.1191 - val_loss: 396252032.0000 - val_rmse: 19906.0781\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399414176.0000 - rmse: 19985.3496 - val_loss: 408723808.0000 - val_rmse: 20216.9180\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 491359104.0000 - rmse: 22166.6191 - val_loss: 377008928.0000 - val_rmse: 19416.7148\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470958336.0000 - rmse: 21701.5742 - val_loss: 391425952.0000 - val_rmse: 19784.4883\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439526528.0000 - rmse: 20964.8867 - val_loss: 506407552.0000 - val_rmse: 22503.4980\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439482688.0000 - rmse: 20963.8418 - val_loss: 385214848.0000 - val_rmse: 19626.8906\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374035136.0000 - rmse: 19339.9883 - val_loss: 443698720.0000 - val_rmse: 21064.1562\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506457824.0000 - rmse: 22504.6172 - val_loss: 392050816.0000 - val_rmse: 19800.2715\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429991680.0000 - rmse: 20736.2402 - val_loss: 411861152.0000 - val_rmse: 20294.3633\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468533312.0000 - rmse: 21645.6289 - val_loss: 399200128.0000 - val_rmse: 19979.9941\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494208800.0000 - rmse: 22230.8047 - val_loss: 411016064.0000 - val_rmse: 20273.5312\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426468992.0000 - rmse: 20651.1250 - val_loss: 394167392.0000 - val_rmse: 19853.6484\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456607968.0000 - rmse: 21368.3867 - val_loss: 418118656.0000 - val_rmse: 20447.9492\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463388448.0000 - rmse: 21526.4590 - val_loss: 397373888.0000 - val_rmse: 19934.2383\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437921760.0000 - rmse: 20926.5801 - val_loss: 598713664.0000 - val_rmse: 24468.6270\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420140544.0000 - rmse: 20497.3301 - val_loss: 414995328.0000 - val_rmse: 20371.4336\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403672992.0000 - rmse: 20091.6152 - val_loss: 421227488.0000 - val_rmse: 20523.8281\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456449152.0000 - rmse: 21364.6699 - val_loss: 393563744.0000 - val_rmse: 19838.4414\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417494240.0000 - rmse: 20432.6758 - val_loss: 410674528.0000 - val_rmse: 20265.1055\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445299776.0000 - rmse: 21102.1270 - val_loss: 404720928.0000 - val_rmse: 20117.6777\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505281440.0000 - rmse: 22478.4668 - val_loss: 386785120.0000 - val_rmse: 19666.8535\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412367456.0000 - rmse: 20306.8320 - val_loss: 392960160.0000 - val_rmse: 19823.2227\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417166656.0000 - rmse: 20424.6582 - val_loss: 389671392.0000 - val_rmse: 19740.0957\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434674464.0000 - rmse: 20848.8457 - val_loss: 431354048.0000 - val_rmse: 20769.0645\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382444608.0000 - rmse: 19556.1914 - val_loss: 374578048.0000 - val_rmse: 19354.0195\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486500736.0000 - rmse: 22056.7617 - val_loss: 390904096.0000 - val_rmse: 19771.2930\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391203680.0000 - rmse: 19778.8691 - val_loss: 376854912.0000 - val_rmse: 19412.7520\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387339776.0000 - rmse: 19680.9492 - val_loss: 384898880.0000 - val_rmse: 19618.8398\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418782784.0000 - rmse: 20464.1816 - val_loss: 373859840.0000 - val_rmse: 19335.4551\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440546976.0000 - rmse: 20989.2109 - val_loss: 374968672.0000 - val_rmse: 19364.1074\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371904864.0000 - rmse: 19284.8359 - val_loss: 397248192.0000 - val_rmse: 19931.0859\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403647072.0000 - rmse: 20090.9688 - val_loss: 400576032.0000 - val_rmse: 20014.3945\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400651232.0000 - rmse: 20016.2734 - val_loss: 379647168.0000 - val_rmse: 19484.5371\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525208160.0000 - rmse: 22917.4199 - val_loss: 384734368.0000 - val_rmse: 19614.6445\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350383680.0000 - rmse: 18718.5332 - val_loss: 409594752.0000 - val_rmse: 20238.4473\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378943776.0000 - rmse: 19466.4785 - val_loss: 439853408.0000 - val_rmse: 20972.6797\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440393024.0000 - rmse: 20985.5430 - val_loss: 437981504.0000 - val_rmse: 20928.0078\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372188352.0000 - rmse: 19292.1836 - val_loss: 383472160.0000 - val_rmse: 19582.4453\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383131488.0000 - rmse: 19573.7441 - val_loss: 383029376.0000 - val_rmse: 19571.1367\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466276928.0000 - rmse: 21593.4453 - val_loss: 411991616.0000 - val_rmse: 20297.5742\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426619936.0000 - rmse: 20654.7793 - val_loss: 393584704.0000 - val_rmse: 19838.9688\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398456448.0000 - rmse: 19961.3730 - val_loss: 385199232.0000 - val_rmse: 19626.4902\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398875744.0000 - rmse: 19971.8730 - val_loss: 390021760.0000 - val_rmse: 19748.9668\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358186688.0000 - rmse: 18925.8203 - val_loss: 458138208.0000 - val_rmse: 21404.1621\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389699424.0000 - rmse: 19740.8047 - val_loss: 378946592.0000 - val_rmse: 19466.5488\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413033664.0000 - rmse: 20323.2285 - val_loss: 391423168.0000 - val_rmse: 19784.4180\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418148704.0000 - rmse: 20448.6855 - val_loss: 371427328.0000 - val_rmse: 19272.4492\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378741696.0000 - rmse: 19461.2871 - val_loss: 402707776.0000 - val_rmse: 20067.5801\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364871840.0000 - rmse: 19101.6191 - val_loss: 426598912.0000 - val_rmse: 20654.2715\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409369600.0000 - rmse: 20232.8828 - val_loss: 379859680.0000 - val_rmse: 19489.9883\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396692320.0000 - rmse: 19917.1348 - val_loss: 480529792.0000 - val_rmse: 21920.9883\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421544640.0000 - rmse: 20531.5508 - val_loss: 371844288.0000 - val_rmse: 19283.2637\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355242496.0000 - rmse: 18847.8770 - val_loss: 388100640.0000 - val_rmse: 19700.2695\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323335744.0000 - rmse: 17981.5391 - val_loss: 367884032.0000 - val_rmse: 19180.3027\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399486272.0000 - rmse: 19987.1504 - val_loss: 394065760.0000 - val_rmse: 19851.0898\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390098944.0000 - rmse: 19750.9219 - val_loss: 482188672.0000 - val_rmse: 21958.7930\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 382555264.0000 - rmse: 19559.0195 - val_loss: 419296320.0000 - val_rmse: 20476.7266\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320624768.0000 - rmse: 17905.9980 - val_loss: 366627200.0000 - val_rmse: 19147.5098\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361866528.0000 - rmse: 19022.7891 - val_loss: 397944576.0000 - val_rmse: 19948.5449\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417890976.0000 - rmse: 20442.3809 - val_loss: 378365920.0000 - val_rmse: 19451.6289\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327300160.0000 - rmse: 18091.4375 - val_loss: 390788928.0000 - val_rmse: 19768.3828\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412277792.0000 - rmse: 20304.6230 - val_loss: 377634784.0000 - val_rmse: 19432.8262\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366994848.0000 - rmse: 19157.1094 - val_loss: 361320704.0000 - val_rmse: 19008.4355\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338621728.0000 - rmse: 18401.6777 - val_loss: 443928768.0000 - val_rmse: 21069.6172\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333268224.0000 - rmse: 18255.6348 - val_loss: 363917344.0000 - val_rmse: 19076.6152\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373549120.0000 - rmse: 19327.4180 - val_loss: 361766816.0000 - val_rmse: 19020.1680\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434151648.0000 - rmse: 20836.3047 - val_loss: 377434880.0000 - val_rmse: 19427.6816\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311890880.0000 - rmse: 17660.4297 - val_loss: 423484704.0000 - val_rmse: 20578.7422\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366704512.0000 - rmse: 19149.5293 - val_loss: 371455360.0000 - val_rmse: 19273.1758\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368513088.0000 - rmse: 19196.6953 - val_loss: 355073568.0000 - val_rmse: 18843.3945\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470059936.0000 - rmse: 21680.8652 - val_loss: 459892640.0000 - val_rmse: 21445.1074\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314582048.0000 - rmse: 17736.4609 - val_loss: 378985760.0000 - val_rmse: 19467.5566\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387285024.0000 - rmse: 19679.5566 - val_loss: 368174464.0000 - val_rmse: 19187.8711\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344383360.0000 - rmse: 18557.5684 - val_loss: 442425280.0000 - val_rmse: 21033.9082\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334045952.0000 - rmse: 18276.9238 - val_loss: 360274432.0000 - val_rmse: 18980.8965\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347227456.0000 - rmse: 18634.0391 - val_loss: 386026432.0000 - val_rmse: 19647.5547\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319869824.0000 - rmse: 17884.9043 - val_loss: 347662400.0000 - val_rmse: 18645.7070\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337217824.0000 - rmse: 18363.4922 - val_loss: 371189792.0000 - val_rmse: 19266.2871\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371430784.0000 - rmse: 19272.5391 - val_loss: 354220736.0000 - val_rmse: 18820.7520\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310658080.0000 - rmse: 17625.4922 - val_loss: 359206752.0000 - val_rmse: 18952.7500\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385899360.0000 - rmse: 19644.3223 - val_loss: 524807552.0000 - val_rmse: 22908.6758\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357143296.0000 - rmse: 18898.2344 - val_loss: 381005984.0000 - val_rmse: 19519.3750\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361983488.0000 - rmse: 19025.8633 - val_loss: 378282528.0000 - val_rmse: 19449.4863\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319560064.0000 - rmse: 17876.2422 - val_loss: 357139136.0000 - val_rmse: 18898.1230\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387694688.0000 - rmse: 19689.9648 - val_loss: 346694560.0000 - val_rmse: 18619.7344\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330696192.0000 - rmse: 18185.0527 - val_loss: 369198368.0000 - val_rmse: 19214.5352\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323575328.0000 - rmse: 17988.1973 - val_loss: 388074912.0000 - val_rmse: 19699.6172\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351430656.0000 - rmse: 18746.4844 - val_loss: 344140128.0000 - val_rmse: 18551.0137\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283466400.0000 - rmse: 16836.4590 - val_loss: 360907872.0000 - val_rmse: 18997.5742\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323749152.0000 - rmse: 17993.0293 - val_loss: 416962848.0000 - val_rmse: 20419.6660\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289328928.0000 - rmse: 17009.6699 - val_loss: 383925792.0000 - val_rmse: 19594.0215\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338240416.0000 - rmse: 18391.3125 - val_loss: 365916608.0000 - val_rmse: 19128.9473\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334024192.0000 - rmse: 18276.3281 - val_loss: 373691872.0000 - val_rmse: 19331.1113\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292426240.0000 - rmse: 17100.4746 - val_loss: 360805152.0000 - val_rmse: 18994.8691\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308090144.0000 - rmse: 17552.4961 - val_loss: 368722816.0000 - val_rmse: 19202.1562\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321161984.0000 - rmse: 17920.9922 - val_loss: 385409248.0000 - val_rmse: 19631.8418\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300101408.0000 - rmse: 17323.4336 - val_loss: 393385984.0000 - val_rmse: 19833.9590\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352943520.0000 - rmse: 18786.7910 - val_loss: 374399872.0000 - val_rmse: 19349.4160\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326738784.0000 - rmse: 18075.9160 - val_loss: 357449280.0000 - val_rmse: 18906.3281\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 397051776.0000 - rmse: 19926.1582 - val_loss: 353584064.0000 - val_rmse: 18803.8320\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360952512.0000 - rmse: 18998.7480 - val_loss: 374907552.0000 - val_rmse: 19362.5293\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247386304.0000 - rmse: 15728.5176 - val_loss: 485323072.0000 - val_rmse: 22030.0488\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293869728.0000 - rmse: 17142.6270 - val_loss: 398848320.0000 - val_rmse: 19971.1875\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381506784.0000 - rmse: 19532.1992 - val_loss: 400686944.0000 - val_rmse: 20017.1641\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295477472.0000 - rmse: 17189.4590 - val_loss: 373146912.0000 - val_rmse: 19317.0098\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254877456.0000 - rmse: 15964.8809 - val_loss: 380248768.0000 - val_rmse: 19499.9688\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330921056.0000 - rmse: 18191.2344 - val_loss: 426969632.0000 - val_rmse: 20663.2441\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360917632.0000 - rmse: 18997.8281 - val_loss: 394239520.0000 - val_rmse: 19855.4648\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336957952.0000 - rmse: 18356.4141 - val_loss: 386601568.0000 - val_rmse: 19662.1855\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292422368.0000 - rmse: 17100.3613 - val_loss: 445415840.0000 - val_rmse: 21104.8730\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338234752.0000 - rmse: 18391.1602 - val_loss: 378808160.0000 - val_rmse: 19462.9941\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357224384.0000 - rmse: 18900.3789 - val_loss: 381029696.0000 - val_rmse: 19519.9824\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378165536.0000 - rmse: 19446.4766 - val_loss: 374745888.0000 - val_rmse: 19358.3535\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299800544.0000 - rmse: 17314.7500 - val_loss: 404102080.0000 - val_rmse: 20102.2891\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326690208.0000 - rmse: 18074.5723 - val_loss: 361741120.0000 - val_rmse: 19019.4922\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351194176.0000 - rmse: 18740.1738 - val_loss: 345819712.0000 - val_rmse: 18596.2285\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299689728.0000 - rmse: 17311.5488 - val_loss: 349206528.0000 - val_rmse: 18687.0684\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341700864.0000 - rmse: 18485.1523 - val_loss: 339130208.0000 - val_rmse: 18415.4883\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344378112.0000 - rmse: 18557.4258 - val_loss: 368559008.0000 - val_rmse: 19197.8906\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341838976.0000 - rmse: 18488.8887 - val_loss: 350647552.0000 - val_rmse: 18725.5859\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276832928.0000 - rmse: 16638.2969 - val_loss: 356984160.0000 - val_rmse: 18894.0254\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308005760.0000 - rmse: 17550.0918 - val_loss: 376352416.0000 - val_rmse: 19399.8027\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410044640.0000 - rmse: 20249.5586 - val_loss: 346437568.0000 - val_rmse: 18612.8320\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319227104.0000 - rmse: 17866.9277 - val_loss: 381670176.0000 - val_rmse: 19536.3809\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333511456.0000 - rmse: 18262.2949 - val_loss: 376424256.0000 - val_rmse: 19401.6543\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344943264.0000 - rmse: 18572.6465 - val_loss: 360984640.0000 - val_rmse: 18999.5957\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352219168.0000 - rmse: 18767.5000 - val_loss: 345908768.0000 - val_rmse: 18598.6211\n",
      "104/104 [==============================] - 0s 748us/step - loss: 460861216.0000 - rmse: 21467.6758\n",
      "[460861216.0, 21467.67578125]\n",
      "Figure(640x480)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "!python train.py forest"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "20345.161274211918\n",
      "[249799.         218020.         572968.33333333  71518.66666667\n",
      "  95852.33333333  58455.33333333  56294.66666667  77270.66666667\n",
      " 149625.66666667 188795.66666667 162384.66666667  82641.66666667\n",
      "  64896.66666667 149918.66666667  32450.        ]\n",
      "[241800. 212500. 600600.  71300.  94400.  59400.  55900.  77600. 151800.\n",
      " 183500. 152900.  83000.  59200. 150500.  13100.]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "!python train.py blend"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16589, 122) (16589,)\n",
      "(13271, 122) (13271,) (3318, 122) (3318,)\n",
      "2021-10-05 23:39:41.374086: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-05 23:39:41.374120: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-05 23:39:41.374502: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-05 23:39:41.512014: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "20072.665825778517 20345.161274211918 21467.67567828786 17962.42230372351\n",
      "[248546.92599357 217564.81946124 575688.57861737  70908.38963769\n",
      "  96178.06876719  62578.58554842  57175.83963182  77451.92437592\n",
      " 153411.00219326 188081.35934194] [241800. 212500. 600600.  71300.  94400.  59400.  55900.  77600. 151800.\n",
      " 183500.]\n",
      "[252556.95122858 223455.02320414 584585.93914122  72729.48759438\n",
      "  97058.1172448   66097.56250515  59585.13080397  77229.43281558\n",
      " 154897.7390817  193013.94259812]\n",
      "[249799.         218020.         572968.33333333  71518.66666667\n",
      "  95852.33333333  58455.33333333  56294.66666667  77270.66666667\n",
      " 149625.66666667 188795.66666667]\n",
      "[244600.34  212805.78  571055.75   69084.86   95762.336  63031.79\n",
      "  56029.75   77754.734 155134.95  183846.19 ]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('media': conda)"
  },
  "interpreter": {
   "hash": "523abd0b41b10f09f9f2dcd44e6a4b66c0ed2d8233204c8b18bb9ef2c73e8b61"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}