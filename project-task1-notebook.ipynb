{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import train_and_get_deep_learning_model, predict, check_result, train_and_get_tree_model, train_and_get_forest, train_and_get_gbr, k_fold, get_blend_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and get the Baseline Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16589, 123) (16589,)\n",
      "(13271, 123) (13271,) (3318, 123) (3318,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_55 (Dense)             (None, 128)               15872     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,873\n",
      "Trainable params: 20,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 24469979136.0000 - rmse: 156428.8281 - val_loss: 20359634944.0000 - val_rmse: 142687.1875\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 8854075392.0000 - rmse: 94096.0938 - val_loss: 2508493056.0000 - val_rmse: 50084.8594\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2697178368.0000 - rmse: 51934.3672 - val_loss: 1709190144.0000 - val_rmse: 41342.3516\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2225362432.0000 - rmse: 47173.7461 - val_loss: 1325283072.0000 - val_rmse: 36404.4375\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1975264128.0000 - rmse: 44443.9453 - val_loss: 1240083456.0000 - val_rmse: 35214.8203\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1869430016.0000 - rmse: 43236.9062 - val_loss: 1250622080.0000 - val_rmse: 35364.1367\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1796811008.0000 - rmse: 42388.8086 - val_loss: 1052386688.0000 - val_rmse: 32440.5098\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1700659328.0000 - rmse: 41239.0508 - val_loss: 1047033280.0000 - val_rmse: 32357.8945\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1594627968.0000 - rmse: 39932.7930 - val_loss: 965699200.0000 - val_rmse: 31075.7012\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1612577664.0000 - rmse: 40156.9141 - val_loss: 958459456.0000 - val_rmse: 30958.9961\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1519734656.0000 - rmse: 38983.7734 - val_loss: 909144512.0000 - val_rmse: 30152.0234\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1486518656.0000 - rmse: 38555.3984 - val_loss: 883522112.0000 - val_rmse: 29724.0996\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1451366912.0000 - rmse: 38096.8086 - val_loss: 875483968.0000 - val_rmse: 29588.5781\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1474352640.0000 - rmse: 38397.3008 - val_loss: 853327680.0000 - val_rmse: 29211.7734\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1387496064.0000 - rmse: 37249.1094 - val_loss: 849450112.0000 - val_rmse: 29145.3281\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1392314368.0000 - rmse: 37313.7305 - val_loss: 814131904.0000 - val_rmse: 28532.9961\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1358761728.0000 - rmse: 36861.3867 - val_loss: 804378368.0000 - val_rmse: 28361.5645\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1308999168.0000 - rmse: 36180.0938 - val_loss: 824438400.0000 - val_rmse: 28713.0352\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1340467712.0000 - rmse: 36612.3984 - val_loss: 776562496.0000 - val_rmse: 27866.8711\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1365084544.0000 - rmse: 36947.0508 - val_loss: 771498240.0000 - val_rmse: 27775.8574\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1287093504.0000 - rmse: 35876.0859 - val_loss: 759977088.0000 - val_rmse: 27567.6816\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1247999232.0000 - rmse: 35327.0312 - val_loss: 748624640.0000 - val_rmse: 27361.0059\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1232544256.0000 - rmse: 35107.6094 - val_loss: 753850240.0000 - val_rmse: 27456.3340\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1263726336.0000 - rmse: 35548.9297 - val_loss: 733991872.0000 - val_rmse: 27092.2852\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1190295168.0000 - rmse: 34500.6562 - val_loss: 751740800.0000 - val_rmse: 27417.8926\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1210317696.0000 - rmse: 34789.6211 - val_loss: 732392512.0000 - val_rmse: 27062.7520\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1240105728.0000 - rmse: 35215.1367 - val_loss: 714932672.0000 - val_rmse: 26738.2246\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1228164352.0000 - rmse: 35045.1758 - val_loss: 763492224.0000 - val_rmse: 27631.3633\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1215288320.0000 - rmse: 34860.9844 - val_loss: 730544320.0000 - val_rmse: 27028.5840\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1184817792.0000 - rmse: 34421.1836 - val_loss: 722158976.0000 - val_rmse: 26873.0137\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1142458624.0000 - rmse: 33800.2773 - val_loss: 704376000.0000 - val_rmse: 26540.0820\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1178083584.0000 - rmse: 34323.2227 - val_loss: 698870016.0000 - val_rmse: 26436.1504\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1156394880.0000 - rmse: 34005.8047 - val_loss: 697889600.0000 - val_rmse: 26417.5996\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1146732672.0000 - rmse: 33863.4414 - val_loss: 696984576.0000 - val_rmse: 26400.4648\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1153096576.0000 - rmse: 33957.2773 - val_loss: 733918592.0000 - val_rmse: 27090.9316\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1136099712.0000 - rmse: 33706.0781 - val_loss: 689681728.0000 - val_rmse: 26261.7930\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1120606720.0000 - rmse: 33475.4648 - val_loss: 701827200.0000 - val_rmse: 26492.0215\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1140522496.0000 - rmse: 33771.6211 - val_loss: 708838784.0000 - val_rmse: 26624.0273\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1104561664.0000 - rmse: 33234.9453 - val_loss: 703952000.0000 - val_rmse: 26532.0938\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1106475904.0000 - rmse: 33263.7344 - val_loss: 695630080.0000 - val_rmse: 26374.8008\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1132071936.0000 - rmse: 33646.2773 - val_loss: 681831296.0000 - val_rmse: 26111.9004\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1061706496.0000 - rmse: 32583.8379 - val_loss: 707453248.0000 - val_rmse: 26597.9941\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1163978880.0000 - rmse: 34117.1328 - val_loss: 749620736.0000 - val_rmse: 27379.2031\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1086767744.0000 - rmse: 32966.1602 - val_loss: 677027392.0000 - val_rmse: 26019.7500\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1068437440.0000 - rmse: 32686.9609 - val_loss: 731676992.0000 - val_rmse: 27049.5293\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1101274112.0000 - rmse: 33185.4492 - val_loss: 697104896.0000 - val_rmse: 26402.7441\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1079514496.0000 - rmse: 32855.9648 - val_loss: 668187264.0000 - val_rmse: 25849.3184\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1054307200.0000 - rmse: 32470.0977 - val_loss: 701936896.0000 - val_rmse: 26494.0918\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1019128832.0000 - rmse: 31923.7969 - val_loss: 676407616.0000 - val_rmse: 26007.8359\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1050060096.0000 - rmse: 32404.6309 - val_loss: 662056448.0000 - val_rmse: 25730.4570\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1079118080.0000 - rmse: 32849.9336 - val_loss: 727675968.0000 - val_rmse: 26975.4707\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1027075648.0000 - rmse: 32048.0215 - val_loss: 658685248.0000 - val_rmse: 25664.8633\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1036737216.0000 - rmse: 32198.4043 - val_loss: 646557824.0000 - val_rmse: 25427.5020\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1017005632.0000 - rmse: 31890.5254 - val_loss: 662860608.0000 - val_rmse: 25746.0801\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1078964096.0000 - rmse: 32847.5898 - val_loss: 643005248.0000 - val_rmse: 25357.5488\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1007558080.0000 - rmse: 31742.0547 - val_loss: 637981056.0000 - val_rmse: 25258.2871\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 985630016.0000 - rmse: 31394.7441 - val_loss: 646336000.0000 - val_rmse: 25423.1387\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1015307200.0000 - rmse: 31863.8848 - val_loss: 631655424.0000 - val_rmse: 25132.7559\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 976566080.0000 - rmse: 31250.0566 - val_loss: 635706560.0000 - val_rmse: 25213.2227\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 977722944.0000 - rmse: 31268.5586 - val_loss: 666060416.0000 - val_rmse: 25808.1465\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 970820800.0000 - rmse: 31157.9980 - val_loss: 627948032.0000 - val_rmse: 25058.8906\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 989673216.0000 - rmse: 31459.0723 - val_loss: 635946112.0000 - val_rmse: 25217.9727\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 973190272.0000 - rmse: 31195.9980 - val_loss: 623207552.0000 - val_rmse: 24964.1250\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 972277760.0000 - rmse: 31181.3691 - val_loss: 629391168.0000 - val_rmse: 25087.6699\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 978190528.0000 - rmse: 31276.0371 - val_loss: 617557888.0000 - val_rmse: 24850.7129\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 930748608.0000 - rmse: 30508.1719 - val_loss: 621415360.0000 - val_rmse: 24928.2031\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 953726272.0000 - rmse: 30882.4590 - val_loss: 622959232.0000 - val_rmse: 24959.1504\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 907409856.0000 - rmse: 30123.2441 - val_loss: 644808896.0000 - val_rmse: 25393.0879\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 956450240.0000 - rmse: 30926.5293 - val_loss: 624297792.0000 - val_rmse: 24985.9512\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 921464576.0000 - rmse: 30355.6348 - val_loss: 631339008.0000 - val_rmse: 25126.4609\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 974869504.0000 - rmse: 31222.9004 - val_loss: 659319104.0000 - val_rmse: 25677.2090\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 931712640.0000 - rmse: 30523.9688 - val_loss: 607668480.0000 - val_rmse: 24650.9336\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 939953280.0000 - rmse: 30658.6582 - val_loss: 626796544.0000 - val_rmse: 25035.9043\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 898758464.0000 - rmse: 29979.3008 - val_loss: 617175360.0000 - val_rmse: 24843.0137\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 868981312.0000 - rmse: 29478.4883 - val_loss: 590325056.0000 - val_rmse: 24296.6055\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 944209408.0000 - rmse: 30727.9883 - val_loss: 611168512.0000 - val_rmse: 24721.8223\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 885673536.0000 - rmse: 29760.2676 - val_loss: 615771200.0000 - val_rmse: 24814.7383\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 875467712.0000 - rmse: 29588.3027 - val_loss: 615052096.0000 - val_rmse: 24800.2422\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 937957376.0000 - rmse: 30626.0898 - val_loss: 579859264.0000 - val_rmse: 24080.2676\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 866132096.0000 - rmse: 29430.1230 - val_loss: 573053376.0000 - val_rmse: 23938.5332\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1009038464.0000 - rmse: 31765.3652 - val_loss: 616592064.0000 - val_rmse: 24831.2715\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 876453568.0000 - rmse: 29604.9590 - val_loss: 574292288.0000 - val_rmse: 23964.3965\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 892365632.0000 - rmse: 29872.4902 - val_loss: 613565376.0000 - val_rmse: 24770.2500\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 814595008.0000 - rmse: 28541.1113 - val_loss: 581251520.0000 - val_rmse: 24109.1582\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 863735360.0000 - rmse: 29389.3750 - val_loss: 562190400.0000 - val_rmse: 23710.5547\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 838278336.0000 - rmse: 28953.0371 - val_loss: 565007744.0000 - val_rmse: 23769.8906\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 858772864.0000 - rmse: 29304.8262 - val_loss: 562486336.0000 - val_rmse: 23716.7930\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 847954944.0000 - rmse: 29119.6660 - val_loss: 592155712.0000 - val_rmse: 24334.2500\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 892707776.0000 - rmse: 29878.2148 - val_loss: 578766784.0000 - val_rmse: 24057.5703\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 888893760.0000 - rmse: 29814.3223 - val_loss: 582846848.0000 - val_rmse: 24142.2207\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 837572672.0000 - rmse: 28940.8477 - val_loss: 556193280.0000 - val_rmse: 23583.7500\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 874428800.0000 - rmse: 29570.7422 - val_loss: 544107840.0000 - val_rmse: 23326.1191\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 826116288.0000 - rmse: 28742.2383 - val_loss: 542516480.0000 - val_rmse: 23291.9824\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 805555904.0000 - rmse: 28382.3164 - val_loss: 527548960.0000 - val_rmse: 22968.4336\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782579776.0000 - rmse: 27974.6270 - val_loss: 553275776.0000 - val_rmse: 23521.8145\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 845573056.0000 - rmse: 29078.7363 - val_loss: 544942848.0000 - val_rmse: 23344.0117\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 816978496.0000 - rmse: 28582.8359 - val_loss: 537177536.0000 - val_rmse: 23177.0898\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 833449344.0000 - rmse: 28869.5234 - val_loss: 523972800.0000 - val_rmse: 22890.4531\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 854669056.0000 - rmse: 29234.7227 - val_loss: 534891008.0000 - val_rmse: 23127.7090\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 827788864.0000 - rmse: 28771.3203 - val_loss: 508772128.0000 - val_rmse: 22555.9766\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 830022400.0000 - rmse: 28810.1094 - val_loss: 513554400.0000 - val_rmse: 22661.7363\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 814697856.0000 - rmse: 28542.9121 - val_loss: 518730336.0000 - val_rmse: 22775.6523\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 803762816.0000 - rmse: 28350.7109 - val_loss: 545238208.0000 - val_rmse: 23350.3359\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 778685248.0000 - rmse: 27904.9316 - val_loss: 527790816.0000 - val_rmse: 22973.6992\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661325184.0000 - rmse: 25716.2441 - val_loss: 504221088.0000 - val_rmse: 22454.8672\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729237120.0000 - rmse: 27004.3906 - val_loss: 496419680.0000 - val_rmse: 22280.4766\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 813252736.0000 - rmse: 28517.5859 - val_loss: 488782432.0000 - val_rmse: 22108.4238\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 854399104.0000 - rmse: 29230.1055 - val_loss: 505208992.0000 - val_rmse: 22476.8555\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751739264.0000 - rmse: 27417.8633 - val_loss: 494297152.0000 - val_rmse: 22232.7949\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 749163136.0000 - rmse: 27370.8438 - val_loss: 479517952.0000 - val_rmse: 21897.8984\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706776704.0000 - rmse: 26585.2715 - val_loss: 483919616.0000 - val_rmse: 21998.1738\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 719569856.0000 - rmse: 26824.7988 - val_loss: 495458784.0000 - val_rmse: 22258.9043\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 765265152.0000 - rmse: 27663.4258 - val_loss: 471811872.0000 - val_rmse: 21721.2305\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 725005376.0000 - rmse: 26925.9238 - val_loss: 485438208.0000 - val_rmse: 22032.6621\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 695524736.0000 - rmse: 26372.8027 - val_loss: 546088192.0000 - val_rmse: 23368.5293\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665289600.0000 - rmse: 25793.2090 - val_loss: 538648128.0000 - val_rmse: 23208.7930\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 787109632.0000 - rmse: 28055.4727 - val_loss: 499601632.0000 - val_rmse: 22351.7695\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 723933888.0000 - rmse: 26906.0176 - val_loss: 461284192.0000 - val_rmse: 21477.5273\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 758308480.0000 - rmse: 27537.4023 - val_loss: 454859936.0000 - val_rmse: 21327.4453\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 694486016.0000 - rmse: 26353.1016 - val_loss: 446132704.0000 - val_rmse: 21121.8535\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 705400448.0000 - rmse: 26559.3750 - val_loss: 478382784.0000 - val_rmse: 21871.9629\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668428608.0000 - rmse: 25853.9863 - val_loss: 470024384.0000 - val_rmse: 21680.0449\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 671035776.0000 - rmse: 25904.3574 - val_loss: 445489632.0000 - val_rmse: 21106.6250\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 646092352.0000 - rmse: 25418.3457 - val_loss: 469203456.0000 - val_rmse: 21661.1035\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 633334016.0000 - rmse: 25166.1289 - val_loss: 465134848.0000 - val_rmse: 21566.9844\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637771648.0000 - rmse: 25254.1406 - val_loss: 441931936.0000 - val_rmse: 21022.1758\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637813376.0000 - rmse: 25254.9668 - val_loss: 429761984.0000 - val_rmse: 20730.7012\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 668120832.0000 - rmse: 25848.0332 - val_loss: 417564736.0000 - val_rmse: 20434.4004\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 665090752.0000 - rmse: 25789.3535 - val_loss: 420877280.0000 - val_rmse: 20515.2930\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569382528.0000 - rmse: 23861.7383 - val_loss: 469950944.0000 - val_rmse: 21678.3516\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 694046528.0000 - rmse: 26344.7637 - val_loss: 422979264.0000 - val_rmse: 20566.4590\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 639844800.0000 - rmse: 25295.1543 - val_loss: 416393888.0000 - val_rmse: 20405.7305\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584011712.0000 - rmse: 24166.3340 - val_loss: 442613664.0000 - val_rmse: 21038.3848\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630635776.0000 - rmse: 25112.4629 - val_loss: 423415744.0000 - val_rmse: 20577.0684\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 677869568.0000 - rmse: 26035.9277 - val_loss: 428611968.0000 - val_rmse: 20702.9434\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 536083232.0000 - rmse: 23153.4707 - val_loss: 427654400.0000 - val_rmse: 20679.8066\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 580161536.0000 - rmse: 24086.5430 - val_loss: 421869088.0000 - val_rmse: 20539.4512\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 581540352.0000 - rmse: 24115.1484 - val_loss: 444665472.0000 - val_rmse: 21087.0918\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629365824.0000 - rmse: 25087.1641 - val_loss: 417121952.0000 - val_rmse: 20423.5645\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684858688.0000 - rmse: 26169.8047 - val_loss: 416621856.0000 - val_rmse: 20411.3145\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565926080.0000 - rmse: 23789.2012 - val_loss: 419642880.0000 - val_rmse: 20485.1875\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560226432.0000 - rmse: 23669.1035 - val_loss: 442634848.0000 - val_rmse: 21038.8887\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531724096.0000 - rmse: 23059.1426 - val_loss: 395084960.0000 - val_rmse: 19876.7441\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604879040.0000 - rmse: 24594.2891 - val_loss: 462444192.0000 - val_rmse: 21504.5156\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 531843744.0000 - rmse: 23061.7363 - val_loss: 457351168.0000 - val_rmse: 21385.7695\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 553761344.0000 - rmse: 23532.1348 - val_loss: 395343552.0000 - val_rmse: 19883.2461\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530003104.0000 - rmse: 23021.7969 - val_loss: 566435648.0000 - val_rmse: 23799.9082\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533624000.0000 - rmse: 23100.3008 - val_loss: 468184288.0000 - val_rmse: 21637.5664\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 563153216.0000 - rmse: 23730.8496 - val_loss: 408232000.0000 - val_rmse: 20204.7520\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601042176.0000 - rmse: 24516.1602 - val_loss: 385356736.0000 - val_rmse: 19630.5059\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490075328.0000 - rmse: 22137.6445 - val_loss: 397307616.0000 - val_rmse: 19932.5762\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515614432.0000 - rmse: 22707.1445 - val_loss: 414205632.0000 - val_rmse: 20352.0430\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 546663040.0000 - rmse: 23380.8262 - val_loss: 426966880.0000 - val_rmse: 20663.1777\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525035136.0000 - rmse: 22913.6445 - val_loss: 378512832.0000 - val_rmse: 19455.4043\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506408928.0000 - rmse: 22503.5312 - val_loss: 370488544.0000 - val_rmse: 19248.0781\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 581365184.0000 - rmse: 24111.5156 - val_loss: 356316800.0000 - val_rmse: 18876.3555\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485948064.0000 - rmse: 22044.2285 - val_loss: 365648768.0000 - val_rmse: 19121.9434\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529345280.0000 - rmse: 23007.5039 - val_loss: 363297568.0000 - val_rmse: 19060.3672\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519583104.0000 - rmse: 22794.3652 - val_loss: 366856032.0000 - val_rmse: 19153.4844\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503651456.0000 - rmse: 22442.1797 - val_loss: 446126784.0000 - val_rmse: 21121.7129\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487844448.0000 - rmse: 22087.2012 - val_loss: 396152736.0000 - val_rmse: 19903.5859\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452393920.0000 - rmse: 21269.5547 - val_loss: 379156544.0000 - val_rmse: 19471.9414\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499620672.0000 - rmse: 22352.1953 - val_loss: 367149376.0000 - val_rmse: 19161.1387\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441662464.0000 - rmse: 21015.7656 - val_loss: 362736768.0000 - val_rmse: 19045.6484\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520714272.0000 - rmse: 22819.1641 - val_loss: 386096736.0000 - val_rmse: 19649.3438\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471967808.0000 - rmse: 21724.8184 - val_loss: 355007296.0000 - val_rmse: 18841.6348\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489725824.0000 - rmse: 22129.7500 - val_loss: 370687552.0000 - val_rmse: 19253.2461\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 504033728.0000 - rmse: 22450.6934 - val_loss: 356341472.0000 - val_rmse: 18877.0078\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446834272.0000 - rmse: 21138.4551 - val_loss: 416632320.0000 - val_rmse: 20411.5723\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475831264.0000 - rmse: 21813.5547 - val_loss: 373797440.0000 - val_rmse: 19333.8398\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488080512.0000 - rmse: 22092.5449 - val_loss: 407434720.0000 - val_rmse: 20185.0117\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444371872.0000 - rmse: 21080.1289 - val_loss: 374505920.0000 - val_rmse: 19352.1543\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436157920.0000 - rmse: 20884.3945 - val_loss: 384119296.0000 - val_rmse: 19598.9609\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470912512.0000 - rmse: 21700.5176 - val_loss: 460697696.0000 - val_rmse: 21463.8691\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 459212576.0000 - rmse: 21429.2461 - val_loss: 378198912.0000 - val_rmse: 19447.3359\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452753120.0000 - rmse: 21277.9961 - val_loss: 466821536.0000 - val_rmse: 21606.0527\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463113024.0000 - rmse: 21520.0605 - val_loss: 367432384.0000 - val_rmse: 19168.5234\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405904000.0000 - rmse: 20147.0586 - val_loss: 366780960.0000 - val_rmse: 19151.5254\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426090336.0000 - rmse: 20641.9551 - val_loss: 376870112.0000 - val_rmse: 19413.1426\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390020192.0000 - rmse: 19748.9297 - val_loss: 368641632.0000 - val_rmse: 19200.0410\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424216896.0000 - rmse: 20596.5254 - val_loss: 482092480.0000 - val_rmse: 21956.6035\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416716672.0000 - rmse: 20413.6387 - val_loss: 375254208.0000 - val_rmse: 19371.4785\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432146848.0000 - rmse: 20788.1426 - val_loss: 393118848.0000 - val_rmse: 19827.2227\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418273504.0000 - rmse: 20451.7363 - val_loss: 379127680.0000 - val_rmse: 19471.2012\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 453744512.0000 - rmse: 21301.2773 - val_loss: 392717568.0000 - val_rmse: 19817.1035\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375513440.0000 - rmse: 19378.1680 - val_loss: 378911808.0000 - val_rmse: 19465.6562\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444786272.0000 - rmse: 21089.9570 - val_loss: 388650816.0000 - val_rmse: 19714.2285\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421052896.0000 - rmse: 20519.5703 - val_loss: 379286656.0000 - val_rmse: 19475.2812\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419407840.0000 - rmse: 20479.4492 - val_loss: 358053056.0000 - val_rmse: 18922.2891\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391488544.0000 - rmse: 19786.0684 - val_loss: 377103744.0000 - val_rmse: 19419.1582\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387730560.0000 - rmse: 19690.8730 - val_loss: 407639456.0000 - val_rmse: 20190.0840\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360728352.0000 - rmse: 18992.8496 - val_loss: 401439808.0000 - val_rmse: 20035.9609\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392507104.0000 - rmse: 19811.7910 - val_loss: 378288288.0000 - val_rmse: 19449.6328\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365659200.0000 - rmse: 19122.2148 - val_loss: 369439456.0000 - val_rmse: 19220.8086\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364867584.0000 - rmse: 19101.5059 - val_loss: 384292032.0000 - val_rmse: 19603.3652\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 429991584.0000 - rmse: 20736.2383 - val_loss: 393829216.0000 - val_rmse: 19845.1289\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381150016.0000 - rmse: 19523.0625 - val_loss: 376834112.0000 - val_rmse: 19412.2129\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365562208.0000 - rmse: 19119.6816 - val_loss: 424489792.0000 - val_rmse: 20603.1484\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374308480.0000 - rmse: 19347.0527 - val_loss: 378664352.0000 - val_rmse: 19459.3008\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324818624.0000 - rmse: 18022.7246 - val_loss: 443943072.0000 - val_rmse: 21069.9570\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416305856.0000 - rmse: 20403.5742 - val_loss: 403860576.0000 - val_rmse: 20096.2812\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376205952.0000 - rmse: 19396.0293 - val_loss: 390700576.0000 - val_rmse: 19766.1465\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401327680.0000 - rmse: 20033.1621 - val_loss: 429712064.0000 - val_rmse: 20729.4961\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400212096.0000 - rmse: 20005.3008 - val_loss: 386386144.0000 - val_rmse: 19656.7070\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339716160.0000 - rmse: 18431.3887 - val_loss: 391938784.0000 - val_rmse: 19797.4434\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416602016.0000 - rmse: 20410.8301 - val_loss: 350761696.0000 - val_rmse: 18728.6328\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385947456.0000 - rmse: 19645.5449 - val_loss: 386622112.0000 - val_rmse: 19662.7070\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353053600.0000 - rmse: 18789.7207 - val_loss: 361468928.0000 - val_rmse: 19012.3359\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326177376.0000 - rmse: 18060.3809 - val_loss: 376859904.0000 - val_rmse: 19412.8789\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380690016.0000 - rmse: 19511.2793 - val_loss: 392399840.0000 - val_rmse: 19809.0840\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377546688.0000 - rmse: 19430.5586 - val_loss: 346528224.0000 - val_rmse: 18615.2695\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348506880.0000 - rmse: 18668.3398 - val_loss: 410720608.0000 - val_rmse: 20266.2422\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331026080.0000 - rmse: 18194.1230 - val_loss: 384011648.0000 - val_rmse: 19596.2129\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356318208.0000 - rmse: 18876.3906 - val_loss: 353470784.0000 - val_rmse: 18800.8164\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389670528.0000 - rmse: 19740.0742 - val_loss: 370694496.0000 - val_rmse: 19253.4277\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364443712.0000 - rmse: 19090.4082 - val_loss: 438117920.0000 - val_rmse: 20931.2637\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405971200.0000 - rmse: 20148.7246 - val_loss: 364389248.0000 - val_rmse: 19088.9805\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 371660224.0000 - rmse: 19278.4922 - val_loss: 390847904.0000 - val_rmse: 19769.8730\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372633760.0000 - rmse: 19303.7227 - val_loss: 395334016.0000 - val_rmse: 19883.0078\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341151712.0000 - rmse: 18470.2910 - val_loss: 389183680.0000 - val_rmse: 19727.7363\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324740864.0000 - rmse: 18020.5684 - val_loss: 353026624.0000 - val_rmse: 18789.0020\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373098144.0000 - rmse: 19315.7480 - val_loss: 370782016.0000 - val_rmse: 19255.7012\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343817600.0000 - rmse: 18542.3184 - val_loss: 376119072.0000 - val_rmse: 19393.7891\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318104896.0000 - rmse: 17835.4941 - val_loss: 430666048.0000 - val_rmse: 20752.4941\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332760640.0000 - rmse: 18241.7266 - val_loss: 351639328.0000 - val_rmse: 18752.0469\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341492544.0000 - rmse: 18479.5156 - val_loss: 379551808.0000 - val_rmse: 19482.0898\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299817600.0000 - rmse: 17315.2422 - val_loss: 407358080.0000 - val_rmse: 20183.1133\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319128576.0000 - rmse: 17864.1699 - val_loss: 414375072.0000 - val_rmse: 20356.2031\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324696224.0000 - rmse: 18019.3301 - val_loss: 423253696.0000 - val_rmse: 20573.1289\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327352448.0000 - rmse: 18092.8848 - val_loss: 430302816.0000 - val_rmse: 20743.7422\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339189664.0000 - rmse: 18417.1016 - val_loss: 358737728.0000 - val_rmse: 18940.3711\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332220576.0000 - rmse: 18226.9180 - val_loss: 403782464.0000 - val_rmse: 20094.3379\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285379168.0000 - rmse: 16893.1680 - val_loss: 447635552.0000 - val_rmse: 21157.3984\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320586720.0000 - rmse: 17904.9355 - val_loss: 392957760.0000 - val_rmse: 19823.1621\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329296512.0000 - rmse: 18146.5293 - val_loss: 437726176.0000 - val_rmse: 20921.9062\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293042720.0000 - rmse: 17118.4902 - val_loss: 352531616.0000 - val_rmse: 18775.8242\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303284672.0000 - rmse: 17415.0703 - val_loss: 369196672.0000 - val_rmse: 19214.4902\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327597056.0000 - rmse: 18099.6406 - val_loss: 414913568.0000 - val_rmse: 20369.4277\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330545856.0000 - rmse: 18180.9180 - val_loss: 373698144.0000 - val_rmse: 19331.2734\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279138784.0000 - rmse: 16707.4453 - val_loss: 362282400.0000 - val_rmse: 19033.7168\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341203648.0000 - rmse: 18471.6973 - val_loss: 392770560.0000 - val_rmse: 19818.4375\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358659968.0000 - rmse: 18938.3184 - val_loss: 401975808.0000 - val_rmse: 20049.3320\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266915856.0000 - rmse: 16337.5586 - val_loss: 378305792.0000 - val_rmse: 19450.0820\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282591456.0000 - rmse: 16810.4551 - val_loss: 360098400.0000 - val_rmse: 18976.2578\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301022336.0000 - rmse: 17349.9922 - val_loss: 346113696.0000 - val_rmse: 18604.1309\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377518592.0000 - rmse: 19429.8359 - val_loss: 348760480.0000 - val_rmse: 18675.1289\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305823648.0000 - rmse: 17487.8145 - val_loss: 359148544.0000 - val_rmse: 18951.2148\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290971456.0000 - rmse: 17057.8828 - val_loss: 411233920.0000 - val_rmse: 20278.9023\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300660800.0000 - rmse: 17339.5723 - val_loss: 397935552.0000 - val_rmse: 19948.3223\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253610688.0000 - rmse: 15925.1572 - val_loss: 375799840.0000 - val_rmse: 19385.5566\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347021728.0000 - rmse: 18628.5195 - val_loss: 377714592.0000 - val_rmse: 19434.8809\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320408096.0000 - rmse: 17899.9453 - val_loss: 379891296.0000 - val_rmse: 19490.7988\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289076960.0000 - rmse: 17002.2637 - val_loss: 339414592.0000 - val_rmse: 18423.2070\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304952000.0000 - rmse: 17462.8730 - val_loss: 372113664.0000 - val_rmse: 19290.2480\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 282381664.0000 - rmse: 16804.2129 - val_loss: 338639264.0000 - val_rmse: 18402.1523\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273587104.0000 - rmse: 16540.4668 - val_loss: 343753504.0000 - val_rmse: 18540.5898\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334798912.0000 - rmse: 18297.5098 - val_loss: 355495040.0000 - val_rmse: 18854.5742\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244069648.0000 - rmse: 15622.7256 - val_loss: 359592064.0000 - val_rmse: 18962.9121\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274076256.0000 - rmse: 16555.2480 - val_loss: 338343296.0000 - val_rmse: 18394.1113\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280175072.0000 - rmse: 16738.4297 - val_loss: 413306240.0000 - val_rmse: 20329.9336\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261052464.0000 - rmse: 16157.1172 - val_loss: 423309504.0000 - val_rmse: 20574.4844\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241163760.0000 - rmse: 15529.4473 - val_loss: 451324832.0000 - val_rmse: 21244.4062\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262059792.0000 - rmse: 16188.2607 - val_loss: 403632864.0000 - val_rmse: 20090.6172\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241831072.0000 - rmse: 15550.9180 - val_loss: 473597504.0000 - val_rmse: 21762.2949\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293602048.0000 - rmse: 17134.8203 - val_loss: 380850112.0000 - val_rmse: 19515.3789\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313734720.0000 - rmse: 17712.5566 - val_loss: 341950432.0000 - val_rmse: 18491.9004\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302382016.0000 - rmse: 17389.1328 - val_loss: 407003232.0000 - val_rmse: 20174.3203\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297781184.0000 - rmse: 17256.3359 - val_loss: 446860448.0000 - val_rmse: 21139.0742\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291565696.0000 - rmse: 17075.2949 - val_loss: 359004928.0000 - val_rmse: 18947.4238\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311066464.0000 - rmse: 17637.0762 - val_loss: 348696672.0000 - val_rmse: 18673.4219\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250874768.0000 - rmse: 15839.0264 - val_loss: 412709696.0000 - val_rmse: 20315.2578\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310406016.0000 - rmse: 17618.3398 - val_loss: 432777344.0000 - val_rmse: 20803.3008\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271020128.0000 - rmse: 16462.6875 - val_loss: 381721184.0000 - val_rmse: 19537.6855\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340860288.0000 - rmse: 18462.4004 - val_loss: 368243776.0000 - val_rmse: 19189.6797\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299087680.0000 - rmse: 17294.1504 - val_loss: 359420768.0000 - val_rmse: 18958.3945\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266275984.0000 - rmse: 16317.9639 - val_loss: 398522016.0000 - val_rmse: 19963.0156\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305114912.0000 - rmse: 17467.5371 - val_loss: 447668352.0000 - val_rmse: 21158.1738\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368551712.0000 - rmse: 19197.6992 - val_loss: 390379424.0000 - val_rmse: 19758.0195\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287153120.0000 - rmse: 16945.5918 - val_loss: 384921632.0000 - val_rmse: 19619.4199\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309396064.0000 - rmse: 17589.6582 - val_loss: 385089760.0000 - val_rmse: 19623.7031\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295734208.0000 - rmse: 17196.9238 - val_loss: 368964288.0000 - val_rmse: 19208.4414\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240529040.0000 - rmse: 15508.9971 - val_loss: 390228832.0000 - val_rmse: 19754.2090\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297031680.0000 - rmse: 17234.6055 - val_loss: 478751264.0000 - val_rmse: 21880.3848\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276133728.0000 - rmse: 16617.2695 - val_loss: 439282592.0000 - val_rmse: 20959.0684\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315096384.0000 - rmse: 17750.9512 - val_loss: 432165376.0000 - val_rmse: 20788.5879\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252336048.0000 - rmse: 15885.0879 - val_loss: 425536128.0000 - val_rmse: 20628.5273\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265735888.0000 - rmse: 16301.4062 - val_loss: 378757984.0000 - val_rmse: 19461.7031\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250519728.0000 - rmse: 15827.8135 - val_loss: 422090656.0000 - val_rmse: 20544.8438\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274081344.0000 - rmse: 16555.4004 - val_loss: 440426592.0000 - val_rmse: 20986.3438\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246477984.0000 - rmse: 15699.6162 - val_loss: 421872736.0000 - val_rmse: 20539.5391\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227999200.0000 - rmse: 15099.6406 - val_loss: 460459296.0000 - val_rmse: 21458.3145\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312415360.0000 - rmse: 17675.2754 - val_loss: 405223616.0000 - val_rmse: 20130.1660\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252929280.0000 - rmse: 15903.7490 - val_loss: 441398912.0000 - val_rmse: 21009.4941\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242172784.0000 - rmse: 15561.9004 - val_loss: 416761920.0000 - val_rmse: 20414.7480\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263051696.0000 - rmse: 16218.8682 - val_loss: 402115072.0000 - val_rmse: 20052.8066\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210081280.0000 - rmse: 14494.1797 - val_loss: 385576448.0000 - val_rmse: 19636.0996\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216861200.0000 - rmse: 14726.2070 - val_loss: 452260416.0000 - val_rmse: 21266.4141\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252114544.0000 - rmse: 15878.1152 - val_loss: 448843200.0000 - val_rmse: 21185.9199\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218221280.0000 - rmse: 14772.3135 - val_loss: 416777408.0000 - val_rmse: 20415.1270\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257801264.0000 - rmse: 16056.1895 - val_loss: 447348192.0000 - val_rmse: 21150.6055\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221225824.0000 - rmse: 14873.6602 - val_loss: 508915168.0000 - val_rmse: 22559.1465\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284945376.0000 - rmse: 16880.3242 - val_loss: 403123968.0000 - val_rmse: 20077.9473\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263969776.0000 - rmse: 16247.1455 - val_loss: 415867104.0000 - val_rmse: 20392.8184\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221254160.0000 - rmse: 14874.6133 - val_loss: 424993824.0000 - val_rmse: 20615.3770\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242490912.0000 - rmse: 15572.1172 - val_loss: 408421344.0000 - val_rmse: 20209.4355\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238843280.0000 - rmse: 15454.5537 - val_loss: 423857120.0000 - val_rmse: 20587.7910\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265146336.0000 - rmse: 16283.3135 - val_loss: 459673248.0000 - val_rmse: 21439.9922\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250295760.0000 - rmse: 15820.7363 - val_loss: 411836256.0000 - val_rmse: 20293.7500\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269649888.0000 - rmse: 16421.0195 - val_loss: 431019840.0000 - val_rmse: 20761.0156\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267440224.0000 - rmse: 16353.5977 - val_loss: 374814912.0000 - val_rmse: 19360.1367\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220612224.0000 - rmse: 14853.0195 - val_loss: 373560608.0000 - val_rmse: 19327.7148\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312630784.0000 - rmse: 17681.3672 - val_loss: 387033344.0000 - val_rmse: 19673.1621\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279023648.0000 - rmse: 16704.0000 - val_loss: 412654688.0000 - val_rmse: 20313.9023\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253007360.0000 - rmse: 15906.2041 - val_loss: 392272576.0000 - val_rmse: 19805.8711\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222750352.0000 - rmse: 14924.8213 - val_loss: 403253856.0000 - val_rmse: 20081.1816\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235470288.0000 - rmse: 15345.0410 - val_loss: 436025088.0000 - val_rmse: 20881.2129\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303494976.0000 - rmse: 17421.1055 - val_loss: 395015008.0000 - val_rmse: 19874.9844\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254430880.0000 - rmse: 15950.8877 - val_loss: 433594016.0000 - val_rmse: 20822.9199\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276615360.0000 - rmse: 16631.7559 - val_loss: 445240736.0000 - val_rmse: 21100.7266\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251022384.0000 - rmse: 15843.6836 - val_loss: 486874880.0000 - val_rmse: 22065.2422\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236225872.0000 - rmse: 15369.6416 - val_loss: 451967520.0000 - val_rmse: 21259.5254\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277801920.0000 - rmse: 16667.3887 - val_loss: 420282304.0000 - val_rmse: 20500.7871\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253702800.0000 - rmse: 15928.0479 - val_loss: 416683968.0000 - val_rmse: 20412.8379\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 188758192.0000 - rmse: 13738.9287 - val_loss: 523757280.0000 - val_rmse: 22885.7441\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277132160.0000 - rmse: 16647.2852 - val_loss: 368774528.0000 - val_rmse: 19203.5020\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222110144.0000 - rmse: 14903.3584 - val_loss: 397976128.0000 - val_rmse: 19949.3379\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239191120.0000 - rmse: 15465.8027 - val_loss: 376402208.0000 - val_rmse: 19401.0879\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242512368.0000 - rmse: 15572.8066 - val_loss: 412239808.0000 - val_rmse: 20303.6875\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263661040.0000 - rmse: 16237.6406 - val_loss: 516376096.0000 - val_rmse: 22723.9062\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237703168.0000 - rmse: 15417.6240 - val_loss: 407521120.0000 - val_rmse: 20187.1523\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228640368.0000 - rmse: 15120.8564 - val_loss: 377536096.0000 - val_rmse: 19430.2891\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229455360.0000 - rmse: 15147.7822 - val_loss: 388916800.0000 - val_rmse: 19720.9727\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222329824.0000 - rmse: 14910.7266 - val_loss: 393170848.0000 - val_rmse: 19828.5352\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228243216.0000 - rmse: 15107.7188 - val_loss: 388348384.0000 - val_rmse: 19706.5566\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249263168.0000 - rmse: 15788.0693 - val_loss: 396282592.0000 - val_rmse: 19906.8477\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218294608.0000 - rmse: 14774.7949 - val_loss: 435747840.0000 - val_rmse: 20874.5723\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215039120.0000 - rmse: 14664.2119 - val_loss: 479799040.0000 - val_rmse: 21904.3164\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 223134480.0000 - rmse: 14937.6855 - val_loss: 521009312.0000 - val_rmse: 22825.6289\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 212900608.0000 - rmse: 14591.1123 - val_loss: 443217216.0000 - val_rmse: 21052.7246\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247378128.0000 - rmse: 15728.2578 - val_loss: 405134176.0000 - val_rmse: 20127.9434\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267112656.0000 - rmse: 16343.5811 - val_loss: 428064352.0000 - val_rmse: 20689.7129\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237830656.0000 - rmse: 15421.7588 - val_loss: 532418816.0000 - val_rmse: 23074.2031\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263539904.0000 - rmse: 16233.9102 - val_loss: 373755904.0000 - val_rmse: 19332.7676\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245458912.0000 - rmse: 15667.1279 - val_loss: 400029792.0000 - val_rmse: 20000.7441\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232016432.0000 - rmse: 15232.0840 - val_loss: 469679360.0000 - val_rmse: 21672.0859\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275463264.0000 - rmse: 16597.0820 - val_loss: 397820480.0000 - val_rmse: 19945.4375\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 212230576.0000 - rmse: 14568.1338 - val_loss: 432149856.0000 - val_rmse: 20788.2129\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224431072.0000 - rmse: 14981.0215 - val_loss: 399404672.0000 - val_rmse: 19985.1113\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229888096.0000 - rmse: 15162.0586 - val_loss: 439092992.0000 - val_rmse: 20954.5449\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217946976.0000 - rmse: 14763.0273 - val_loss: 427677024.0000 - val_rmse: 20680.3535\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232093024.0000 - rmse: 15234.5986 - val_loss: 447222784.0000 - val_rmse: 21147.6426\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 208672720.0000 - rmse: 14445.5078 - val_loss: 428388768.0000 - val_rmse: 20697.5547\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239803728.0000 - rmse: 15485.5967 - val_loss: 390133088.0000 - val_rmse: 19751.7871\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216155232.0000 - rmse: 14702.2168 - val_loss: 472582240.0000 - val_rmse: 21738.9551\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 213227600.0000 - rmse: 14602.3125 - val_loss: 455718528.0000 - val_rmse: 21347.5645\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248347440.0000 - rmse: 15759.0420 - val_loss: 394075872.0000 - val_rmse: 19851.3418\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232175552.0000 - rmse: 15237.3066 - val_loss: 398340608.0000 - val_rmse: 19958.4727\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260800704.0000 - rmse: 16149.3232 - val_loss: 417806432.0000 - val_rmse: 20440.3125\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228291520.0000 - rmse: 15109.3184 - val_loss: 388645824.0000 - val_rmse: 19714.0996\n",
      "104/104 [==============================] - 0s 710us/step - loss: 460440768.0000 - rmse: 21457.8809\n",
      "[460440768.0, 21457.880859375]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "baseline_model = train_and_get_deep_learning_model(\"baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run k-fold for baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 26000998400.0000 - rmse: 161248.2500 - val_loss: 22052696064.0000 - val_rmse: 148501.5000\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 19512244224.0000 - rmse: 139686.2344 - val_loss: 7671927808.0000 - val_rmse: 87589.5391\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 4290976512.0000 - rmse: 65505.5469 - val_loss: 2159055616.0000 - val_rmse: 46465.6406\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 2670384896.0000 - rmse: 51675.7656 - val_loss: 1653729792.0000 - val_rmse: 40666.0781\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 2255241216.0000 - rmse: 47489.3789 - val_loss: 1396337920.0000 - val_rmse: 37367.6055\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2087958400.0000 - rmse: 45694.1836 - val_loss: 1248311296.0000 - val_rmse: 35331.4492\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1913190656.0000 - rmse: 43740.0352 - val_loss: 1159184128.0000 - val_rmse: 34046.7930\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1888238848.0000 - rmse: 43453.8711 - val_loss: 1125417344.0000 - val_rmse: 33547.2422\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1797387264.0000 - rmse: 42395.6055 - val_loss: 1051706816.0000 - val_rmse: 32430.0293\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1753002368.0000 - rmse: 41868.8711 - val_loss: 994212992.0000 - val_rmse: 31531.1426\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1702148352.0000 - rmse: 41257.1016 - val_loss: 952890368.0000 - val_rmse: 30868.9219\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1690540928.0000 - rmse: 41116.1875 - val_loss: 922839616.0000 - val_rmse: 30378.2754\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1628062336.0000 - rmse: 40349.2539 - val_loss: 877108480.0000 - val_rmse: 29616.0176\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1625552128.0000 - rmse: 40318.1367 - val_loss: 856874752.0000 - val_rmse: 29272.4238\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1590369024.0000 - rmse: 39879.4297 - val_loss: 826463424.0000 - val_rmse: 28748.2773\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1546546816.0000 - rmse: 39326.1602 - val_loss: 800299008.0000 - val_rmse: 28289.5566\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1538352256.0000 - rmse: 39221.8320 - val_loss: 785504896.0000 - val_rmse: 28026.8594\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1492876160.0000 - rmse: 38637.7539 - val_loss: 763333504.0000 - val_rmse: 27628.4902\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1517807232.0000 - rmse: 38959.0469 - val_loss: 768848704.0000 - val_rmse: 27728.1211\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1471700864.0000 - rmse: 38362.7539 - val_loss: 739321856.0000 - val_rmse: 27190.4727\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1429421696.0000 - rmse: 37807.6953 - val_loss: 724819392.0000 - val_rmse: 26922.4707\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1449544064.0000 - rmse: 38072.8789 - val_loss: 719989248.0000 - val_rmse: 26832.6152\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1413225728.0000 - rmse: 37592.8945 - val_loss: 715138496.0000 - val_rmse: 26742.0742\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1377198336.0000 - rmse: 37110.6211 - val_loss: 713394560.0000 - val_rmse: 26709.4473\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1410719232.0000 - rmse: 37559.5430 - val_loss: 703249152.0000 - val_rmse: 26518.8457\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1289320832.0000 - rmse: 35907.1133 - val_loss: 741174400.0000 - val_rmse: 27224.5176\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1377488384.0000 - rmse: 37114.5312 - val_loss: 715794240.0000 - val_rmse: 26754.3320\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1388412672.0000 - rmse: 37261.4102 - val_loss: 689838912.0000 - val_rmse: 26264.7852\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1297475456.0000 - rmse: 36020.4883 - val_loss: 684563904.0000 - val_rmse: 26164.1719\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1306564864.0000 - rmse: 36146.4336 - val_loss: 682972032.0000 - val_rmse: 26133.7344\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1319457408.0000 - rmse: 36324.3359 - val_loss: 681299904.0000 - val_rmse: 26101.7227\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1324471040.0000 - rmse: 36393.2812 - val_loss: 680483584.0000 - val_rmse: 26086.0801\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1314732416.0000 - rmse: 36259.2383 - val_loss: 682085824.0000 - val_rmse: 26116.7734\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1350714112.0000 - rmse: 36752.0625 - val_loss: 682694592.0000 - val_rmse: 26128.4258\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1279516160.0000 - rmse: 35770.3242 - val_loss: 676891776.0000 - val_rmse: 26017.1445\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1264819968.0000 - rmse: 35564.3086 - val_loss: 718009472.0000 - val_rmse: 26795.6992\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1349039360.0000 - rmse: 36729.2695 - val_loss: 679189824.0000 - val_rmse: 26061.2715\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1291887616.0000 - rmse: 35942.8398 - val_loss: 677375424.0000 - val_rmse: 26026.4375\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1239040768.0000 - rmse: 35200.0117 - val_loss: 685294656.0000 - val_rmse: 26178.1328\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1259066752.0000 - rmse: 35483.3320 - val_loss: 685495040.0000 - val_rmse: 26181.9609\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1236797696.0000 - rmse: 35168.1328 - val_loss: 706247232.0000 - val_rmse: 26575.3125\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1229070848.0000 - rmse: 35058.1055 - val_loss: 696034240.0000 - val_rmse: 26382.4609\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1265413888.0000 - rmse: 35572.6562 - val_loss: 702380352.0000 - val_rmse: 26502.4590\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1224176640.0000 - rmse: 34988.2344 - val_loss: 685127808.0000 - val_rmse: 26174.9453\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1252217856.0000 - rmse: 35386.6914 - val_loss: 717191616.0000 - val_rmse: 26780.4336\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1196401792.0000 - rmse: 34589.0430 - val_loss: 681839872.0000 - val_rmse: 26112.0645\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1244347904.0000 - rmse: 35275.3164 - val_loss: 691817792.0000 - val_rmse: 26302.4297\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1188812928.0000 - rmse: 34479.1680 - val_loss: 692764480.0000 - val_rmse: 26320.4199\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1220822144.0000 - rmse: 34940.2656 - val_loss: 684206336.0000 - val_rmse: 26157.3379\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1220007680.0000 - rmse: 34928.6094 - val_loss: 683197248.0000 - val_rmse: 26138.0430\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1157673088.0000 - rmse: 34024.5938 - val_loss: 693229248.0000 - val_rmse: 26329.2461\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1155597824.0000 - rmse: 33994.0859 - val_loss: 688559552.0000 - val_rmse: 26240.4180\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1189374720.0000 - rmse: 34487.3125 - val_loss: 690324992.0000 - val_rmse: 26274.0371\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1163972480.0000 - rmse: 34117.0391 - val_loss: 704458752.0000 - val_rmse: 26541.6426\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1130281728.0000 - rmse: 33619.6641 - val_loss: 696593856.0000 - val_rmse: 26393.0645\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1132939904.0000 - rmse: 33659.1719 - val_loss: 701142656.0000 - val_rmse: 26479.0977\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1166272128.0000 - rmse: 34150.7266 - val_loss: 700907008.0000 - val_rmse: 26474.6484\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1127747712.0000 - rmse: 33581.9570 - val_loss: 679583680.0000 - val_rmse: 26068.8262\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1118562560.0000 - rmse: 33444.9180 - val_loss: 686012096.0000 - val_rmse: 26191.8320\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1164084352.0000 - rmse: 34118.6797 - val_loss: 682657408.0000 - val_rmse: 26127.7129\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1068496960.0000 - rmse: 32687.8711 - val_loss: 688290432.0000 - val_rmse: 26235.2891\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1113002752.0000 - rmse: 33361.6953 - val_loss: 683474944.0000 - val_rmse: 26143.3535\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1138057088.0000 - rmse: 33735.1016 - val_loss: 681140672.0000 - val_rmse: 26098.6719\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1131953920.0000 - rmse: 33644.5234 - val_loss: 678803392.0000 - val_rmse: 26053.8555\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1160939264.0000 - rmse: 34072.5586 - val_loss: 699917376.0000 - val_rmse: 26455.9512\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1056844288.0000 - rmse: 32509.1426 - val_loss: 688836352.0000 - val_rmse: 26245.6914\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1093405952.0000 - rmse: 33066.6914 - val_loss: 689883520.0000 - val_rmse: 26265.6348\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1109236736.0000 - rmse: 33305.2070 - val_loss: 714202752.0000 - val_rmse: 26724.5723\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1097482752.0000 - rmse: 33128.2773 - val_loss: 773181952.0000 - val_rmse: 27806.1504\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1085912064.0000 - rmse: 32953.1797 - val_loss: 730918272.0000 - val_rmse: 27035.5000\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1071112576.0000 - rmse: 32727.8555 - val_loss: 707191616.0000 - val_rmse: 26593.0742\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1013812608.0000 - rmse: 31840.4238 - val_loss: 692741312.0000 - val_rmse: 26319.9785\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1043962496.0000 - rmse: 32310.4082 - val_loss: 703849856.0000 - val_rmse: 26530.1660\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1061890816.0000 - rmse: 32586.6660 - val_loss: 709073280.0000 - val_rmse: 26628.4297\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1078951936.0000 - rmse: 32847.4023 - val_loss: 720058624.0000 - val_rmse: 26833.9082\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1062333440.0000 - rmse: 32593.4570 - val_loss: 721172736.0000 - val_rmse: 26854.6582\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1044724544.0000 - rmse: 32322.1992 - val_loss: 702886528.0000 - val_rmse: 26512.0059\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1008713408.0000 - rmse: 31760.2480 - val_loss: 712386240.0000 - val_rmse: 26690.5645\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1022790464.0000 - rmse: 31981.0957 - val_loss: 726429248.0000 - val_rmse: 26952.3516\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1050156544.0000 - rmse: 32406.1191 - val_loss: 716805568.0000 - val_rmse: 26773.2246\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 981781056.0000 - rmse: 31333.3828 - val_loss: 720164416.0000 - val_rmse: 26835.8789\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 948132672.0000 - rmse: 30791.7637 - val_loss: 702649344.0000 - val_rmse: 26507.5332\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 974242304.0000 - rmse: 31212.8555 - val_loss: 727635456.0000 - val_rmse: 26974.7188\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1018754432.0000 - rmse: 31917.9336 - val_loss: 714486592.0000 - val_rmse: 26729.8809\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 977267520.0000 - rmse: 31261.2773 - val_loss: 720458688.0000 - val_rmse: 26841.3613\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1058366592.0000 - rmse: 32532.5469 - val_loss: 739872000.0000 - val_rmse: 27200.5879\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 969045888.0000 - rmse: 31129.5020 - val_loss: 764254144.0000 - val_rmse: 27645.1465\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 962730752.0000 - rmse: 31027.9023 - val_loss: 763536704.0000 - val_rmse: 27632.1680\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 861678976.0000 - rmse: 29354.3691 - val_loss: 849282112.0000 - val_rmse: 29142.4453\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 954613376.0000 - rmse: 30896.8184 - val_loss: 744750144.0000 - val_rmse: 27290.1113\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 923032512.0000 - rmse: 30381.4492 - val_loss: 731623296.0000 - val_rmse: 27048.5352\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 944369728.0000 - rmse: 30730.5996 - val_loss: 779028736.0000 - val_rmse: 27911.0859\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 975744320.0000 - rmse: 31236.9062 - val_loss: 743667840.0000 - val_rmse: 27270.2734\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 928129216.0000 - rmse: 30465.2129 - val_loss: 735569024.0000 - val_rmse: 27121.3750\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 929723712.0000 - rmse: 30491.3711 - val_loss: 774459264.0000 - val_rmse: 27829.1074\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 928018176.0000 - rmse: 30463.3906 - val_loss: 757681344.0000 - val_rmse: 27526.0117\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 934560640.0000 - rmse: 30570.5840 - val_loss: 735607232.0000 - val_rmse: 27122.0801\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 912984064.0000 - rmse: 30215.6270 - val_loss: 848083008.0000 - val_rmse: 29121.8652\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 915895552.0000 - rmse: 30263.7656 - val_loss: 826659776.0000 - val_rmse: 28751.6914\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 899012800.0000 - rmse: 29983.5430 - val_loss: 731927168.0000 - val_rmse: 27054.1523\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 871953088.0000 - rmse: 29528.8516 - val_loss: 779142400.0000 - val_rmse: 27913.1230\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 846260864.0000 - rmse: 29090.5625 - val_loss: 758027008.0000 - val_rmse: 27532.2910\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 886501376.0000 - rmse: 29774.1738 - val_loss: 794599744.0000 - val_rmse: 28188.6465\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 885473344.0000 - rmse: 29756.9043 - val_loss: 732219456.0000 - val_rmse: 27059.5547\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 834751744.0000 - rmse: 28892.0684 - val_loss: 782537472.0000 - val_rmse: 27973.8691\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800805440.0000 - rmse: 28298.5039 - val_loss: 811302848.0000 - val_rmse: 28483.3789\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 797089408.0000 - rmse: 28232.7715 - val_loss: 778336960.0000 - val_rmse: 27898.6914\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 891438080.0000 - rmse: 29856.9609 - val_loss: 788478720.0000 - val_rmse: 28079.8613\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 817090752.0000 - rmse: 28584.7988 - val_loss: 783874880.0000 - val_rmse: 27997.7656\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 884981824.0000 - rmse: 29748.6445 - val_loss: 886035072.0000 - val_rmse: 29766.3398\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 808316160.0000 - rmse: 28430.9023 - val_loss: 834252864.0000 - val_rmse: 28883.4355\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 843561536.0000 - rmse: 29044.1289 - val_loss: 789197760.0000 - val_rmse: 28092.6641\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 817523456.0000 - rmse: 28592.3672 - val_loss: 711044096.0000 - val_rmse: 26665.4082\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 752686464.0000 - rmse: 27435.1328 - val_loss: 696327488.0000 - val_rmse: 26388.0176\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 728896768.0000 - rmse: 26998.0879 - val_loss: 735769792.0000 - val_rmse: 27125.0762\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 750760832.0000 - rmse: 27400.0156 - val_loss: 865604992.0000 - val_rmse: 29421.1660\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 755353344.0000 - rmse: 27483.6914 - val_loss: 827800640.0000 - val_rmse: 28771.5254\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 789770880.0000 - rmse: 28102.8613 - val_loss: 767605120.0000 - val_rmse: 27705.6875\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706192832.0000 - rmse: 26574.2871 - val_loss: 696610176.0000 - val_rmse: 26393.3730\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 751523456.0000 - rmse: 27413.9277 - val_loss: 711982144.0000 - val_rmse: 26682.9941\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 735804480.0000 - rmse: 27125.7168 - val_loss: 767912000.0000 - val_rmse: 27711.2246\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 738904960.0000 - rmse: 27182.8066 - val_loss: 813176000.0000 - val_rmse: 28516.2402\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 764176832.0000 - rmse: 27643.7480 - val_loss: 753768064.0000 - val_rmse: 27454.8359\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730461440.0000 - rmse: 27027.0508 - val_loss: 770020992.0000 - val_rmse: 27749.2500\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 729297472.0000 - rmse: 27005.5078 - val_loss: 772528640.0000 - val_rmse: 27794.4004\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 688430208.0000 - rmse: 26237.9531 - val_loss: 852471424.0000 - val_rmse: 29197.1113\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 711751360.0000 - rmse: 26678.6680 - val_loss: 1004238592.0000 - val_rmse: 31689.7246\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 678585856.0000 - rmse: 26049.6797 - val_loss: 847638528.0000 - val_rmse: 29114.2324\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 658121472.0000 - rmse: 25653.8770 - val_loss: 884095488.0000 - val_rmse: 29733.7441\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692596096.0000 - rmse: 26317.2188 - val_loss: 864382656.0000 - val_rmse: 29400.3848\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 681023872.0000 - rmse: 26096.4336 - val_loss: 795444224.0000 - val_rmse: 28203.6211\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 682477952.0000 - rmse: 26124.2773 - val_loss: 727686144.0000 - val_rmse: 26975.6582\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 698670656.0000 - rmse: 26432.3770 - val_loss: 674376960.0000 - val_rmse: 25968.7676\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 639462784.0000 - rmse: 25287.6016 - val_loss: 771588352.0000 - val_rmse: 27777.4785\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 672725696.0000 - rmse: 25936.9570 - val_loss: 687913216.0000 - val_rmse: 26228.0996\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 698110144.0000 - rmse: 26421.7715 - val_loss: 823965760.0000 - val_rmse: 28704.8008\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 734722560.0000 - rmse: 27105.7637 - val_loss: 679329920.0000 - val_rmse: 26063.9590\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 634045824.0000 - rmse: 25180.2656 - val_loss: 963948288.0000 - val_rmse: 31047.5176\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 658012992.0000 - rmse: 25651.7617 - val_loss: 678410816.0000 - val_rmse: 26046.3203\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 641217536.0000 - rmse: 25322.2734 - val_loss: 706933504.0000 - val_rmse: 26588.2207\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 582738624.0000 - rmse: 24139.9785 - val_loss: 629813952.0000 - val_rmse: 25096.0918\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587576192.0000 - rmse: 24239.9707 - val_loss: 758117184.0000 - val_rmse: 27533.9258\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 585463744.0000 - rmse: 24196.3574 - val_loss: 627824896.0000 - val_rmse: 25056.4336\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 619609472.0000 - rmse: 24891.9551 - val_loss: 625020864.0000 - val_rmse: 25000.4160\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564858240.0000 - rmse: 23766.7461 - val_loss: 708968768.0000 - val_rmse: 26626.4668\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 659742336.0000 - rmse: 25685.4492 - val_loss: 710244096.0000 - val_rmse: 26650.4043\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 618278208.0000 - rmse: 24865.1973 - val_loss: 714724096.0000 - val_rmse: 26734.3223\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 514795936.0000 - rmse: 22689.1152 - val_loss: 825099072.0000 - val_rmse: 28724.5371\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 550876288.0000 - rmse: 23470.7539 - val_loss: 782823552.0000 - val_rmse: 27978.9844\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577096320.0000 - rmse: 24022.8301 - val_loss: 818242880.0000 - val_rmse: 28604.9453\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 552847296.0000 - rmse: 23512.7031 - val_loss: 601957632.0000 - val_rmse: 24534.8242\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 545485440.0000 - rmse: 23355.6289 - val_loss: 811321728.0000 - val_rmse: 28483.7090\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 527236032.0000 - rmse: 22961.6211 - val_loss: 703820224.0000 - val_rmse: 26529.6094\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 552000000.0000 - rmse: 23494.6797 - val_loss: 840541760.0000 - val_rmse: 28992.0977\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 554821952.0000 - rmse: 23554.6582 - val_loss: 601457088.0000 - val_rmse: 24524.6211\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 504783968.0000 - rmse: 22467.3945 - val_loss: 834433664.0000 - val_rmse: 28886.5645\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 467432640.0000 - rmse: 21620.1895 - val_loss: 841316928.0000 - val_rmse: 29005.4629\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465592832.0000 - rmse: 21577.5996 - val_loss: 788513472.0000 - val_rmse: 28080.4824\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490365152.0000 - rmse: 22144.1895 - val_loss: 929483712.0000 - val_rmse: 30487.4355\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521525952.0000 - rmse: 22836.9434 - val_loss: 857746944.0000 - val_rmse: 29287.3145\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470307712.0000 - rmse: 21686.5781 - val_loss: 642787008.0000 - val_rmse: 25353.2441\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467247936.0000 - rmse: 21615.9141 - val_loss: 832695936.0000 - val_rmse: 28856.4707\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 461921440.0000 - rmse: 21492.3574 - val_loss: 712827264.0000 - val_rmse: 26698.8242\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 471155008.0000 - rmse: 21706.1055 - val_loss: 652643968.0000 - val_rmse: 25546.8965\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412044736.0000 - rmse: 20298.8848 - val_loss: 886724352.0000 - val_rmse: 29777.9180\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 510384928.0000 - rmse: 22591.6992 - val_loss: 1021842432.0000 - val_rmse: 31966.2695\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510474464.0000 - rmse: 22593.6816 - val_loss: 792562944.0000 - val_rmse: 28152.4941\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446466048.0000 - rmse: 21129.7441 - val_loss: 731028672.0000 - val_rmse: 27037.5410\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470612640.0000 - rmse: 21693.6074 - val_loss: 1058289152.0000 - val_rmse: 32531.3555\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479190976.0000 - rmse: 21890.4316 - val_loss: 733856768.0000 - val_rmse: 27089.7891\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428382208.0000 - rmse: 20697.3945 - val_loss: 767614528.0000 - val_rmse: 27705.8574\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420285632.0000 - rmse: 20500.8672 - val_loss: 716740032.0000 - val_rmse: 26772.0000\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 444996000.0000 - rmse: 21094.9258 - val_loss: 645350016.0000 - val_rmse: 25403.7383\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438819872.0000 - rmse: 20948.0273 - val_loss: 1029431872.0000 - val_rmse: 32084.7617\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461414816.0000 - rmse: 21480.5684 - val_loss: 794886400.0000 - val_rmse: 28193.7305\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479038368.0000 - rmse: 21886.9453 - val_loss: 707357248.0000 - val_rmse: 26596.1895\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470021376.0000 - rmse: 21679.9746 - val_loss: 840679808.0000 - val_rmse: 28994.4785\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 449904832.0000 - rmse: 21210.9609 - val_loss: 1124710144.0000 - val_rmse: 33536.6992\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486604640.0000 - rmse: 22059.1172 - val_loss: 750964096.0000 - val_rmse: 27403.7246\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 467757088.0000 - rmse: 21627.6914 - val_loss: 805070144.0000 - val_rmse: 28373.7578\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 425618304.0000 - rmse: 20630.5176 - val_loss: 680283008.0000 - val_rmse: 26082.2363\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 436902720.0000 - rmse: 20902.2168 - val_loss: 920360320.0000 - val_rmse: 30337.4414\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 400980480.0000 - rmse: 20024.4961 - val_loss: 678080768.0000 - val_rmse: 26039.9844\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455597472.0000 - rmse: 21344.7266 - val_loss: 764287104.0000 - val_rmse: 27645.7422\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385810112.0000 - rmse: 19642.0488 - val_loss: 742018112.0000 - val_rmse: 27240.0098\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456208736.0000 - rmse: 21359.0430 - val_loss: 783261952.0000 - val_rmse: 27986.8184\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407169280.0000 - rmse: 20178.4355 - val_loss: 1203372416.0000 - val_rmse: 34689.6602\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401296448.0000 - rmse: 20032.3848 - val_loss: 982707136.0000 - val_rmse: 31348.1602\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440921120.0000 - rmse: 20998.1211 - val_loss: 741800256.0000 - val_rmse: 27236.0098\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448524736.0000 - rmse: 21178.4023 - val_loss: 987409984.0000 - val_rmse: 31423.0801\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 419329024.0000 - rmse: 20477.5234 - val_loss: 998280704.0000 - val_rmse: 31595.5801\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 389770144.0000 - rmse: 19742.5977 - val_loss: 1023072512.0000 - val_rmse: 31985.5039\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 396990784.0000 - rmse: 19924.6270 - val_loss: 788255872.0000 - val_rmse: 28075.8945\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375476448.0000 - rmse: 19377.2148 - val_loss: 960838912.0000 - val_rmse: 30997.4004\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 397091840.0000 - rmse: 19927.1641 - val_loss: 752487040.0000 - val_rmse: 27431.4980\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 348554432.0000 - rmse: 18669.6133 - val_loss: 1047943424.0000 - val_rmse: 32371.9551\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362135008.0000 - rmse: 19029.8438 - val_loss: 1091924736.0000 - val_rmse: 33044.2812\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323759424.0000 - rmse: 17993.3145 - val_loss: 963804544.0000 - val_rmse: 31045.2012\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373053536.0000 - rmse: 19314.5918 - val_loss: 769601216.0000 - val_rmse: 27741.6875\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 368878528.0000 - rmse: 19206.2090 - val_loss: 540773248.0000 - val_rmse: 23254.5312\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 377376800.0000 - rmse: 19426.1895 - val_loss: 704861888.0000 - val_rmse: 26549.2344\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 357240480.0000 - rmse: 18900.8066 - val_loss: 674484928.0000 - val_rmse: 25970.8477\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 371966624.0000 - rmse: 19286.4355 - val_loss: 942530688.0000 - val_rmse: 30700.6621\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383755712.0000 - rmse: 19589.6836 - val_loss: 543503488.0000 - val_rmse: 23313.1621\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398831808.0000 - rmse: 19970.7734 - val_loss: 905461376.0000 - val_rmse: 30090.8848\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341746560.0000 - rmse: 18486.3887 - val_loss: 1133173120.0000 - val_rmse: 33662.6367\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 382240096.0000 - rmse: 19550.9609 - val_loss: 588183872.0000 - val_rmse: 24252.5020\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365528512.0000 - rmse: 19118.8008 - val_loss: 672815296.0000 - val_rmse: 25938.6816\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 378659552.0000 - rmse: 19459.1758 - val_loss: 903684928.0000 - val_rmse: 30061.3535\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340025760.0000 - rmse: 18439.7871 - val_loss: 810300160.0000 - val_rmse: 28465.7715\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357280160.0000 - rmse: 18901.8555 - val_loss: 638105920.0000 - val_rmse: 25260.7578\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367910816.0000 - rmse: 19181.0000 - val_loss: 787457600.0000 - val_rmse: 28061.6758\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332272736.0000 - rmse: 18228.3496 - val_loss: 1193971072.0000 - val_rmse: 34553.8867\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 318560384.0000 - rmse: 17848.2598 - val_loss: 630938624.0000 - val_rmse: 25118.4902\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 406735584.0000 - rmse: 20167.6855 - val_loss: 795257856.0000 - val_rmse: 28200.3164\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 308800896.0000 - rmse: 17572.7305 - val_loss: 854180352.0000 - val_rmse: 29226.3633\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332077760.0000 - rmse: 18223.0000 - val_loss: 716849216.0000 - val_rmse: 26774.0391\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352269824.0000 - rmse: 18768.8516 - val_loss: 911295680.0000 - val_rmse: 30187.6738\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318955360.0000 - rmse: 17859.3203 - val_loss: 853186816.0000 - val_rmse: 29209.3613\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335407360.0000 - rmse: 18314.1309 - val_loss: 614551872.0000 - val_rmse: 24790.1562\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325103200.0000 - rmse: 18030.6191 - val_loss: 1003153920.0000 - val_rmse: 31672.6055\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291149440.0000 - rmse: 17063.0996 - val_loss: 831522752.0000 - val_rmse: 28836.1348\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333447232.0000 - rmse: 18260.5371 - val_loss: 975180352.0000 - val_rmse: 31227.8770\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 332715456.0000 - rmse: 18240.4883 - val_loss: 773627968.0000 - val_rmse: 27814.1680\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317513344.0000 - rmse: 17818.9023 - val_loss: 733280064.0000 - val_rmse: 27079.1426\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297109536.0000 - rmse: 17236.8633 - val_loss: 1144565376.0000 - val_rmse: 33831.4258\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 382721184.0000 - rmse: 19563.2598 - val_loss: 996593280.0000 - val_rmse: 31568.8652\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 324329984.0000 - rmse: 18009.1621 - val_loss: 911575808.0000 - val_rmse: 30192.3145\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295494720.0000 - rmse: 17189.9570 - val_loss: 962953024.0000 - val_rmse: 31031.4844\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 328406048.0000 - rmse: 18121.9766 - val_loss: 1239930496.0000 - val_rmse: 35212.6484\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317497472.0000 - rmse: 17818.4590 - val_loss: 747568512.0000 - val_rmse: 27341.6992\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317443232.0000 - rmse: 17816.9355 - val_loss: 1448827136.0000 - val_rmse: 38063.4609\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 309295104.0000 - rmse: 17586.7852 - val_loss: 1107166720.0000 - val_rmse: 33274.1133\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 342588032.0000 - rmse: 18509.1309 - val_loss: 1167901312.0000 - val_rmse: 34174.5703\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 292209312.0000 - rmse: 17094.1289 - val_loss: 984642752.0000 - val_rmse: 31379.0176\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328027392.0000 - rmse: 18111.5234 - val_loss: 744845184.0000 - val_rmse: 27291.8516\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 286892640.0000 - rmse: 16937.9062 - val_loss: 1217098112.0000 - val_rmse: 34886.9336\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 343846464.0000 - rmse: 18543.0977 - val_loss: 1505932416.0000 - val_rmse: 38806.3438\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339427104.0000 - rmse: 18423.5469 - val_loss: 919417408.0000 - val_rmse: 30321.8965\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 319962912.0000 - rmse: 17887.5039 - val_loss: 922308288.0000 - val_rmse: 30369.5293\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304262944.0000 - rmse: 17443.1328 - val_loss: 1535390976.0000 - val_rmse: 39184.0664\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307531744.0000 - rmse: 17536.5820 - val_loss: 1477797248.0000 - val_rmse: 38442.1250\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 262101408.0000 - rmse: 16189.5449 - val_loss: 1026242688.0000 - val_rmse: 32035.0234\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 314345216.0000 - rmse: 17729.7812 - val_loss: 1423034368.0000 - val_rmse: 37723.1250\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295287232.0000 - rmse: 17183.9238 - val_loss: 1124455936.0000 - val_rmse: 33532.9102\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281560992.0000 - rmse: 16779.7773 - val_loss: 1217841280.0000 - val_rmse: 34897.5820\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330054720.0000 - rmse: 18167.4082 - val_loss: 1274154368.0000 - val_rmse: 35695.3008\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267824128.0000 - rmse: 16365.3320 - val_loss: 1023625408.0000 - val_rmse: 31994.1465\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288827040.0000 - rmse: 16994.9121 - val_loss: 1169409536.0000 - val_rmse: 34196.6289\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273596288.0000 - rmse: 16540.7461 - val_loss: 1177846784.0000 - val_rmse: 34319.7695\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300661856.0000 - rmse: 17339.6035 - val_loss: 1340293760.0000 - val_rmse: 36610.0234\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292590336.0000 - rmse: 17105.2715 - val_loss: 1244681984.0000 - val_rmse: 35280.0508\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 271305888.0000 - rmse: 16471.3652 - val_loss: 1506003072.0000 - val_rmse: 38807.2539\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273717120.0000 - rmse: 16544.3984 - val_loss: 1170770688.0000 - val_rmse: 34216.5234\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254133248.0000 - rmse: 15941.5566 - val_loss: 821072128.0000 - val_rmse: 28654.3555\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246209984.0000 - rmse: 15691.0781 - val_loss: 1509118336.0000 - val_rmse: 38847.3711\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287168672.0000 - rmse: 16946.0508 - val_loss: 813624768.0000 - val_rmse: 28524.1074\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228516752.0000 - rmse: 15116.7705 - val_loss: 1561031424.0000 - val_rmse: 39509.8906\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262748048.0000 - rmse: 16209.5039 - val_loss: 1275563648.0000 - val_rmse: 35715.0352\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254930144.0000 - rmse: 15966.5312 - val_loss: 1321356032.0000 - val_rmse: 36350.4570\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265473152.0000 - rmse: 16293.3457 - val_loss: 1411082112.0000 - val_rmse: 37564.3711\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263081696.0000 - rmse: 16219.7930 - val_loss: 1213309568.0000 - val_rmse: 34832.5898\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293820480.0000 - rmse: 17141.1914 - val_loss: 882382016.0000 - val_rmse: 29704.9160\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302673472.0000 - rmse: 17397.5117 - val_loss: 1147681280.0000 - val_rmse: 33877.4453\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268567232.0000 - rmse: 16388.0195 - val_loss: 1240008704.0000 - val_rmse: 35213.7578\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286481248.0000 - rmse: 16925.7559 - val_loss: 1764661760.0000 - val_rmse: 42007.8789\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 340022112.0000 - rmse: 18439.6875 - val_loss: 1734165760.0000 - val_rmse: 41643.3164\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249193600.0000 - rmse: 15785.8652 - val_loss: 1481014912.0000 - val_rmse: 38483.9570\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 271021824.0000 - rmse: 16462.7402 - val_loss: 1190044544.0000 - val_rmse: 34497.0234\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 274036352.0000 - rmse: 16554.0430 - val_loss: 1007937984.0000 - val_rmse: 31748.0391\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 242692480.0000 - rmse: 15578.5908 - val_loss: 1481158272.0000 - val_rmse: 38485.8203\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267205168.0000 - rmse: 16346.4111 - val_loss: 982620864.0000 - val_rmse: 31346.7832\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246002560.0000 - rmse: 15684.4678 - val_loss: 1404023168.0000 - val_rmse: 37470.2930\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304276544.0000 - rmse: 17443.5234 - val_loss: 1339248000.0000 - val_rmse: 36595.7383\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278044224.0000 - rmse: 16674.6562 - val_loss: 1029464640.0000 - val_rmse: 32085.2695\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246061376.0000 - rmse: 15686.3418 - val_loss: 1112812288.0000 - val_rmse: 33358.8398\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 235411456.0000 - rmse: 15343.1221 - val_loss: 1716662528.0000 - val_rmse: 41432.6250\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266219104.0000 - rmse: 16316.2207 - val_loss: 1352754432.0000 - val_rmse: 36779.8086\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237991952.0000 - rmse: 15426.9863 - val_loss: 1819508224.0000 - val_rmse: 42655.6953\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266852144.0000 - rmse: 16335.6074 - val_loss: 1457922176.0000 - val_rmse: 38182.7461\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271404608.0000 - rmse: 16474.3613 - val_loss: 1925940224.0000 - val_rmse: 43885.5352\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 271039744.0000 - rmse: 16463.2852 - val_loss: 1327158784.0000 - val_rmse: 36430.1914\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261062960.0000 - rmse: 16157.4404 - val_loss: 1389059840.0000 - val_rmse: 37270.0938\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 270458368.0000 - rmse: 16445.6152 - val_loss: 1446557568.0000 - val_rmse: 38033.6367\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 259258832.0000 - rmse: 16101.5146 - val_loss: 1316533376.0000 - val_rmse: 36284.0664\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234326512.0000 - rmse: 15307.7266 - val_loss: 1862621824.0000 - val_rmse: 43158.1016\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303050976.0000 - rmse: 17408.3594 - val_loss: 1658195712.0000 - val_rmse: 40720.9492\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249141008.0000 - rmse: 15784.1982 - val_loss: 1459187712.0000 - val_rmse: 38199.3164\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 213526240.0000 - rmse: 14612.5352 - val_loss: 1349227520.0000 - val_rmse: 36731.8320\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244550320.0000 - rmse: 15638.1035 - val_loss: 1442771712.0000 - val_rmse: 37983.8359\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253618176.0000 - rmse: 15925.3926 - val_loss: 1719003904.0000 - val_rmse: 41460.8672\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248795392.0000 - rmse: 15773.2461 - val_loss: 917354048.0000 - val_rmse: 30287.8535\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 225828912.0000 - rmse: 15027.6035 - val_loss: 1518933760.0000 - val_rmse: 38973.5000\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304606912.0000 - rmse: 17452.9922 - val_loss: 1296617728.0000 - val_rmse: 36008.5781\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244891440.0000 - rmse: 15649.0059 - val_loss: 1164231680.0000 - val_rmse: 34120.8359\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312629472.0000 - rmse: 17681.3301 - val_loss: 1460559360.0000 - val_rmse: 38217.2656\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270382592.0000 - rmse: 16443.3145 - val_loss: 1255924992.0000 - val_rmse: 35439.0312\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267320560.0000 - rmse: 16349.9395 - val_loss: 1132362752.0000 - val_rmse: 33650.5977\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 225083072.0000 - rmse: 15002.7676 - val_loss: 2137958144.0000 - val_rmse: 46238.0586\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253645360.0000 - rmse: 15926.2461 - val_loss: 1185445632.0000 - val_rmse: 34430.3008\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260896240.0000 - rmse: 16152.2812 - val_loss: 1048297856.0000 - val_rmse: 32377.4277\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241555520.0000 - rmse: 15542.0547 - val_loss: 1302411520.0000 - val_rmse: 36088.9375\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 228744464.0000 - rmse: 15124.2988 - val_loss: 1612423168.0000 - val_rmse: 40154.9883\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 258022992.0000 - rmse: 16063.0928 - val_loss: 927311424.0000 - val_rmse: 30451.7891\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224690848.0000 - rmse: 14989.6904 - val_loss: 2158621952.0000 - val_rmse: 46460.9688\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256420176.0000 - rmse: 16013.1230 - val_loss: 1875021184.0000 - val_rmse: 43301.5156\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 225648416.0000 - rmse: 15021.5977 - val_loss: 2041412992.0000 - val_rmse: 45182.0000\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252358000.0000 - rmse: 15885.7783 - val_loss: 1724724992.0000 - val_rmse: 41529.8086\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252244192.0000 - rmse: 15882.1963 - val_loss: 1716274048.0000 - val_rmse: 41427.9375\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242088720.0000 - rmse: 15559.1973 - val_loss: 1534815616.0000 - val_rmse: 39176.7227\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206712864.0000 - rmse: 14377.5117 - val_loss: 1275393792.0000 - val_rmse: 35712.6562\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230608416.0000 - rmse: 15185.7959 - val_loss: 1343252608.0000 - val_rmse: 36650.4102\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 195891456.0000 - rmse: 13996.1211 - val_loss: 1048384128.0000 - val_rmse: 32378.7598\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247302016.0000 - rmse: 15725.8389 - val_loss: 1748104320.0000 - val_rmse: 41810.3359\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288725472.0000 - rmse: 16991.9219 - val_loss: 1072264512.0000 - val_rmse: 32745.4492\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235889664.0000 - rmse: 15358.7002 - val_loss: 1909670784.0000 - val_rmse: 43699.7812\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236202416.0000 - rmse: 15368.8770 - val_loss: 1465481984.0000 - val_rmse: 38281.6133\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243897136.0000 - rmse: 15617.2051 - val_loss: 1283096576.0000 - val_rmse: 35820.3359\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 260627664.0000 - rmse: 16143.9648 - val_loss: 1191356288.0000 - val_rmse: 34516.0273\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 219966256.0000 - rmse: 14831.2578 - val_loss: 1348215680.0000 - val_rmse: 36718.0547\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 248550992.0000 - rmse: 15765.5000 - val_loss: 1437043456.0000 - val_rmse: 37908.3555\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 220566736.0000 - rmse: 14851.4873 - val_loss: 1491500800.0000 - val_rmse: 38619.9531\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 202738624.0000 - rmse: 14238.6299 - val_loss: 1026553728.0000 - val_rmse: 32039.8750\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 217610800.0000 - rmse: 14751.6367 - val_loss: 1670931072.0000 - val_rmse: 40877.0234\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219389760.0000 - rmse: 14811.8115 - val_loss: 1218926720.0000 - val_rmse: 34913.1289\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202142368.0000 - rmse: 14217.6748 - val_loss: 1568538240.0000 - val_rmse: 39604.7734\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 220537376.0000 - rmse: 14850.4980 - val_loss: 1048527616.0000 - val_rmse: 32380.9766\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 223839152.0000 - rmse: 14961.2539 - val_loss: 1550483072.0000 - val_rmse: 39376.1758\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 234881392.0000 - rmse: 15325.8389 - val_loss: 1432785536.0000 - val_rmse: 37852.1523\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211839792.0000 - rmse: 14554.7158 - val_loss: 956289344.0000 - val_rmse: 30923.9277\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 179726544.0000 - rmse: 13406.2119 - val_loss: 1568977536.0000 - val_rmse: 39610.3203\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 209557632.0000 - rmse: 14476.1025 - val_loss: 1241899904.0000 - val_rmse: 35240.6016\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243249312.0000 - rmse: 15596.4521 - val_loss: 1354067072.0000 - val_rmse: 36797.6484\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216210672.0000 - rmse: 14704.1025 - val_loss: 1108685312.0000 - val_rmse: 33296.9258\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267568304.0000 - rmse: 16357.5137 - val_loss: 1288861568.0000 - val_rmse: 35900.7188\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251859216.0000 - rmse: 15870.0713 - val_loss: 1388191872.0000 - val_rmse: 37258.4453\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240655680.0000 - rmse: 15513.0801 - val_loss: 1771207168.0000 - val_rmse: 42085.7109\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 197802656.0000 - rmse: 14064.2305 - val_loss: 1419733504.0000 - val_rmse: 37679.3516\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238470512.0000 - rmse: 15442.4902 - val_loss: 1200312704.0000 - val_rmse: 34645.5312\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 200465136.0000 - rmse: 14158.5693 - val_loss: 1170570496.0000 - val_rmse: 34213.6016\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224583472.0000 - rmse: 14986.1074 - val_loss: 1852860032.0000 - val_rmse: 43044.8594\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268787424.0000 - rmse: 16394.7383 - val_loss: 1151248000.0000 - val_rmse: 33930.0469\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 225635616.0000 - rmse: 15021.1699 - val_loss: 1969496320.0000 - val_rmse: 44379.0078\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 200502976.0000 - rmse: 14159.9053 - val_loss: 1674791680.0000 - val_rmse: 40924.2188\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 254959840.0000 - rmse: 15967.4609 - val_loss: 1367020544.0000 - val_rmse: 36973.2422\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 222196128.0000 - rmse: 14906.2441 - val_loss: 1070361728.0000 - val_rmse: 32716.3828\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 201914976.0000 - rmse: 14209.6777 - val_loss: 1047489408.0000 - val_rmse: 32364.9414\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265356256.0000 - rmse: 16289.7568 - val_loss: 1404074752.0000 - val_rmse: 37470.9844\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219094624.0000 - rmse: 14801.8447 - val_loss: 1270925696.0000 - val_rmse: 35650.0391\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228126848.0000 - rmse: 15103.8662 - val_loss: 1570027776.0000 - val_rmse: 39623.5742\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227387392.0000 - rmse: 15079.3691 - val_loss: 971652672.0000 - val_rmse: 31171.3398\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 195137808.0000 - rmse: 13969.1719 - val_loss: 1253593088.0000 - val_rmse: 35406.1172\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 203155312.0000 - rmse: 14253.2559 - val_loss: 2098739840.0000 - val_rmse: 45812.0039\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 181212000.0000 - rmse: 13461.4990 - val_loss: 876149632.0000 - val_rmse: 29599.8223\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224854288.0000 - rmse: 14995.1406 - val_loss: 1913582720.0000 - val_rmse: 43744.5156\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207812752.0000 - rmse: 14415.7100 - val_loss: 2188209920.0000 - val_rmse: 46778.3008\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217331248.0000 - rmse: 14742.1572 - val_loss: 1360375680.0000 - val_rmse: 36883.2695\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 198131120.0000 - rmse: 14075.9033 - val_loss: 1547237248.0000 - val_rmse: 39334.9336\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234515168.0000 - rmse: 15313.8857 - val_loss: 1469253760.0000 - val_rmse: 38330.8477\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 200163568.0000 - rmse: 14147.9160 - val_loss: 1327309056.0000 - val_rmse: 36432.2500\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217065696.0000 - rmse: 14733.1475 - val_loss: 1785427328.0000 - val_rmse: 42254.3164\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 194885840.0000 - rmse: 13960.1504 - val_loss: 1370930816.0000 - val_rmse: 37026.0820\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 233270848.0000 - rmse: 15273.2061 - val_loss: 1814247424.0000 - val_rmse: 42593.9844\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 193558368.0000 - rmse: 13912.5234 - val_loss: 1564003200.0000 - val_rmse: 39547.4805\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 186921328.0000 - rmse: 13671.9170 - val_loss: 1977474560.0000 - val_rmse: 44468.8047\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 208758432.0000 - rmse: 14448.4736 - val_loss: 1501763200.0000 - val_rmse: 38752.5898\n",
      "104/104 [==============================] - 0s 658us/step - loss: 400149056.0000 - rmse: 20003.7246\n",
      "[400149056.0, 20003.724609375]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 25328785408.0000 - rmse: 159150.2031 - val_loss: 18157766656.0000 - val_rmse: 134750.7500\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 10388385792.0000 - rmse: 101923.4297 - val_loss: 2453396224.0000 - val_rmse: 49531.7695\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 2691952896.0000 - rmse: 51884.0312 - val_loss: 1760165120.0000 - val_rmse: 41954.3203\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 2125508992.0000 - rmse: 46103.2422 - val_loss: 1410764544.0000 - val_rmse: 37560.1445\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1764407936.0000 - rmse: 42004.8555 - val_loss: 1285136768.0000 - val_rmse: 35848.8047\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1741526144.0000 - rmse: 41731.5977 - val_loss: 1204125312.0000 - val_rmse: 34700.5078\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1653108864.0000 - rmse: 40658.4414 - val_loss: 1153605760.0000 - val_rmse: 33964.7734\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1501168256.0000 - rmse: 38744.9141 - val_loss: 1083964032.0000 - val_rmse: 32923.6094\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1466973824.0000 - rmse: 38301.0938 - val_loss: 1047853760.0000 - val_rmse: 32370.5703\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1403409792.0000 - rmse: 37462.1133 - val_loss: 1012974464.0000 - val_rmse: 31827.2598\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1366596096.0000 - rmse: 36967.5000 - val_loss: 991297472.0000 - val_rmse: 31484.8770\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1301240448.0000 - rmse: 36072.7109 - val_loss: 970676864.0000 - val_rmse: 31155.6875\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1278742144.0000 - rmse: 35759.5039 - val_loss: 956785024.0000 - val_rmse: 30931.9414\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1261480704.0000 - rmse: 35517.3281 - val_loss: 940640896.0000 - val_rmse: 30669.8691\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1195901568.0000 - rmse: 34581.8086 - val_loss: 913283840.0000 - val_rmse: 30220.5859\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1207949312.0000 - rmse: 34755.5664 - val_loss: 961740864.0000 - val_rmse: 31011.9473\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1158703744.0000 - rmse: 34039.7383 - val_loss: 892494400.0000 - val_rmse: 29874.6445\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1160255744.0000 - rmse: 34062.5273 - val_loss: 883180416.0000 - val_rmse: 29718.3516\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1143883392.0000 - rmse: 33821.3438 - val_loss: 883392512.0000 - val_rmse: 29721.9199\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1091285760.0000 - rmse: 33034.6133 - val_loss: 863389312.0000 - val_rmse: 29383.4863\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1121443968.0000 - rmse: 33487.9688 - val_loss: 860911872.0000 - val_rmse: 29341.2988\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1077069184.0000 - rmse: 32818.7305 - val_loss: 854198464.0000 - val_rmse: 29226.6719\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1153373824.0000 - rmse: 33961.3594 - val_loss: 855775872.0000 - val_rmse: 29253.6465\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1092035712.0000 - rmse: 33045.9648 - val_loss: 841943552.0000 - val_rmse: 29016.2637\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1050048832.0000 - rmse: 32404.4570 - val_loss: 868020864.0000 - val_rmse: 29462.1934\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1056454080.0000 - rmse: 32503.1387 - val_loss: 831321792.0000 - val_rmse: 28832.6523\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1014462016.0000 - rmse: 31850.6211 - val_loss: 829144064.0000 - val_rmse: 28794.8613\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1069924928.0000 - rmse: 32709.7070 - val_loss: 842222656.0000 - val_rmse: 29021.0723\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1046730048.0000 - rmse: 32353.2070 - val_loss: 820225152.0000 - val_rmse: 28639.5723\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1069760448.0000 - rmse: 32707.1934 - val_loss: 817305856.0000 - val_rmse: 28588.5625\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1075098112.0000 - rmse: 32788.6875 - val_loss: 844636864.0000 - val_rmse: 29062.6367\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1048771968.0000 - rmse: 32384.7480 - val_loss: 807896704.0000 - val_rmse: 28423.5234\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1030956224.0000 - rmse: 32108.5078 - val_loss: 805320192.0000 - val_rmse: 28378.1641\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1011430976.0000 - rmse: 31803.0020 - val_loss: 828116928.0000 - val_rmse: 28777.0215\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 980828672.0000 - rmse: 31318.1836 - val_loss: 818811648.0000 - val_rmse: 28614.8848\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 979146944.0000 - rmse: 31291.3242 - val_loss: 797724736.0000 - val_rmse: 28244.0215\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 954193216.0000 - rmse: 30890.0176 - val_loss: 813844736.0000 - val_rmse: 28527.9648\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 984046400.0000 - rmse: 31369.5137 - val_loss: 791454272.0000 - val_rmse: 28132.7969\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 976869312.0000 - rmse: 31254.9082 - val_loss: 795062272.0000 - val_rmse: 28196.8477\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 974813376.0000 - rmse: 31222.0020 - val_loss: 792844800.0000 - val_rmse: 28157.5000\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 968581760.0000 - rmse: 31122.0469 - val_loss: 782675776.0000 - val_rmse: 27976.3438\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 930893504.0000 - rmse: 30510.5469 - val_loss: 801279552.0000 - val_rmse: 28306.8809\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 896674112.0000 - rmse: 29944.5176 - val_loss: 789004544.0000 - val_rmse: 28089.2246\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 954332672.0000 - rmse: 30892.2754 - val_loss: 776215680.0000 - val_rmse: 27860.6465\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 946833536.0000 - rmse: 30770.6602 - val_loss: 773199232.0000 - val_rmse: 27806.4609\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 876865088.0000 - rmse: 29611.9082 - val_loss: 782246720.0000 - val_rmse: 27968.6738\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 907233984.0000 - rmse: 30120.3242 - val_loss: 771150848.0000 - val_rmse: 27769.6016\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 947057792.0000 - rmse: 30774.3047 - val_loss: 764995712.0000 - val_rmse: 27658.5566\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 913625280.0000 - rmse: 30226.2344 - val_loss: 764280448.0000 - val_rmse: 27645.6230\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 889770368.0000 - rmse: 29829.0195 - val_loss: 819331200.0000 - val_rmse: 28623.9629\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 898928320.0000 - rmse: 29982.1328 - val_loss: 758977920.0000 - val_rmse: 27549.5547\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 845703104.0000 - rmse: 29080.9746 - val_loss: 757830400.0000 - val_rmse: 27528.7188\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 889453376.0000 - rmse: 29823.7051 - val_loss: 762682624.0000 - val_rmse: 27616.7090\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 888403456.0000 - rmse: 29806.0977 - val_loss: 746570944.0000 - val_rmse: 27323.4512\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 898725952.0000 - rmse: 29978.7578 - val_loss: 758306560.0000 - val_rmse: 27537.3672\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 890902016.0000 - rmse: 29847.9824 - val_loss: 736200192.0000 - val_rmse: 27133.0078\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 866779008.0000 - rmse: 29441.1113 - val_loss: 798963584.0000 - val_rmse: 28265.9434\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 915340864.0000 - rmse: 30254.6016 - val_loss: 741422016.0000 - val_rmse: 27229.0664\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 895066560.0000 - rmse: 29917.6621 - val_loss: 732802496.0000 - val_rmse: 27070.3223\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 879143360.0000 - rmse: 29650.3516 - val_loss: 729050880.0000 - val_rmse: 27000.9414\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 879454272.0000 - rmse: 29655.5938 - val_loss: 726232512.0000 - val_rmse: 26948.7012\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 838170368.0000 - rmse: 28951.1719 - val_loss: 729739136.0000 - val_rmse: 27013.6836\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 860753984.0000 - rmse: 29338.6094 - val_loss: 718614528.0000 - val_rmse: 26806.9863\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 851332608.0000 - rmse: 29177.6055 - val_loss: 715256128.0000 - val_rmse: 26744.2734\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 847869184.0000 - rmse: 29118.1934 - val_loss: 721795712.0000 - val_rmse: 26866.2559\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 876845312.0000 - rmse: 29611.5723 - val_loss: 710636928.0000 - val_rmse: 26657.7734\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 892781888.0000 - rmse: 29879.4551 - val_loss: 708090432.0000 - val_rmse: 26609.9688\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 837824384.0000 - rmse: 28945.1953 - val_loss: 700608064.0000 - val_rmse: 26469.0000\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 838241792.0000 - rmse: 28952.4062 - val_loss: 706121472.0000 - val_rmse: 26572.9453\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 841594240.0000 - rmse: 29010.2441 - val_loss: 700191168.0000 - val_rmse: 26461.1250\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 815921472.0000 - rmse: 28564.3398 - val_loss: 698933248.0000 - val_rmse: 26437.3457\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 774562368.0000 - rmse: 27830.9609 - val_loss: 708409344.0000 - val_rmse: 26615.9609\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 811475328.0000 - rmse: 28486.4062 - val_loss: 739970752.0000 - val_rmse: 27202.4043\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 886052032.0000 - rmse: 29766.6270 - val_loss: 685648384.0000 - val_rmse: 26184.8867\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 789809280.0000 - rmse: 28103.5449 - val_loss: 710710784.0000 - val_rmse: 26659.1602\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 809285504.0000 - rmse: 28447.9434 - val_loss: 692214272.0000 - val_rmse: 26309.9648\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 778210624.0000 - rmse: 27896.4258 - val_loss: 690347584.0000 - val_rmse: 26274.4668\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 786731456.0000 - rmse: 28048.7344 - val_loss: 699739840.0000 - val_rmse: 26452.5957\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 778971200.0000 - rmse: 27910.0547 - val_loss: 685766528.0000 - val_rmse: 26187.1426\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782866048.0000 - rmse: 27979.7441 - val_loss: 682866048.0000 - val_rmse: 26131.7051\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 758984576.0000 - rmse: 27549.6738 - val_loss: 699625920.0000 - val_rmse: 26450.4434\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 770732864.0000 - rmse: 27762.0762 - val_loss: 673738368.0000 - val_rmse: 25956.4707\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 769813760.0000 - rmse: 27745.5176 - val_loss: 677952192.0000 - val_rmse: 26037.5156\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718733952.0000 - rmse: 26809.2129 - val_loss: 674418304.0000 - val_rmse: 25969.5645\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 715856960.0000 - rmse: 26755.5039 - val_loss: 684528064.0000 - val_rmse: 26163.4863\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 723788480.0000 - rmse: 26903.3164 - val_loss: 673805440.0000 - val_rmse: 25957.7617\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 755387968.0000 - rmse: 27484.3223 - val_loss: 741467264.0000 - val_rmse: 27229.8965\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729721024.0000 - rmse: 27013.3496 - val_loss: 724730304.0000 - val_rmse: 26920.8145\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 734038208.0000 - rmse: 27093.1387 - val_loss: 686836224.0000 - val_rmse: 26207.5605\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 685170560.0000 - rmse: 26175.7637 - val_loss: 670436224.0000 - val_rmse: 25892.7832\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 707666112.0000 - rmse: 26601.9941 - val_loss: 675757056.0000 - val_rmse: 25995.3281\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 703074048.0000 - rmse: 26515.5430 - val_loss: 669993664.0000 - val_rmse: 25884.2363\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 746632960.0000 - rmse: 27324.5859 - val_loss: 678968960.0000 - val_rmse: 26057.0332\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 675431616.0000 - rmse: 25989.0664 - val_loss: 665772416.0000 - val_rmse: 25802.5664\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 741742336.0000 - rmse: 27234.9473 - val_loss: 668445312.0000 - val_rmse: 25854.3086\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 694489792.0000 - rmse: 26353.1738 - val_loss: 672375168.0000 - val_rmse: 25930.1973\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 717173248.0000 - rmse: 26780.0898 - val_loss: 671666176.0000 - val_rmse: 25916.5234\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 686915200.0000 - rmse: 26209.0664 - val_loss: 670958848.0000 - val_rmse: 25902.8730\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 702107712.0000 - rmse: 26497.3145 - val_loss: 677821632.0000 - val_rmse: 26035.0059\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 696869440.0000 - rmse: 26398.2852 - val_loss: 694573184.0000 - val_rmse: 26354.7559\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 722416512.0000 - rmse: 26877.8066 - val_loss: 664945216.0000 - val_rmse: 25786.5312\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 715662912.0000 - rmse: 26751.8750 - val_loss: 656454400.0000 - val_rmse: 25621.3633\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 711712384.0000 - rmse: 26677.9375 - val_loss: 670553088.0000 - val_rmse: 25895.0391\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 654493248.0000 - rmse: 25583.0664 - val_loss: 661264960.0000 - val_rmse: 25715.0723\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 648086016.0000 - rmse: 25457.5332 - val_loss: 683687360.0000 - val_rmse: 26147.4160\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 696062912.0000 - rmse: 26383.0039 - val_loss: 661631360.0000 - val_rmse: 25722.1953\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 652474368.0000 - rmse: 25543.5762 - val_loss: 689873536.0000 - val_rmse: 26265.4434\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 619310144.0000 - rmse: 24885.9414 - val_loss: 700673024.0000 - val_rmse: 26470.2285\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 644462976.0000 - rmse: 25386.2754 - val_loss: 681806144.0000 - val_rmse: 26111.4180\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 706893632.0000 - rmse: 26587.4707 - val_loss: 660337472.0000 - val_rmse: 25697.0332\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 664076736.0000 - rmse: 25769.6855 - val_loss: 677915712.0000 - val_rmse: 26036.8145\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 668825664.0000 - rmse: 25861.6641 - val_loss: 657643648.0000 - val_rmse: 25644.5645\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 656967680.0000 - rmse: 25631.3809 - val_loss: 672958464.0000 - val_rmse: 25941.4414\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 633578624.0000 - rmse: 25170.9883 - val_loss: 687457856.0000 - val_rmse: 26219.4180\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666556608.0000 - rmse: 25817.7578 - val_loss: 727711424.0000 - val_rmse: 26976.1270\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 630677888.0000 - rmse: 25113.3008 - val_loss: 657729856.0000 - val_rmse: 25646.2441\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 630801728.0000 - rmse: 25115.7656 - val_loss: 662196544.0000 - val_rmse: 25733.1777\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 658156480.0000 - rmse: 25654.5586 - val_loss: 671289856.0000 - val_rmse: 25909.2617\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 640464192.0000 - rmse: 25307.3945 - val_loss: 655323136.0000 - val_rmse: 25599.2793\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 660447168.0000 - rmse: 25699.1660 - val_loss: 669569472.0000 - val_rmse: 25876.0410\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 674433216.0000 - rmse: 25969.8516 - val_loss: 672749248.0000 - val_rmse: 25937.4082\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 598866304.0000 - rmse: 24471.7441 - val_loss: 677648704.0000 - val_rmse: 26031.6875\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 590758080.0000 - rmse: 24305.5156 - val_loss: 675698752.0000 - val_rmse: 25994.2051\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 649624704.0000 - rmse: 25487.7363 - val_loss: 656501184.0000 - val_rmse: 25622.2773\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 669895424.0000 - rmse: 25882.3379 - val_loss: 672586112.0000 - val_rmse: 25934.2637\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 669364672.0000 - rmse: 25872.0801 - val_loss: 664647872.0000 - val_rmse: 25780.7637\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 665251712.0000 - rmse: 25792.4746 - val_loss: 682537664.0000 - val_rmse: 26125.4219\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 672038720.0000 - rmse: 25923.7090 - val_loss: 662192576.0000 - val_rmse: 25733.1035\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 631356096.0000 - rmse: 25126.7988 - val_loss: 648870848.0000 - val_rmse: 25472.9414\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605734016.0000 - rmse: 24611.6641 - val_loss: 653890816.0000 - val_rmse: 25571.2871\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 597561856.0000 - rmse: 24445.0762 - val_loss: 690367872.0000 - val_rmse: 26274.8516\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623566080.0000 - rmse: 24971.3047 - val_loss: 654679744.0000 - val_rmse: 25586.7090\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 546682304.0000 - rmse: 23381.2383 - val_loss: 667570304.0000 - val_rmse: 25837.3828\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 573442944.0000 - rmse: 23946.6680 - val_loss: 672999744.0000 - val_rmse: 25942.2383\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 604439552.0000 - rmse: 24585.3516 - val_loss: 643999616.0000 - val_rmse: 25377.1484\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604768448.0000 - rmse: 24592.0410 - val_loss: 653499520.0000 - val_rmse: 25563.6367\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 590648064.0000 - rmse: 24303.2520 - val_loss: 656203136.0000 - val_rmse: 25616.4629\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574945664.0000 - rmse: 23978.0234 - val_loss: 649623040.0000 - val_rmse: 25487.7031\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610416448.0000 - rmse: 24706.6055 - val_loss: 690943936.0000 - val_rmse: 26285.8105\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571673280.0000 - rmse: 23909.6895 - val_loss: 664284992.0000 - val_rmse: 25773.7266\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596073984.0000 - rmse: 24414.6270 - val_loss: 673418944.0000 - val_rmse: 25950.3164\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582226688.0000 - rmse: 24129.3730 - val_loss: 659820416.0000 - val_rmse: 25686.9707\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605925952.0000 - rmse: 24615.5625 - val_loss: 639462976.0000 - val_rmse: 25287.6035\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 537164992.0000 - rmse: 23176.8203 - val_loss: 663507584.0000 - val_rmse: 25758.6406\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 593390656.0000 - rmse: 24359.6113 - val_loss: 693556288.0000 - val_rmse: 26335.4570\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 566843776.0000 - rmse: 23808.4805 - val_loss: 664740736.0000 - val_rmse: 25782.5645\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 596563456.0000 - rmse: 24424.6465 - val_loss: 668394112.0000 - val_rmse: 25853.3184\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 552785024.0000 - rmse: 23511.3809 - val_loss: 664042560.0000 - val_rmse: 25769.0234\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 549206144.0000 - rmse: 23435.1484 - val_loss: 691048640.0000 - val_rmse: 26287.8027\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584571648.0000 - rmse: 24177.9160 - val_loss: 671142528.0000 - val_rmse: 25906.4180\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594200640.0000 - rmse: 24376.2305 - val_loss: 665737728.0000 - val_rmse: 25801.8945\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574968192.0000 - rmse: 23978.4941 - val_loss: 686642432.0000 - val_rmse: 26203.8633\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 576326336.0000 - rmse: 24006.7969 - val_loss: 751127104.0000 - val_rmse: 27406.6973\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509462048.0000 - rmse: 22571.2656 - val_loss: 639111040.0000 - val_rmse: 25280.6465\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540924160.0000 - rmse: 23257.7754 - val_loss: 657028672.0000 - val_rmse: 25632.5703\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565244416.0000 - rmse: 23774.8672 - val_loss: 684569344.0000 - val_rmse: 26164.2754\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560029824.0000 - rmse: 23664.9492 - val_loss: 694129856.0000 - val_rmse: 26346.3438\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544619264.0000 - rmse: 23337.0801 - val_loss: 675777792.0000 - val_rmse: 25995.7266\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578956352.0000 - rmse: 24061.5098 - val_loss: 690655424.0000 - val_rmse: 26280.3223\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570886592.0000 - rmse: 23893.2324 - val_loss: 699227072.0000 - val_rmse: 26442.9004\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561434304.0000 - rmse: 23694.6035 - val_loss: 680308288.0000 - val_rmse: 26082.7207\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518740000.0000 - rmse: 22775.8633 - val_loss: 734930432.0000 - val_rmse: 27109.5996\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512644704.0000 - rmse: 22641.6582 - val_loss: 673604608.0000 - val_rmse: 25953.8945\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616978368.0000 - rmse: 24839.0488 - val_loss: 649907136.0000 - val_rmse: 25493.2754\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 530139360.0000 - rmse: 23024.7559 - val_loss: 655191232.0000 - val_rmse: 25596.7031\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530939488.0000 - rmse: 23042.1230 - val_loss: 663150656.0000 - val_rmse: 25751.7109\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508161312.0000 - rmse: 22542.4336 - val_loss: 689248576.0000 - val_rmse: 26253.5449\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542578496.0000 - rmse: 23293.3145 - val_loss: 703060288.0000 - val_rmse: 26515.2832\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473008096.0000 - rmse: 21748.7500 - val_loss: 677013568.0000 - val_rmse: 26019.4824\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516486144.0000 - rmse: 22726.3320 - val_loss: 693726656.0000 - val_rmse: 26338.6914\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532581280.0000 - rmse: 23077.7227 - val_loss: 694301568.0000 - val_rmse: 26349.6035\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493530528.0000 - rmse: 22215.5449 - val_loss: 677556352.0000 - val_rmse: 26029.9121\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527756160.0000 - rmse: 22972.9434 - val_loss: 667434624.0000 - val_rmse: 25834.7539\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568033728.0000 - rmse: 23833.4590 - val_loss: 704813760.0000 - val_rmse: 26548.3281\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562176448.0000 - rmse: 23710.2598 - val_loss: 738526976.0000 - val_rmse: 27175.8516\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558671104.0000 - rmse: 23636.2246 - val_loss: 690711104.0000 - val_rmse: 26281.3828\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471941760.0000 - rmse: 21724.2207 - val_loss: 683040896.0000 - val_rmse: 26135.0508\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510361760.0000 - rmse: 22591.1875 - val_loss: 703043520.0000 - val_rmse: 26514.9688\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532230944.0000 - rmse: 23070.1289 - val_loss: 676072384.0000 - val_rmse: 26001.3926\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510035680.0000 - rmse: 22583.9688 - val_loss: 696708416.0000 - val_rmse: 26395.2344\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 471500064.0000 - rmse: 21714.0527 - val_loss: 700741120.0000 - val_rmse: 26471.5156\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582334784.0000 - rmse: 24131.6133 - val_loss: 725498176.0000 - val_rmse: 26935.0742\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511592736.0000 - rmse: 22618.4160 - val_loss: 721457472.0000 - val_rmse: 26859.9609\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 556685632.0000 - rmse: 23594.1855 - val_loss: 698762432.0000 - val_rmse: 26434.1152\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500849952.0000 - rmse: 22379.6777 - val_loss: 701660224.0000 - val_rmse: 26488.8691\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492534880.0000 - rmse: 22193.1270 - val_loss: 717540224.0000 - val_rmse: 26786.9395\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 505555072.0000 - rmse: 22484.5488 - val_loss: 683622784.0000 - val_rmse: 26146.1816\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 502950976.0000 - rmse: 22426.5684 - val_loss: 696471488.0000 - val_rmse: 26390.7461\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499588672.0000 - rmse: 22351.4805 - val_loss: 697584832.0000 - val_rmse: 26411.8320\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 490427520.0000 - rmse: 22145.5977 - val_loss: 706418944.0000 - val_rmse: 26578.5430\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 485427584.0000 - rmse: 22032.4219 - val_loss: 707094912.0000 - val_rmse: 26591.2559\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 456148032.0000 - rmse: 21357.6230 - val_loss: 694404352.0000 - val_rmse: 26351.5527\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 467288128.0000 - rmse: 21616.8477 - val_loss: 689635008.0000 - val_rmse: 26260.9004\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495408992.0000 - rmse: 22257.7852 - val_loss: 707324032.0000 - val_rmse: 26595.5645\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441355872.0000 - rmse: 21008.4707 - val_loss: 710207488.0000 - val_rmse: 26649.7168\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 480180288.0000 - rmse: 21913.0156 - val_loss: 690682688.0000 - val_rmse: 26280.8418\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483946432.0000 - rmse: 21998.7812 - val_loss: 691137344.0000 - val_rmse: 26289.4902\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423290752.0000 - rmse: 20574.0312 - val_loss: 734281216.0000 - val_rmse: 27097.6230\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460643008.0000 - rmse: 21462.5957 - val_loss: 750023744.0000 - val_rmse: 27386.5605\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517218208.0000 - rmse: 22742.4297 - val_loss: 693166272.0000 - val_rmse: 26328.0508\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 538666368.0000 - rmse: 23209.1875 - val_loss: 695271232.0000 - val_rmse: 26367.9961\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488502880.0000 - rmse: 22102.0996 - val_loss: 738888192.0000 - val_rmse: 27182.4980\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446048448.0000 - rmse: 21119.8574 - val_loss: 698090688.0000 - val_rmse: 26421.4062\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473726304.0000 - rmse: 21765.2539 - val_loss: 682541696.0000 - val_rmse: 26125.4980\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513776896.0000 - rmse: 22666.6465 - val_loss: 733287168.0000 - val_rmse: 27079.2754\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475232640.0000 - rmse: 21799.8301 - val_loss: 699490944.0000 - val_rmse: 26447.8887\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447340896.0000 - rmse: 21150.4336 - val_loss: 692523776.0000 - val_rmse: 26315.8457\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430028960.0000 - rmse: 20737.1387 - val_loss: 700493568.0000 - val_rmse: 26466.8398\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505711040.0000 - rmse: 22488.0195 - val_loss: 721784000.0000 - val_rmse: 26866.0371\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430284544.0000 - rmse: 20743.2988 - val_loss: 792915008.0000 - val_rmse: 28158.7461\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 496650816.0000 - rmse: 22285.6621 - val_loss: 696559296.0000 - val_rmse: 26392.4102\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419153152.0000 - rmse: 20473.2305 - val_loss: 743019392.0000 - val_rmse: 27258.3828\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418935136.0000 - rmse: 20467.9043 - val_loss: 733482688.0000 - val_rmse: 27082.8848\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512278080.0000 - rmse: 22633.5605 - val_loss: 719361280.0000 - val_rmse: 26820.9082\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451763488.0000 - rmse: 21254.7285 - val_loss: 701139904.0000 - val_rmse: 26479.0449\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478318624.0000 - rmse: 21870.4961 - val_loss: 681699072.0000 - val_rmse: 26109.3672\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446639104.0000 - rmse: 21133.8359 - val_loss: 714665984.0000 - val_rmse: 26733.2383\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473324096.0000 - rmse: 21756.0137 - val_loss: 779915712.0000 - val_rmse: 27926.9707\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456359616.0000 - rmse: 21362.5742 - val_loss: 720323456.0000 - val_rmse: 26838.8398\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423050816.0000 - rmse: 20568.1992 - val_loss: 705274752.0000 - val_rmse: 26557.0098\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 444684576.0000 - rmse: 21087.5449 - val_loss: 697754240.0000 - val_rmse: 26415.0391\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489021824.0000 - rmse: 22113.8379 - val_loss: 709257088.0000 - val_rmse: 26631.8809\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470334144.0000 - rmse: 21687.1855 - val_loss: 693246464.0000 - val_rmse: 26329.5723\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428279584.0000 - rmse: 20694.9160 - val_loss: 695172544.0000 - val_rmse: 26366.1250\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 408091136.0000 - rmse: 20201.2656 - val_loss: 720451520.0000 - val_rmse: 26841.2285\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 408662624.0000 - rmse: 20215.4043 - val_loss: 731853824.0000 - val_rmse: 27052.7949\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 454127328.0000 - rmse: 21310.2617 - val_loss: 728711232.0000 - val_rmse: 26994.6504\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405267872.0000 - rmse: 20131.2637 - val_loss: 739829888.0000 - val_rmse: 27199.8145\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 444904448.0000 - rmse: 21092.7559 - val_loss: 726802624.0000 - val_rmse: 26959.2773\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483470816.0000 - rmse: 21987.9688 - val_loss: 739888832.0000 - val_rmse: 27200.8984\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439625696.0000 - rmse: 20967.2520 - val_loss: 742148800.0000 - val_rmse: 27242.4082\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 472772640.0000 - rmse: 21743.3359 - val_loss: 783876032.0000 - val_rmse: 27997.7852\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462855584.0000 - rmse: 21514.0781 - val_loss: 735840320.0000 - val_rmse: 27126.3770\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363494144.0000 - rmse: 19065.5215 - val_loss: 739907264.0000 - val_rmse: 27201.2363\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 404355584.0000 - rmse: 20108.5938 - val_loss: 742803136.0000 - val_rmse: 27254.4141\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385143008.0000 - rmse: 19625.0605 - val_loss: 745062144.0000 - val_rmse: 27295.8262\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402226144.0000 - rmse: 20055.5762 - val_loss: 725783360.0000 - val_rmse: 26940.3652\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411532608.0000 - rmse: 20286.2656 - val_loss: 694249600.0000 - val_rmse: 26348.6172\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 448093472.0000 - rmse: 21168.2168 - val_loss: 709239936.0000 - val_rmse: 26631.5586\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451314912.0000 - rmse: 21244.1719 - val_loss: 695435072.0000 - val_rmse: 26371.1035\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 391911744.0000 - rmse: 19796.7598 - val_loss: 689571136.0000 - val_rmse: 26259.6855\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 395165600.0000 - rmse: 19878.7715 - val_loss: 714608000.0000 - val_rmse: 26732.1523\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 399266784.0000 - rmse: 19981.6621 - val_loss: 727350592.0000 - val_rmse: 26969.4375\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420194112.0000 - rmse: 20498.6367 - val_loss: 715984960.0000 - val_rmse: 26757.8945\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410650176.0000 - rmse: 20264.5039 - val_loss: 723039360.0000 - val_rmse: 26889.3906\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428606816.0000 - rmse: 20702.8223 - val_loss: 697713088.0000 - val_rmse: 26414.2578\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447948448.0000 - rmse: 21164.7910 - val_loss: 717590016.0000 - val_rmse: 26787.8711\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427638304.0000 - rmse: 20679.4160 - val_loss: 741141632.0000 - val_rmse: 27223.9160\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353889920.0000 - rmse: 18811.9609 - val_loss: 725444672.0000 - val_rmse: 26934.0801\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375430752.0000 - rmse: 19376.0352 - val_loss: 719542656.0000 - val_rmse: 26824.2930\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450245504.0000 - rmse: 21218.9883 - val_loss: 751396352.0000 - val_rmse: 27411.6094\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355430432.0000 - rmse: 18852.8633 - val_loss: 705983744.0000 - val_rmse: 26570.3555\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416663168.0000 - rmse: 20412.3281 - val_loss: 704756096.0000 - val_rmse: 26547.2422\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358329696.0000 - rmse: 18929.5977 - val_loss: 766928960.0000 - val_rmse: 27693.4824\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403299328.0000 - rmse: 20082.3125 - val_loss: 711652864.0000 - val_rmse: 26676.8223\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 423215136.0000 - rmse: 20572.1934 - val_loss: 732299904.0000 - val_rmse: 27061.0410\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402457984.0000 - rmse: 20061.3535 - val_loss: 732789632.0000 - val_rmse: 27070.0879\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386901728.0000 - rmse: 19669.8184 - val_loss: 755978944.0000 - val_rmse: 27495.0703\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405957920.0000 - rmse: 20148.3965 - val_loss: 692356864.0000 - val_rmse: 26312.6738\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413293536.0000 - rmse: 20329.6230 - val_loss: 738302080.0000 - val_rmse: 27171.7129\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388530688.0000 - rmse: 19711.1816 - val_loss: 767207040.0000 - val_rmse: 27698.5000\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370298304.0000 - rmse: 19243.1348 - val_loss: 685560640.0000 - val_rmse: 26183.2129\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 355900064.0000 - rmse: 18865.3145 - val_loss: 703283136.0000 - val_rmse: 26519.4844\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 387562880.0000 - rmse: 19686.6172 - val_loss: 706303424.0000 - val_rmse: 26576.3672\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 421303776.0000 - rmse: 20525.6855 - val_loss: 707651712.0000 - val_rmse: 26601.7227\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 405427616.0000 - rmse: 20135.2324 - val_loss: 694592640.0000 - val_rmse: 26355.1230\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 353577920.0000 - rmse: 18803.6660 - val_loss: 738093760.0000 - val_rmse: 27167.8809\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 389141024.0000 - rmse: 19726.6582 - val_loss: 698593344.0000 - val_rmse: 26430.9160\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 366478208.0000 - rmse: 19143.6172 - val_loss: 711943680.0000 - val_rmse: 26682.2734\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377700768.0000 - rmse: 19434.5254 - val_loss: 680706560.0000 - val_rmse: 26090.3535\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 366973312.0000 - rmse: 19156.5469 - val_loss: 698374912.0000 - val_rmse: 26426.7832\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362264672.0000 - rmse: 19033.2500 - val_loss: 677023936.0000 - val_rmse: 26019.6816\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344686912.0000 - rmse: 18565.7441 - val_loss: 705794624.0000 - val_rmse: 26566.7949\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336872928.0000 - rmse: 18354.0977 - val_loss: 777239168.0000 - val_rmse: 27879.0098\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381674208.0000 - rmse: 19536.4824 - val_loss: 753741824.0000 - val_rmse: 27454.3594\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364201120.0000 - rmse: 19084.0527 - val_loss: 691353600.0000 - val_rmse: 26293.6035\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380356736.0000 - rmse: 19502.7363 - val_loss: 679486016.0000 - val_rmse: 26066.9512\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388922208.0000 - rmse: 19721.1113 - val_loss: 719919232.0000 - val_rmse: 26831.3086\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381564704.0000 - rmse: 19533.6797 - val_loss: 699519360.0000 - val_rmse: 26448.4277\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364364448.0000 - rmse: 19088.3320 - val_loss: 677176128.0000 - val_rmse: 26022.6074\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355461088.0000 - rmse: 18853.6758 - val_loss: 663614656.0000 - val_rmse: 25760.7188\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359970080.0000 - rmse: 18972.8770 - val_loss: 739662592.0000 - val_rmse: 27196.7383\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398015104.0000 - rmse: 19950.3164 - val_loss: 662488768.0000 - val_rmse: 25738.8555\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314929504.0000 - rmse: 17746.2500 - val_loss: 716073408.0000 - val_rmse: 26759.5488\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377225056.0000 - rmse: 19422.2812 - val_loss: 692779840.0000 - val_rmse: 26320.7109\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307227296.0000 - rmse: 17527.9004 - val_loss: 670597248.0000 - val_rmse: 25895.8926\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323379200.0000 - rmse: 17982.7461 - val_loss: 680813248.0000 - val_rmse: 26092.3965\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304668672.0000 - rmse: 17454.7598 - val_loss: 727358336.0000 - val_rmse: 26969.5820\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373431040.0000 - rmse: 19324.3633 - val_loss: 675918464.0000 - val_rmse: 25998.4316\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 435735072.0000 - rmse: 20874.2676 - val_loss: 683459840.0000 - val_rmse: 26143.0625\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353074048.0000 - rmse: 18790.2656 - val_loss: 663407744.0000 - val_rmse: 25756.7031\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 375621056.0000 - rmse: 19380.9434 - val_loss: 656299904.0000 - val_rmse: 25618.3516\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 285971072.0000 - rmse: 16910.6777 - val_loss: 649218688.0000 - val_rmse: 25479.7676\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299805760.0000 - rmse: 17314.9004 - val_loss: 669744768.0000 - val_rmse: 25879.4258\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 397630144.0000 - rmse: 19940.6641 - val_loss: 672756672.0000 - val_rmse: 25937.5527\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 401039200.0000 - rmse: 20025.9629 - val_loss: 688586880.0000 - val_rmse: 26240.9395\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304015648.0000 - rmse: 17436.0430 - val_loss: 713054016.0000 - val_rmse: 26703.0703\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341567104.0000 - rmse: 18481.5332 - val_loss: 741168000.0000 - val_rmse: 27224.4004\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326219136.0000 - rmse: 18061.5371 - val_loss: 685825984.0000 - val_rmse: 26188.2773\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288300640.0000 - rmse: 16979.4160 - val_loss: 667697472.0000 - val_rmse: 25839.8418\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305170176.0000 - rmse: 17469.1191 - val_loss: 723289984.0000 - val_rmse: 26894.0508\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342367360.0000 - rmse: 18503.1699 - val_loss: 798534144.0000 - val_rmse: 28258.3457\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258323952.0000 - rmse: 16072.4580 - val_loss: 656978880.0000 - val_rmse: 25631.5977\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356120096.0000 - rmse: 18871.1426 - val_loss: 686531008.0000 - val_rmse: 26201.7363\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323056512.0000 - rmse: 17973.7715 - val_loss: 702802560.0000 - val_rmse: 26510.4238\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345632128.0000 - rmse: 18591.1816 - val_loss: 741104320.0000 - val_rmse: 27223.2305\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302767328.0000 - rmse: 17400.2090 - val_loss: 699055424.0000 - val_rmse: 26439.6543\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485352864.0000 - rmse: 22030.7246 - val_loss: 687676096.0000 - val_rmse: 26223.5781\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335923552.0000 - rmse: 18328.2168 - val_loss: 644401536.0000 - val_rmse: 25385.0645\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320884576.0000 - rmse: 17913.2500 - val_loss: 647053248.0000 - val_rmse: 25437.2422\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269787232.0000 - rmse: 16425.1992 - val_loss: 665503808.0000 - val_rmse: 25797.3574\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303169792.0000 - rmse: 17411.7695 - val_loss: 645526464.0000 - val_rmse: 25407.2129\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372555552.0000 - rmse: 19301.6973 - val_loss: 652388224.0000 - val_rmse: 25541.8906\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307594112.0000 - rmse: 17538.3594 - val_loss: 695411200.0000 - val_rmse: 26370.6484\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335438112.0000 - rmse: 18314.9688 - val_loss: 654174912.0000 - val_rmse: 25576.8438\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344784640.0000 - rmse: 18568.3770 - val_loss: 679419072.0000 - val_rmse: 26065.6680\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 263671312.0000 - rmse: 16237.9590 - val_loss: 734520192.0000 - val_rmse: 27102.0332\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 289547872.0000 - rmse: 17016.1055 - val_loss: 678473216.0000 - val_rmse: 26047.5176\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297970592.0000 - rmse: 17261.8223 - val_loss: 666401408.0000 - val_rmse: 25814.7500\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356747072.0000 - rmse: 18887.7480 - val_loss: 674894784.0000 - val_rmse: 25978.7363\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 326722560.0000 - rmse: 18075.4668 - val_loss: 664406656.0000 - val_rmse: 25776.0859\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 314285024.0000 - rmse: 17728.0840 - val_loss: 657309120.0000 - val_rmse: 25638.0410\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 302241760.0000 - rmse: 17385.1016 - val_loss: 663163136.0000 - val_rmse: 25751.9531\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257349456.0000 - rmse: 16042.1133 - val_loss: 687433088.0000 - val_rmse: 26218.9434\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 307483328.0000 - rmse: 17535.2012 - val_loss: 636438400.0000 - val_rmse: 25227.7305\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286757920.0000 - rmse: 16933.9277 - val_loss: 663817728.0000 - val_rmse: 25764.6602\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275249984.0000 - rmse: 16590.6582 - val_loss: 679351872.0000 - val_rmse: 26064.3789\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312339648.0000 - rmse: 17673.1328 - val_loss: 633515328.0000 - val_rmse: 25169.7285\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258226160.0000 - rmse: 16069.4160 - val_loss: 725398720.0000 - val_rmse: 26933.2266\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364013216.0000 - rmse: 19079.1289 - val_loss: 699999296.0000 - val_rmse: 26457.4980\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299437024.0000 - rmse: 17304.2480 - val_loss: 697661568.0000 - val_rmse: 26413.2832\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 300304160.0000 - rmse: 17329.2852 - val_loss: 668217472.0000 - val_rmse: 25849.9023\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279020992.0000 - rmse: 16703.9180 - val_loss: 688487296.0000 - val_rmse: 26239.0410\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 273439488.0000 - rmse: 16536.0039 - val_loss: 651116928.0000 - val_rmse: 25516.9922\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268926752.0000 - rmse: 16398.9844 - val_loss: 661112384.0000 - val_rmse: 25712.1055\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404710304.0000 - rmse: 20117.4121 - val_loss: 709229312.0000 - val_rmse: 26631.3594\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351724096.0000 - rmse: 18754.3066 - val_loss: 679537664.0000 - val_rmse: 26067.9414\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324588288.0000 - rmse: 18016.3340 - val_loss: 648425408.0000 - val_rmse: 25464.1992\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251808352.0000 - rmse: 15868.4688 - val_loss: 756100032.0000 - val_rmse: 27497.2715\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299220992.0000 - rmse: 17298.0039 - val_loss: 687801728.0000 - val_rmse: 26225.9746\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361824768.0000 - rmse: 19021.6914 - val_loss: 640595840.0000 - val_rmse: 25309.9941\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288968288.0000 - rmse: 16999.0664 - val_loss: 675434432.0000 - val_rmse: 25989.1191\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273994912.0000 - rmse: 16552.7910 - val_loss: 727515136.0000 - val_rmse: 26972.4883\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293828320.0000 - rmse: 17141.4199 - val_loss: 659844288.0000 - val_rmse: 25687.4336\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286753152.0000 - rmse: 16933.7871 - val_loss: 716612608.0000 - val_rmse: 26769.6191\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295426272.0000 - rmse: 17187.9688 - val_loss: 672724992.0000 - val_rmse: 25936.9414\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258859440.0000 - rmse: 16089.1084 - val_loss: 684461888.0000 - val_rmse: 26162.2227\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309667072.0000 - rmse: 17597.3574 - val_loss: 680933440.0000 - val_rmse: 26094.7012\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292770528.0000 - rmse: 17110.5371 - val_loss: 622426496.0000 - val_rmse: 24948.4766\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280686400.0000 - rmse: 16753.6953 - val_loss: 685272960.0000 - val_rmse: 26177.7168\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280402112.0000 - rmse: 16745.2109 - val_loss: 670820352.0000 - val_rmse: 25900.1992\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313589568.0000 - rmse: 17708.4609 - val_loss: 632460096.0000 - val_rmse: 25148.7598\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308348512.0000 - rmse: 17559.8535 - val_loss: 677707520.0000 - val_rmse: 26032.8145\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252707056.0000 - rmse: 15896.7627 - val_loss: 781758592.0000 - val_rmse: 27959.9453\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272912640.0000 - rmse: 16520.0664 - val_loss: 723233408.0000 - val_rmse: 26892.9980\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292290624.0000 - rmse: 17096.5098 - val_loss: 719289344.0000 - val_rmse: 26819.5703\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243226720.0000 - rmse: 15595.7275 - val_loss: 657647104.0000 - val_rmse: 25644.6309\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267918528.0000 - rmse: 16368.2158 - val_loss: 669715712.0000 - val_rmse: 25878.8652\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326712992.0000 - rmse: 18075.2012 - val_loss: 688470720.0000 - val_rmse: 26238.7227\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279732672.0000 - rmse: 16725.2090 - val_loss: 641509888.0000 - val_rmse: 25328.0449\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265242928.0000 - rmse: 16286.2793 - val_loss: 692943232.0000 - val_rmse: 26323.8145\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250103776.0000 - rmse: 15814.6680 - val_loss: 710112256.0000 - val_rmse: 26647.9297\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256936976.0000 - rmse: 16029.2520 - val_loss: 733145088.0000 - val_rmse: 27076.6523\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298030496.0000 - rmse: 17263.5586 - val_loss: 666073152.0000 - val_rmse: 25808.3926\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270265536.0000 - rmse: 16439.7539 - val_loss: 677533312.0000 - val_rmse: 26029.4707\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287398656.0000 - rmse: 16952.8359 - val_loss: 630680768.0000 - val_rmse: 25113.3574\n",
      "Epoch 367/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269803072.0000 - rmse: 16425.6816 - val_loss: 692543104.0000 - val_rmse: 26316.2109\n",
      "Epoch 368/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240270736.0000 - rmse: 15500.6670 - val_loss: 698625088.0000 - val_rmse: 26431.5156\n",
      "Epoch 369/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245212048.0000 - rmse: 15659.2471 - val_loss: 661095296.0000 - val_rmse: 25711.7734\n",
      "Epoch 370/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266079376.0000 - rmse: 16311.9385 - val_loss: 741113408.0000 - val_rmse: 27223.3965\n",
      "Epoch 371/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270872320.0000 - rmse: 16458.1953 - val_loss: 627514112.0000 - val_rmse: 25050.2324\n",
      "Epoch 372/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277654560.0000 - rmse: 16662.9668 - val_loss: 670035904.0000 - val_rmse: 25885.0508\n",
      "Epoch 373/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286908768.0000 - rmse: 16938.3809 - val_loss: 694509440.0000 - val_rmse: 26353.5469\n",
      "Epoch 374/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303827552.0000 - rmse: 17430.6484 - val_loss: 666296128.0000 - val_rmse: 25812.7109\n",
      "Epoch 375/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309672768.0000 - rmse: 17597.5195 - val_loss: 679083712.0000 - val_rmse: 26059.2344\n",
      "Epoch 376/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318928864.0000 - rmse: 17858.5781 - val_loss: 703275264.0000 - val_rmse: 26519.3359\n",
      "Epoch 377/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257219888.0000 - rmse: 16038.0742 - val_loss: 643059776.0000 - val_rmse: 25358.6211\n",
      "Epoch 378/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245369952.0000 - rmse: 15664.2881 - val_loss: 661436928.0000 - val_rmse: 25718.4141\n",
      "Epoch 379/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313016096.0000 - rmse: 17692.2598 - val_loss: 663040640.0000 - val_rmse: 25749.5742\n",
      "Epoch 380/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248797232.0000 - rmse: 15773.3037 - val_loss: 697535360.0000 - val_rmse: 26410.8945\n",
      "Epoch 381/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259220512.0000 - rmse: 16100.3252 - val_loss: 664084288.0000 - val_rmse: 25769.8320\n",
      "Epoch 382/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245897120.0000 - rmse: 15681.1045 - val_loss: 634570240.0000 - val_rmse: 25190.6777\n",
      "Epoch 383/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270578656.0000 - rmse: 16449.2734 - val_loss: 670753024.0000 - val_rmse: 25898.9004\n",
      "Epoch 384/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265497968.0000 - rmse: 16294.1055 - val_loss: 711288448.0000 - val_rmse: 26669.9883\n",
      "Epoch 385/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262832944.0000 - rmse: 16212.1221 - val_loss: 679513600.0000 - val_rmse: 26067.4785\n",
      "Epoch 386/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261597216.0000 - rmse: 16173.9648 - val_loss: 698993216.0000 - val_rmse: 26438.4805\n",
      "Epoch 387/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 231815744.0000 - rmse: 15225.4961 - val_loss: 707893504.0000 - val_rmse: 26606.2676\n",
      "Epoch 388/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217118096.0000 - rmse: 14734.9268 - val_loss: 734874240.0000 - val_rmse: 27108.5645\n",
      "Epoch 389/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 338903008.0000 - rmse: 18409.3164 - val_loss: 636666496.0000 - val_rmse: 25232.2500\n",
      "Epoch 390/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 243755968.0000 - rmse: 15612.6855 - val_loss: 674005824.0000 - val_rmse: 25961.6230\n",
      "Epoch 391/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231636400.0000 - rmse: 15219.6045 - val_loss: 726977152.0000 - val_rmse: 26962.5117\n",
      "Epoch 392/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 211292960.0000 - rmse: 14535.9170 - val_loss: 636924160.0000 - val_rmse: 25237.3555\n",
      "Epoch 393/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253784304.0000 - rmse: 15930.6074 - val_loss: 698400832.0000 - val_rmse: 26427.2734\n",
      "Epoch 394/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281243040.0000 - rmse: 16770.3008 - val_loss: 670567104.0000 - val_rmse: 25895.3086\n",
      "Epoch 395/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 254932176.0000 - rmse: 15966.5938 - val_loss: 724825024.0000 - val_rmse: 26922.5742\n",
      "Epoch 396/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 208852288.0000 - rmse: 14451.7217 - val_loss: 836191744.0000 - val_rmse: 28916.9805\n",
      "Epoch 397/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247490256.0000 - rmse: 15731.8223 - val_loss: 693707968.0000 - val_rmse: 26338.3340\n",
      "Epoch 398/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 269408096.0000 - rmse: 16413.6543 - val_loss: 655054592.0000 - val_rmse: 25594.0352\n",
      "Epoch 399/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240944608.0000 - rmse: 15522.3887 - val_loss: 704612160.0000 - val_rmse: 26544.5312\n",
      "Epoch 400/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217221072.0000 - rmse: 14738.4219 - val_loss: 679277440.0000 - val_rmse: 26062.9512\n",
      "104/104 [==============================] - 0s 634us/step - loss: 671667136.0000 - rmse: 25916.5371\n",
      "[671667136.0, 25916.537109375]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 22844540928.0000 - rmse: 151144.1094 - val_loss: 12772165632.0000 - val_rmse: 113014.0078\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 4534493696.0000 - rmse: 67338.6484 - val_loss: 2207866880.0000 - val_rmse: 46987.9453\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2145467136.0000 - rmse: 46319.1875 - val_loss: 1687322624.0000 - val_rmse: 41077.0312\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1801194240.0000 - rmse: 42440.4805 - val_loss: 1441167872.0000 - val_rmse: 37962.7188\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1616021376.0000 - rmse: 40199.7695 - val_loss: 1321228800.0000 - val_rmse: 36348.7109\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1454697088.0000 - rmse: 38140.4922 - val_loss: 1293755136.0000 - val_rmse: 35968.8086\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1442960896.0000 - rmse: 37986.3242 - val_loss: 1185501568.0000 - val_rmse: 34431.1133\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1366707840.0000 - rmse: 36969.0117 - val_loss: 1104337536.0000 - val_rmse: 33231.5781\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1330871296.0000 - rmse: 36481.1094 - val_loss: 1062021888.0000 - val_rmse: 32588.6777\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1255434624.0000 - rmse: 35432.1133 - val_loss: 1050071360.0000 - val_rmse: 32404.8047\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1208781696.0000 - rmse: 34767.5391 - val_loss: 1019460160.0000 - val_rmse: 31928.9863\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1220072704.0000 - rmse: 34929.5391 - val_loss: 1043651072.0000 - val_rmse: 32305.5879\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1184709760.0000 - rmse: 34419.6133 - val_loss: 967136512.0000 - val_rmse: 31098.8184\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1135544576.0000 - rmse: 33697.8438 - val_loss: 948580672.0000 - val_rmse: 30799.0371\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1126249216.0000 - rmse: 33559.6367 - val_loss: 943290752.0000 - val_rmse: 30713.0410\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1103388928.0000 - rmse: 33217.2969 - val_loss: 923870272.0000 - val_rmse: 30395.2344\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1088755072.0000 - rmse: 32996.2891 - val_loss: 937203008.0000 - val_rmse: 30613.7715\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1097273600.0000 - rmse: 33125.1211 - val_loss: 912970048.0000 - val_rmse: 30215.3945\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1060863104.0000 - rmse: 32570.8926 - val_loss: 910382592.0000 - val_rmse: 30172.5469\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1031605312.0000 - rmse: 32118.6133 - val_loss: 939475328.0000 - val_rmse: 30650.8613\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1020084096.0000 - rmse: 31938.7559 - val_loss: 895008768.0000 - val_rmse: 29916.6973\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1079179648.0000 - rmse: 32850.8672 - val_loss: 889652928.0000 - val_rmse: 29827.0508\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1004257344.0000 - rmse: 31690.0195 - val_loss: 901487552.0000 - val_rmse: 30024.7832\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1025144960.0000 - rmse: 32017.8848 - val_loss: 896738432.0000 - val_rmse: 29945.5918\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1021955328.0000 - rmse: 31968.0352 - val_loss: 920161984.0000 - val_rmse: 30334.1719\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1019150080.0000 - rmse: 31924.1309 - val_loss: 981077824.0000 - val_rmse: 31322.1621\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 972032832.0000 - rmse: 31177.4414 - val_loss: 890910848.0000 - val_rmse: 29848.1289\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 972354944.0000 - rmse: 31182.6055 - val_loss: 982055232.0000 - val_rmse: 31337.7598\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 942029760.0000 - rmse: 30692.5020 - val_loss: 910924416.0000 - val_rmse: 30181.5254\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 964428160.0000 - rmse: 31055.2441 - val_loss: 893744064.0000 - val_rmse: 29895.5527\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 943218496.0000 - rmse: 30711.8633 - val_loss: 919349504.0000 - val_rmse: 30320.7773\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 989637376.0000 - rmse: 31458.5020 - val_loss: 962814720.0000 - val_rmse: 31029.2559\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 982864960.0000 - rmse: 31350.6777 - val_loss: 877638144.0000 - val_rmse: 29624.9590\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 949871552.0000 - rmse: 30819.9863 - val_loss: 888637376.0000 - val_rmse: 29810.0215\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 987997696.0000 - rmse: 31432.4297 - val_loss: 865152128.0000 - val_rmse: 29413.4688\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 935567360.0000 - rmse: 30587.0449 - val_loss: 861897472.0000 - val_rmse: 29358.0898\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 934054336.0000 - rmse: 30562.3027 - val_loss: 904274048.0000 - val_rmse: 30071.1504\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 954562816.0000 - rmse: 30896.0000 - val_loss: 864072064.0000 - val_rmse: 29395.1035\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 888412928.0000 - rmse: 29806.2559 - val_loss: 912675840.0000 - val_rmse: 30210.5254\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 914738432.0000 - rmse: 30244.6426 - val_loss: 895388608.0000 - val_rmse: 29923.0449\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 909017280.0000 - rmse: 30149.9141 - val_loss: 839561152.0000 - val_rmse: 28975.1816\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 946438528.0000 - rmse: 30764.2402 - val_loss: 850494080.0000 - val_rmse: 29163.2324\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 955492032.0000 - rmse: 30911.0332 - val_loss: 857270912.0000 - val_rmse: 29279.1895\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 951823552.0000 - rmse: 30851.6387 - val_loss: 835179648.0000 - val_rmse: 28899.4746\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 892957120.0000 - rmse: 29882.3887 - val_loss: 835621376.0000 - val_rmse: 28907.1172\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 901402304.0000 - rmse: 30023.3633 - val_loss: 823478208.0000 - val_rmse: 28696.3105\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 852478976.0000 - rmse: 29197.2422 - val_loss: 824504384.0000 - val_rmse: 28714.1836\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 885761536.0000 - rmse: 29761.7461 - val_loss: 828140352.0000 - val_rmse: 28777.4277\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 928476224.0000 - rmse: 30470.9082 - val_loss: 849108096.0000 - val_rmse: 29139.4590\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 844841408.0000 - rmse: 29066.1562 - val_loss: 841885056.0000 - val_rmse: 29015.2559\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 833768640.0000 - rmse: 28875.0527 - val_loss: 863619456.0000 - val_rmse: 29387.4004\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 867264384.0000 - rmse: 29449.3535 - val_loss: 945521984.0000 - val_rmse: 30749.3418\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 822875968.0000 - rmse: 28685.8145 - val_loss: 846927360.0000 - val_rmse: 29102.0156\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 830275904.0000 - rmse: 28814.5078 - val_loss: 873239488.0000 - val_rmse: 29550.6230\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 809009088.0000 - rmse: 28443.0859 - val_loss: 826576256.0000 - val_rmse: 28750.2383\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 934254272.0000 - rmse: 30565.5742 - val_loss: 868601472.0000 - val_rmse: 29472.0449\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 831698368.0000 - rmse: 28839.1816 - val_loss: 838328704.0000 - val_rmse: 28953.9062\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 863302912.0000 - rmse: 29382.0176 - val_loss: 906267264.0000 - val_rmse: 30104.2734\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 866099904.0000 - rmse: 29429.5762 - val_loss: 911537472.0000 - val_rmse: 30191.6797\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 840321664.0000 - rmse: 28988.3027 - val_loss: 891856512.0000 - val_rmse: 29863.9668\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 817953216.0000 - rmse: 28599.8809 - val_loss: 854455936.0000 - val_rmse: 29231.0781\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 876608064.0000 - rmse: 29607.5684 - val_loss: 898126400.0000 - val_rmse: 29968.7578\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 835284800.0000 - rmse: 28901.2949 - val_loss: 838807872.0000 - val_rmse: 28962.1797\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 851957696.0000 - rmse: 29188.3125 - val_loss: 823343808.0000 - val_rmse: 28693.9688\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 827062656.0000 - rmse: 28758.6973 - val_loss: 864092992.0000 - val_rmse: 29395.4590\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 800511552.0000 - rmse: 28293.3125 - val_loss: 851945792.0000 - val_rmse: 29188.1113\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 848459776.0000 - rmse: 29128.3320 - val_loss: 837086912.0000 - val_rmse: 28932.4551\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 804101120.0000 - rmse: 28356.6777 - val_loss: 824297728.0000 - val_rmse: 28710.5859\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 790706752.0000 - rmse: 28119.5078 - val_loss: 877938432.0000 - val_rmse: 29630.0254\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 765606912.0000 - rmse: 27669.6035 - val_loss: 875396864.0000 - val_rmse: 29587.1035\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 772311360.0000 - rmse: 27790.4902 - val_loss: 892973760.0000 - val_rmse: 29882.6660\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 802425536.0000 - rmse: 28327.1172 - val_loss: 884933248.0000 - val_rmse: 29747.8281\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 732971968.0000 - rmse: 27073.4551 - val_loss: 867937536.0000 - val_rmse: 29460.7793\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 798790400.0000 - rmse: 28262.8809 - val_loss: 857770304.0000 - val_rmse: 29287.7168\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 822883520.0000 - rmse: 28685.9473 - val_loss: 915646848.0000 - val_rmse: 30259.6562\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 745718016.0000 - rmse: 27307.8379 - val_loss: 887241600.0000 - val_rmse: 29786.5996\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 824828416.0000 - rmse: 28719.8262 - val_loss: 858629888.0000 - val_rmse: 29302.3867\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 833036160.0000 - rmse: 28862.3652 - val_loss: 848938176.0000 - val_rmse: 29136.5430\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 763873728.0000 - rmse: 27638.2656 - val_loss: 894673152.0000 - val_rmse: 29911.0879\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 765165120.0000 - rmse: 27661.6191 - val_loss: 825654080.0000 - val_rmse: 28734.1973\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 798023936.0000 - rmse: 28249.3164 - val_loss: 904654080.0000 - val_rmse: 30077.4688\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 701196480.0000 - rmse: 26480.1152 - val_loss: 894151168.0000 - val_rmse: 29902.3613\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 720472000.0000 - rmse: 26841.6094 - val_loss: 851815232.0000 - val_rmse: 29185.8730\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 775612480.0000 - rmse: 27849.8203 - val_loss: 811679744.0000 - val_rmse: 28489.9941\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 731818752.0000 - rmse: 27052.1484 - val_loss: 867076864.0000 - val_rmse: 29446.1680\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 777326528.0000 - rmse: 27880.5762 - val_loss: 861352832.0000 - val_rmse: 29348.8125\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 779395584.0000 - rmse: 27917.6562 - val_loss: 844025216.0000 - val_rmse: 29052.1113\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 744432896.0000 - rmse: 27284.2969 - val_loss: 914950400.0000 - val_rmse: 30248.1465\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 727618624.0000 - rmse: 26974.4062 - val_loss: 824192704.0000 - val_rmse: 28708.7559\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 742393344.0000 - rmse: 27246.8945 - val_loss: 822377024.0000 - val_rmse: 28677.1152\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 795130944.0000 - rmse: 28198.0664 - val_loss: 833968512.0000 - val_rmse: 28878.5137\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 727800064.0000 - rmse: 26977.7695 - val_loss: 820914432.0000 - val_rmse: 28651.6035\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 745975872.0000 - rmse: 27312.5586 - val_loss: 832083584.0000 - val_rmse: 28845.8594\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 627633664.0000 - rmse: 25052.6172 - val_loss: 821956032.0000 - val_rmse: 28669.7754\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 736996096.0000 - rmse: 27147.6719 - val_loss: 825269248.0000 - val_rmse: 28727.4980\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751743040.0000 - rmse: 27417.9297 - val_loss: 865075712.0000 - val_rmse: 29412.1699\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 716742272.0000 - rmse: 26772.0430 - val_loss: 862920896.0000 - val_rmse: 29375.5156\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 659876608.0000 - rmse: 25688.0645 - val_loss: 849572992.0000 - val_rmse: 29147.4355\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 710260608.0000 - rmse: 26650.7148 - val_loss: 868732288.0000 - val_rmse: 29474.2656\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 670466432.0000 - rmse: 25893.3672 - val_loss: 866095744.0000 - val_rmse: 29429.5020\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 663321408.0000 - rmse: 25755.0254 - val_loss: 869593728.0000 - val_rmse: 29488.8730\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 689432000.0000 - rmse: 26257.0371 - val_loss: 918149056.0000 - val_rmse: 30300.9746\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656157312.0000 - rmse: 25615.5684 - val_loss: 885834752.0000 - val_rmse: 29762.9766\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 686794688.0000 - rmse: 26206.7676 - val_loss: 885334272.0000 - val_rmse: 29754.5664\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698429440.0000 - rmse: 26427.8164 - val_loss: 939255360.0000 - val_rmse: 30647.2734\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698828672.0000 - rmse: 26435.3672 - val_loss: 908181760.0000 - val_rmse: 30136.0547\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649607232.0000 - rmse: 25487.3945 - val_loss: 862478464.0000 - val_rmse: 29367.9844\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640041152.0000 - rmse: 25299.0352 - val_loss: 876621184.0000 - val_rmse: 29607.7891\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684549376.0000 - rmse: 26163.8945 - val_loss: 912469184.0000 - val_rmse: 30207.1055\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 667491904.0000 - rmse: 25835.8652 - val_loss: 903270720.0000 - val_rmse: 30054.4629\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629222272.0000 - rmse: 25084.3027 - val_loss: 932070720.0000 - val_rmse: 30529.8340\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601145600.0000 - rmse: 24518.2715 - val_loss: 938004544.0000 - val_rmse: 30626.8594\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 600878144.0000 - rmse: 24512.8164 - val_loss: 927805120.0000 - val_rmse: 30459.8945\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614370880.0000 - rmse: 24786.5059 - val_loss: 1085557760.0000 - val_rmse: 32947.8047\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651584320.0000 - rmse: 25526.1465 - val_loss: 912490496.0000 - val_rmse: 30207.4551\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585798656.0000 - rmse: 24203.2754 - val_loss: 962929856.0000 - val_rmse: 31031.1113\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586309376.0000 - rmse: 24213.8262 - val_loss: 946282880.0000 - val_rmse: 30761.7109\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589092288.0000 - rmse: 24271.2227 - val_loss: 1135047424.0000 - val_rmse: 33690.4648\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 620814784.0000 - rmse: 24916.1543 - val_loss: 1098785280.0000 - val_rmse: 33147.9297\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 630236288.0000 - rmse: 25104.5078 - val_loss: 1110889600.0000 - val_rmse: 33330.0117\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 638063168.0000 - rmse: 25259.9121 - val_loss: 1043175744.0000 - val_rmse: 32298.2305\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 593152896.0000 - rmse: 24354.7305 - val_loss: 969258624.0000 - val_rmse: 31132.9180\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 630802496.0000 - rmse: 25115.7812 - val_loss: 1082110208.0000 - val_rmse: 32895.4453\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588042496.0000 - rmse: 24249.5879 - val_loss: 952413376.0000 - val_rmse: 30861.1953\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594636352.0000 - rmse: 24385.1660 - val_loss: 1023082048.0000 - val_rmse: 31985.6543\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620828480.0000 - rmse: 24916.4297 - val_loss: 941942208.0000 - val_rmse: 30691.0762\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 565174144.0000 - rmse: 23773.3906 - val_loss: 968411840.0000 - val_rmse: 31119.3164\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573585600.0000 - rmse: 23949.6465 - val_loss: 1018855488.0000 - val_rmse: 31919.5156\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552609728.0000 - rmse: 23507.6504 - val_loss: 1019805056.0000 - val_rmse: 31934.3867\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560632576.0000 - rmse: 23677.6797 - val_loss: 1105241088.0000 - val_rmse: 33245.1680\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586985152.0000 - rmse: 24227.7773 - val_loss: 993048064.0000 - val_rmse: 31512.6660\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572926592.0000 - rmse: 23935.8828 - val_loss: 1006289472.0000 - val_rmse: 31722.0664\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599170944.0000 - rmse: 24477.9688 - val_loss: 976254592.0000 - val_rmse: 31245.0723\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 619694912.0000 - rmse: 24893.6699 - val_loss: 934955776.0000 - val_rmse: 30577.0469\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566370496.0000 - rmse: 23798.5391 - val_loss: 956146368.0000 - val_rmse: 30921.6172\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561097152.0000 - rmse: 23687.4902 - val_loss: 1041897728.0000 - val_rmse: 32278.4414\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520023168.0000 - rmse: 22804.0156 - val_loss: 945894144.0000 - val_rmse: 30755.3926\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 512429792.0000 - rmse: 22636.9102 - val_loss: 874967872.0000 - val_rmse: 29579.8555\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559538496.0000 - rmse: 23654.5664 - val_loss: 990998784.0000 - val_rmse: 31480.1328\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584248512.0000 - rmse: 24171.2324 - val_loss: 910994752.0000 - val_rmse: 30182.6855\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525488832.0000 - rmse: 22923.5430 - val_loss: 894973440.0000 - val_rmse: 29916.1074\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533346592.0000 - rmse: 23094.2969 - val_loss: 947927616.0000 - val_rmse: 30788.4336\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516140992.0000 - rmse: 22718.7344 - val_loss: 906529792.0000 - val_rmse: 30108.6328\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 566004480.0000 - rmse: 23790.8496 - val_loss: 939731904.0000 - val_rmse: 30655.0469\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533680992.0000 - rmse: 23101.5371 - val_loss: 1020413760.0000 - val_rmse: 31943.9141\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507077664.0000 - rmse: 22518.3828 - val_loss: 960995328.0000 - val_rmse: 30999.9238\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542787264.0000 - rmse: 23297.7949 - val_loss: 944197696.0000 - val_rmse: 30727.7949\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594497088.0000 - rmse: 24382.3105 - val_loss: 922217472.0000 - val_rmse: 30368.0332\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508266048.0000 - rmse: 22544.7539 - val_loss: 918195520.0000 - val_rmse: 30301.7402\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554740224.0000 - rmse: 23552.9238 - val_loss: 964437248.0000 - val_rmse: 31055.3887\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492233376.0000 - rmse: 22186.3340 - val_loss: 1049057344.0000 - val_rmse: 32389.1543\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508375008.0000 - rmse: 22547.1738 - val_loss: 857839360.0000 - val_rmse: 29288.8945\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503018816.0000 - rmse: 22428.0801 - val_loss: 930570624.0000 - val_rmse: 30505.2559\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498795040.0000 - rmse: 22333.7188 - val_loss: 1003823232.0000 - val_rmse: 31683.1699\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494965152.0000 - rmse: 22247.8125 - val_loss: 1000913792.0000 - val_rmse: 31637.2188\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569514624.0000 - rmse: 23864.5059 - val_loss: 979555328.0000 - val_rmse: 31297.8477\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465455392.0000 - rmse: 21574.4141 - val_loss: 946646144.0000 - val_rmse: 30767.6152\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573587840.0000 - rmse: 23949.6934 - val_loss: 966359872.0000 - val_rmse: 31086.3301\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516145696.0000 - rmse: 22718.8379 - val_loss: 917536512.0000 - val_rmse: 30290.8652\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517366016.0000 - rmse: 22745.6797 - val_loss: 971818688.0000 - val_rmse: 31174.0059\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519093472.0000 - rmse: 22783.6211 - val_loss: 913902080.0000 - val_rmse: 30230.8125\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509151712.0000 - rmse: 22564.3887 - val_loss: 1001205760.0000 - val_rmse: 31641.8359\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467205216.0000 - rmse: 21614.9297 - val_loss: 1036833728.0000 - val_rmse: 32199.9023\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461052864.0000 - rmse: 21472.1406 - val_loss: 1039181440.0000 - val_rmse: 32236.3379\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538017472.0000 - rmse: 23195.2012 - val_loss: 1009182720.0000 - val_rmse: 31767.6367\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487217888.0000 - rmse: 22073.0117 - val_loss: 922633216.0000 - val_rmse: 30374.8789\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466827040.0000 - rmse: 21606.1777 - val_loss: 1086071040.0000 - val_rmse: 32955.5938\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467703232.0000 - rmse: 21626.4453 - val_loss: 1002965504.0000 - val_rmse: 31669.6309\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463013728.0000 - rmse: 21517.7520 - val_loss: 951259648.0000 - val_rmse: 30842.4980\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479126528.0000 - rmse: 21888.9590 - val_loss: 924082944.0000 - val_rmse: 30398.7324\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501885760.0000 - rmse: 22402.8066 - val_loss: 955016256.0000 - val_rmse: 30903.3359\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463607104.0000 - rmse: 21531.5352 - val_loss: 899438784.0000 - val_rmse: 29990.6445\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456991136.0000 - rmse: 21377.3516 - val_loss: 932543872.0000 - val_rmse: 30537.5820\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481729024.0000 - rmse: 21948.3262 - val_loss: 982034880.0000 - val_rmse: 31337.4355\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463223104.0000 - rmse: 21522.6191 - val_loss: 1095779840.0000 - val_rmse: 33102.5664\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480299424.0000 - rmse: 21915.7324 - val_loss: 1070498944.0000 - val_rmse: 32718.4785\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427664672.0000 - rmse: 20680.0547 - val_loss: 960239872.0000 - val_rmse: 30987.7363\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466573600.0000 - rmse: 21600.3145 - val_loss: 983147008.0000 - val_rmse: 31355.1758\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477488320.0000 - rmse: 21851.5059 - val_loss: 991505728.0000 - val_rmse: 31488.1836\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481427808.0000 - rmse: 21941.4609 - val_loss: 1084459392.0000 - val_rmse: 32931.1328\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452064608.0000 - rmse: 21261.8105 - val_loss: 985676416.0000 - val_rmse: 31395.4844\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469584608.0000 - rmse: 21669.9004 - val_loss: 1007446720.0000 - val_rmse: 31740.3008\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489042624.0000 - rmse: 22114.3066 - val_loss: 1021127296.0000 - val_rmse: 31955.0820\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505765312.0000 - rmse: 22489.2266 - val_loss: 1060182976.0000 - val_rmse: 32560.4512\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438669184.0000 - rmse: 20944.4316 - val_loss: 948962176.0000 - val_rmse: 30805.2305\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401097952.0000 - rmse: 20027.4277 - val_loss: 949706048.0000 - val_rmse: 30817.3008\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436877088.0000 - rmse: 20901.6055 - val_loss: 997426432.0000 - val_rmse: 31582.0586\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455763520.0000 - rmse: 21348.6172 - val_loss: 943855360.0000 - val_rmse: 30722.2285\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410865664.0000 - rmse: 20269.8223 - val_loss: 1007191936.0000 - val_rmse: 31736.2871\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457534176.0000 - rmse: 21390.0488 - val_loss: 960850752.0000 - val_rmse: 30997.5918\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446248640.0000 - rmse: 21124.5977 - val_loss: 1002371200.0000 - val_rmse: 31660.2461\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447787584.0000 - rmse: 21160.9922 - val_loss: 1172523136.0000 - val_rmse: 34242.1250\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399913088.0000 - rmse: 19997.8262 - val_loss: 979079616.0000 - val_rmse: 31290.2480\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461978976.0000 - rmse: 21493.6953 - val_loss: 992717440.0000 - val_rmse: 31507.4180\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390027968.0000 - rmse: 19749.1250 - val_loss: 1042155968.0000 - val_rmse: 32282.4414\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416464736.0000 - rmse: 20407.4668 - val_loss: 1074104448.0000 - val_rmse: 32773.5312\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448813344.0000 - rmse: 21185.2129 - val_loss: 959691008.0000 - val_rmse: 30978.8809\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496206784.0000 - rmse: 22275.6992 - val_loss: 981561024.0000 - val_rmse: 31329.8750\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392163136.0000 - rmse: 19803.1074 - val_loss: 998621696.0000 - val_rmse: 31600.9766\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437418816.0000 - rmse: 20914.5605 - val_loss: 1195140096.0000 - val_rmse: 34570.7969\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448815520.0000 - rmse: 21185.2656 - val_loss: 900976576.0000 - val_rmse: 30016.2715\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392563296.0000 - rmse: 19813.2090 - val_loss: 942391872.0000 - val_rmse: 30698.4023\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420890752.0000 - rmse: 20515.6211 - val_loss: 1095461504.0000 - val_rmse: 33097.7578\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406353760.0000 - rmse: 20158.2188 - val_loss: 998923456.0000 - val_rmse: 31605.7500\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395477056.0000 - rmse: 19886.6035 - val_loss: 987831872.0000 - val_rmse: 31429.7930\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433335328.0000 - rmse: 20816.7070 - val_loss: 993161152.0000 - val_rmse: 31514.4590\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405958976.0000 - rmse: 20148.4238 - val_loss: 1092437120.0000 - val_rmse: 33052.0352\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423073152.0000 - rmse: 20568.7422 - val_loss: 922382720.0000 - val_rmse: 30370.7539\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439924992.0000 - rmse: 20974.3887 - val_loss: 1055483584.0000 - val_rmse: 32488.2070\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362955520.0000 - rmse: 19051.3906 - val_loss: 918590016.0000 - val_rmse: 30308.2500\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389450976.0000 - rmse: 19734.5117 - val_loss: 1003604224.0000 - val_rmse: 31679.7129\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415240160.0000 - rmse: 20377.4434 - val_loss: 940750016.0000 - val_rmse: 30671.6484\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379592608.0000 - rmse: 19483.1348 - val_loss: 998945280.0000 - val_rmse: 31606.0957\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391354976.0000 - rmse: 19782.6934 - val_loss: 1076502912.0000 - val_rmse: 32810.1055\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408917152.0000 - rmse: 20221.6992 - val_loss: 1088078336.0000 - val_rmse: 32986.0312\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387602976.0000 - rmse: 19687.6348 - val_loss: 1096174976.0000 - val_rmse: 33108.5312\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484863136.0000 - rmse: 22019.6074 - val_loss: 1074375424.0000 - val_rmse: 32777.6680\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428005600.0000 - rmse: 20688.2969 - val_loss: 1025948992.0000 - val_rmse: 32030.4375\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354248960.0000 - rmse: 18821.5020 - val_loss: 1193707264.0000 - val_rmse: 34550.0703\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409658720.0000 - rmse: 20240.0254 - val_loss: 1003573824.0000 - val_rmse: 31679.2305\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391453856.0000 - rmse: 19785.1914 - val_loss: 909293184.0000 - val_rmse: 30154.4883\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419837184.0000 - rmse: 20489.9297 - val_loss: 898866304.0000 - val_rmse: 29981.0996\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327548224.0000 - rmse: 18098.2910 - val_loss: 942788032.0000 - val_rmse: 30704.8535\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358767296.0000 - rmse: 18941.1543 - val_loss: 1022922816.0000 - val_rmse: 31983.1641\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403151904.0000 - rmse: 20078.6426 - val_loss: 1052193280.0000 - val_rmse: 32437.5293\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411724992.0000 - rmse: 20291.0078 - val_loss: 1006166528.0000 - val_rmse: 31720.1289\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363745952.0000 - rmse: 19072.1230 - val_loss: 986359296.0000 - val_rmse: 31406.3574\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356325024.0000 - rmse: 18876.5723 - val_loss: 1027760000.0000 - val_rmse: 32058.6953\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385913760.0000 - rmse: 19644.6855 - val_loss: 1101652096.0000 - val_rmse: 33191.1445\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358577024.0000 - rmse: 18936.1289 - val_loss: 1118628224.0000 - val_rmse: 33445.8984\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388857792.0000 - rmse: 19719.4766 - val_loss: 1000535232.0000 - val_rmse: 31631.2344\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349316896.0000 - rmse: 18690.0195 - val_loss: 1026715968.0000 - val_rmse: 32042.4082\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402526976.0000 - rmse: 20063.0742 - val_loss: 1008319360.0000 - val_rmse: 31754.0449\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324365408.0000 - rmse: 18010.1465 - val_loss: 978491136.0000 - val_rmse: 31280.8438\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363933248.0000 - rmse: 19077.0352 - val_loss: 1071122176.0000 - val_rmse: 32728.0020\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337506016.0000 - rmse: 18371.3359 - val_loss: 1077296768.0000 - val_rmse: 32822.1992\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393829472.0000 - rmse: 19845.1367 - val_loss: 1190191488.0000 - val_rmse: 34499.1523\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341840800.0000 - rmse: 18488.9375 - val_loss: 1182328064.0000 - val_rmse: 34384.9961\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364053952.0000 - rmse: 19080.1973 - val_loss: 1357135232.0000 - val_rmse: 36839.3164\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355714752.0000 - rmse: 18860.4004 - val_loss: 1099296384.0000 - val_rmse: 33155.6406\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350762784.0000 - rmse: 18728.6621 - val_loss: 1222226304.0000 - val_rmse: 34960.3516\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400938176.0000 - rmse: 20023.4395 - val_loss: 1174436864.0000 - val_rmse: 34270.0586\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353365472.0000 - rmse: 18798.0176 - val_loss: 1356983168.0000 - val_rmse: 36837.2539\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368587136.0000 - rmse: 19198.6230 - val_loss: 1124001152.0000 - val_rmse: 33526.1250\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399934048.0000 - rmse: 19998.3496 - val_loss: 1037242560.0000 - val_rmse: 32206.2500\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333443968.0000 - rmse: 18260.4473 - val_loss: 1149288320.0000 - val_rmse: 33901.1562\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324529120.0000 - rmse: 18014.6895 - val_loss: 1219558400.0000 - val_rmse: 34922.1719\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350852640.0000 - rmse: 18731.0586 - val_loss: 1291657472.0000 - val_rmse: 35939.6367\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376994656.0000 - rmse: 19416.3496 - val_loss: 1195522432.0000 - val_rmse: 34576.3281\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346605472.0000 - rmse: 18617.3438 - val_loss: 1236136832.0000 - val_rmse: 35158.7383\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388613536.0000 - rmse: 19713.2832 - val_loss: 1187669504.0000 - val_rmse: 34462.5820\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367701248.0000 - rmse: 19175.5352 - val_loss: 1243696128.0000 - val_rmse: 35266.0742\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353076224.0000 - rmse: 18790.3223 - val_loss: 1157187584.0000 - val_rmse: 34017.4609\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339497216.0000 - rmse: 18425.4492 - val_loss: 1243216256.0000 - val_rmse: 35259.2734\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330814432.0000 - rmse: 18188.3047 - val_loss: 1221794816.0000 - val_rmse: 34954.1836\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378657856.0000 - rmse: 19459.1328 - val_loss: 1390171520.0000 - val_rmse: 37285.0039\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350601760.0000 - rmse: 18724.3613 - val_loss: 1199561984.0000 - val_rmse: 34634.6914\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324582048.0000 - rmse: 18016.1602 - val_loss: 1062241344.0000 - val_rmse: 32592.0410\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330435360.0000 - rmse: 18177.8809 - val_loss: 1251437184.0000 - val_rmse: 35375.6562\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329118272.0000 - rmse: 18141.6172 - val_loss: 1496682240.0000 - val_rmse: 38686.9766\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377880960.0000 - rmse: 19439.1582 - val_loss: 1180261504.0000 - val_rmse: 34354.9336\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329962720.0000 - rmse: 18164.8770 - val_loss: 1157512448.0000 - val_rmse: 34022.2344\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298465440.0000 - rmse: 17276.1523 - val_loss: 1205650176.0000 - val_rmse: 34722.4727\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322478048.0000 - rmse: 17957.6719 - val_loss: 1481987072.0000 - val_rmse: 38496.5859\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412058848.0000 - rmse: 20299.2324 - val_loss: 1178585216.0000 - val_rmse: 34330.5273\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333651296.0000 - rmse: 18266.1250 - val_loss: 1211961600.0000 - val_rmse: 34813.2383\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342933696.0000 - rmse: 18518.4668 - val_loss: 1488471296.0000 - val_rmse: 38580.7109\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348224000.0000 - rmse: 18660.7598 - val_loss: 1311545984.0000 - val_rmse: 36215.2734\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320250240.0000 - rmse: 17895.5371 - val_loss: 1167625856.0000 - val_rmse: 34170.5391\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336021312.0000 - rmse: 18330.8828 - val_loss: 1179433344.0000 - val_rmse: 34342.8789\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320553600.0000 - rmse: 17904.0098 - val_loss: 1313279232.0000 - val_rmse: 36239.1953\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354869056.0000 - rmse: 18837.9668 - val_loss: 1127022976.0000 - val_rmse: 33571.1641\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356688608.0000 - rmse: 18886.2012 - val_loss: 1194390016.0000 - val_rmse: 34559.9492\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277118912.0000 - rmse: 16646.8887 - val_loss: 1308318976.0000 - val_rmse: 36170.6914\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321450496.0000 - rmse: 17929.0391 - val_loss: 1395288192.0000 - val_rmse: 37353.5547\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289983808.0000 - rmse: 17028.9082 - val_loss: 1285386240.0000 - val_rmse: 35852.2852\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307105632.0000 - rmse: 17524.4277 - val_loss: 1401116288.0000 - val_rmse: 37431.4883\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322743584.0000 - rmse: 17965.0645 - val_loss: 1215284608.0000 - val_rmse: 34860.9336\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281800000.0000 - rmse: 16786.8984 - val_loss: 1561015424.0000 - val_rmse: 39509.6836\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300949376.0000 - rmse: 17347.8926 - val_loss: 1085136768.0000 - val_rmse: 32941.4141\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290423456.0000 - rmse: 17041.8125 - val_loss: 1282295552.0000 - val_rmse: 35809.1562\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303899552.0000 - rmse: 17432.7129 - val_loss: 1327923456.0000 - val_rmse: 36440.6836\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260948432.0000 - rmse: 16153.8955 - val_loss: 1331857664.0000 - val_rmse: 36494.6250\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324663328.0000 - rmse: 18018.4160 - val_loss: 1362330624.0000 - val_rmse: 36909.7617\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298146688.0000 - rmse: 17266.9238 - val_loss: 1482154240.0000 - val_rmse: 38498.7578\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278475968.0000 - rmse: 16687.5977 - val_loss: 1409824000.0000 - val_rmse: 37547.6211\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299153216.0000 - rmse: 17296.0449 - val_loss: 1327121920.0000 - val_rmse: 36429.6836\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336232128.0000 - rmse: 18336.6309 - val_loss: 1487480832.0000 - val_rmse: 38567.8711\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350807584.0000 - rmse: 18729.8574 - val_loss: 1372419200.0000 - val_rmse: 37046.1758\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347912128.0000 - rmse: 18652.4004 - val_loss: 1426510080.0000 - val_rmse: 37769.1680\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312836128.0000 - rmse: 17687.1738 - val_loss: 1403006720.0000 - val_rmse: 37456.7305\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275137952.0000 - rmse: 16587.2812 - val_loss: 1227411968.0000 - val_rmse: 35034.4414\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362751360.0000 - rmse: 19046.0332 - val_loss: 1121557632.0000 - val_rmse: 33489.6641\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328887392.0000 - rmse: 18135.2520 - val_loss: 1233209472.0000 - val_rmse: 35117.0820\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283061824.0000 - rmse: 16824.4395 - val_loss: 1225610112.0000 - val_rmse: 35008.7148\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295762976.0000 - rmse: 17197.7617 - val_loss: 1091643008.0000 - val_rmse: 33040.0195\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335281120.0000 - rmse: 18310.6816 - val_loss: 1473471104.0000 - val_rmse: 38385.8203\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243793616.0000 - rmse: 15613.8906 - val_loss: 1513859456.0000 - val_rmse: 38908.3477\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270231488.0000 - rmse: 16438.7188 - val_loss: 1077084672.0000 - val_rmse: 32818.9688\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298888000.0000 - rmse: 17288.3750 - val_loss: 1163071744.0000 - val_rmse: 34103.8359\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280138176.0000 - rmse: 16737.3281 - val_loss: 1304271488.0000 - val_rmse: 36114.6992\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259234912.0000 - rmse: 16100.7725 - val_loss: 1302790656.0000 - val_rmse: 36094.1914\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308752480.0000 - rmse: 17571.3535 - val_loss: 1152193536.0000 - val_rmse: 33943.9766\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295742688.0000 - rmse: 17197.1699 - val_loss: 1264145920.0000 - val_rmse: 35554.8281\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255831280.0000 - rmse: 15994.7256 - val_loss: 1330219264.0000 - val_rmse: 36472.1680\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294327200.0000 - rmse: 17155.9648 - val_loss: 1113103104.0000 - val_rmse: 33363.1992\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266310112.0000 - rmse: 16319.0098 - val_loss: 1014105664.0000 - val_rmse: 31845.0254\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271683296.0000 - rmse: 16482.8164 - val_loss: 1077666176.0000 - val_rmse: 32827.8242\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327685760.0000 - rmse: 18102.0918 - val_loss: 1433644288.0000 - val_rmse: 37863.4961\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 233670752.0000 - rmse: 15286.2920 - val_loss: 1300714880.0000 - val_rmse: 36065.4258\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366146944.0000 - rmse: 19134.9668 - val_loss: 1552284544.0000 - val_rmse: 39399.0391\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271611712.0000 - rmse: 16480.6445 - val_loss: 1472254592.0000 - val_rmse: 38369.9688\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278666464.0000 - rmse: 16693.3047 - val_loss: 1530393472.0000 - val_rmse: 39120.2422\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293547392.0000 - rmse: 17133.2246 - val_loss: 1252499072.0000 - val_rmse: 35390.6641\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264671776.0000 - rmse: 16268.7344 - val_loss: 1193835008.0000 - val_rmse: 34551.9180\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262591648.0000 - rmse: 16204.6797 - val_loss: 1275903616.0000 - val_rmse: 35719.7930\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259828736.0000 - rmse: 16119.2021 - val_loss: 1277376256.0000 - val_rmse: 35740.4023\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301611232.0000 - rmse: 17366.9570 - val_loss: 1260427136.0000 - val_rmse: 35502.4961\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275405312.0000 - rmse: 16595.3379 - val_loss: 1469532800.0000 - val_rmse: 38334.4844\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270348960.0000 - rmse: 16442.2910 - val_loss: 1404054912.0000 - val_rmse: 37470.7227\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278924832.0000 - rmse: 16701.0410 - val_loss: 1616764928.0000 - val_rmse: 40209.0156\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271982048.0000 - rmse: 16491.8770 - val_loss: 1550168704.0000 - val_rmse: 39372.1836\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272222880.0000 - rmse: 16499.1777 - val_loss: 1332619008.0000 - val_rmse: 36505.0547\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302960992.0000 - rmse: 17405.7754 - val_loss: 1444934784.0000 - val_rmse: 38012.2969\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299282976.0000 - rmse: 17299.7969 - val_loss: 1428507392.0000 - val_rmse: 37795.6016\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250114048.0000 - rmse: 15814.9922 - val_loss: 1579161472.0000 - val_rmse: 39738.6641\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238064736.0000 - rmse: 15429.3447 - val_loss: 1457024000.0000 - val_rmse: 38170.9844\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237515504.0000 - rmse: 15411.5361 - val_loss: 1611374208.0000 - val_rmse: 40141.9258\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280841376.0000 - rmse: 16758.3223 - val_loss: 1470519808.0000 - val_rmse: 38347.3555\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252076496.0000 - rmse: 15876.9160 - val_loss: 1409959680.0000 - val_rmse: 37549.4297\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273578464.0000 - rmse: 16540.2070 - val_loss: 1520476800.0000 - val_rmse: 38993.2930\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292303680.0000 - rmse: 17096.8887 - val_loss: 1489085952.0000 - val_rmse: 38588.6758\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274570528.0000 - rmse: 16570.1680 - val_loss: 1469509760.0000 - val_rmse: 38334.1836\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291623872.0000 - rmse: 17076.9980 - val_loss: 1514914816.0000 - val_rmse: 38921.9062\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269229120.0000 - rmse: 16408.2012 - val_loss: 1538254720.0000 - val_rmse: 39220.5898\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316033056.0000 - rmse: 17777.3184 - val_loss: 1314341376.0000 - val_rmse: 36253.8477\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264053504.0000 - rmse: 16249.7217 - val_loss: 1610427776.0000 - val_rmse: 40130.1328\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217951104.0000 - rmse: 14763.1660 - val_loss: 1494195328.0000 - val_rmse: 38654.8242\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253243376.0000 - rmse: 15913.6211 - val_loss: 1341654528.0000 - val_rmse: 36628.6016\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232157584.0000 - rmse: 15236.7158 - val_loss: 1152615424.0000 - val_rmse: 33950.1914\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251608080.0000 - rmse: 15862.1572 - val_loss: 1490109696.0000 - val_rmse: 38601.9375\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276579456.0000 - rmse: 16630.6777 - val_loss: 1702329728.0000 - val_rmse: 41259.2969\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255171104.0000 - rmse: 15974.0752 - val_loss: 1459582336.0000 - val_rmse: 38204.4805\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307325888.0000 - rmse: 17530.7109 - val_loss: 1587959296.0000 - val_rmse: 39849.2070\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265714128.0000 - rmse: 16300.7383 - val_loss: 1745099008.0000 - val_rmse: 41774.3828\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256554272.0000 - rmse: 16017.3105 - val_loss: 1601843072.0000 - val_rmse: 40023.0312\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244576320.0000 - rmse: 15638.9346 - val_loss: 1696113152.0000 - val_rmse: 41183.8945\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229043680.0000 - rmse: 15134.1885 - val_loss: 1781311232.0000 - val_rmse: 42205.5820\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260400784.0000 - rmse: 16136.9365 - val_loss: 1354767360.0000 - val_rmse: 36807.1641\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243039264.0000 - rmse: 15589.7148 - val_loss: 1648624896.0000 - val_rmse: 40603.2617\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296351296.0000 - rmse: 17214.8555 - val_loss: 1584674688.0000 - val_rmse: 39807.9727\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271450112.0000 - rmse: 16475.7402 - val_loss: 1543575808.0000 - val_rmse: 39288.3672\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224910592.0000 - rmse: 14997.0195 - val_loss: 1472744064.0000 - val_rmse: 38376.3477\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243218048.0000 - rmse: 15595.4492 - val_loss: 1560745600.0000 - val_rmse: 39506.2695\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248016544.0000 - rmse: 15748.5391 - val_loss: 1926203520.0000 - val_rmse: 43888.5352\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250020928.0000 - rmse: 15812.0498 - val_loss: 1610482048.0000 - val_rmse: 40130.8125\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259167056.0000 - rmse: 16098.6650 - val_loss: 1868737024.0000 - val_rmse: 43228.8906\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284469888.0000 - rmse: 16866.2344 - val_loss: 1654145664.0000 - val_rmse: 40671.1875\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234542624.0000 - rmse: 15314.7842 - val_loss: 1883362304.0000 - val_rmse: 43397.7227\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219053408.0000 - rmse: 14800.4521 - val_loss: 1947352576.0000 - val_rmse: 44128.8164\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210626288.0000 - rmse: 14512.9688 - val_loss: 2237614336.0000 - val_rmse: 47303.4297\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300982592.0000 - rmse: 17348.8496 - val_loss: 1831249536.0000 - val_rmse: 42793.1016\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239378208.0000 - rmse: 15471.8525 - val_loss: 1958963840.0000 - val_rmse: 44260.1836\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247285360.0000 - rmse: 15725.3066 - val_loss: 1478702080.0000 - val_rmse: 38453.8945\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250923712.0000 - rmse: 15840.5713 - val_loss: 1966837504.0000 - val_rmse: 44349.0430\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254227584.0000 - rmse: 15944.5137 - val_loss: 1397483008.0000 - val_rmse: 37382.9258\n",
      "Epoch 367/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272744832.0000 - rmse: 16514.9863 - val_loss: 1508192768.0000 - val_rmse: 38835.4570\n",
      "Epoch 368/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232691440.0000 - rmse: 15254.2256 - val_loss: 1618032256.0000 - val_rmse: 40224.7734\n",
      "Epoch 369/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211296528.0000 - rmse: 14536.0410 - val_loss: 1405758336.0000 - val_rmse: 37493.4453\n",
      "Epoch 370/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244103456.0000 - rmse: 15623.8096 - val_loss: 1716571392.0000 - val_rmse: 41431.5273\n",
      "Epoch 371/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228420496.0000 - rmse: 15113.5850 - val_loss: 1717808768.0000 - val_rmse: 41446.4570\n",
      "Epoch 372/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217356176.0000 - rmse: 14743.0020 - val_loss: 1897815680.0000 - val_rmse: 43563.9258\n",
      "Epoch 373/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285729024.0000 - rmse: 16903.5195 - val_loss: 1479902592.0000 - val_rmse: 38469.5039\n",
      "Epoch 374/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 212998848.0000 - rmse: 14594.4775 - val_loss: 1658774144.0000 - val_rmse: 40728.0508\n",
      "Epoch 375/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252722624.0000 - rmse: 15897.2520 - val_loss: 1701492864.0000 - val_rmse: 41249.1562\n",
      "Epoch 376/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219735760.0000 - rmse: 14823.4863 - val_loss: 1493814144.0000 - val_rmse: 38649.8945\n",
      "Epoch 377/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 201369232.0000 - rmse: 14190.4609 - val_loss: 1505853440.0000 - val_rmse: 38805.3281\n",
      "Epoch 378/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240741280.0000 - rmse: 15515.8398 - val_loss: 1527695616.0000 - val_rmse: 39085.7461\n",
      "Epoch 379/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 194319456.0000 - rmse: 13939.8506 - val_loss: 1300375552.0000 - val_rmse: 36060.7188\n",
      "Epoch 380/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247622288.0000 - rmse: 15736.0176 - val_loss: 1284369408.0000 - val_rmse: 35838.1016\n",
      "Epoch 381/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259562608.0000 - rmse: 16110.9463 - val_loss: 1648447744.0000 - val_rmse: 40601.0820\n",
      "Epoch 382/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236087536.0000 - rmse: 15365.1387 - val_loss: 1666814592.0000 - val_rmse: 40826.6406\n",
      "Epoch 383/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250942448.0000 - rmse: 15841.1631 - val_loss: 1597000320.0000 - val_rmse: 39962.4883\n",
      "Epoch 384/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202347152.0000 - rmse: 14224.8760 - val_loss: 1682024320.0000 - val_rmse: 41012.4883\n",
      "Epoch 385/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278445280.0000 - rmse: 16686.6777 - val_loss: 1965373696.0000 - val_rmse: 44332.5352\n",
      "Epoch 386/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237879904.0000 - rmse: 15423.3545 - val_loss: 1794046592.0000 - val_rmse: 42356.1875\n",
      "Epoch 387/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255537808.0000 - rmse: 15985.5488 - val_loss: 1818875520.0000 - val_rmse: 42648.2773\n",
      "Epoch 388/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229741136.0000 - rmse: 15157.2139 - val_loss: 1528707968.0000 - val_rmse: 39098.6953\n",
      "Epoch 389/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210985968.0000 - rmse: 14525.3545 - val_loss: 1809003136.0000 - val_rmse: 42532.3750\n",
      "Epoch 390/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253350688.0000 - rmse: 15916.9932 - val_loss: 2340869888.0000 - val_rmse: 48382.5352\n",
      "Epoch 391/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266420608.0000 - rmse: 16322.3945 - val_loss: 1977399424.0000 - val_rmse: 44467.9609\n",
      "Epoch 392/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215727456.0000 - rmse: 14687.6611 - val_loss: 2050840704.0000 - val_rmse: 45286.2070\n",
      "Epoch 393/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 209571456.0000 - rmse: 14476.5811 - val_loss: 1623097728.0000 - val_rmse: 40287.6836\n",
      "Epoch 394/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218542912.0000 - rmse: 14783.1943 - val_loss: 1874828160.0000 - val_rmse: 43299.2852\n",
      "Epoch 395/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286503520.0000 - rmse: 16926.4121 - val_loss: 1686282624.0000 - val_rmse: 41064.3672\n",
      "Epoch 396/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214149152.0000 - rmse: 14633.8330 - val_loss: 1846517632.0000 - val_rmse: 42971.1250\n",
      "Epoch 397/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207093616.0000 - rmse: 14390.7451 - val_loss: 1721049856.0000 - val_rmse: 41485.5391\n",
      "Epoch 398/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 205177424.0000 - rmse: 14324.0146 - val_loss: 1718098304.0000 - val_rmse: 41449.9492\n",
      "Epoch 399/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222496480.0000 - rmse: 14916.3145 - val_loss: 1666172416.0000 - val_rmse: 40818.7734\n",
      "Epoch 400/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 233555616.0000 - rmse: 15282.5254 - val_loss: 1839214976.0000 - val_rmse: 42886.0703\n",
      "104/104 [==============================] - 0s 696us/step - loss: 884871872.0000 - rmse: 29746.7930\n",
      "[884871872.0, 29746.79296875]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 23473182720.0000 - rmse: 153209.6094 - val_loss: 12384513024.0000 - val_rmse: 111285.7266\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 5129243648.0000 - rmse: 71618.7344 - val_loss: 1967848576.0000 - val_rmse: 44360.4375\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2342409216.0000 - rmse: 48398.4414 - val_loss: 1494126720.0000 - val_rmse: 38653.9336\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1950848128.0000 - rmse: 44168.4062 - val_loss: 1296668672.0000 - val_rmse: 36009.2852\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1870164352.0000 - rmse: 43245.3984 - val_loss: 1194167296.0000 - val_rmse: 34556.7266\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1756542336.0000 - rmse: 41911.1250 - val_loss: 1138920832.0000 - val_rmse: 33747.9023\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1648684800.0000 - rmse: 40604.0000 - val_loss: 1061180608.0000 - val_rmse: 32575.7676\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1586731008.0000 - rmse: 39833.7930 - val_loss: 1028436224.0000 - val_rmse: 32069.2402\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1525058048.0000 - rmse: 39051.9922 - val_loss: 985186304.0000 - val_rmse: 31387.6777\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1551741696.0000 - rmse: 39392.1523 - val_loss: 953484864.0000 - val_rmse: 30878.5508\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1491077504.0000 - rmse: 38614.4727 - val_loss: 916821248.0000 - val_rmse: 30279.0566\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1405670272.0000 - rmse: 37492.2695 - val_loss: 927677760.0000 - val_rmse: 30457.8027\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1332654592.0000 - rmse: 36505.5391 - val_loss: 892220288.0000 - val_rmse: 29870.0566\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1416122240.0000 - rmse: 37631.3984 - val_loss: 892322176.0000 - val_rmse: 29871.7617\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1364889344.0000 - rmse: 36944.4102 - val_loss: 857370624.0000 - val_rmse: 29280.8926\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1311572992.0000 - rmse: 36215.6445 - val_loss: 846064576.0000 - val_rmse: 29087.1895\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1257848448.0000 - rmse: 35466.1602 - val_loss: 830867136.0000 - val_rmse: 28824.7656\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1238807552.0000 - rmse: 35196.6992 - val_loss: 842756608.0000 - val_rmse: 29030.2695\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1242920064.0000 - rmse: 35255.0703 - val_loss: 824257856.0000 - val_rmse: 28709.8906\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1174212608.0000 - rmse: 34266.7852 - val_loss: 898540352.0000 - val_rmse: 29975.6621\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1228342144.0000 - rmse: 35047.7109 - val_loss: 850872128.0000 - val_rmse: 29169.7129\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1147384576.0000 - rmse: 33873.0664 - val_loss: 891298752.0000 - val_rmse: 29854.6270\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1180103680.0000 - rmse: 34352.6367 - val_loss: 906954048.0000 - val_rmse: 30115.6777\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1112332544.0000 - rmse: 33351.6484 - val_loss: 857143744.0000 - val_rmse: 29277.0176\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1140465408.0000 - rmse: 33770.7773 - val_loss: 856217088.0000 - val_rmse: 29261.1875\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1096025216.0000 - rmse: 33106.2734 - val_loss: 834359424.0000 - val_rmse: 28885.2812\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1120202240.0000 - rmse: 33469.4219 - val_loss: 834949696.0000 - val_rmse: 28895.4941\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1090692480.0000 - rmse: 33025.6328 - val_loss: 871607552.0000 - val_rmse: 29523.0000\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1043875200.0000 - rmse: 32309.0566 - val_loss: 853606208.0000 - val_rmse: 29216.5391\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1089548160.0000 - rmse: 33008.3047 - val_loss: 867202240.0000 - val_rmse: 29448.2969\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1031787328.0000 - rmse: 32121.4473 - val_loss: 874525504.0000 - val_rmse: 29572.3770\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1047511616.0000 - rmse: 32365.2832 - val_loss: 844051648.0000 - val_rmse: 29052.5664\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1024731264.0000 - rmse: 32011.4238 - val_loss: 845756224.0000 - val_rmse: 29081.8887\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1055313984.0000 - rmse: 32485.5957 - val_loss: 1045567936.0000 - val_rmse: 32335.2422\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1067580928.0000 - rmse: 32673.8574 - val_loss: 899371072.0000 - val_rmse: 29989.5156\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 981668160.0000 - rmse: 31331.5840 - val_loss: 856469888.0000 - val_rmse: 29265.5059\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 965953920.0000 - rmse: 31079.7988 - val_loss: 821747136.0000 - val_rmse: 28666.1328\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1005745408.0000 - rmse: 31713.4902 - val_loss: 827461952.0000 - val_rmse: 28765.6387\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 946865536.0000 - rmse: 30771.1797 - val_loss: 842828928.0000 - val_rmse: 29031.5156\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 945128320.0000 - rmse: 30742.9395 - val_loss: 838695808.0000 - val_rmse: 28960.2461\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 940533440.0000 - rmse: 30668.1172 - val_loss: 854726080.0000 - val_rmse: 29235.6992\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 903863232.0000 - rmse: 30064.3184 - val_loss: 880623936.0000 - val_rmse: 29675.3086\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1005354048.0000 - rmse: 31707.3184 - val_loss: 843271232.0000 - val_rmse: 29039.1328\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 877439040.0000 - rmse: 29621.5977 - val_loss: 876054016.0000 - val_rmse: 29598.2090\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 898025536.0000 - rmse: 29967.0742 - val_loss: 805603520.0000 - val_rmse: 28383.1562\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 890248640.0000 - rmse: 29837.0352 - val_loss: 869365376.0000 - val_rmse: 29485.0020\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 885563200.0000 - rmse: 29758.4141 - val_loss: 872795072.0000 - val_rmse: 29543.1055\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 902121984.0000 - rmse: 30035.3418 - val_loss: 815374272.0000 - val_rmse: 28554.7598\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 881891520.0000 - rmse: 29696.6582 - val_loss: 999985728.0000 - val_rmse: 31622.5508\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 826684800.0000 - rmse: 28752.1270 - val_loss: 846571392.0000 - val_rmse: 29095.9004\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 810857984.0000 - rmse: 28475.5684 - val_loss: 851454592.0000 - val_rmse: 29179.6953\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 813029120.0000 - rmse: 28513.6660 - val_loss: 875236992.0000 - val_rmse: 29584.4043\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 853035520.0000 - rmse: 29206.7715 - val_loss: 1107736064.0000 - val_rmse: 33282.6680\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724364032.0000 - rmse: 26914.0117 - val_loss: 826025984.0000 - val_rmse: 28740.6680\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 790898944.0000 - rmse: 28122.9258 - val_loss: 846065344.0000 - val_rmse: 29087.2031\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 764308416.0000 - rmse: 27646.1289 - val_loss: 796780032.0000 - val_rmse: 28227.2930\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 814868032.0000 - rmse: 28545.8926 - val_loss: 923167616.0000 - val_rmse: 30383.6738\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 713346304.0000 - rmse: 26708.5430 - val_loss: 1019023232.0000 - val_rmse: 31922.1426\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 693245952.0000 - rmse: 26329.5645 - val_loss: 861783424.0000 - val_rmse: 29356.1484\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 699066368.0000 - rmse: 26439.8633 - val_loss: 863970560.0000 - val_rmse: 29393.3770\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 705962944.0000 - rmse: 26569.9629 - val_loss: 886177920.0000 - val_rmse: 29768.7402\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 781316672.0000 - rmse: 27952.0430 - val_loss: 766573888.0000 - val_rmse: 27687.0703\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649213696.0000 - rmse: 25479.6719 - val_loss: 1251971840.0000 - val_rmse: 35383.2148\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 638871744.0000 - rmse: 25275.9102 - val_loss: 844178496.0000 - val_rmse: 29054.7500\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630362752.0000 - rmse: 25107.0254 - val_loss: 858778624.0000 - val_rmse: 29304.9258\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 705049984.0000 - rmse: 26552.7773 - val_loss: 810210752.0000 - val_rmse: 28464.2012\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 627739264.0000 - rmse: 25054.7246 - val_loss: 813287488.0000 - val_rmse: 28518.1953\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 696741760.0000 - rmse: 26395.8672 - val_loss: 784793408.0000 - val_rmse: 28014.1641\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621658496.0000 - rmse: 24933.0801 - val_loss: 880124160.0000 - val_rmse: 29666.8867\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579508160.0000 - rmse: 24072.9766 - val_loss: 879889728.0000 - val_rmse: 29662.9336\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626830272.0000 - rmse: 25036.5781 - val_loss: 996189632.0000 - val_rmse: 31562.4727\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 725023360.0000 - rmse: 26926.2578 - val_loss: 932530624.0000 - val_rmse: 30537.3652\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661376256.0000 - rmse: 25717.2344 - val_loss: 823810944.0000 - val_rmse: 28702.1055\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558835712.0000 - rmse: 23639.7070 - val_loss: 1505987200.0000 - val_rmse: 38807.0508\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618689216.0000 - rmse: 24873.4648 - val_loss: 887759872.0000 - val_rmse: 29795.2988\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576137536.0000 - rmse: 24002.8652 - val_loss: 835409088.0000 - val_rmse: 28903.4434\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583413248.0000 - rmse: 24153.9473 - val_loss: 837066176.0000 - val_rmse: 28932.0957\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497180544.0000 - rmse: 22297.5449 - val_loss: 717992896.0000 - val_rmse: 26795.3887\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515880032.0000 - rmse: 22712.9922 - val_loss: 1297340800.0000 - val_rmse: 36018.6172\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588004352.0000 - rmse: 24248.8008 - val_loss: 781340928.0000 - val_rmse: 27952.4766\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549876480.0000 - rmse: 23449.4453 - val_loss: 745555904.0000 - val_rmse: 27304.8691\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555444480.0000 - rmse: 23567.8691 - val_loss: 880275904.0000 - val_rmse: 29669.4434\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514479136.0000 - rmse: 22682.1328 - val_loss: 1043488640.0000 - val_rmse: 32303.0742\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522714848.0000 - rmse: 22862.9570 - val_loss: 819272768.0000 - val_rmse: 28622.9414\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520363104.0000 - rmse: 22811.4688 - val_loss: 894160960.0000 - val_rmse: 29902.5234\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520803712.0000 - rmse: 22821.1250 - val_loss: 784332480.0000 - val_rmse: 28005.9355\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526487776.0000 - rmse: 22945.3223 - val_loss: 752855744.0000 - val_rmse: 27438.2168\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508388320.0000 - rmse: 22547.4668 - val_loss: 789658944.0000 - val_rmse: 28100.8711\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512360256.0000 - rmse: 22635.3770 - val_loss: 797156032.0000 - val_rmse: 28233.9492\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495537696.0000 - rmse: 22260.6758 - val_loss: 937632192.0000 - val_rmse: 30620.7793\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502338528.0000 - rmse: 22412.9082 - val_loss: 950585088.0000 - val_rmse: 30831.5605\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486318912.0000 - rmse: 22052.6387 - val_loss: 749453952.0000 - val_rmse: 27376.1562\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418751136.0000 - rmse: 20463.4102 - val_loss: 992777152.0000 - val_rmse: 31508.3672\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473655104.0000 - rmse: 21763.6191 - val_loss: 777349952.0000 - val_rmse: 27880.9961\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482260704.0000 - rmse: 21960.4355 - val_loss: 1045895040.0000 - val_rmse: 32340.3008\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485153984.0000 - rmse: 22026.2109 - val_loss: 781137024.0000 - val_rmse: 27948.8281\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427679648.0000 - rmse: 20680.4141 - val_loss: 1058980224.0000 - val_rmse: 32541.9766\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481204512.0000 - rmse: 21936.3750 - val_loss: 906413952.0000 - val_rmse: 30106.7090\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374379776.0000 - rmse: 19348.8965 - val_loss: 1131764992.0000 - val_rmse: 33641.7148\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448838080.0000 - rmse: 21185.7988 - val_loss: 1027402560.0000 - val_rmse: 32053.1211\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438577088.0000 - rmse: 20942.2324 - val_loss: 1108049792.0000 - val_rmse: 33287.3789\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425152288.0000 - rmse: 20619.2207 - val_loss: 1123441408.0000 - val_rmse: 33517.7773\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425944000.0000 - rmse: 20638.4102 - val_loss: 1440085632.0000 - val_rmse: 37948.4570\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530825408.0000 - rmse: 23039.6484 - val_loss: 707860992.0000 - val_rmse: 26605.6562\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435170848.0000 - rmse: 20860.7480 - val_loss: 733542080.0000 - val_rmse: 27083.9824\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464646080.0000 - rmse: 21555.6504 - val_loss: 834849280.0000 - val_rmse: 28893.7578\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407098496.0000 - rmse: 20176.6816 - val_loss: 890900672.0000 - val_rmse: 29847.9590\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378118496.0000 - rmse: 19445.2695 - val_loss: 1195057664.0000 - val_rmse: 34569.6055\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418122048.0000 - rmse: 20448.0312 - val_loss: 981945216.0000 - val_rmse: 31336.0059\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447088672.0000 - rmse: 21144.4707 - val_loss: 1022777408.0000 - val_rmse: 31980.8887\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420970240.0000 - rmse: 20517.5586 - val_loss: 811702656.0000 - val_rmse: 28490.3965\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367980256.0000 - rmse: 19182.8105 - val_loss: 639707712.0000 - val_rmse: 25292.4434\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348256224.0000 - rmse: 18661.6250 - val_loss: 1020411968.0000 - val_rmse: 31943.8887\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497639040.0000 - rmse: 22307.8242 - val_loss: 987955264.0000 - val_rmse: 31431.7559\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441410144.0000 - rmse: 21009.7617 - val_loss: 1639559296.0000 - val_rmse: 40491.4727\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420688192.0000 - rmse: 20510.6855 - val_loss: 1742144896.0000 - val_rmse: 41739.0078\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439347456.0000 - rmse: 20960.6172 - val_loss: 1007896000.0000 - val_rmse: 31747.3750\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327728448.0000 - rmse: 18103.2715 - val_loss: 1913668224.0000 - val_rmse: 43745.4922\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364824256.0000 - rmse: 19100.3730 - val_loss: 1718488832.0000 - val_rmse: 41454.6602\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424923680.0000 - rmse: 20613.6777 - val_loss: 1007096640.0000 - val_rmse: 31734.7852\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354946592.0000 - rmse: 18840.0254 - val_loss: 1148971776.0000 - val_rmse: 33896.4844\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379568448.0000 - rmse: 19482.5156 - val_loss: 1211358080.0000 - val_rmse: 34804.5703\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334150112.0000 - rmse: 18279.7734 - val_loss: 1240818560.0000 - val_rmse: 35225.2539\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539149504.0000 - rmse: 23219.5938 - val_loss: 2115231488.0000 - val_rmse: 45991.6445\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403161344.0000 - rmse: 20078.8789 - val_loss: 1254797184.0000 - val_rmse: 35423.1133\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428246144.0000 - rmse: 20694.1094 - val_loss: 1147463424.0000 - val_rmse: 33874.2266\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367987520.0000 - rmse: 19183.0000 - val_loss: 1361784576.0000 - val_rmse: 36902.3672\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353697504.0000 - rmse: 18806.8457 - val_loss: 794792832.0000 - val_rmse: 28192.0703\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432327712.0000 - rmse: 20792.4922 - val_loss: 1258806912.0000 - val_rmse: 35479.6641\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436501952.0000 - rmse: 20892.6289 - val_loss: 1833100416.0000 - val_rmse: 42814.7227\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434523968.0000 - rmse: 20845.2383 - val_loss: 1569620864.0000 - val_rmse: 39618.4414\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395795072.0000 - rmse: 19894.5996 - val_loss: 1341184128.0000 - val_rmse: 36622.1797\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400008128.0000 - rmse: 20000.2031 - val_loss: 1187131648.0000 - val_rmse: 34454.7773\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362269408.0000 - rmse: 19033.3750 - val_loss: 801789504.0000 - val_rmse: 28315.8887\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352462400.0000 - rmse: 18773.9824 - val_loss: 1417986944.0000 - val_rmse: 37656.1680\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345865760.0000 - rmse: 18597.4648 - val_loss: 1248605312.0000 - val_rmse: 35335.6094\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337264448.0000 - rmse: 18364.7598 - val_loss: 1240826752.0000 - val_rmse: 35225.3711\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396857952.0000 - rmse: 19921.2930 - val_loss: 1582153472.0000 - val_rmse: 39776.2930\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341426624.0000 - rmse: 18477.7324 - val_loss: 904130240.0000 - val_rmse: 30068.7559\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350142848.0000 - rmse: 18712.1035 - val_loss: 1083243904.0000 - val_rmse: 32912.6719\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386083104.0000 - rmse: 19648.9961 - val_loss: 1082246144.0000 - val_rmse: 32897.5078\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362117088.0000 - rmse: 19029.3750 - val_loss: 1107311104.0000 - val_rmse: 33276.2852\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322503776.0000 - rmse: 17958.3887 - val_loss: 739220544.0000 - val_rmse: 27188.6113\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333512768.0000 - rmse: 18262.3320 - val_loss: 1256834432.0000 - val_rmse: 35451.8594\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279438560.0000 - rmse: 16716.4141 - val_loss: 1410840192.0000 - val_rmse: 37561.1523\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350089920.0000 - rmse: 18710.6895 - val_loss: 957640960.0000 - val_rmse: 30945.7754\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350540224.0000 - rmse: 18722.7188 - val_loss: 1087745024.0000 - val_rmse: 32980.9805\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312170624.0000 - rmse: 17668.3516 - val_loss: 1165833216.0000 - val_rmse: 34144.2969\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357639232.0000 - rmse: 18911.3516 - val_loss: 1352339328.0000 - val_rmse: 36774.1680\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309814944.0000 - rmse: 17601.5605 - val_loss: 1273218304.0000 - val_rmse: 35682.1836\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319758048.0000 - rmse: 17881.7793 - val_loss: 1387730688.0000 - val_rmse: 37252.2578\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395600224.0000 - rmse: 19889.7012 - val_loss: 1212197632.0000 - val_rmse: 34816.6289\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387741856.0000 - rmse: 19691.1621 - val_loss: 1138802944.0000 - val_rmse: 33746.1523\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296967040.0000 - rmse: 17232.7324 - val_loss: 972991424.0000 - val_rmse: 31192.8105\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306970016.0000 - rmse: 17520.5605 - val_loss: 1273294464.0000 - val_rmse: 35683.2500\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287805408.0000 - rmse: 16964.8281 - val_loss: 1359812608.0000 - val_rmse: 36875.6367\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249357248.0000 - rmse: 15791.0488 - val_loss: 839364672.0000 - val_rmse: 28971.7910\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319769120.0000 - rmse: 17882.0898 - val_loss: 1028548288.0000 - val_rmse: 32070.9863\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253280016.0000 - rmse: 15914.7734 - val_loss: 1273513856.0000 - val_rmse: 35686.3242\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289983168.0000 - rmse: 17028.8926 - val_loss: 1189757824.0000 - val_rmse: 34492.8672\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296556928.0000 - rmse: 17220.8281 - val_loss: 935067840.0000 - val_rmse: 30578.8789\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242734720.0000 - rmse: 15579.9453 - val_loss: 1482265728.0000 - val_rmse: 38500.2031\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305023872.0000 - rmse: 17464.9336 - val_loss: 1467542528.0000 - val_rmse: 38308.5195\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304254144.0000 - rmse: 17442.8828 - val_loss: 1939954688.0000 - val_rmse: 44044.9180\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264270384.0000 - rmse: 16256.3945 - val_loss: 1184352000.0000 - val_rmse: 34414.4141\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311852960.0000 - rmse: 17659.3594 - val_loss: 1552276864.0000 - val_rmse: 39398.9414\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323959520.0000 - rmse: 17998.8750 - val_loss: 1844886400.0000 - val_rmse: 42952.1406\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264052032.0000 - rmse: 16249.6768 - val_loss: 1878818688.0000 - val_rmse: 43345.3438\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281629824.0000 - rmse: 16781.8301 - val_loss: 1980205312.0000 - val_rmse: 44499.4961\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263429360.0000 - rmse: 16230.5068 - val_loss: 1649232512.0000 - val_rmse: 40610.7422\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323425888.0000 - rmse: 17984.0449 - val_loss: 1538075008.0000 - val_rmse: 39218.3008\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277465216.0000 - rmse: 16657.2871 - val_loss: 1377970560.0000 - val_rmse: 37121.0273\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301104480.0000 - rmse: 17352.3633 - val_loss: 1299624832.0000 - val_rmse: 36050.3086\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323550848.0000 - rmse: 17987.5195 - val_loss: 1655891712.0000 - val_rmse: 40692.6484\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350490784.0000 - rmse: 18721.3984 - val_loss: 1705551488.0000 - val_rmse: 41298.3242\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275168096.0000 - rmse: 16588.1914 - val_loss: 1700180864.0000 - val_rmse: 41233.2461\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303562592.0000 - rmse: 17423.0469 - val_loss: 2014990208.0000 - val_rmse: 44888.6406\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273805248.0000 - rmse: 16547.0625 - val_loss: 1354489088.0000 - val_rmse: 36803.3828\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272548384.0000 - rmse: 16509.0391 - val_loss: 1827142912.0000 - val_rmse: 42745.0938\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275218464.0000 - rmse: 16589.7090 - val_loss: 2133054720.0000 - val_rmse: 46185.0039\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254104112.0000 - rmse: 15940.6436 - val_loss: 2142510464.0000 - val_rmse: 46287.2617\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309842112.0000 - rmse: 17602.3320 - val_loss: 1914876672.0000 - val_rmse: 43759.3047\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263319472.0000 - rmse: 16227.1211 - val_loss: 1946306560.0000 - val_rmse: 44116.9648\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420925920.0000 - rmse: 20516.4785 - val_loss: 1103533696.0000 - val_rmse: 33219.4766\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244989984.0000 - rmse: 15652.1553 - val_loss: 2701182976.0000 - val_rmse: 51972.9062\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318124928.0000 - rmse: 17836.0566 - val_loss: 1591277824.0000 - val_rmse: 39890.8242\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319396224.0000 - rmse: 17871.6582 - val_loss: 1360270592.0000 - val_rmse: 36881.8477\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293434048.0000 - rmse: 17129.9160 - val_loss: 1392480896.0000 - val_rmse: 37315.9609\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258737344.0000 - rmse: 16085.3145 - val_loss: 1409232896.0000 - val_rmse: 37539.7500\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267273712.0000 - rmse: 16348.5078 - val_loss: 1524614656.0000 - val_rmse: 39046.3125\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257246848.0000 - rmse: 16038.9160 - val_loss: 1386576384.0000 - val_rmse: 37236.7617\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297113472.0000 - rmse: 17236.9766 - val_loss: 1543605888.0000 - val_rmse: 39288.7500\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323297344.0000 - rmse: 17980.4707 - val_loss: 1542517376.0000 - val_rmse: 39274.8945\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271546432.0000 - rmse: 16478.6660 - val_loss: 1266398592.0000 - val_rmse: 35586.4961\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267474960.0000 - rmse: 16354.6611 - val_loss: 1887436032.0000 - val_rmse: 43444.6328\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338242080.0000 - rmse: 18391.3574 - val_loss: 1579480448.0000 - val_rmse: 39742.6797\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245549584.0000 - rmse: 15670.0215 - val_loss: 1424381824.0000 - val_rmse: 37740.9844\n",
      "104/104 [==============================] - 0s 754us/step - loss: 488868000.0000 - rmse: 22110.3574\n",
      "[488868000.0, 22110.357421875]\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 22289817600.0000 - rmse: 149297.7500 - val_loss: 7491725824.0000 - val_rmse: 86554.7578\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 3978954240.0000 - rmse: 63078.9531 - val_loss: 1853703424.0000 - val_rmse: 43054.6562\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2334997504.0000 - rmse: 48321.8125 - val_loss: 1428681344.0000 - val_rmse: 37797.9023\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2000231808.0000 - rmse: 44723.9492 - val_loss: 1311543808.0000 - val_rmse: 36215.2422\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1826005504.0000 - rmse: 42731.7852 - val_loss: 1184201984.0000 - val_rmse: 34412.2344\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1785039232.0000 - rmse: 42249.7266 - val_loss: 1091854336.0000 - val_rmse: 33043.2188\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1708462464.0000 - rmse: 41333.5508 - val_loss: 1050860032.0000 - val_rmse: 32416.9707\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1610120064.0000 - rmse: 40126.3008 - val_loss: 1021125632.0000 - val_rmse: 31955.0566\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1517356544.0000 - rmse: 38953.2617 - val_loss: 1095739392.0000 - val_rmse: 33101.9531\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1501670528.0000 - rmse: 38751.3945 - val_loss: 964270080.0000 - val_rmse: 31052.6992\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1466432128.0000 - rmse: 38294.0234 - val_loss: 930072000.0000 - val_rmse: 30497.0820\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1437570176.0000 - rmse: 37915.3008 - val_loss: 921628480.0000 - val_rmse: 30358.3340\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1372551680.0000 - rmse: 37047.9648 - val_loss: 912689024.0000 - val_rmse: 30210.7441\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1298998912.0000 - rmse: 36041.6289 - val_loss: 897204480.0000 - val_rmse: 29953.3711\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1354680448.0000 - rmse: 36805.9844 - val_loss: 910774080.0000 - val_rmse: 30179.0332\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1322107904.0000 - rmse: 36360.8008 - val_loss: 917427008.0000 - val_rmse: 30289.0566\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1350877056.0000 - rmse: 36754.2812 - val_loss: 864495808.0000 - val_rmse: 29402.3086\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1282138368.0000 - rmse: 35806.9609 - val_loss: 866773760.0000 - val_rmse: 29441.0215\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1245635584.0000 - rmse: 35293.5625 - val_loss: 988236288.0000 - val_rmse: 31436.2266\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1256200832.0000 - rmse: 35442.9219 - val_loss: 882053952.0000 - val_rmse: 29699.3926\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1139510912.0000 - rmse: 33756.6406 - val_loss: 865401216.0000 - val_rmse: 29417.7031\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1201258624.0000 - rmse: 34659.1797 - val_loss: 892351040.0000 - val_rmse: 29872.2461\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1166920704.0000 - rmse: 34160.2227 - val_loss: 1012244992.0000 - val_rmse: 31815.7969\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1176831232.0000 - rmse: 34304.9727 - val_loss: 866832320.0000 - val_rmse: 29442.0156\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1158219904.0000 - rmse: 34032.6289 - val_loss: 852633664.0000 - val_rmse: 29199.8906\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1093664128.0000 - rmse: 33070.5898 - val_loss: 865033024.0000 - val_rmse: 29411.4434\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1119183872.0000 - rmse: 33454.2070 - val_loss: 854072448.0000 - val_rmse: 29224.5176\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1082040960.0000 - rmse: 32894.3906 - val_loss: 862678080.0000 - val_rmse: 29371.3828\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1095596032.0000 - rmse: 33099.7891 - val_loss: 846603264.0000 - val_rmse: 29096.4473\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1127472384.0000 - rmse: 33577.8555 - val_loss: 890011584.0000 - val_rmse: 29833.0625\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1050936768.0000 - rmse: 32418.1543 - val_loss: 909254144.0000 - val_rmse: 30153.8418\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1059913600.0000 - rmse: 32556.3145 - val_loss: 852759744.0000 - val_rmse: 29202.0508\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1099400448.0000 - rmse: 33157.2070 - val_loss: 851842688.0000 - val_rmse: 29186.3438\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1051252608.0000 - rmse: 32423.0254 - val_loss: 862710848.0000 - val_rmse: 29371.9395\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1023148480.0000 - rmse: 31986.6914 - val_loss: 864760896.0000 - val_rmse: 29406.8164\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1072827264.0000 - rmse: 32754.0410 - val_loss: 867369408.0000 - val_rmse: 29451.1367\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1072281984.0000 - rmse: 32745.7168 - val_loss: 862331328.0000 - val_rmse: 29365.4785\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 999883904.0000 - rmse: 31620.9414 - val_loss: 912113792.0000 - val_rmse: 30201.2227\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1043636224.0000 - rmse: 32305.3594 - val_loss: 877558784.0000 - val_rmse: 29623.6191\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 972770560.0000 - rmse: 31189.2695 - val_loss: 855353664.0000 - val_rmse: 29246.4297\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 995218432.0000 - rmse: 31547.0820 - val_loss: 865198720.0000 - val_rmse: 29414.2598\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 937516352.0000 - rmse: 30618.8887 - val_loss: 860892544.0000 - val_rmse: 29340.9707\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 944826048.0000 - rmse: 30738.0234 - val_loss: 908267328.0000 - val_rmse: 30137.4746\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 994827520.0000 - rmse: 31540.8867 - val_loss: 870068800.0000 - val_rmse: 29496.9277\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 968673856.0000 - rmse: 31123.5254 - val_loss: 923383808.0000 - val_rmse: 30387.2305\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 978965056.0000 - rmse: 31288.4180 - val_loss: 883351616.0000 - val_rmse: 29721.2324\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 963381312.0000 - rmse: 31038.3848 - val_loss: 846811712.0000 - val_rmse: 29100.0293\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 931429184.0000 - rmse: 30519.3242 - val_loss: 864287488.0000 - val_rmse: 29398.7676\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 973491840.0000 - rmse: 31200.8301 - val_loss: 853704000.0000 - val_rmse: 29218.2129\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 995294400.0000 - rmse: 31548.2871 - val_loss: 864924992.0000 - val_rmse: 29409.6074\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 877745344.0000 - rmse: 29626.7676 - val_loss: 872182336.0000 - val_rmse: 29532.7324\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 905160128.0000 - rmse: 30085.8789 - val_loss: 833036928.0000 - val_rmse: 28862.3789\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 897004672.0000 - rmse: 29950.0371 - val_loss: 978771264.0000 - val_rmse: 31285.3203\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 941451008.0000 - rmse: 30683.0742 - val_loss: 864977344.0000 - val_rmse: 29410.4980\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 879828928.0000 - rmse: 29661.9102 - val_loss: 811987200.0000 - val_rmse: 28495.3887\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 922153024.0000 - rmse: 30366.9727 - val_loss: 920499136.0000 - val_rmse: 30339.7285\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 910094016.0000 - rmse: 30167.7637 - val_loss: 886353472.0000 - val_rmse: 29771.6895\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 878316288.0000 - rmse: 29636.4023 - val_loss: 938427520.0000 - val_rmse: 30633.7637\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 857914240.0000 - rmse: 29290.1738 - val_loss: 907702272.0000 - val_rmse: 30128.0977\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 903259200.0000 - rmse: 30054.2715 - val_loss: 856607424.0000 - val_rmse: 29267.8574\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 865990528.0000 - rmse: 29427.7168 - val_loss: 839796736.0000 - val_rmse: 28979.2461\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 815509440.0000 - rmse: 28557.1250 - val_loss: 854021632.0000 - val_rmse: 29223.6484\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 801658752.0000 - rmse: 28313.5781 - val_loss: 869075712.0000 - val_rmse: 29480.0898\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 824819072.0000 - rmse: 28719.6641 - val_loss: 857329664.0000 - val_rmse: 29280.1914\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 801665856.0000 - rmse: 28313.7051 - val_loss: 889678016.0000 - val_rmse: 29827.4707\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 813262848.0000 - rmse: 28517.7637 - val_loss: 870568384.0000 - val_rmse: 29505.3965\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 816765568.0000 - rmse: 28579.1113 - val_loss: 986331456.0000 - val_rmse: 31405.9141\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800677056.0000 - rmse: 28296.2383 - val_loss: 917848896.0000 - val_rmse: 30296.0215\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 821879872.0000 - rmse: 28668.4473 - val_loss: 934082688.0000 - val_rmse: 30562.7637\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 792165504.0000 - rmse: 28145.4355 - val_loss: 972074368.0000 - val_rmse: 31178.1074\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 784452352.0000 - rmse: 28008.0742 - val_loss: 966769408.0000 - val_rmse: 31092.9160\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 773388160.0000 - rmse: 27809.8574 - val_loss: 1080403968.0000 - val_rmse: 32869.5000\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 759127872.0000 - rmse: 27552.2734 - val_loss: 1004585984.0000 - val_rmse: 31695.2051\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 839071936.0000 - rmse: 28966.7383 - val_loss: 935960704.0000 - val_rmse: 30593.4746\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742972352.0000 - rmse: 27257.5176 - val_loss: 960276096.0000 - val_rmse: 30988.3203\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 786311552.0000 - rmse: 28041.2480 - val_loss: 964714816.0000 - val_rmse: 31059.8574\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 728249024.0000 - rmse: 26986.0898 - val_loss: 944886272.0000 - val_rmse: 30739.0020\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 754599744.0000 - rmse: 27469.9785 - val_loss: 973049216.0000 - val_rmse: 31193.7363\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721421312.0000 - rmse: 26859.2871 - val_loss: 943210496.0000 - val_rmse: 30711.7324\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691697088.0000 - rmse: 26300.1348 - val_loss: 969393792.0000 - val_rmse: 31135.0898\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 749808896.0000 - rmse: 27382.6367 - val_loss: 1128572544.0000 - val_rmse: 33594.2344\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 743746240.0000 - rmse: 27271.7109 - val_loss: 974320704.0000 - val_rmse: 31214.1113\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 713440832.0000 - rmse: 26710.3125 - val_loss: 1115239808.0000 - val_rmse: 33395.2070\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 674771456.0000 - rmse: 25976.3633 - val_loss: 1000129600.0000 - val_rmse: 31624.8242\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 713843648.0000 - rmse: 26717.8535 - val_loss: 983437696.0000 - val_rmse: 31359.8105\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 708948544.0000 - rmse: 26626.0859 - val_loss: 962364096.0000 - val_rmse: 31021.9941\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 725318720.0000 - rmse: 26931.7422 - val_loss: 1022576448.0000 - val_rmse: 31977.7500\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685921920.0000 - rmse: 26190.1113 - val_loss: 999426240.0000 - val_rmse: 31613.7031\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676561024.0000 - rmse: 26010.7852 - val_loss: 1080604032.0000 - val_rmse: 32872.5430\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 660795840.0000 - rmse: 25705.9492 - val_loss: 1037259456.0000 - val_rmse: 32206.5117\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599785088.0000 - rmse: 24490.5098 - val_loss: 1012828736.0000 - val_rmse: 31824.9707\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691104256.0000 - rmse: 26288.8613 - val_loss: 983810112.0000 - val_rmse: 31365.7480\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620096320.0000 - rmse: 24901.7324 - val_loss: 1001570560.0000 - val_rmse: 31647.5996\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614978368.0000 - rmse: 24798.7559 - val_loss: 988094912.0000 - val_rmse: 31433.9746\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647482048.0000 - rmse: 25445.6680 - val_loss: 1079055488.0000 - val_rmse: 32848.9766\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607518208.0000 - rmse: 24647.8848 - val_loss: 1056249664.0000 - val_rmse: 32499.9941\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 678288320.0000 - rmse: 26043.9668 - val_loss: 1127531264.0000 - val_rmse: 33578.7305\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632526080.0000 - rmse: 25150.0703 - val_loss: 1045265472.0000 - val_rmse: 32330.5664\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642047040.0000 - rmse: 25338.6465 - val_loss: 1043882752.0000 - val_rmse: 32309.1738\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610009664.0000 - rmse: 24698.3730 - val_loss: 1010774016.0000 - val_rmse: 31792.6719\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630512512.0000 - rmse: 25110.0078 - val_loss: 1151051264.0000 - val_rmse: 33927.1445\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572197440.0000 - rmse: 23920.6484 - val_loss: 1091610624.0000 - val_rmse: 33039.5312\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 670478912.0000 - rmse: 25893.6074 - val_loss: 1076336128.0000 - val_rmse: 32807.5625\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574288512.0000 - rmse: 23964.3184 - val_loss: 1100820736.0000 - val_rmse: 33178.6172\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614414272.0000 - rmse: 24787.3809 - val_loss: 1172850688.0000 - val_rmse: 34246.9062\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535534976.0000 - rmse: 23141.6289 - val_loss: 1245052544.0000 - val_rmse: 35285.3008\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576317760.0000 - rmse: 24006.6191 - val_loss: 1191638144.0000 - val_rmse: 34520.1133\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567107520.0000 - rmse: 23814.0176 - val_loss: 1121787648.0000 - val_rmse: 33493.0977\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676717632.0000 - rmse: 26013.7969 - val_loss: 1119873280.0000 - val_rmse: 33464.5078\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 602720320.0000 - rmse: 24550.3633 - val_loss: 1302592896.0000 - val_rmse: 36091.4531\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548172928.0000 - rmse: 23413.0938 - val_loss: 1186153472.0000 - val_rmse: 34440.5781\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533521824.0000 - rmse: 23098.0898 - val_loss: 1389540224.0000 - val_rmse: 37276.5391\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582509568.0000 - rmse: 24135.2344 - val_loss: 1088637824.0000 - val_rmse: 32994.5117\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520134624.0000 - rmse: 22806.4609 - val_loss: 1147266688.0000 - val_rmse: 33871.3242\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589436864.0000 - rmse: 24278.3203 - val_loss: 1055741248.0000 - val_rmse: 32492.1719\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533828992.0000 - rmse: 23104.7402 - val_loss: 1079733760.0000 - val_rmse: 32859.3008\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578875008.0000 - rmse: 24059.8223 - val_loss: 1053381696.0000 - val_rmse: 32455.8418\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510775712.0000 - rmse: 22600.3477 - val_loss: 1082760192.0000 - val_rmse: 32905.3203\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560763200.0000 - rmse: 23680.4395 - val_loss: 1075920000.0000 - val_rmse: 32801.2188\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561027712.0000 - rmse: 23686.0234 - val_loss: 1053131776.0000 - val_rmse: 32451.9922\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525590688.0000 - rmse: 22925.7656 - val_loss: 1038550656.0000 - val_rmse: 32226.5527\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444227136.0000 - rmse: 21076.6953 - val_loss: 1132294528.0000 - val_rmse: 33649.5859\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498100992.0000 - rmse: 22318.1758 - val_loss: 1161855104.0000 - val_rmse: 34085.9961\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534642784.0000 - rmse: 23122.3438 - val_loss: 1098572032.0000 - val_rmse: 33144.7148\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507327520.0000 - rmse: 22523.9316 - val_loss: 1190637056.0000 - val_rmse: 34505.6055\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495703456.0000 - rmse: 22264.3984 - val_loss: 1083853184.0000 - val_rmse: 32921.9219\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436347424.0000 - rmse: 20888.9297 - val_loss: 1286837120.0000 - val_rmse: 35872.5117\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461049408.0000 - rmse: 21472.0605 - val_loss: 1192898432.0000 - val_rmse: 34538.3633\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455231104.0000 - rmse: 21336.1445 - val_loss: 1276908800.0000 - val_rmse: 35733.8594\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496615040.0000 - rmse: 22284.8613 - val_loss: 1191369600.0000 - val_rmse: 34516.2227\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461403776.0000 - rmse: 21480.3105 - val_loss: 1178362880.0000 - val_rmse: 34327.2891\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542167104.0000 - rmse: 23284.4824 - val_loss: 1263175808.0000 - val_rmse: 35541.1836\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449361248.0000 - rmse: 21198.1426 - val_loss: 1141514880.0000 - val_rmse: 33786.3125\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485481440.0000 - rmse: 22033.6426 - val_loss: 1240712448.0000 - val_rmse: 35223.7500\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517123712.0000 - rmse: 22740.3516 - val_loss: 1127147904.0000 - val_rmse: 33573.0234\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529450304.0000 - rmse: 23009.7871 - val_loss: 1196477440.0000 - val_rmse: 34590.1367\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399561792.0000 - rmse: 19989.0410 - val_loss: 1191108736.0000 - val_rmse: 34512.4414\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408193824.0000 - rmse: 20203.8066 - val_loss: 1272355200.0000 - val_rmse: 35670.0859\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452440256.0000 - rmse: 21270.6426 - val_loss: 1203664896.0000 - val_rmse: 34693.8750\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468651264.0000 - rmse: 21648.3535 - val_loss: 1217154688.0000 - val_rmse: 34887.7422\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457803520.0000 - rmse: 21396.3438 - val_loss: 1213192704.0000 - val_rmse: 34830.9141\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455909280.0000 - rmse: 21352.0312 - val_loss: 1306636800.0000 - val_rmse: 36147.4297\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421972128.0000 - rmse: 20541.9609 - val_loss: 1263143424.0000 - val_rmse: 35540.7305\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441813024.0000 - rmse: 21019.3496 - val_loss: 1238923648.0000 - val_rmse: 35198.3477\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426712640.0000 - rmse: 20657.0234 - val_loss: 1110182912.0000 - val_rmse: 33319.4062\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399985248.0000 - rmse: 19999.6309 - val_loss: 1239345920.0000 - val_rmse: 35204.3438\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413780096.0000 - rmse: 20341.5840 - val_loss: 1191246336.0000 - val_rmse: 34514.4375\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414491328.0000 - rmse: 20359.0605 - val_loss: 1276054784.0000 - val_rmse: 35721.9062\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416916064.0000 - rmse: 20418.5234 - val_loss: 1064631232.0000 - val_rmse: 32628.6875\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353660896.0000 - rmse: 18805.8730 - val_loss: 1188861568.0000 - val_rmse: 34479.8711\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398285472.0000 - rmse: 19957.0898 - val_loss: 1323302016.0000 - val_rmse: 36377.2148\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417329792.0000 - rmse: 20428.6484 - val_loss: 1223575296.0000 - val_rmse: 34979.6406\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398170048.0000 - rmse: 19954.1992 - val_loss: 1093420928.0000 - val_rmse: 33066.9141\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445513088.0000 - rmse: 21107.1816 - val_loss: 1094812800.0000 - val_rmse: 33087.9570\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396722528.0000 - rmse: 19917.8945 - val_loss: 1030998400.0000 - val_rmse: 32109.1641\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391347776.0000 - rmse: 19782.5117 - val_loss: 1042125056.0000 - val_rmse: 32281.9609\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391325664.0000 - rmse: 19781.9531 - val_loss: 1105993216.0000 - val_rmse: 33256.4766\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361097728.0000 - rmse: 19002.5703 - val_loss: 1139402752.0000 - val_rmse: 33755.0352\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424978912.0000 - rmse: 20615.0176 - val_loss: 1116403328.0000 - val_rmse: 33412.6211\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490601536.0000 - rmse: 22149.5273 - val_loss: 1200547840.0000 - val_rmse: 34648.9219\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373510880.0000 - rmse: 19326.4277 - val_loss: 1031670656.0000 - val_rmse: 32119.6270\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392277536.0000 - rmse: 19805.9961 - val_loss: 1242743040.0000 - val_rmse: 35252.5625\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450100672.0000 - rmse: 21215.5762 - val_loss: 1235440000.0000 - val_rmse: 35148.8281\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365745280.0000 - rmse: 19124.4688 - val_loss: 1162473088.0000 - val_rmse: 34095.0586\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389834336.0000 - rmse: 19744.2227 - val_loss: 1211806464.0000 - val_rmse: 34811.0117\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385889856.0000 - rmse: 19644.0801 - val_loss: 1184737664.0000 - val_rmse: 34420.0195\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389323488.0000 - rmse: 19731.2812 - val_loss: 1172148864.0000 - val_rmse: 34236.6602\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344312288.0000 - rmse: 18555.6543 - val_loss: 1227000064.0000 - val_rmse: 35028.5625\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374201088.0000 - rmse: 19344.2773 - val_loss: 1221574016.0000 - val_rmse: 34951.0234\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468737120.0000 - rmse: 21650.3359 - val_loss: 1216388864.0000 - val_rmse: 34876.7656\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352079840.0000 - rmse: 18763.7891 - val_loss: 1116695808.0000 - val_rmse: 33417.0000\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336027488.0000 - rmse: 18331.0527 - val_loss: 1388409984.0000 - val_rmse: 37261.3750\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369087648.0000 - rmse: 19211.6523 - val_loss: 1492264704.0000 - val_rmse: 38629.8438\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334668064.0000 - rmse: 18293.9336 - val_loss: 1326657920.0000 - val_rmse: 36423.3164\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405075552.0000 - rmse: 20126.4863 - val_loss: 1322974080.0000 - val_rmse: 36372.7109\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367515232.0000 - rmse: 19170.6875 - val_loss: 1599321856.0000 - val_rmse: 39991.5234\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327362560.0000 - rmse: 18093.1621 - val_loss: 1116529280.0000 - val_rmse: 33414.5078\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351430880.0000 - rmse: 18746.4902 - val_loss: 1488471552.0000 - val_rmse: 38580.7148\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341352128.0000 - rmse: 18475.7168 - val_loss: 1339271424.0000 - val_rmse: 36596.0586\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321405248.0000 - rmse: 17927.7793 - val_loss: 1246328960.0000 - val_rmse: 35303.3828\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321021664.0000 - rmse: 17917.0762 - val_loss: 1459418752.0000 - val_rmse: 38202.3359\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435396608.0000 - rmse: 20866.1602 - val_loss: 1252421632.0000 - val_rmse: 35389.5703\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360905632.0000 - rmse: 18997.5156 - val_loss: 1171552128.0000 - val_rmse: 34227.9453\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330560864.0000 - rmse: 18181.3320 - val_loss: 1278668032.0000 - val_rmse: 35758.4688\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321074400.0000 - rmse: 17918.5488 - val_loss: 1571475968.0000 - val_rmse: 39641.8477\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373158592.0000 - rmse: 19317.3125 - val_loss: 1187201024.0000 - val_rmse: 34455.7852\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361944384.0000 - rmse: 19024.8359 - val_loss: 1214174848.0000 - val_rmse: 34845.0117\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324227168.0000 - rmse: 18006.3086 - val_loss: 1292272896.0000 - val_rmse: 35948.1992\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340374528.0000 - rmse: 18449.2422 - val_loss: 1212265344.0000 - val_rmse: 34817.6016\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333923552.0000 - rmse: 18273.5742 - val_loss: 1197971584.0000 - val_rmse: 34611.7266\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392378976.0000 - rmse: 19808.5586 - val_loss: 1257198720.0000 - val_rmse: 35457.0000\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359924384.0000 - rmse: 18971.6719 - val_loss: 1230621952.0000 - val_rmse: 35080.2227\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322261184.0000 - rmse: 17951.6309 - val_loss: 1306932352.0000 - val_rmse: 36151.5195\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325162496.0000 - rmse: 18032.2617 - val_loss: 1396562688.0000 - val_rmse: 37370.6133\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355336288.0000 - rmse: 18850.3652 - val_loss: 1447488128.0000 - val_rmse: 38045.8672\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383460384.0000 - rmse: 19582.1445 - val_loss: 1247364608.0000 - val_rmse: 35318.0469\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310404064.0000 - rmse: 17618.2871 - val_loss: 1597643904.0000 - val_rmse: 39970.5391\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354831040.0000 - rmse: 18836.9590 - val_loss: 1375192704.0000 - val_rmse: 37083.5898\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312084288.0000 - rmse: 17665.9082 - val_loss: 1265674624.0000 - val_rmse: 35576.3203\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300991360.0000 - rmse: 17349.1035 - val_loss: 1082160384.0000 - val_rmse: 32896.2070\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295485376.0000 - rmse: 17189.6875 - val_loss: 1231796096.0000 - val_rmse: 35096.9531\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296719488.0000 - rmse: 17225.5469 - val_loss: 1297048832.0000 - val_rmse: 36014.5625\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359415584.0000 - rmse: 18958.2598 - val_loss: 1142029056.0000 - val_rmse: 33793.9219\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285865216.0000 - rmse: 16907.5488 - val_loss: 1307217536.0000 - val_rmse: 36155.4648\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285179264.0000 - rmse: 16887.2520 - val_loss: 1383875840.0000 - val_rmse: 37200.4805\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361064416.0000 - rmse: 19001.6934 - val_loss: 1139093888.0000 - val_rmse: 33750.4648\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331139424.0000 - rmse: 18197.2363 - val_loss: 1216533248.0000 - val_rmse: 34878.8359\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322705600.0000 - rmse: 17964.0078 - val_loss: 1160536704.0000 - val_rmse: 34066.6523\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307735680.0000 - rmse: 17542.3945 - val_loss: 1080130816.0000 - val_rmse: 32865.3438\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305237856.0000 - rmse: 17471.0566 - val_loss: 1406588032.0000 - val_rmse: 37504.5078\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379601760.0000 - rmse: 19483.3711 - val_loss: 1368041216.0000 - val_rmse: 36987.0430\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315011072.0000 - rmse: 17748.5508 - val_loss: 1423577344.0000 - val_rmse: 37730.3242\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324396896.0000 - rmse: 18011.0195 - val_loss: 1151682816.0000 - val_rmse: 33936.4531\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372447136.0000 - rmse: 19298.8887 - val_loss: 1504859008.0000 - val_rmse: 38792.5117\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310996128.0000 - rmse: 17635.0820 - val_loss: 1430034048.0000 - val_rmse: 37815.7891\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335055072.0000 - rmse: 18304.5098 - val_loss: 1392023936.0000 - val_rmse: 37309.8359\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293285760.0000 - rmse: 17125.5879 - val_loss: 1364549888.0000 - val_rmse: 36939.8125\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290948416.0000 - rmse: 17057.2090 - val_loss: 1340275840.0000 - val_rmse: 36609.7773\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256923616.0000 - rmse: 16028.8359 - val_loss: 1432241408.0000 - val_rmse: 37844.9648\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297548640.0000 - rmse: 17249.5977 - val_loss: 1375744768.0000 - val_rmse: 37091.0352\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394775424.0000 - rmse: 19868.9551 - val_loss: 1359700992.0000 - val_rmse: 36874.1250\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353630816.0000 - rmse: 18805.0742 - val_loss: 1277888768.0000 - val_rmse: 35747.5703\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306578016.0000 - rmse: 17509.3691 - val_loss: 1548262784.0000 - val_rmse: 39347.9648\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250023376.0000 - rmse: 15812.1270 - val_loss: 1203128064.0000 - val_rmse: 34686.1328\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288241472.0000 - rmse: 16977.6738 - val_loss: 1359375104.0000 - val_rmse: 36869.7031\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314651520.0000 - rmse: 17738.4180 - val_loss: 1519556608.0000 - val_rmse: 38981.4922\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290191616.0000 - rmse: 17035.0098 - val_loss: 1295032064.0000 - val_rmse: 35986.5547\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304617408.0000 - rmse: 17453.2910 - val_loss: 1805746944.0000 - val_rmse: 42494.0820\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284429792.0000 - rmse: 16865.0449 - val_loss: 1410188928.0000 - val_rmse: 37552.4805\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291499520.0000 - rmse: 17073.3555 - val_loss: 1431756288.0000 - val_rmse: 37838.5547\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319253728.0000 - rmse: 17867.6719 - val_loss: 1304315648.0000 - val_rmse: 36115.3086\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278620896.0000 - rmse: 16691.9395 - val_loss: 1337832448.0000 - val_rmse: 36576.3906\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415251008.0000 - rmse: 20377.7070 - val_loss: 1505081216.0000 - val_rmse: 38795.3750\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328166016.0000 - rmse: 18115.3516 - val_loss: 1342451456.0000 - val_rmse: 36639.4805\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304295840.0000 - rmse: 17444.0762 - val_loss: 1183504384.0000 - val_rmse: 34402.0977\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258910640.0000 - rmse: 16090.7002 - val_loss: 1125808512.0000 - val_rmse: 33553.0703\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292415328.0000 - rmse: 17100.1562 - val_loss: 1244465792.0000 - val_rmse: 35276.9883\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257830416.0000 - rmse: 16057.0977 - val_loss: 1283745920.0000 - val_rmse: 35829.3984\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260826320.0000 - rmse: 16150.1172 - val_loss: 1595960192.0000 - val_rmse: 39949.4688\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338583936.0000 - rmse: 18400.6504 - val_loss: 1266921856.0000 - val_rmse: 35593.8477\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247346992.0000 - rmse: 15727.2686 - val_loss: 2228975616.0000 - val_rmse: 47212.0273\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268882368.0000 - rmse: 16397.6328 - val_loss: 1428451712.0000 - val_rmse: 37794.8633\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253357920.0000 - rmse: 15917.2197 - val_loss: 1688629760.0000 - val_rmse: 41092.9414\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238848784.0000 - rmse: 15454.7314 - val_loss: 1540331264.0000 - val_rmse: 39247.0547\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222531760.0000 - rmse: 14917.4971 - val_loss: 1376698112.0000 - val_rmse: 37103.8828\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294596928.0000 - rmse: 17163.8262 - val_loss: 1486847360.0000 - val_rmse: 38559.6602\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245451920.0000 - rmse: 15666.9033 - val_loss: 1921676800.0000 - val_rmse: 43836.9336\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222430112.0000 - rmse: 14914.0898 - val_loss: 1771410944.0000 - val_rmse: 42088.1328\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348982048.0000 - rmse: 18681.0605 - val_loss: 1422121216.0000 - val_rmse: 37711.0234\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210913136.0000 - rmse: 14522.8477 - val_loss: 1477329536.0000 - val_rmse: 38436.0430\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272189856.0000 - rmse: 16498.1758 - val_loss: 1417814016.0000 - val_rmse: 37653.8672\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301120800.0000 - rmse: 17352.8320 - val_loss: 1558782464.0000 - val_rmse: 39481.4180\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239942592.0000 - rmse: 15490.0801 - val_loss: 1436595968.0000 - val_rmse: 37902.4531\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279962944.0000 - rmse: 16732.0918 - val_loss: 1563830912.0000 - val_rmse: 39545.3008\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277618016.0000 - rmse: 16661.8730 - val_loss: 1611719808.0000 - val_rmse: 40146.2305\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338914208.0000 - rmse: 18409.6230 - val_loss: 1638738048.0000 - val_rmse: 40481.3281\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312526848.0000 - rmse: 17678.4297 - val_loss: 1304255104.0000 - val_rmse: 36114.4727\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 209620208.0000 - rmse: 14478.2666 - val_loss: 1447257984.0000 - val_rmse: 38042.8438\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218527632.0000 - rmse: 14782.6787 - val_loss: 1325218816.0000 - val_rmse: 36403.5547\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255061376.0000 - rmse: 15970.6406 - val_loss: 1704265728.0000 - val_rmse: 41282.7539\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242761568.0000 - rmse: 15580.8066 - val_loss: 1706271488.0000 - val_rmse: 41307.0391\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296305344.0000 - rmse: 17213.5215 - val_loss: 2193030656.0000 - val_rmse: 46829.8047\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289174400.0000 - rmse: 17005.1270 - val_loss: 1939398912.0000 - val_rmse: 44038.6055\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229850272.0000 - rmse: 15160.8135 - val_loss: 1557356416.0000 - val_rmse: 39463.3555\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246970416.0000 - rmse: 15715.2920 - val_loss: 1834026624.0000 - val_rmse: 42825.5352\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331071168.0000 - rmse: 18195.3613 - val_loss: 1586722432.0000 - val_rmse: 39833.6836\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259553296.0000 - rmse: 16110.6572 - val_loss: 1694508160.0000 - val_rmse: 41164.4023\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256062688.0000 - rmse: 16001.9590 - val_loss: 1483484160.0000 - val_rmse: 38516.0234\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242130544.0000 - rmse: 15560.5430 - val_loss: 1339604736.0000 - val_rmse: 36600.6094\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254069984.0000 - rmse: 15939.5713 - val_loss: 1630904704.0000 - val_rmse: 40384.4609\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276838112.0000 - rmse: 16638.4531 - val_loss: 1688905472.0000 - val_rmse: 41096.2969\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217114336.0000 - rmse: 14734.7988 - val_loss: 1364924928.0000 - val_rmse: 36944.8906\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249659360.0000 - rmse: 15800.6104 - val_loss: 1808207744.0000 - val_rmse: 42523.0273\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302185824.0000 - rmse: 17383.4922 - val_loss: 1392226048.0000 - val_rmse: 37312.5469\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244244064.0000 - rmse: 15628.3086 - val_loss: 1798678656.0000 - val_rmse: 42410.8320\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297561376.0000 - rmse: 17249.9668 - val_loss: 1492249344.0000 - val_rmse: 38629.6445\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276341664.0000 - rmse: 16623.5273 - val_loss: 1173472384.0000 - val_rmse: 34255.9844\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276639392.0000 - rmse: 16632.4785 - val_loss: 1244647680.0000 - val_rmse: 35279.5664\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290373504.0000 - rmse: 17040.3477 - val_loss: 1224495360.0000 - val_rmse: 34992.7891\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222678112.0000 - rmse: 14922.4023 - val_loss: 1520472320.0000 - val_rmse: 38993.2344\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267987248.0000 - rmse: 16370.3145 - val_loss: 1404167424.0000 - val_rmse: 37472.2227\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267590528.0000 - rmse: 16358.1943 - val_loss: 1386106880.0000 - val_rmse: 37230.4570\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228288640.0000 - rmse: 15109.2236 - val_loss: 1042450304.0000 - val_rmse: 32286.9980\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228228192.0000 - rmse: 15107.2236 - val_loss: 1177995392.0000 - val_rmse: 34321.9375\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260111344.0000 - rmse: 16127.9658 - val_loss: 1513121664.0000 - val_rmse: 38898.8633\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226299872.0000 - rmse: 15043.2666 - val_loss: 1385660032.0000 - val_rmse: 37224.4531\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 198126736.0000 - rmse: 14075.7490 - val_loss: 1654134656.0000 - val_rmse: 40671.0547\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239728080.0000 - rmse: 15483.1543 - val_loss: 1295934208.0000 - val_rmse: 35999.0859\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292613696.0000 - rmse: 17105.9531 - val_loss: 1116013312.0000 - val_rmse: 33406.7852\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257094672.0000 - rmse: 16034.1699 - val_loss: 1152924160.0000 - val_rmse: 33954.7383\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 199427792.0000 - rmse: 14121.8896 - val_loss: 1214540160.0000 - val_rmse: 34850.2539\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257746688.0000 - rmse: 16054.4902 - val_loss: 1488658432.0000 - val_rmse: 38583.1367\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220053152.0000 - rmse: 14834.1885 - val_loss: 1646473728.0000 - val_rmse: 40576.7617\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221581776.0000 - rmse: 14885.6211 - val_loss: 1362021632.0000 - val_rmse: 36905.5781\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256380592.0000 - rmse: 16011.8877 - val_loss: 1146516224.0000 - val_rmse: 33860.2461\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 198763440.0000 - rmse: 14098.3467 - val_loss: 1379856512.0000 - val_rmse: 37146.4180\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 233542128.0000 - rmse: 15282.0840 - val_loss: 1295774976.0000 - val_rmse: 35996.8750\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219038880.0000 - rmse: 14799.9609 - val_loss: 1670972544.0000 - val_rmse: 40877.5312\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237968704.0000 - rmse: 15426.2324 - val_loss: 1264084096.0000 - val_rmse: 35553.9570\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234134784.0000 - rmse: 15301.4619 - val_loss: 1464036992.0000 - val_rmse: 38262.7383\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211433520.0000 - rmse: 14540.7529 - val_loss: 2282111744.0000 - val_rmse: 47771.4531\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280515552.0000 - rmse: 16748.5977 - val_loss: 1114802176.0000 - val_rmse: 33388.6523\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277742016.0000 - rmse: 16665.5918 - val_loss: 1450052352.0000 - val_rmse: 38079.5547\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270662880.0000 - rmse: 16451.8359 - val_loss: 1242053120.0000 - val_rmse: 35242.7734\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227315264.0000 - rmse: 15076.9766 - val_loss: 1124362240.0000 - val_rmse: 33531.5117\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 208129984.0000 - rmse: 14426.7100 - val_loss: 1454915456.0000 - val_rmse: 38143.3555\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226367696.0000 - rmse: 15045.5205 - val_loss: 1569998336.0000 - val_rmse: 39623.2031\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236616032.0000 - rmse: 15382.3281 - val_loss: 2155824128.0000 - val_rmse: 46430.8516\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254520608.0000 - rmse: 15953.7012 - val_loss: 1397113088.0000 - val_rmse: 37377.9766\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262756256.0000 - rmse: 16209.7578 - val_loss: 1421820160.0000 - val_rmse: 37707.0312\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206308496.0000 - rmse: 14363.4404 - val_loss: 1384620544.0000 - val_rmse: 37210.4883\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251968256.0000 - rmse: 15873.5068 - val_loss: 1605678208.0000 - val_rmse: 40070.9141\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324316000.0000 - rmse: 18008.7754 - val_loss: 1482245760.0000 - val_rmse: 38499.9453\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236241824.0000 - rmse: 15370.1602 - val_loss: 1677087872.0000 - val_rmse: 40952.2617\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273152128.0000 - rmse: 16527.3145 - val_loss: 1544022528.0000 - val_rmse: 39294.0469\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229980144.0000 - rmse: 15165.0947 - val_loss: 2040769280.0000 - val_rmse: 45174.8750\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265871552.0000 - rmse: 16305.5674 - val_loss: 1476461568.0000 - val_rmse: 38424.7539\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248201472.0000 - rmse: 15754.4111 - val_loss: 1339606784.0000 - val_rmse: 36600.6406\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265487376.0000 - rmse: 16293.7832 - val_loss: 1233293440.0000 - val_rmse: 35118.2773\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241362768.0000 - rmse: 15535.8525 - val_loss: 1088923904.0000 - val_rmse: 32998.8477\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 205017984.0000 - rmse: 14318.4473 - val_loss: 1201520512.0000 - val_rmse: 34662.9531\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266241664.0000 - rmse: 16316.9121 - val_loss: 1146895360.0000 - val_rmse: 33865.8438\n",
      "104/104 [==============================] - 0s 713us/step - loss: 466765184.0000 - rmse: 21604.7500\n",
      "[466765184.0, 21604.75]\n",
      "[20003.724609375, 25916.537109375, 29746.79296875, 22110.357421875, 21604.75]\n",
      "23876.432421875\n"
     ]
    }
   ],
   "source": [
    "k_fold(\"baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Result of Baseline Model and Generate the CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[171288.61  296763.6   225639.78  164268.11   67766.63  113638.914\n",
      "  44488.5    44426.99   75725.62  112283.48 ] (5000,)\n"
     ]
    }
   ],
   "source": [
    "predict(baseline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and get the Embedding Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16589, 123) (16589,)\n",
      "(13271, 123) (13271,) (3318, 123) (3318,)\n",
      "<src.model.emb_model object at 0x7f8468412410>\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 23887517696.0000 - rmse: 154555.8750 - val_loss: 10976982016.0000 - val_rmse: 104771.0938\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 3628226560.0000 - rmse: 60234.7617 - val_loss: 1828237056.0000 - val_rmse: 42757.8906\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2356172288.0000 - rmse: 48540.4180 - val_loss: 1341848192.0000 - val_rmse: 36631.2461\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1954442112.0000 - rmse: 44209.0742 - val_loss: 1200975232.0000 - val_rmse: 34655.0898\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1864350976.0000 - rmse: 43178.1289 - val_loss: 1099503104.0000 - val_rmse: 33158.7578\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1770189696.0000 - rmse: 42073.6211 - val_loss: 1205104640.0000 - val_rmse: 34714.6172\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1691198848.0000 - rmse: 41124.1875 - val_loss: 986866880.0000 - val_rmse: 31414.4375\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1701554816.0000 - rmse: 41249.9062 - val_loss: 974720000.0000 - val_rmse: 31220.5059\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1569862272.0000 - rmse: 39621.4883 - val_loss: 938933376.0000 - val_rmse: 30642.0195\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1572438016.0000 - rmse: 39653.9805 - val_loss: 926136064.0000 - val_rmse: 30432.4844\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1553016320.0000 - rmse: 39408.3281 - val_loss: 886487168.0000 - val_rmse: 29773.9336\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1583277312.0000 - rmse: 39790.4180 - val_loss: 882082368.0000 - val_rmse: 29699.8711\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1460976256.0000 - rmse: 38222.7188 - val_loss: 852127680.0000 - val_rmse: 29191.2266\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1474974336.0000 - rmse: 38405.3945 - val_loss: 836922048.0000 - val_rmse: 28929.6055\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1409494400.0000 - rmse: 37543.2344 - val_loss: 813394496.0000 - val_rmse: 28520.0723\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1382552704.0000 - rmse: 37182.6953 - val_loss: 788482752.0000 - val_rmse: 28079.9355\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1386231936.0000 - rmse: 37232.1367 - val_loss: 761762624.0000 - val_rmse: 27600.0469\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1303696000.0000 - rmse: 36106.7305 - val_loss: 741255424.0000 - val_rmse: 27226.0059\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1304815872.0000 - rmse: 36122.2344 - val_loss: 771722304.0000 - val_rmse: 27779.8906\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1220095872.0000 - rmse: 34929.8711 - val_loss: 715033280.0000 - val_rmse: 26740.1055\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1084248832.0000 - rmse: 32927.9336 - val_loss: 713721024.0000 - val_rmse: 26715.5586\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1063186944.0000 - rmse: 32606.5469 - val_loss: 685801856.0000 - val_rmse: 26187.8184\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1157883392.0000 - rmse: 34027.6875 - val_loss: 703269632.0000 - val_rmse: 26519.2305\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1072365120.0000 - rmse: 32746.9863 - val_loss: 746634112.0000 - val_rmse: 27324.6055\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1087494528.0000 - rmse: 32977.1836 - val_loss: 681822336.0000 - val_rmse: 26111.7285\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1078854656.0000 - rmse: 32845.9219 - val_loss: 665880832.0000 - val_rmse: 25804.6660\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1003803904.0000 - rmse: 31682.8652 - val_loss: 653723776.0000 - val_rmse: 25568.0234\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1029576704.0000 - rmse: 32087.0176 - val_loss: 634913792.0000 - val_rmse: 25197.4961\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 995771456.0000 - rmse: 31555.8457 - val_loss: 607738048.0000 - val_rmse: 24652.3438\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1069213568.0000 - rmse: 32698.8320 - val_loss: 611780352.0000 - val_rmse: 24734.1934\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 972849984.0000 - rmse: 31190.5430 - val_loss: 634754688.0000 - val_rmse: 25194.3379\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 988578432.0000 - rmse: 31441.6680 - val_loss: 610174976.0000 - val_rmse: 24701.7207\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1015695232.0000 - rmse: 31869.9746 - val_loss: 573127680.0000 - val_rmse: 23940.0859\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 972273920.0000 - rmse: 31181.3066 - val_loss: 575364544.0000 - val_rmse: 23986.7578\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 936807360.0000 - rmse: 30607.3086 - val_loss: 565139264.0000 - val_rmse: 23772.6582\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 894764224.0000 - rmse: 29912.6094 - val_loss: 584141056.0000 - val_rmse: 24169.0098\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 810297792.0000 - rmse: 28465.7305 - val_loss: 578158400.0000 - val_rmse: 24044.9238\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 821831488.0000 - rmse: 28667.6035 - val_loss: 580780992.0000 - val_rmse: 24099.3984\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 810052800.0000 - rmse: 28461.4258 - val_loss: 516052544.0000 - val_rmse: 22716.7891\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 844766272.0000 - rmse: 29064.8633 - val_loss: 494058560.0000 - val_rmse: 22227.4277\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 832593728.0000 - rmse: 28854.7012 - val_loss: 509034880.0000 - val_rmse: 22561.8008\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 799573120.0000 - rmse: 28276.7246 - val_loss: 564358144.0000 - val_rmse: 23756.2227\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 783966336.0000 - rmse: 27999.3984 - val_loss: 539143872.0000 - val_rmse: 23219.4727\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 792136064.0000 - rmse: 28144.9121 - val_loss: 479496832.0000 - val_rmse: 21897.4160\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 738714432.0000 - rmse: 27179.3008 - val_loss: 492961504.0000 - val_rmse: 22202.7363\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 714360704.0000 - rmse: 26727.5273 - val_loss: 468790048.0000 - val_rmse: 21651.5605\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 712817088.0000 - rmse: 26698.6348 - val_loss: 455606272.0000 - val_rmse: 21344.9355\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 740595776.0000 - rmse: 27213.8887 - val_loss: 501314720.0000 - val_rmse: 22390.0586\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 764769344.0000 - rmse: 27654.4629 - val_loss: 475643936.0000 - val_rmse: 21809.2617\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 682555840.0000 - rmse: 26125.7695 - val_loss: 491960512.0000 - val_rmse: 22180.1836\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 657542080.0000 - rmse: 25642.5840 - val_loss: 466622304.0000 - val_rmse: 21601.4414\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 730287488.0000 - rmse: 27023.8320 - val_loss: 484489696.0000 - val_rmse: 22011.1270\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 691894848.0000 - rmse: 26303.8945 - val_loss: 454165568.0000 - val_rmse: 21311.1602\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 697280640.0000 - rmse: 26406.0723 - val_loss: 446326368.0000 - val_rmse: 21126.4375\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 666426944.0000 - rmse: 25815.2461 - val_loss: 459500928.0000 - val_rmse: 21435.9727\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 711441536.0000 - rmse: 26672.8613 - val_loss: 432146464.0000 - val_rmse: 20788.1328\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 715872576.0000 - rmse: 26755.7949 - val_loss: 434393632.0000 - val_rmse: 20842.1113\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 631249536.0000 - rmse: 25124.6797 - val_loss: 864825984.0000 - val_rmse: 29407.9238\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 630125632.0000 - rmse: 25102.3027 - val_loss: 463641312.0000 - val_rmse: 21532.3320\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 630709952.0000 - rmse: 25113.9395 - val_loss: 449308384.0000 - val_rmse: 21196.8965\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 558903744.0000 - rmse: 23641.1445 - val_loss: 415295744.0000 - val_rmse: 20378.8066\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 597051968.0000 - rmse: 24434.6465 - val_loss: 423010432.0000 - val_rmse: 20567.2168\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 649516480.0000 - rmse: 25485.6133 - val_loss: 459078368.0000 - val_rmse: 21426.1133\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 650121984.0000 - rmse: 25497.4902 - val_loss: 401442112.0000 - val_rmse: 20036.0195\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 639141056.0000 - rmse: 25281.2383 - val_loss: 416083040.0000 - val_rmse: 20398.1133\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 601020224.0000 - rmse: 24515.7129 - val_loss: 418098240.0000 - val_rmse: 20447.4512\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 633456448.0000 - rmse: 25168.5605 - val_loss: 413774528.0000 - val_rmse: 20341.4492\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 609028608.0000 - rmse: 24678.5059 - val_loss: 509351456.0000 - val_rmse: 22568.8164\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 603653760.0000 - rmse: 24569.3672 - val_loss: 448813440.0000 - val_rmse: 21185.2168\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 557652864.0000 - rmse: 23614.6738 - val_loss: 408234752.0000 - val_rmse: 20204.8203\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 587477312.0000 - rmse: 24237.9316 - val_loss: 400139648.0000 - val_rmse: 20003.4902\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 613922112.0000 - rmse: 24777.4512 - val_loss: 433924608.0000 - val_rmse: 20830.8574\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 584652096.0000 - rmse: 24179.5801 - val_loss: 466304992.0000 - val_rmse: 21594.0957\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 551134976.0000 - rmse: 23476.2637 - val_loss: 401571552.0000 - val_rmse: 20039.2500\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 603513792.0000 - rmse: 24566.5176 - val_loss: 400908384.0000 - val_rmse: 20022.6973\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 546536192.0000 - rmse: 23378.1133 - val_loss: 421340800.0000 - val_rmse: 20526.5879\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 578810560.0000 - rmse: 24058.4824 - val_loss: 458926624.0000 - val_rmse: 21422.5723\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 563374976.0000 - rmse: 23735.5215 - val_loss: 449527936.0000 - val_rmse: 21202.0742\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 497275648.0000 - rmse: 22299.6777 - val_loss: 423654880.0000 - val_rmse: 20582.8789\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 520818976.0000 - rmse: 22821.4590 - val_loss: 404444832.0000 - val_rmse: 20110.8145\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 608316864.0000 - rmse: 24664.0801 - val_loss: 406401184.0000 - val_rmse: 20159.3945\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 530653920.0000 - rmse: 23035.9258 - val_loss: 376818624.0000 - val_rmse: 19411.8164\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 590769920.0000 - rmse: 24305.7598 - val_loss: 377241920.0000 - val_rmse: 19422.7168\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 533274688.0000 - rmse: 23092.7402 - val_loss: 379717568.0000 - val_rmse: 19486.3438\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 595622528.0000 - rmse: 24405.3789 - val_loss: 416049024.0000 - val_rmse: 20397.2793\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 521147104.0000 - rmse: 22828.6465 - val_loss: 375040896.0000 - val_rmse: 19365.9727\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 566745600.0000 - rmse: 23806.4199 - val_loss: 376103712.0000 - val_rmse: 19393.3926\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 586151296.0000 - rmse: 24210.5625 - val_loss: 369253856.0000 - val_rmse: 19215.9785\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 542608960.0000 - rmse: 23293.9688 - val_loss: 525862048.0000 - val_rmse: 22931.6816\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 506208000.0000 - rmse: 22499.0664 - val_loss: 393648896.0000 - val_rmse: 19840.5879\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 522645440.0000 - rmse: 22861.4395 - val_loss: 365010848.0000 - val_rmse: 19105.2578\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 540412800.0000 - rmse: 23246.7812 - val_loss: 381879808.0000 - val_rmse: 19541.7461\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 494311616.0000 - rmse: 22233.1191 - val_loss: 366300000.0000 - val_rmse: 19138.9648\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 566748608.0000 - rmse: 23806.4824 - val_loss: 385774240.0000 - val_rmse: 19641.1367\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 511414304.0000 - rmse: 22614.4707 - val_loss: 381896576.0000 - val_rmse: 19542.1738\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 560322880.0000 - rmse: 23671.1406 - val_loss: 363275840.0000 - val_rmse: 19059.7969\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 527109408.0000 - rmse: 22958.8633 - val_loss: 383269056.0000 - val_rmse: 19577.2578\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 540322944.0000 - rmse: 23244.8477 - val_loss: 369960064.0000 - val_rmse: 19234.3457\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 548840320.0000 - rmse: 23427.3418 - val_loss: 360893184.0000 - val_rmse: 18997.1895\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 563232064.0000 - rmse: 23732.5098 - val_loss: 397363392.0000 - val_rmse: 19933.9766\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 489650720.0000 - rmse: 22128.0527 - val_loss: 378944160.0000 - val_rmse: 19466.4883\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 555782592.0000 - rmse: 23575.0410 - val_loss: 377770560.0000 - val_rmse: 19436.3203\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 476056064.0000 - rmse: 21818.7090 - val_loss: 418107136.0000 - val_rmse: 20447.6680\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 481906496.0000 - rmse: 21952.3691 - val_loss: 347140384.0000 - val_rmse: 18631.7031\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 486946304.0000 - rmse: 22066.8594 - val_loss: 357542112.0000 - val_rmse: 18908.7832\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 528188160.0000 - rmse: 22982.3438 - val_loss: 383481760.0000 - val_rmse: 19582.6895\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 600198656.0000 - rmse: 24498.9512 - val_loss: 371006784.0000 - val_rmse: 19261.5371\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 509513888.0000 - rmse: 22572.4141 - val_loss: 380395840.0000 - val_rmse: 19503.7383\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 455276416.0000 - rmse: 21337.2070 - val_loss: 349441664.0000 - val_rmse: 18693.3594\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 475388576.0000 - rmse: 21803.4082 - val_loss: 402153120.0000 - val_rmse: 20053.7559\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 495775936.0000 - rmse: 22266.0273 - val_loss: 353791968.0000 - val_rmse: 18809.3594\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 527659360.0000 - rmse: 22970.8379 - val_loss: 473946688.0000 - val_rmse: 21770.3164\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 477735360.0000 - rmse: 21857.1582 - val_loss: 374529920.0000 - val_rmse: 19352.7754\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 497939360.0000 - rmse: 22314.5547 - val_loss: 375632416.0000 - val_rmse: 19381.2383\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 489806080.0000 - rmse: 22131.5625 - val_loss: 384464864.0000 - val_rmse: 19607.7754\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 504307264.0000 - rmse: 22456.7871 - val_loss: 409126176.0000 - val_rmse: 20226.8672\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 517431968.0000 - rmse: 22747.1309 - val_loss: 373709824.0000 - val_rmse: 19331.5762\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 479951136.0000 - rmse: 21907.7871 - val_loss: 402258336.0000 - val_rmse: 20056.3789\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 507640288.0000 - rmse: 22530.8750 - val_loss: 382742432.0000 - val_rmse: 19563.8047\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 433949376.0000 - rmse: 20831.4512 - val_loss: 398073728.0000 - val_rmse: 19951.7852\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 497524384.0000 - rmse: 22305.2539 - val_loss: 364677760.0000 - val_rmse: 19096.5371\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 483439552.0000 - rmse: 21987.2598 - val_loss: 365338528.0000 - val_rmse: 19113.8301\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 421488576.0000 - rmse: 20530.1875 - val_loss: 550969600.0000 - val_rmse: 23472.7422\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 507218304.0000 - rmse: 22521.5078 - val_loss: 364375360.0000 - val_rmse: 19088.6191\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 469910368.0000 - rmse: 21677.4160 - val_loss: 387325472.0000 - val_rmse: 19680.5859\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 524559168.0000 - rmse: 22903.2559 - val_loss: 433377056.0000 - val_rmse: 20817.7109\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 444153824.0000 - rmse: 21074.9570 - val_loss: 369196224.0000 - val_rmse: 19214.4805\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 427688320.0000 - rmse: 20680.6270 - val_loss: 380384864.0000 - val_rmse: 19503.4570\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 504256448.0000 - rmse: 22455.6543 - val_loss: 386611488.0000 - val_rmse: 19662.4395\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 438885408.0000 - rmse: 20949.5918 - val_loss: 358933824.0000 - val_rmse: 18945.5488\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 456589248.0000 - rmse: 21367.9492 - val_loss: 354810240.0000 - val_rmse: 18836.4082\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 451393408.0000 - rmse: 21246.0215 - val_loss: 375393216.0000 - val_rmse: 19375.0664\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 473665472.0000 - rmse: 21763.8574 - val_loss: 382878528.0000 - val_rmse: 19567.2812\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 406014624.0000 - rmse: 20149.8047 - val_loss: 378660512.0000 - val_rmse: 19459.2012\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 423824832.0000 - rmse: 20587.0059 - val_loss: 392894944.0000 - val_rmse: 19821.5781\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 538307584.0000 - rmse: 23201.4570 - val_loss: 424129792.0000 - val_rmse: 20594.4121\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 446639488.0000 - rmse: 21133.8477 - val_loss: 464996640.0000 - val_rmse: 21563.7812\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 421345376.0000 - rmse: 20526.6992 - val_loss: 412407488.0000 - val_rmse: 20307.8184\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 498051392.0000 - rmse: 22317.0645 - val_loss: 393223392.0000 - val_rmse: 19829.8613\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 491040960.0000 - rmse: 22159.4434 - val_loss: 477365760.0000 - val_rmse: 21848.7012\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 488519520.0000 - rmse: 22102.4785 - val_loss: 369169216.0000 - val_rmse: 19213.7773\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 481192576.0000 - rmse: 21936.1016 - val_loss: 481223872.0000 - val_rmse: 21936.8164\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 449598432.0000 - rmse: 21203.7363 - val_loss: 426199584.0000 - val_rmse: 20644.6016\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 470641632.0000 - rmse: 21694.2773 - val_loss: 359842048.0000 - val_rmse: 18969.5039\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 415792896.0000 - rmse: 20391.0000 - val_loss: 349179104.0000 - val_rmse: 18686.3340\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 394794080.0000 - rmse: 19869.4258 - val_loss: 479536960.0000 - val_rmse: 21898.3320\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 428847424.0000 - rmse: 20708.6309 - val_loss: 354326176.0000 - val_rmse: 18823.5547\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 440320000.0000 - rmse: 20983.8027 - val_loss: 375440896.0000 - val_rmse: 19376.2969\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 404535712.0000 - rmse: 20113.0723 - val_loss: 347413184.0000 - val_rmse: 18639.0234\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 435254560.0000 - rmse: 20862.7559 - val_loss: 355731200.0000 - val_rmse: 18860.8379\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 400911552.0000 - rmse: 20022.7754 - val_loss: 354950048.0000 - val_rmse: 18840.1172\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 423710144.0000 - rmse: 20584.2207 - val_loss: 395495552.0000 - val_rmse: 19887.0703\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 398036736.0000 - rmse: 19950.8574 - val_loss: 335672160.0000 - val_rmse: 18321.3574\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 418645312.0000 - rmse: 20460.8242 - val_loss: 340346656.0000 - val_rmse: 18448.4863\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 428762144.0000 - rmse: 20706.5723 - val_loss: 367177440.0000 - val_rmse: 19161.8750\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 417221888.0000 - rmse: 20426.0098 - val_loss: 364804608.0000 - val_rmse: 19099.8594\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 419829824.0000 - rmse: 20489.7500 - val_loss: 342722400.0000 - val_rmse: 18512.7637\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 362133632.0000 - rmse: 19029.8086 - val_loss: 363156672.0000 - val_rmse: 19056.6699\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 417534336.0000 - rmse: 20433.6562 - val_loss: 369747360.0000 - val_rmse: 19228.8164\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 393566432.0000 - rmse: 19838.5098 - val_loss: 377767040.0000 - val_rmse: 19436.2305\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 422228448.0000 - rmse: 20548.1973 - val_loss: 489879808.0000 - val_rmse: 22133.2285\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 425506720.0000 - rmse: 20627.8145 - val_loss: 374364736.0000 - val_rmse: 19348.5078\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 402496672.0000 - rmse: 20062.3203 - val_loss: 420642240.0000 - val_rmse: 20509.5645\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 381332384.0000 - rmse: 19527.7344 - val_loss: 390898272.0000 - val_rmse: 19771.1484\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 399626144.0000 - rmse: 19990.6523 - val_loss: 485797216.0000 - val_rmse: 22040.8086\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 381957760.0000 - rmse: 19543.7402 - val_loss: 385139488.0000 - val_rmse: 19624.9707\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 392107712.0000 - rmse: 19801.7090 - val_loss: 362392576.0000 - val_rmse: 19036.6113\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 383851424.0000 - rmse: 19592.1270 - val_loss: 384935104.0000 - val_rmse: 19619.7637\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 415857120.0000 - rmse: 20392.5742 - val_loss: 371458144.0000 - val_rmse: 19273.2500\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 383668992.0000 - rmse: 19587.4707 - val_loss: 493336800.0000 - val_rmse: 22211.1855\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 358789696.0000 - rmse: 18941.7441 - val_loss: 421910208.0000 - val_rmse: 20540.4531\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 395083072.0000 - rmse: 19876.6973 - val_loss: 345194912.0000 - val_rmse: 18579.4219\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 386800800.0000 - rmse: 19667.2520 - val_loss: 349532864.0000 - val_rmse: 18695.7988\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 403411648.0000 - rmse: 20085.1094 - val_loss: 402205216.0000 - val_rmse: 20055.0547\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 451415872.0000 - rmse: 21246.5488 - val_loss: 363841984.0000 - val_rmse: 19074.6426\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 422187008.0000 - rmse: 20547.1895 - val_loss: 439284512.0000 - val_rmse: 20959.1152\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 417669280.0000 - rmse: 20436.9590 - val_loss: 357287776.0000 - val_rmse: 18902.0566\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 384520064.0000 - rmse: 19609.1836 - val_loss: 376493920.0000 - val_rmse: 19403.4512\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 366427104.0000 - rmse: 19142.2852 - val_loss: 385212992.0000 - val_rmse: 19626.8438\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 381611616.0000 - rmse: 19534.8828 - val_loss: 363721184.0000 - val_rmse: 19071.4766\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 409144864.0000 - rmse: 20227.3301 - val_loss: 366316832.0000 - val_rmse: 19139.4043\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 415376544.0000 - rmse: 20380.7891 - val_loss: 350653184.0000 - val_rmse: 18725.7363\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 425761408.0000 - rmse: 20633.9863 - val_loss: 360132480.0000 - val_rmse: 18977.1562\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 410749216.0000 - rmse: 20266.9492 - val_loss: 466620480.0000 - val_rmse: 21601.4004\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 385254400.0000 - rmse: 19627.8984 - val_loss: 347117376.0000 - val_rmse: 18631.0859\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 362929984.0000 - rmse: 19050.7207 - val_loss: 355438560.0000 - val_rmse: 18853.0781\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 397806944.0000 - rmse: 19945.0977 - val_loss: 379231520.0000 - val_rmse: 19473.8672\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 373971360.0000 - rmse: 19338.3398 - val_loss: 352441984.0000 - val_rmse: 18773.4375\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 375913152.0000 - rmse: 19388.4805 - val_loss: 352234880.0000 - val_rmse: 18767.9219\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 438892608.0000 - rmse: 20949.7637 - val_loss: 365173344.0000 - val_rmse: 19109.5098\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 424493952.0000 - rmse: 20603.2520 - val_loss: 357866592.0000 - val_rmse: 18917.3613\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 399765440.0000 - rmse: 19994.1348 - val_loss: 370531520.0000 - val_rmse: 19249.1953\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 372333312.0000 - rmse: 19295.9395 - val_loss: 365359488.0000 - val_rmse: 19114.3789\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 342258240.0000 - rmse: 18500.2227 - val_loss: 372966848.0000 - val_rmse: 19312.3496\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 351483296.0000 - rmse: 18747.8887 - val_loss: 376774112.0000 - val_rmse: 19410.6699\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 395638816.0000 - rmse: 19890.6719 - val_loss: 405200384.0000 - val_rmse: 20129.5898\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 384250880.0000 - rmse: 19602.3184 - val_loss: 360959488.0000 - val_rmse: 18998.9336\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 353215712.0000 - rmse: 18794.0332 - val_loss: 380952192.0000 - val_rmse: 19517.9961\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 349101280.0000 - rmse: 18684.2520 - val_loss: 405691968.0000 - val_rmse: 20141.7969\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 355335264.0000 - rmse: 18850.3379 - val_loss: 387105280.0000 - val_rmse: 19674.9922\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 404206976.0000 - rmse: 20104.8984 - val_loss: 356658272.0000 - val_rmse: 18885.3984\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 387342240.0000 - rmse: 19681.0117 - val_loss: 368666016.0000 - val_rmse: 19200.6777\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 373918816.0000 - rmse: 19336.9805 - val_loss: 359994048.0000 - val_rmse: 18973.5098\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 389641184.0000 - rmse: 19739.3301 - val_loss: 378067616.0000 - val_rmse: 19443.9609\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 330465024.0000 - rmse: 18178.6973 - val_loss: 359272768.0000 - val_rmse: 18954.4922\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 354139328.0000 - rmse: 18818.5898 - val_loss: 385327328.0000 - val_rmse: 19629.7559\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 359531584.0000 - rmse: 18961.3184 - val_loss: 352925888.0000 - val_rmse: 18786.3223\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 330132032.0000 - rmse: 18169.5352 - val_loss: 383590464.0000 - val_rmse: 19585.4648\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 394820384.0000 - rmse: 19870.0879 - val_loss: 342809856.0000 - val_rmse: 18515.1250\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 383870368.0000 - rmse: 19592.6094 - val_loss: 366389376.0000 - val_rmse: 19141.3008\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 334347648.0000 - rmse: 18285.1758 - val_loss: 470023552.0000 - val_rmse: 21680.0273\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 357484640.0000 - rmse: 18907.2637 - val_loss: 406152896.0000 - val_rmse: 20153.2363\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 374636096.0000 - rmse: 19355.5176 - val_loss: 392956608.0000 - val_rmse: 19823.1328\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 339421536.0000 - rmse: 18423.3965 - val_loss: 361631456.0000 - val_rmse: 19016.6094\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 417105664.0000 - rmse: 20423.1641 - val_loss: 375172224.0000 - val_rmse: 19369.3633\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 379824960.0000 - rmse: 19489.0977 - val_loss: 463048928.0000 - val_rmse: 21518.5723\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 352432320.0000 - rmse: 18773.1816 - val_loss: 369694240.0000 - val_rmse: 19227.4336\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 375069152.0000 - rmse: 19366.7012 - val_loss: 417222752.0000 - val_rmse: 20426.0312\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 387018496.0000 - rmse: 19672.7852 - val_loss: 340139648.0000 - val_rmse: 18442.8750\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 338271360.0000 - rmse: 18392.1543 - val_loss: 369298080.0000 - val_rmse: 19217.1289\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 324848320.0000 - rmse: 18023.5488 - val_loss: 397595296.0000 - val_rmse: 19939.7910\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 337992224.0000 - rmse: 18384.5645 - val_loss: 580040576.0000 - val_rmse: 24084.0312\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 369915360.0000 - rmse: 19233.1836 - val_loss: 415460864.0000 - val_rmse: 20382.8574\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 333720512.0000 - rmse: 18268.0195 - val_loss: 407813312.0000 - val_rmse: 20194.3887\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 342300032.0000 - rmse: 18501.3516 - val_loss: 387468320.0000 - val_rmse: 19684.2148\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 341734336.0000 - rmse: 18486.0586 - val_loss: 360086464.0000 - val_rmse: 18975.9453\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 318990624.0000 - rmse: 17860.3086 - val_loss: 359032128.0000 - val_rmse: 18948.1426\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 362633152.0000 - rmse: 19042.9297 - val_loss: 354557760.0000 - val_rmse: 18829.7051\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 405672864.0000 - rmse: 20141.3223 - val_loss: 371305344.0000 - val_rmse: 19269.2852\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 321857856.0000 - rmse: 17940.3965 - val_loss: 362820224.0000 - val_rmse: 19047.8398\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 335525792.0000 - rmse: 18317.3633 - val_loss: 349561536.0000 - val_rmse: 18696.5645\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 346580544.0000 - rmse: 18616.6738 - val_loss: 366459104.0000 - val_rmse: 19143.1211\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 324221792.0000 - rmse: 18006.1602 - val_loss: 354320832.0000 - val_rmse: 18823.4121\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309607072.0000 - rmse: 17595.6543 - val_loss: 381740288.0000 - val_rmse: 19538.1758\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 334090144.0000 - rmse: 18278.1328 - val_loss: 412273888.0000 - val_rmse: 20304.5293\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 349389504.0000 - rmse: 18691.9629 - val_loss: 378469440.0000 - val_rmse: 19454.2910\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 396225024.0000 - rmse: 19905.4023 - val_loss: 374317504.0000 - val_rmse: 19347.2871\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 339454944.0000 - rmse: 18424.3027 - val_loss: 383751776.0000 - val_rmse: 19589.5840\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 339754944.0000 - rmse: 18432.4434 - val_loss: 392106400.0000 - val_rmse: 19801.6758\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 342126912.0000 - rmse: 18496.6738 - val_loss: 428645984.0000 - val_rmse: 20703.7676\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 380165184.0000 - rmse: 19497.8242 - val_loss: 349969760.0000 - val_rmse: 18707.4785\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299921632.0000 - rmse: 17318.2461 - val_loss: 334044928.0000 - val_rmse: 18276.8965\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 373462464.0000 - rmse: 19325.1777 - val_loss: 351666976.0000 - val_rmse: 18752.7852\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 325219520.0000 - rmse: 18033.8438 - val_loss: 438916000.0000 - val_rmse: 20950.3223\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 383656864.0000 - rmse: 19587.1602 - val_loss: 428019008.0000 - val_rmse: 20688.6211\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 333204864.0000 - rmse: 18253.9004 - val_loss: 388215424.0000 - val_rmse: 19703.1836\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 372276992.0000 - rmse: 19294.4805 - val_loss: 380580576.0000 - val_rmse: 19508.4746\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 313287136.0000 - rmse: 17699.9199 - val_loss: 392047648.0000 - val_rmse: 19800.1934\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 322994400.0000 - rmse: 17972.0449 - val_loss: 373492768.0000 - val_rmse: 19325.9609\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 332115936.0000 - rmse: 18224.0488 - val_loss: 388184256.0000 - val_rmse: 19702.3926\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 360914304.0000 - rmse: 18997.7441 - val_loss: 373179872.0000 - val_rmse: 19317.8633\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 331007456.0000 - rmse: 18193.6094 - val_loss: 375324736.0000 - val_rmse: 19373.2988\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 343277696.0000 - rmse: 18527.7539 - val_loss: 402153024.0000 - val_rmse: 20053.7539\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 298241120.0000 - rmse: 17269.6582 - val_loss: 358681088.0000 - val_rmse: 18938.8770\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 310197504.0000 - rmse: 17612.4238 - val_loss: 350804224.0000 - val_rmse: 18729.7676\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 332392224.0000 - rmse: 18231.6270 - val_loss: 361447456.0000 - val_rmse: 19011.7715\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286801920.0000 - rmse: 16935.2266 - val_loss: 422767392.0000 - val_rmse: 20561.3086\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 298237440.0000 - rmse: 17269.5527 - val_loss: 357078368.0000 - val_rmse: 18896.5176\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 342977920.0000 - rmse: 18519.6621 - val_loss: 362264960.0000 - val_rmse: 19033.2598\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 287960576.0000 - rmse: 16969.4004 - val_loss: 446057568.0000 - val_rmse: 21120.0742\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 320610816.0000 - rmse: 17905.6094 - val_loss: 353172896.0000 - val_rmse: 18792.8945\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 313449024.0000 - rmse: 17704.4922 - val_loss: 339480160.0000 - val_rmse: 18424.9883\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 371047424.0000 - rmse: 19262.5918 - val_loss: 397239456.0000 - val_rmse: 19930.8672\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 326130816.0000 - rmse: 18059.0918 - val_loss: 356007296.0000 - val_rmse: 18868.1562\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 306157408.0000 - rmse: 17497.3535 - val_loss: 472197760.0000 - val_rmse: 21730.1113\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 302909568.0000 - rmse: 17404.2969 - val_loss: 338260192.0000 - val_rmse: 18391.8516\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 314563712.0000 - rmse: 17735.9434 - val_loss: 378708736.0000 - val_rmse: 19460.4395\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 355893504.0000 - rmse: 18865.1406 - val_loss: 388057024.0000 - val_rmse: 19699.1621\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 294476896.0000 - rmse: 17160.3301 - val_loss: 341745472.0000 - val_rmse: 18486.3594\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 341196096.0000 - rmse: 18471.4941 - val_loss: 352091200.0000 - val_rmse: 18764.0938\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 347291616.0000 - rmse: 18635.7617 - val_loss: 339145376.0000 - val_rmse: 18415.9004\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 312030560.0000 - rmse: 17664.3867 - val_loss: 370235360.0000 - val_rmse: 19241.5020\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 294227040.0000 - rmse: 17153.0469 - val_loss: 358944608.0000 - val_rmse: 18945.8340\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 373691072.0000 - rmse: 19331.0898 - val_loss: 423374944.0000 - val_rmse: 20576.0762\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 277930240.0000 - rmse: 16671.2402 - val_loss: 336214496.0000 - val_rmse: 18336.1523\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 287946432.0000 - rmse: 16968.9844 - val_loss: 350296704.0000 - val_rmse: 18716.2148\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 292116064.0000 - rmse: 17091.4023 - val_loss: 348712544.0000 - val_rmse: 18673.8457\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 306066848.0000 - rmse: 17494.7656 - val_loss: 383203648.0000 - val_rmse: 19575.5879\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 306180992.0000 - rmse: 17498.0273 - val_loss: 360781440.0000 - val_rmse: 18994.2480\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 287719328.0000 - rmse: 16962.2910 - val_loss: 330328800.0000 - val_rmse: 18174.9492\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 298584544.0000 - rmse: 17279.5996 - val_loss: 376919520.0000 - val_rmse: 19414.4160\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 329816544.0000 - rmse: 18160.8516 - val_loss: 323859616.0000 - val_rmse: 17996.0996\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 324682144.0000 - rmse: 18018.9395 - val_loss: 373432064.0000 - val_rmse: 19324.3906\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 339195712.0000 - rmse: 18417.2676 - val_loss: 329294432.0000 - val_rmse: 18146.4707\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 290100000.0000 - rmse: 17032.3223 - val_loss: 352978912.0000 - val_rmse: 18787.7324\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 343894592.0000 - rmse: 18544.3945 - val_loss: 351504512.0000 - val_rmse: 18748.4531\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 295466080.0000 - rmse: 17189.1270 - val_loss: 357143136.0000 - val_rmse: 18898.2305\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 316911104.0000 - rmse: 17801.9980 - val_loss: 412208640.0000 - val_rmse: 20302.9219\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 306150688.0000 - rmse: 17497.1621 - val_loss: 343167616.0000 - val_rmse: 18524.7832\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 295603008.0000 - rmse: 17193.1094 - val_loss: 404566400.0000 - val_rmse: 20113.8359\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 307839744.0000 - rmse: 17545.3633 - val_loss: 364132064.0000 - val_rmse: 19082.2441\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 293872224.0000 - rmse: 17142.7012 - val_loss: 435731424.0000 - val_rmse: 20874.1816\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 306653600.0000 - rmse: 17511.5273 - val_loss: 356248960.0000 - val_rmse: 18874.5586\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 292322912.0000 - rmse: 17097.4531 - val_loss: 318871680.0000 - val_rmse: 17856.9785\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 331222496.0000 - rmse: 18199.5195 - val_loss: 365136416.0000 - val_rmse: 19108.5430\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 325292224.0000 - rmse: 18035.8594 - val_loss: 341549696.0000 - val_rmse: 18481.0625\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 294991648.0000 - rmse: 17175.3203 - val_loss: 362820640.0000 - val_rmse: 19047.8516\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 332644128.0000 - rmse: 18238.5332 - val_loss: 344192736.0000 - val_rmse: 18552.4316\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 271811392.0000 - rmse: 16486.7031 - val_loss: 331816608.0000 - val_rmse: 18215.8340\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 298445952.0000 - rmse: 17275.5879 - val_loss: 331603872.0000 - val_rmse: 18209.9941\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 287084640.0000 - rmse: 16943.5723 - val_loss: 404229408.0000 - val_rmse: 20105.4570\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 301625536.0000 - rmse: 17367.3691 - val_loss: 340962816.0000 - val_rmse: 18465.1777\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 348066336.0000 - rmse: 18656.5352 - val_loss: 357243648.0000 - val_rmse: 18900.8906\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 290684512.0000 - rmse: 17049.4727 - val_loss: 381025952.0000 - val_rmse: 19519.8867\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 302801856.0000 - rmse: 17401.2031 - val_loss: 352035200.0000 - val_rmse: 18762.6016\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 289184320.0000 - rmse: 17005.4199 - val_loss: 324592928.0000 - val_rmse: 18016.4629\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 285603104.0000 - rmse: 16899.7969 - val_loss: 386585344.0000 - val_rmse: 19661.7734\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 296979744.0000 - rmse: 17233.0996 - val_loss: 370451488.0000 - val_rmse: 19247.1172\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 286049536.0000 - rmse: 16913.0000 - val_loss: 374328288.0000 - val_rmse: 19347.5645\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 272176416.0000 - rmse: 16497.7695 - val_loss: 338554688.0000 - val_rmse: 18399.8555\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 346813984.0000 - rmse: 18622.9434 - val_loss: 366736896.0000 - val_rmse: 19150.3750\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264661376.0000 - rmse: 16268.4170 - val_loss: 353350624.0000 - val_rmse: 18797.6230\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 316009664.0000 - rmse: 17776.6602 - val_loss: 349422368.0000 - val_rmse: 18692.8418\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 253027856.0000 - rmse: 15906.8496 - val_loss: 343211104.0000 - val_rmse: 18525.9570\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 258364128.0000 - rmse: 16073.7090 - val_loss: 347998144.0000 - val_rmse: 18654.7090\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 269572096.0000 - rmse: 16418.6504 - val_loss: 336146752.0000 - val_rmse: 18334.3047\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 323091200.0000 - rmse: 17974.7383 - val_loss: 361241952.0000 - val_rmse: 19006.3652\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 275791808.0000 - rmse: 16606.9805 - val_loss: 346505824.0000 - val_rmse: 18614.6660\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 277994144.0000 - rmse: 16673.1562 - val_loss: 343414592.0000 - val_rmse: 18531.4492\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 282733920.0000 - rmse: 16814.6934 - val_loss: 360752000.0000 - val_rmse: 18993.4727\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268948128.0000 - rmse: 16399.6387 - val_loss: 335464576.0000 - val_rmse: 18315.6914\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 246754128.0000 - rmse: 15708.4092 - val_loss: 348403200.0000 - val_rmse: 18665.5625\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 284469216.0000 - rmse: 16866.2148 - val_loss: 330924448.0000 - val_rmse: 18191.3281\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 265627712.0000 - rmse: 16298.0889 - val_loss: 360706304.0000 - val_rmse: 18992.2695\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 283075488.0000 - rmse: 16824.8477 - val_loss: 352816192.0000 - val_rmse: 18783.4023\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 248086848.0000 - rmse: 15750.7725 - val_loss: 327475008.0000 - val_rmse: 18096.2715\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 301587936.0000 - rmse: 17366.2871 - val_loss: 342263232.0000 - val_rmse: 18500.3574\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 301277856.0000 - rmse: 17357.3574 - val_loss: 347962240.0000 - val_rmse: 18653.7461\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 291555968.0000 - rmse: 17075.0098 - val_loss: 387627616.0000 - val_rmse: 19688.2617\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 265287312.0000 - rmse: 16287.6426 - val_loss: 328811904.0000 - val_rmse: 18133.1719\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 254707808.0000 - rmse: 15959.5684 - val_loss: 334325728.0000 - val_rmse: 18284.5762\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 351642912.0000 - rmse: 18752.1445 - val_loss: 317716128.0000 - val_rmse: 17824.5938\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 285055136.0000 - rmse: 16883.5762 - val_loss: 355334976.0000 - val_rmse: 18850.3301\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 271803104.0000 - rmse: 16486.4512 - val_loss: 332879296.0000 - val_rmse: 18244.9805\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286619744.0000 - rmse: 16929.8477 - val_loss: 326890464.0000 - val_rmse: 18080.1133\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 276974880.0000 - rmse: 16642.5625 - val_loss: 324708320.0000 - val_rmse: 18019.6641\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 252012448.0000 - rmse: 15874.9004 - val_loss: 328749792.0000 - val_rmse: 18131.4590\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 265744848.0000 - rmse: 16301.6826 - val_loss: 407735872.0000 - val_rmse: 20192.4707\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 249171920.0000 - rmse: 15785.1807 - val_loss: 324870976.0000 - val_rmse: 18024.1777\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 273725824.0000 - rmse: 16544.6621 - val_loss: 339460704.0000 - val_rmse: 18424.4590\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 240235568.0000 - rmse: 15499.5342 - val_loss: 323887776.0000 - val_rmse: 17996.8828\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 284952000.0000 - rmse: 16880.5215 - val_loss: 329971616.0000 - val_rmse: 18165.1211\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 269657440.0000 - rmse: 16421.2500 - val_loss: 317479968.0000 - val_rmse: 17817.9668\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 282151232.0000 - rmse: 16797.3574 - val_loss: 375527200.0000 - val_rmse: 19378.5234\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 246184208.0000 - rmse: 15690.2588 - val_loss: 366757920.0000 - val_rmse: 19150.9238\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 277339744.0000 - rmse: 16653.5195 - val_loss: 324144352.0000 - val_rmse: 18004.0098\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 250578208.0000 - rmse: 15829.6621 - val_loss: 337264512.0000 - val_rmse: 18364.7637\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 267908800.0000 - rmse: 16367.9199 - val_loss: 395163136.0000 - val_rmse: 19878.7109\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 273021792.0000 - rmse: 16523.3711 - val_loss: 327397952.0000 - val_rmse: 18094.1406\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280463840.0000 - rmse: 16747.0547 - val_loss: 376391040.0000 - val_rmse: 19400.8008\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 276919776.0000 - rmse: 16640.9062 - val_loss: 319901056.0000 - val_rmse: 17885.7773\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 282987904.0000 - rmse: 16822.2441 - val_loss: 322915456.0000 - val_rmse: 17969.8477\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 285582912.0000 - rmse: 16899.1992 - val_loss: 335156992.0000 - val_rmse: 18307.2930\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 251658976.0000 - rmse: 15863.7627 - val_loss: 344306240.0000 - val_rmse: 18555.4902\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 266977040.0000 - rmse: 16339.4316 - val_loss: 331544416.0000 - val_rmse: 18208.3613\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286491328.0000 - rmse: 16926.0547 - val_loss: 368796608.0000 - val_rmse: 19204.0781\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 259820416.0000 - rmse: 16118.9463 - val_loss: 360366176.0000 - val_rmse: 18983.3125\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 278464672.0000 - rmse: 16687.2617 - val_loss: 339222560.0000 - val_rmse: 18417.9961\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 261343056.0000 - rmse: 16166.1084 - val_loss: 323307424.0000 - val_rmse: 17980.7520\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 279475872.0000 - rmse: 16717.5312 - val_loss: 312416960.0000 - val_rmse: 17675.3203\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 240237200.0000 - rmse: 15499.5869 - val_loss: 356405984.0000 - val_rmse: 18878.7168\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 270561600.0000 - rmse: 16448.7559 - val_loss: 341587552.0000 - val_rmse: 18482.0879\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 252577600.0000 - rmse: 15892.6904 - val_loss: 341410752.0000 - val_rmse: 18477.3027\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 305397376.0000 - rmse: 17475.6230 - val_loss: 477903168.0000 - val_rmse: 21860.9961\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 243568592.0000 - rmse: 15606.6846 - val_loss: 327991168.0000 - val_rmse: 18110.5273\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 247416704.0000 - rmse: 15729.4854 - val_loss: 315725248.0000 - val_rmse: 17768.6582\n",
      "Epoch 367/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 287529216.0000 - rmse: 16956.6855 - val_loss: 415193632.0000 - val_rmse: 20376.3008\n",
      "Epoch 368/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 253114704.0000 - rmse: 15909.5791 - val_loss: 323475616.0000 - val_rmse: 17985.4277\n",
      "Epoch 369/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 275054976.0000 - rmse: 16584.7812 - val_loss: 308324800.0000 - val_rmse: 17559.1797\n",
      "Epoch 370/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 224382352.0000 - rmse: 14979.3975 - val_loss: 320741472.0000 - val_rmse: 17909.2559\n",
      "Epoch 371/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 325441376.0000 - rmse: 18039.9941 - val_loss: 322784224.0000 - val_rmse: 17966.1973\n",
      "Epoch 372/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 246989968.0000 - rmse: 15715.9141 - val_loss: 347770880.0000 - val_rmse: 18648.6152\n",
      "Epoch 373/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 288158176.0000 - rmse: 16975.2227 - val_loss: 337237824.0000 - val_rmse: 18364.0371\n",
      "Epoch 374/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 229770944.0000 - rmse: 15158.1973 - val_loss: 336318368.0000 - val_rmse: 18338.9844\n",
      "Epoch 375/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 250099056.0000 - rmse: 15814.5205 - val_loss: 321086080.0000 - val_rmse: 17918.8750\n",
      "Epoch 376/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 247997120.0000 - rmse: 15747.9238 - val_loss: 357021120.0000 - val_rmse: 18895.0020\n",
      "Epoch 377/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 250733504.0000 - rmse: 15834.5664 - val_loss: 353241472.0000 - val_rmse: 18794.7188\n",
      "Epoch 378/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 275518272.0000 - rmse: 16598.7422 - val_loss: 377957184.0000 - val_rmse: 19441.1211\n",
      "Epoch 379/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 241863008.0000 - rmse: 15551.9453 - val_loss: 549587840.0000 - val_rmse: 23443.2891\n",
      "Epoch 380/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 318341760.0000 - rmse: 17842.1348 - val_loss: 352772256.0000 - val_rmse: 18782.2324\n",
      "Epoch 381/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 267463600.0000 - rmse: 16354.3145 - val_loss: 372447552.0000 - val_rmse: 19298.9004\n",
      "Epoch 382/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 275414016.0000 - rmse: 16595.6016 - val_loss: 389513088.0000 - val_rmse: 19736.0859\n",
      "Epoch 383/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 229257664.0000 - rmse: 15141.2568 - val_loss: 365698368.0000 - val_rmse: 19123.2422\n",
      "Epoch 384/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 267870432.0000 - rmse: 16366.7480 - val_loss: 363088800.0000 - val_rmse: 19054.8887\n",
      "Epoch 385/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 261803856.0000 - rmse: 16180.3545 - val_loss: 347184928.0000 - val_rmse: 18632.8984\n",
      "Epoch 386/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 246131760.0000 - rmse: 15688.5869 - val_loss: 336904416.0000 - val_rmse: 18354.9570\n",
      "Epoch 387/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 307884992.0000 - rmse: 17546.6523 - val_loss: 398615328.0000 - val_rmse: 19965.3535\n",
      "Epoch 388/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 239008208.0000 - rmse: 15459.8906 - val_loss: 331477888.0000 - val_rmse: 18206.5352\n",
      "Epoch 389/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 262248352.0000 - rmse: 16194.0840 - val_loss: 459384640.0000 - val_rmse: 21433.2598\n",
      "Epoch 390/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 238116000.0000 - rmse: 15431.0078 - val_loss: 355858944.0000 - val_rmse: 18864.2246\n",
      "Epoch 391/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309126976.0000 - rmse: 17582.0078 - val_loss: 332594400.0000 - val_rmse: 18237.1699\n",
      "Epoch 392/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 253016864.0000 - rmse: 15906.5039 - val_loss: 380352256.0000 - val_rmse: 19502.6211\n",
      "Epoch 393/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 275524032.0000 - rmse: 16598.9160 - val_loss: 360984480.0000 - val_rmse: 18999.5918\n",
      "Epoch 394/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 271982720.0000 - rmse: 16491.8984 - val_loss: 338027072.0000 - val_rmse: 18385.5117\n",
      "Epoch 395/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 220519776.0000 - rmse: 14849.9082 - val_loss: 351655904.0000 - val_rmse: 18752.4902\n",
      "Epoch 396/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 255436464.0000 - rmse: 15982.3799 - val_loss: 317769120.0000 - val_rmse: 17826.0801\n",
      "Epoch 397/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 248804608.0000 - rmse: 15773.5410 - val_loss: 321635744.0000 - val_rmse: 17934.2051\n",
      "Epoch 398/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 249590208.0000 - rmse: 15798.4238 - val_loss: 312458432.0000 - val_rmse: 17676.4941\n",
      "Epoch 399/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 234419920.0000 - rmse: 15310.7783 - val_loss: 325038304.0000 - val_rmse: 18028.8184\n",
      "Epoch 400/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 261865440.0000 - rmse: 16182.2568 - val_loss: 336038752.0000 - val_rmse: 18331.3594\n",
      "104/104 [==============================] - 0s 960us/step - loss: 404800608.0000 - rmse: 20119.6582\n",
      "[404800608.0, 20119.658203125]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"emb_model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_61 (Dense)             multiple                  860       \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             multiple                  39        \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             multiple                  4         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             multiple                  36        \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             multiple                  156       \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             multiple                  3840      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             multiple                  4128      \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             multiple                  528       \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             multiple                  272       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             multiple                  68        \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             multiple                  5         \n",
      "=================================================================\n",
      "Total params: 9,936\n",
      "Trainable params: 9,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "emb_model = train_and_get_deep_learning_model(\"emb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run k-fold for embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<src.model.emb_model object at 0x7f8798433cd0>\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 21486362624.0000 - rmse: 146582.2656 - val_loss: 3449835264.0000 - val_rmse: 58735.2969\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2821952768.0000 - rmse: 53122.0547 - val_loss: 1395036544.0000 - val_rmse: 37350.1875\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2125130240.0000 - rmse: 46099.1367 - val_loss: 1193041408.0000 - val_rmse: 34540.4297\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1947135744.0000 - rmse: 44126.3594 - val_loss: 1061434688.0000 - val_rmse: 32579.6660\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1750267136.0000 - rmse: 41836.1953 - val_loss: 964750272.0000 - val_rmse: 31060.4297\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1706206592.0000 - rmse: 41306.2539 - val_loss: 880832384.0000 - val_rmse: 29678.8203\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1713645824.0000 - rmse: 41396.2070 - val_loss: 837002688.0000 - val_rmse: 28930.9980\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1639913728.0000 - rmse: 40495.8477 - val_loss: 797237056.0000 - val_rmse: 28235.3867\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1576545536.0000 - rmse: 39705.7383 - val_loss: 801862592.0000 - val_rmse: 28317.1777\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1603340544.0000 - rmse: 40041.7344 - val_loss: 734169344.0000 - val_rmse: 27095.5586\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1457061376.0000 - rmse: 38171.4727 - val_loss: 724141376.0000 - val_rmse: 26909.8750\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1477238656.0000 - rmse: 38434.8633 - val_loss: 695461120.0000 - val_rmse: 26371.5977\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1413431936.0000 - rmse: 37595.6367 - val_loss: 687916096.0000 - val_rmse: 26228.1543\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1366361088.0000 - rmse: 36964.3203 - val_loss: 659662144.0000 - val_rmse: 25683.8887\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1412894848.0000 - rmse: 37588.4922 - val_loss: 654143360.0000 - val_rmse: 25576.2266\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1312564352.0000 - rmse: 36229.3281 - val_loss: 638581376.0000 - val_rmse: 25270.1680\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1333220608.0000 - rmse: 36513.2930 - val_loss: 633364864.0000 - val_rmse: 25166.7422\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1307420928.0000 - rmse: 36158.2773 - val_loss: 609038784.0000 - val_rmse: 24678.7109\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1258179328.0000 - rmse: 35470.8242 - val_loss: 594223104.0000 - val_rmse: 24376.6914\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1211943296.0000 - rmse: 34812.9766 - val_loss: 601344384.0000 - val_rmse: 24522.3242\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1156898432.0000 - rmse: 34013.2109 - val_loss: 632925696.0000 - val_rmse: 25158.0137\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1192766080.0000 - rmse: 34536.4453 - val_loss: 590249408.0000 - val_rmse: 24295.0488\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1200443264.0000 - rmse: 34647.4141 - val_loss: 638500160.0000 - val_rmse: 25268.5605\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1114886016.0000 - rmse: 33389.9102 - val_loss: 576587072.0000 - val_rmse: 24012.2285\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1095096832.0000 - rmse: 33092.2461 - val_loss: 643619648.0000 - val_rmse: 25369.6602\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1156303872.0000 - rmse: 34004.4688 - val_loss: 556937344.0000 - val_rmse: 23599.5195\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1085197696.0000 - rmse: 32942.3398 - val_loss: 550083392.0000 - val_rmse: 23453.8574\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1096846720.0000 - rmse: 33118.6758 - val_loss: 573642240.0000 - val_rmse: 23950.8301\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1066970496.0000 - rmse: 32664.5137 - val_loss: 590817536.0000 - val_rmse: 24306.7383\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1115543808.0000 - rmse: 33399.7578 - val_loss: 635997312.0000 - val_rmse: 25218.9863\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1086539520.0000 - rmse: 32962.6992 - val_loss: 563576960.0000 - val_rmse: 23739.7754\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1075680000.0000 - rmse: 32797.5625 - val_loss: 565825728.0000 - val_rmse: 23787.0918\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1076035200.0000 - rmse: 32802.9766 - val_loss: 556424960.0000 - val_rmse: 23588.6621\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1093498240.0000 - rmse: 33068.0859 - val_loss: 556028288.0000 - val_rmse: 23580.2520\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 998237568.0000 - rmse: 31594.8984 - val_loss: 668040192.0000 - val_rmse: 25846.4727\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1045841152.0000 - rmse: 32339.4668 - val_loss: 543516224.0000 - val_rmse: 23313.4336\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 979879552.0000 - rmse: 31303.0273 - val_loss: 548229632.0000 - val_rmse: 23414.3047\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1054916160.0000 - rmse: 32479.4727 - val_loss: 536944960.0000 - val_rmse: 23172.0723\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 907770176.0000 - rmse: 30129.2246 - val_loss: 594286912.0000 - val_rmse: 24378.0000\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1051648448.0000 - rmse: 32429.1289 - val_loss: 549975744.0000 - val_rmse: 23451.5625\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1093702656.0000 - rmse: 33071.1758 - val_loss: 516273664.0000 - val_rmse: 22721.6562\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 937869760.0000 - rmse: 30624.6602 - val_loss: 528790304.0000 - val_rmse: 22995.4414\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 966668928.0000 - rmse: 31091.3008 - val_loss: 519664000.0000 - val_rmse: 22796.1406\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 948511552.0000 - rmse: 30797.9141 - val_loss: 517019808.0000 - val_rmse: 22738.0703\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 935071424.0000 - rmse: 30578.9375 - val_loss: 544617408.0000 - val_rmse: 23337.0391\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 905382848.0000 - rmse: 30089.5801 - val_loss: 516660544.0000 - val_rmse: 22730.1680\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 942065856.0000 - rmse: 30693.0918 - val_loss: 494786912.0000 - val_rmse: 22243.8066\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 862673024.0000 - rmse: 29371.2969 - val_loss: 497701280.0000 - val_rmse: 22309.2188\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 906355968.0000 - rmse: 30105.7461 - val_loss: 541815360.0000 - val_rmse: 23276.9277\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 949995072.0000 - rmse: 30821.9902 - val_loss: 532229728.0000 - val_rmse: 23070.1055\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 968690240.0000 - rmse: 31123.7891 - val_loss: 482964000.0000 - val_rmse: 21976.4414\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 827735488.0000 - rmse: 28770.3926 - val_loss: 520044320.0000 - val_rmse: 22804.4805\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 844835968.0000 - rmse: 29066.0625 - val_loss: 467058560.0000 - val_rmse: 21611.5371\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 868114048.0000 - rmse: 29463.7754 - val_loss: 487741216.0000 - val_rmse: 22084.8633\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 779401152.0000 - rmse: 27917.7578 - val_loss: 474845024.0000 - val_rmse: 21790.9395\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 839433600.0000 - rmse: 28972.9805 - val_loss: 448851936.0000 - val_rmse: 21186.1270\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 896706112.0000 - rmse: 29945.0508 - val_loss: 447286816.0000 - val_rmse: 21149.1562\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 814080640.0000 - rmse: 28532.0977 - val_loss: 471035488.0000 - val_rmse: 21703.3516\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 828809536.0000 - rmse: 28789.0527 - val_loss: 448647520.0000 - val_rmse: 21181.3008\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 849081664.0000 - rmse: 29139.0059 - val_loss: 435868160.0000 - val_rmse: 20877.4551\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 796253696.0000 - rmse: 28217.9668 - val_loss: 425209408.0000 - val_rmse: 20620.6055\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 889379200.0000 - rmse: 29822.4609 - val_loss: 437075008.0000 - val_rmse: 20906.3398\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 874645632.0000 - rmse: 29574.4082 - val_loss: 458990848.0000 - val_rmse: 21424.0723\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 814624640.0000 - rmse: 28541.6309 - val_loss: 428238496.0000 - val_rmse: 20693.9238\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 882910976.0000 - rmse: 29713.8184 - val_loss: 416710528.0000 - val_rmse: 20413.4883\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 800429248.0000 - rmse: 28291.8574 - val_loss: 417631488.0000 - val_rmse: 20436.0332\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 849548160.0000 - rmse: 29147.0098 - val_loss: 415541536.0000 - val_rmse: 20384.8359\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 730418240.0000 - rmse: 27026.2500 - val_loss: 435969280.0000 - val_rmse: 20879.8770\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 789726208.0000 - rmse: 28102.0684 - val_loss: 398383776.0000 - val_rmse: 19959.5527\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 834110976.0000 - rmse: 28880.9785 - val_loss: 500542336.0000 - val_rmse: 22372.8027\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 827629888.0000 - rmse: 28768.5566 - val_loss: 417061760.0000 - val_rmse: 20422.0898\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 784872000.0000 - rmse: 28015.5664 - val_loss: 395802272.0000 - val_rmse: 19894.7793\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 785244672.0000 - rmse: 28022.2168 - val_loss: 401559904.0000 - val_rmse: 20038.9590\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 784482688.0000 - rmse: 28008.6172 - val_loss: 428447456.0000 - val_rmse: 20698.9727\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 784979904.0000 - rmse: 28017.4922 - val_loss: 438169536.0000 - val_rmse: 20932.5000\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 750395072.0000 - rmse: 27393.3398 - val_loss: 452455168.0000 - val_rmse: 21270.9941\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 785200576.0000 - rmse: 28021.4297 - val_loss: 392684160.0000 - val_rmse: 19816.2598\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 729702592.0000 - rmse: 27013.0078 - val_loss: 389276544.0000 - val_rmse: 19730.0918\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 720089984.0000 - rmse: 26834.4922 - val_loss: 375570336.0000 - val_rmse: 19379.6367\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 799112128.0000 - rmse: 28268.5723 - val_loss: 383874144.0000 - val_rmse: 19592.7070\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 671407808.0000 - rmse: 25911.5391 - val_loss: 376579296.0000 - val_rmse: 19405.6504\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 716927232.0000 - rmse: 26775.4961 - val_loss: 375456608.0000 - val_rmse: 19376.7031\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 723285888.0000 - rmse: 26893.9746 - val_loss: 367828992.0000 - val_rmse: 19178.8691\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 730706368.0000 - rmse: 27031.5801 - val_loss: 361102688.0000 - val_rmse: 19002.7012\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 722364480.0000 - rmse: 26876.8398 - val_loss: 354055200.0000 - val_rmse: 18816.3555\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 756056768.0000 - rmse: 27496.4863 - val_loss: 359584672.0000 - val_rmse: 18962.7188\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 669862464.0000 - rmse: 25881.7012 - val_loss: 369048544.0000 - val_rmse: 19210.6367\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 730406464.0000 - rmse: 27026.0332 - val_loss: 379268160.0000 - val_rmse: 19474.8086\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 746580224.0000 - rmse: 27323.6211 - val_loss: 363560320.0000 - val_rmse: 19067.2578\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 682583744.0000 - rmse: 26126.3027 - val_loss: 361529760.0000 - val_rmse: 19013.9355\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 672289920.0000 - rmse: 25928.5547 - val_loss: 348144096.0000 - val_rmse: 18658.6191\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 688459008.0000 - rmse: 26238.5020 - val_loss: 359685248.0000 - val_rmse: 18965.3691\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 671181760.0000 - rmse: 25907.1758 - val_loss: 462074304.0000 - val_rmse: 21495.9141\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 663216704.0000 - rmse: 25752.9941 - val_loss: 356580896.0000 - val_rmse: 18883.3496\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 696409664.0000 - rmse: 26389.5742 - val_loss: 348907840.0000 - val_rmse: 18679.0742\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 688492672.0000 - rmse: 26239.1445 - val_loss: 424151072.0000 - val_rmse: 20594.9277\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 650961664.0000 - rmse: 25513.9512 - val_loss: 396537440.0000 - val_rmse: 19913.2480\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 695798464.0000 - rmse: 26377.9922 - val_loss: 338475552.0000 - val_rmse: 18397.7051\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 625105472.0000 - rmse: 25002.1094 - val_loss: 334570848.0000 - val_rmse: 18291.2773\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 636523136.0000 - rmse: 25229.4102 - val_loss: 366644864.0000 - val_rmse: 19147.9727\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 681863232.0000 - rmse: 26112.5117 - val_loss: 355895520.0000 - val_rmse: 18865.1934\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 654110080.0000 - rmse: 25575.5762 - val_loss: 378849792.0000 - val_rmse: 19464.0645\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 644049088.0000 - rmse: 25378.1230 - val_loss: 467467584.0000 - val_rmse: 21620.9980\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 670669248.0000 - rmse: 25897.2832 - val_loss: 348104736.0000 - val_rmse: 18657.5645\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 593111296.0000 - rmse: 24353.8770 - val_loss: 352568032.0000 - val_rmse: 18776.7949\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 677614400.0000 - rmse: 26031.0273 - val_loss: 361964128.0000 - val_rmse: 19025.3555\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 616332736.0000 - rmse: 24826.0488 - val_loss: 340254880.0000 - val_rmse: 18446.0000\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 692701120.0000 - rmse: 26319.2148 - val_loss: 399486272.0000 - val_rmse: 19987.1523\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 654790336.0000 - rmse: 25588.8711 - val_loss: 338096224.0000 - val_rmse: 18387.3926\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 623819328.0000 - rmse: 24976.3750 - val_loss: 332657856.0000 - val_rmse: 18238.9102\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 625165440.0000 - rmse: 25003.3086 - val_loss: 331316160.0000 - val_rmse: 18202.0918\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 577964864.0000 - rmse: 24040.9004 - val_loss: 388231680.0000 - val_rmse: 19703.5957\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 642488768.0000 - rmse: 25347.3613 - val_loss: 347221504.0000 - val_rmse: 18633.8809\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 687778816.0000 - rmse: 26225.5371 - val_loss: 341096960.0000 - val_rmse: 18468.8105\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 611356416.0000 - rmse: 24725.6230 - val_loss: 343402848.0000 - val_rmse: 18531.1328\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 571784512.0000 - rmse: 23912.0156 - val_loss: 345778048.0000 - val_rmse: 18595.1074\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 649422656.0000 - rmse: 25483.7715 - val_loss: 344423840.0000 - val_rmse: 18558.6602\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 561027520.0000 - rmse: 23686.0195 - val_loss: 454115008.0000 - val_rmse: 21309.9746\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 669388480.0000 - rmse: 25872.5430 - val_loss: 434798272.0000 - val_rmse: 20851.8164\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 605631232.0000 - rmse: 24609.5762 - val_loss: 367049024.0000 - val_rmse: 19158.5234\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 552781888.0000 - rmse: 23511.3145 - val_loss: 377343040.0000 - val_rmse: 19425.3203\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 612002816.0000 - rmse: 24738.6914 - val_loss: 373270144.0000 - val_rmse: 19320.2012\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 591979712.0000 - rmse: 24330.6328 - val_loss: 377561760.0000 - val_rmse: 19430.9492\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 568087616.0000 - rmse: 23834.5879 - val_loss: 351109664.0000 - val_rmse: 18737.9199\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 547817920.0000 - rmse: 23405.5098 - val_loss: 387952000.0000 - val_rmse: 19696.4980\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 608892928.0000 - rmse: 24675.7559 - val_loss: 404656256.0000 - val_rmse: 20116.0703\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 559282176.0000 - rmse: 23649.1484 - val_loss: 476640128.0000 - val_rmse: 21832.0898\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 575466496.0000 - rmse: 23988.8828 - val_loss: 352486560.0000 - val_rmse: 18774.6250\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 617605440.0000 - rmse: 24851.6680 - val_loss: 409396160.0000 - val_rmse: 20233.5410\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 613453952.0000 - rmse: 24768.0020 - val_loss: 350921920.0000 - val_rmse: 18732.9102\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 598688704.0000 - rmse: 24468.1152 - val_loss: 337842560.0000 - val_rmse: 18380.4941\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 576229184.0000 - rmse: 24004.7734 - val_loss: 352139648.0000 - val_rmse: 18765.3848\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 536855264.0000 - rmse: 23170.1367 - val_loss: 365268768.0000 - val_rmse: 19112.0059\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 567574080.0000 - rmse: 23823.8125 - val_loss: 360771456.0000 - val_rmse: 18993.9844\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 629253888.0000 - rmse: 25084.9336 - val_loss: 354583072.0000 - val_rmse: 18830.3770\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 576939008.0000 - rmse: 24019.5547 - val_loss: 364773088.0000 - val_rmse: 19099.0332\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 605489920.0000 - rmse: 24606.7051 - val_loss: 388717792.0000 - val_rmse: 19715.9277\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 517189568.0000 - rmse: 22741.8027 - val_loss: 407847712.0000 - val_rmse: 20195.2402\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 565138880.0000 - rmse: 23772.6504 - val_loss: 373514560.0000 - val_rmse: 19326.5254\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 553645888.0000 - rmse: 23529.6816 - val_loss: 384466016.0000 - val_rmse: 19607.8047\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 540582784.0000 - rmse: 23250.4355 - val_loss: 377681760.0000 - val_rmse: 19434.0352\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 547390976.0000 - rmse: 23396.3887 - val_loss: 357388544.0000 - val_rmse: 18904.7227\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 516532576.0000 - rmse: 22727.3535 - val_loss: 339298240.0000 - val_rmse: 18420.0508\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 533807424.0000 - rmse: 23104.2734 - val_loss: 452884960.0000 - val_rmse: 21281.0938\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 555388480.0000 - rmse: 23566.6816 - val_loss: 342872096.0000 - val_rmse: 18516.8066\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 512387008.0000 - rmse: 22635.9668 - val_loss: 364461248.0000 - val_rmse: 19090.8691\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 516524224.0000 - rmse: 22727.1699 - val_loss: 359002112.0000 - val_rmse: 18947.3516\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 574673152.0000 - rmse: 23972.3418 - val_loss: 586151040.0000 - val_rmse: 24210.5566\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 526955552.0000 - rmse: 22955.5117 - val_loss: 388420064.0000 - val_rmse: 19708.3750\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 538200576.0000 - rmse: 23199.1504 - val_loss: 380058016.0000 - val_rmse: 19495.0762\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 527869760.0000 - rmse: 22975.4160 - val_loss: 331097952.0000 - val_rmse: 18196.0977\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 512444736.0000 - rmse: 22637.2422 - val_loss: 386694880.0000 - val_rmse: 19664.5586\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 475225440.0000 - rmse: 21799.6660 - val_loss: 356946144.0000 - val_rmse: 18893.0176\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 561608192.0000 - rmse: 23698.2734 - val_loss: 350616672.0000 - val_rmse: 18724.7617\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 496140320.0000 - rmse: 22274.2070 - val_loss: 340423616.0000 - val_rmse: 18450.5723\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 520260832.0000 - rmse: 22809.2266 - val_loss: 350240416.0000 - val_rmse: 18714.7109\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 476945280.0000 - rmse: 21839.0762 - val_loss: 346369504.0000 - val_rmse: 18611.0059\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 479192032.0000 - rmse: 21890.4551 - val_loss: 376427616.0000 - val_rmse: 19401.7422\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 496847520.0000 - rmse: 22290.0762 - val_loss: 348526240.0000 - val_rmse: 18668.8574\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 526134208.0000 - rmse: 22937.6152 - val_loss: 345978432.0000 - val_rmse: 18600.4961\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 490053216.0000 - rmse: 22137.1465 - val_loss: 347536320.0000 - val_rmse: 18642.3262\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 496627552.0000 - rmse: 22285.1426 - val_loss: 405211840.0000 - val_rmse: 20129.8750\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 493092864.0000 - rmse: 22205.6953 - val_loss: 341372960.0000 - val_rmse: 18476.2812\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 471679744.0000 - rmse: 21718.1895 - val_loss: 354487072.0000 - val_rmse: 18827.8262\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 466991520.0000 - rmse: 21609.9863 - val_loss: 345298944.0000 - val_rmse: 18582.2207\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 478729472.0000 - rmse: 21879.8867 - val_loss: 344547520.0000 - val_rmse: 18561.9922\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 487685216.0000 - rmse: 22083.5957 - val_loss: 346462688.0000 - val_rmse: 18613.5078\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 470245504.0000 - rmse: 21685.1445 - val_loss: 331043872.0000 - val_rmse: 18194.6113\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 436437120.0000 - rmse: 20891.0781 - val_loss: 366479360.0000 - val_rmse: 19143.6504\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 483514624.0000 - rmse: 21988.9668 - val_loss: 342429664.0000 - val_rmse: 18504.8555\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 454680288.0000 - rmse: 21323.2344 - val_loss: 342373344.0000 - val_rmse: 18503.3340\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 574423488.0000 - rmse: 23967.1328 - val_loss: 351292544.0000 - val_rmse: 18742.7988\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 507153856.0000 - rmse: 22520.0762 - val_loss: 391239712.0000 - val_rmse: 19779.7812\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 498750880.0000 - rmse: 22332.7305 - val_loss: 369091872.0000 - val_rmse: 19211.7637\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 519456128.0000 - rmse: 22791.5801 - val_loss: 345286560.0000 - val_rmse: 18581.8887\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 479073408.0000 - rmse: 21887.7461 - val_loss: 378881280.0000 - val_rmse: 19464.8730\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 466971296.0000 - rmse: 21609.5195 - val_loss: 335144512.0000 - val_rmse: 18306.9531\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 493157408.0000 - rmse: 22207.1484 - val_loss: 371868288.0000 - val_rmse: 19283.8867\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 457525472.0000 - rmse: 21389.8457 - val_loss: 354837568.0000 - val_rmse: 18837.1328\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 571246080.0000 - rmse: 23900.7539 - val_loss: 337193024.0000 - val_rmse: 18362.8164\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 480039008.0000 - rmse: 21909.7930 - val_loss: 336322688.0000 - val_rmse: 18339.1035\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 473536416.0000 - rmse: 21760.8926 - val_loss: 354417472.0000 - val_rmse: 18825.9785\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 482169152.0000 - rmse: 21958.3496 - val_loss: 370718752.0000 - val_rmse: 19254.0586\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 447499072.0000 - rmse: 21154.1738 - val_loss: 401375072.0000 - val_rmse: 20034.3477\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 592562496.0000 - rmse: 24342.6074 - val_loss: 384300192.0000 - val_rmse: 19603.5762\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 426860896.0000 - rmse: 20660.6113 - val_loss: 332681312.0000 - val_rmse: 18239.5527\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 439543520.0000 - rmse: 20965.2930 - val_loss: 328994112.0000 - val_rmse: 18138.1953\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 487497152.0000 - rmse: 22079.3379 - val_loss: 352931616.0000 - val_rmse: 18786.4746\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 421573888.0000 - rmse: 20532.2637 - val_loss: 418339136.0000 - val_rmse: 20453.3398\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 454923296.0000 - rmse: 21328.9316 - val_loss: 355391040.0000 - val_rmse: 18851.8184\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 466497408.0000 - rmse: 21598.5508 - val_loss: 342118912.0000 - val_rmse: 18496.4570\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 455130880.0000 - rmse: 21333.7969 - val_loss: 391735840.0000 - val_rmse: 19792.3184\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 514658688.0000 - rmse: 22686.0898 - val_loss: 429305728.0000 - val_rmse: 20719.6934\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 418948544.0000 - rmse: 20468.2324 - val_loss: 349858336.0000 - val_rmse: 18704.5000\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 462023904.0000 - rmse: 21494.7422 - val_loss: 360273856.0000 - val_rmse: 18980.8809\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 465844224.0000 - rmse: 21583.4238 - val_loss: 347743232.0000 - val_rmse: 18647.8750\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 398933792.0000 - rmse: 19973.3262 - val_loss: 547337984.0000 - val_rmse: 23395.2559\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 482981600.0000 - rmse: 21976.8418 - val_loss: 371385216.0000 - val_rmse: 19271.3574\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 455133792.0000 - rmse: 21333.8652 - val_loss: 348331424.0000 - val_rmse: 18663.6387\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 400129856.0000 - rmse: 20003.2461 - val_loss: 371071424.0000 - val_rmse: 19263.2148\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 446483840.0000 - rmse: 21130.1641 - val_loss: 393617216.0000 - val_rmse: 19839.7891\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 438870944.0000 - rmse: 20949.2461 - val_loss: 360412864.0000 - val_rmse: 18984.5430\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 428129600.0000 - rmse: 20691.2930 - val_loss: 348232736.0000 - val_rmse: 18660.9941\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 426939936.0000 - rmse: 20662.5254 - val_loss: 406273472.0000 - val_rmse: 20156.2266\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 403833376.0000 - rmse: 20095.6055 - val_loss: 367502400.0000 - val_rmse: 19170.3516\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 420378464.0000 - rmse: 20503.1328 - val_loss: 367889600.0000 - val_rmse: 19180.4492\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 479265024.0000 - rmse: 21892.1230 - val_loss: 424852192.0000 - val_rmse: 20611.9434\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 414847680.0000 - rmse: 20367.8105 - val_loss: 363239872.0000 - val_rmse: 19058.8535\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 449104864.0000 - rmse: 21192.0938 - val_loss: 362965632.0000 - val_rmse: 19051.6562\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 417919360.0000 - rmse: 20443.0762 - val_loss: 364166816.0000 - val_rmse: 19083.1562\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 376226944.0000 - rmse: 19396.5703 - val_loss: 360969504.0000 - val_rmse: 18999.1973\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 470552736.0000 - rmse: 21692.2285 - val_loss: 416069536.0000 - val_rmse: 20397.7832\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 426124000.0000 - rmse: 20642.7715 - val_loss: 392783456.0000 - val_rmse: 19818.7656\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 401907328.0000 - rmse: 20047.6270 - val_loss: 348735968.0000 - val_rmse: 18674.4746\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 409806400.0000 - rmse: 20243.6758 - val_loss: 402905696.0000 - val_rmse: 20072.5117\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 427004288.0000 - rmse: 20664.0820 - val_loss: 373821376.0000 - val_rmse: 19334.4609\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 409161472.0000 - rmse: 20227.7402 - val_loss: 359732480.0000 - val_rmse: 18966.6152\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 439929184.0000 - rmse: 20974.4883 - val_loss: 363878592.0000 - val_rmse: 19075.6016\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 379440800.0000 - rmse: 19479.2402 - val_loss: 346575904.0000 - val_rmse: 18616.5488\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 377802656.0000 - rmse: 19437.1465 - val_loss: 359243136.0000 - val_rmse: 18953.7109\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 428292736.0000 - rmse: 20695.2344 - val_loss: 350457664.0000 - val_rmse: 18720.5137\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 390273440.0000 - rmse: 19755.3398 - val_loss: 377680384.0000 - val_rmse: 19434.0000\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 467488192.0000 - rmse: 21621.4746 - val_loss: 715354176.0000 - val_rmse: 26746.1055\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 407621184.0000 - rmse: 20189.6309 - val_loss: 355328416.0000 - val_rmse: 18850.1562\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 400166944.0000 - rmse: 20004.1738 - val_loss: 377776352.0000 - val_rmse: 19436.4688\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 358955936.0000 - rmse: 18946.1328 - val_loss: 374537056.0000 - val_rmse: 19352.9590\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 428549920.0000 - rmse: 20701.4473 - val_loss: 343050816.0000 - val_rmse: 18521.6309\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 378628256.0000 - rmse: 19458.3730 - val_loss: 382435040.0000 - val_rmse: 19555.9473\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 390942176.0000 - rmse: 19772.2578 - val_loss: 341291776.0000 - val_rmse: 18474.0840\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 421419552.0000 - rmse: 20528.5059 - val_loss: 376701088.0000 - val_rmse: 19408.7891\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 478146368.0000 - rmse: 21866.5586 - val_loss: 463828800.0000 - val_rmse: 21536.6855\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 419091296.0000 - rmse: 20471.7188 - val_loss: 353078336.0000 - val_rmse: 18790.3789\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 380843840.0000 - rmse: 19515.2207 - val_loss: 359333216.0000 - val_rmse: 18956.0859\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 383922272.0000 - rmse: 19593.9355 - val_loss: 388120320.0000 - val_rmse: 19700.7695\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 407521056.0000 - rmse: 20187.1504 - val_loss: 391710816.0000 - val_rmse: 19791.6855\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 370598176.0000 - rmse: 19250.9258 - val_loss: 335322816.0000 - val_rmse: 18311.8223\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 404339680.0000 - rmse: 20108.1992 - val_loss: 478801088.0000 - val_rmse: 21881.5234\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 413420256.0000 - rmse: 20332.7383 - val_loss: 351306368.0000 - val_rmse: 18743.1680\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 355857728.0000 - rmse: 18864.1914 - val_loss: 330611808.0000 - val_rmse: 18182.7344\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 351684320.0000 - rmse: 18753.2480 - val_loss: 341269120.0000 - val_rmse: 18473.4707\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 389758400.0000 - rmse: 19742.2988 - val_loss: 339424224.0000 - val_rmse: 18423.4688\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 349883520.0000 - rmse: 18705.1738 - val_loss: 401134464.0000 - val_rmse: 20028.3418\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 384893504.0000 - rmse: 19618.7031 - val_loss: 381102688.0000 - val_rmse: 19521.8516\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 392682368.0000 - rmse: 19816.2148 - val_loss: 363293376.0000 - val_rmse: 19060.2559\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 393204928.0000 - rmse: 19829.3965 - val_loss: 370470848.0000 - val_rmse: 19247.6191\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 352452096.0000 - rmse: 18773.7070 - val_loss: 356989120.0000 - val_rmse: 18894.1562\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 351026080.0000 - rmse: 18735.6895 - val_loss: 347107872.0000 - val_rmse: 18630.8320\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 387206528.0000 - rmse: 19677.5645 - val_loss: 436942400.0000 - val_rmse: 20903.1680\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 393666016.0000 - rmse: 19841.0176 - val_loss: 405579680.0000 - val_rmse: 20139.0098\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 395670880.0000 - rmse: 19891.4785 - val_loss: 359731680.0000 - val_rmse: 18966.5938\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 340605888.0000 - rmse: 18455.5117 - val_loss: 327202688.0000 - val_rmse: 18088.7441\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 328838400.0000 - rmse: 18133.9023 - val_loss: 346389344.0000 - val_rmse: 18611.5371\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 356424832.0000 - rmse: 18879.2168 - val_loss: 355365376.0000 - val_rmse: 18851.1367\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 385316832.0000 - rmse: 19629.4883 - val_loss: 380121376.0000 - val_rmse: 19496.7012\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 342049728.0000 - rmse: 18494.5859 - val_loss: 424425696.0000 - val_rmse: 20601.5938\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 368911392.0000 - rmse: 19207.0664 - val_loss: 344342592.0000 - val_rmse: 18556.4707\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 397231264.0000 - rmse: 19930.6621 - val_loss: 346706976.0000 - val_rmse: 18620.0684\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 385444832.0000 - rmse: 19632.7480 - val_loss: 330020256.0000 - val_rmse: 18166.4590\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 315582848.0000 - rmse: 17764.6523 - val_loss: 330252416.0000 - val_rmse: 18172.8477\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 357240512.0000 - rmse: 18900.8066 - val_loss: 368493888.0000 - val_rmse: 19196.1953\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 392799296.0000 - rmse: 19819.1641 - val_loss: 339382912.0000 - val_rmse: 18422.3477\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 371481568.0000 - rmse: 19273.8574 - val_loss: 352732160.0000 - val_rmse: 18781.1641\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 382737952.0000 - rmse: 19563.6895 - val_loss: 388228544.0000 - val_rmse: 19703.5156\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 366489472.0000 - rmse: 19143.9141 - val_loss: 337919456.0000 - val_rmse: 18382.5859\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 328532416.0000 - rmse: 18125.4629 - val_loss: 385021568.0000 - val_rmse: 19621.9668\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 315675712.0000 - rmse: 17767.2656 - val_loss: 371135616.0000 - val_rmse: 19264.8809\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 349380320.0000 - rmse: 18691.7188 - val_loss: 451138624.0000 - val_rmse: 21240.0234\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 347883840.0000 - rmse: 18651.6445 - val_loss: 335219392.0000 - val_rmse: 18308.9980\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 369836256.0000 - rmse: 19231.1270 - val_loss: 352703840.0000 - val_rmse: 18780.4102\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 329033824.0000 - rmse: 18139.2891 - val_loss: 335983168.0000 - val_rmse: 18329.8438\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 308941952.0000 - rmse: 17576.7441 - val_loss: 372908736.0000 - val_rmse: 19310.8457\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 337929312.0000 - rmse: 18382.8535 - val_loss: 339852896.0000 - val_rmse: 18435.0996\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 334672224.0000 - rmse: 18294.0488 - val_loss: 358698368.0000 - val_rmse: 18939.3340\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 383428352.0000 - rmse: 19581.3262 - val_loss: 316829952.0000 - val_rmse: 17799.7168\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 349171040.0000 - rmse: 18686.1191 - val_loss: 343480416.0000 - val_rmse: 18533.2246\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 335255584.0000 - rmse: 18309.9863 - val_loss: 417927552.0000 - val_rmse: 20443.2773\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 403825856.0000 - rmse: 20095.4180 - val_loss: 356517088.0000 - val_rmse: 18881.6602\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 364449408.0000 - rmse: 19090.5586 - val_loss: 347267904.0000 - val_rmse: 18635.1250\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 381013312.0000 - rmse: 19519.5625 - val_loss: 368151168.0000 - val_rmse: 19187.2656\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 288454144.0000 - rmse: 16983.9375 - val_loss: 340178720.0000 - val_rmse: 18443.9336\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 303527840.0000 - rmse: 17422.0508 - val_loss: 420980320.0000 - val_rmse: 20517.8047\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 345687936.0000 - rmse: 18592.6855 - val_loss: 396246400.0000 - val_rmse: 19905.9395\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 334111168.0000 - rmse: 18278.7090 - val_loss: 356727648.0000 - val_rmse: 18887.2344\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 330289696.0000 - rmse: 18173.8730 - val_loss: 339128832.0000 - val_rmse: 18415.4512\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 328832128.0000 - rmse: 18133.7285 - val_loss: 350762880.0000 - val_rmse: 18728.6641\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 344671776.0000 - rmse: 18565.3379 - val_loss: 427864928.0000 - val_rmse: 20684.8965\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 293194784.0000 - rmse: 17122.9316 - val_loss: 381813856.0000 - val_rmse: 19540.0586\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 327242976.0000 - rmse: 18089.8574 - val_loss: 341948928.0000 - val_rmse: 18491.8613\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 322876640.0000 - rmse: 17968.7676 - val_loss: 343801120.0000 - val_rmse: 18541.8750\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 321883040.0000 - rmse: 17941.0996 - val_loss: 344669472.0000 - val_rmse: 18565.2754\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 316672960.0000 - rmse: 17795.3066 - val_loss: 382778240.0000 - val_rmse: 19564.7188\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309112448.0000 - rmse: 17581.5938 - val_loss: 355738656.0000 - val_rmse: 18861.0352\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 373698560.0000 - rmse: 19331.2852 - val_loss: 363256960.0000 - val_rmse: 19059.3008\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 361893024.0000 - rmse: 19023.4863 - val_loss: 354998432.0000 - val_rmse: 18841.4023\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 372546048.0000 - rmse: 19301.4512 - val_loss: 379811520.0000 - val_rmse: 19488.7539\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299698016.0000 - rmse: 17311.7891 - val_loss: 363837248.0000 - val_rmse: 19074.5176\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 294252032.0000 - rmse: 17153.7754 - val_loss: 340706272.0000 - val_rmse: 18458.2305\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 304667968.0000 - rmse: 17454.7402 - val_loss: 369226112.0000 - val_rmse: 19215.2578\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286521632.0000 - rmse: 16926.9492 - val_loss: 363756064.0000 - val_rmse: 19072.3906\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 276479808.0000 - rmse: 16627.6816 - val_loss: 353757536.0000 - val_rmse: 18808.4434\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 285309312.0000 - rmse: 16891.1016 - val_loss: 339262976.0000 - val_rmse: 18419.0918\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 294782848.0000 - rmse: 17169.2422 - val_loss: 348849376.0000 - val_rmse: 18677.5098\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 324780128.0000 - rmse: 18021.6562 - val_loss: 366970656.0000 - val_rmse: 19156.4785\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 298820096.0000 - rmse: 17286.4141 - val_loss: 382534592.0000 - val_rmse: 19558.4922\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 380660160.0000 - rmse: 19510.5137 - val_loss: 389532544.0000 - val_rmse: 19736.5781\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 304955712.0000 - rmse: 17462.9805 - val_loss: 383871168.0000 - val_rmse: 19592.6309\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 278017408.0000 - rmse: 16673.8535 - val_loss: 372588416.0000 - val_rmse: 19302.5488\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 288191296.0000 - rmse: 16976.1973 - val_loss: 387653280.0000 - val_rmse: 19688.9121\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 337951072.0000 - rmse: 18383.4453 - val_loss: 397655008.0000 - val_rmse: 19941.2891\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 296345376.0000 - rmse: 17214.6855 - val_loss: 385640512.0000 - val_rmse: 19637.7324\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 308532416.0000 - rmse: 17565.0918 - val_loss: 376614720.0000 - val_rmse: 19406.5645\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 323140448.0000 - rmse: 17976.1074 - val_loss: 380223200.0000 - val_rmse: 19499.3125\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 291619040.0000 - rmse: 17076.8574 - val_loss: 375684000.0000 - val_rmse: 19382.5703\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 270434048.0000 - rmse: 16444.8789 - val_loss: 340750304.0000 - val_rmse: 18459.4238\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 261715936.0000 - rmse: 16177.6367 - val_loss: 350341632.0000 - val_rmse: 18717.4160\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309529920.0000 - rmse: 17593.4629 - val_loss: 326397088.0000 - val_rmse: 18066.4629\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 318435168.0000 - rmse: 17844.7520 - val_loss: 362706784.0000 - val_rmse: 19044.8633\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309694112.0000 - rmse: 17598.1289 - val_loss: 361924704.0000 - val_rmse: 19024.3184\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 281464608.0000 - rmse: 16776.9062 - val_loss: 370549792.0000 - val_rmse: 19249.6699\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 304470592.0000 - rmse: 17449.0859 - val_loss: 391971392.0000 - val_rmse: 19798.2676\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 296949152.0000 - rmse: 17232.2129 - val_loss: 394077344.0000 - val_rmse: 19851.3809\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 275783328.0000 - rmse: 16606.7246 - val_loss: 340992832.0000 - val_rmse: 18465.9922\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 275725984.0000 - rmse: 16604.9980 - val_loss: 336021312.0000 - val_rmse: 18330.8848\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286770272.0000 - rmse: 16934.2930 - val_loss: 370015552.0000 - val_rmse: 19235.7891\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 258696864.0000 - rmse: 16084.0566 - val_loss: 409274912.0000 - val_rmse: 20230.5449\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 276756000.0000 - rmse: 16635.9844 - val_loss: 340159232.0000 - val_rmse: 18443.4062\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 310199136.0000 - rmse: 17612.4707 - val_loss: 396786016.0000 - val_rmse: 19919.4883\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 273664192.0000 - rmse: 16542.7988 - val_loss: 364006528.0000 - val_rmse: 19078.9551\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 275403424.0000 - rmse: 16595.2832 - val_loss: 408063584.0000 - val_rmse: 20200.5840\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 275609024.0000 - rmse: 16601.4766 - val_loss: 400946496.0000 - val_rmse: 20023.6484\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 265155104.0000 - rmse: 16283.5840 - val_loss: 373374400.0000 - val_rmse: 19322.8984\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 301064288.0000 - rmse: 17351.2051 - val_loss: 332364544.0000 - val_rmse: 18230.8672\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 277043776.0000 - rmse: 16644.6328 - val_loss: 369002528.0000 - val_rmse: 19209.4395\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 276584416.0000 - rmse: 16630.8281 - val_loss: 360891232.0000 - val_rmse: 18997.1367\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280129728.0000 - rmse: 16737.0762 - val_loss: 373128096.0000 - val_rmse: 19316.5234\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 254254944.0000 - rmse: 15945.3740 - val_loss: 337660160.0000 - val_rmse: 18375.5312\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 269736704.0000 - rmse: 16423.6621 - val_loss: 357442784.0000 - val_rmse: 18906.1582\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 291544320.0000 - rmse: 17074.6699 - val_loss: 336284032.0000 - val_rmse: 18338.0488\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 279238272.0000 - rmse: 16710.4238 - val_loss: 391589216.0000 - val_rmse: 19788.6133\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 259513472.0000 - rmse: 16109.4219 - val_loss: 336648160.0000 - val_rmse: 18347.9746\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 256136640.0000 - rmse: 16004.2695 - val_loss: 367932992.0000 - val_rmse: 19181.5801\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 272849824.0000 - rmse: 16518.1660 - val_loss: 391960896.0000 - val_rmse: 19798.0020\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309469280.0000 - rmse: 17591.7383 - val_loss: 419658272.0000 - val_rmse: 20485.5625\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 257252576.0000 - rmse: 16039.0957 - val_loss: 368294400.0000 - val_rmse: 19190.9980\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 313324768.0000 - rmse: 17700.9824 - val_loss: 400038048.0000 - val_rmse: 20000.9512\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 257194272.0000 - rmse: 16037.2773 - val_loss: 370918592.0000 - val_rmse: 19259.2461\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 254773520.0000 - rmse: 15961.6270 - val_loss: 351127840.0000 - val_rmse: 18738.4062\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260338256.0000 - rmse: 16135.0010 - val_loss: 362138272.0000 - val_rmse: 19029.9316\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 270348672.0000 - rmse: 16442.2832 - val_loss: 366992896.0000 - val_rmse: 19157.0586\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 239982704.0000 - rmse: 15491.3750 - val_loss: 376171392.0000 - val_rmse: 19395.1387\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 244979024.0000 - rmse: 15651.8057 - val_loss: 363266464.0000 - val_rmse: 19059.5508\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 244468000.0000 - rmse: 15635.4727 - val_loss: 366159200.0000 - val_rmse: 19135.2871\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 282298208.0000 - rmse: 16801.7324 - val_loss: 369875744.0000 - val_rmse: 19232.1543\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 272950048.0000 - rmse: 16521.1992 - val_loss: 373189248.0000 - val_rmse: 19318.1074\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264036368.0000 - rmse: 16249.1963 - val_loss: 398931168.0000 - val_rmse: 19973.2617\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 238673520.0000 - rmse: 15449.0625 - val_loss: 351085792.0000 - val_rmse: 18737.2832\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 249822592.0000 - rmse: 15805.7773 - val_loss: 359804256.0000 - val_rmse: 18968.5078\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 250852720.0000 - rmse: 15838.3311 - val_loss: 355827232.0000 - val_rmse: 18863.3828\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 251221040.0000 - rmse: 15849.9541 - val_loss: 357389664.0000 - val_rmse: 18904.7520\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 254497728.0000 - rmse: 15952.9854 - val_loss: 414376640.0000 - val_rmse: 20356.2441\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280914176.0000 - rmse: 16760.4941 - val_loss: 370970144.0000 - val_rmse: 19260.5859\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 275197472.0000 - rmse: 16589.0762 - val_loss: 385105984.0000 - val_rmse: 19624.1172\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 256298208.0000 - rmse: 16009.3164 - val_loss: 427224608.0000 - val_rmse: 20669.4121\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 240235664.0000 - rmse: 15499.5371 - val_loss: 367567168.0000 - val_rmse: 19172.0410\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 240801456.0000 - rmse: 15517.7783 - val_loss: 368422080.0000 - val_rmse: 19194.3242\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260302048.0000 - rmse: 16133.8789 - val_loss: 400451552.0000 - val_rmse: 20011.2852\n",
      "Epoch 367/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 293724928.0000 - rmse: 17138.4043 - val_loss: 331589248.0000 - val_rmse: 18209.5918\n",
      "Epoch 368/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 252493936.0000 - rmse: 15890.0576 - val_loss: 371509952.0000 - val_rmse: 19274.5938\n",
      "Epoch 369/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 255500784.0000 - rmse: 15984.3916 - val_loss: 387069312.0000 - val_rmse: 19674.0781\n",
      "Epoch 370/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 251752592.0000 - rmse: 15866.7129 - val_loss: 359336000.0000 - val_rmse: 18956.1602\n",
      "Epoch 371/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 241184304.0000 - rmse: 15530.1094 - val_loss: 366989024.0000 - val_rmse: 19156.9570\n",
      "Epoch 372/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 251160528.0000 - rmse: 15848.0449 - val_loss: 347618400.0000 - val_rmse: 18644.5273\n",
      "Epoch 373/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 236007680.0000 - rmse: 15362.5410 - val_loss: 368032768.0000 - val_rmse: 19184.1797\n",
      "Epoch 374/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 224052400.0000 - rmse: 14968.3799 - val_loss: 342904160.0000 - val_rmse: 18517.6719\n",
      "Epoch 375/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 228678464.0000 - rmse: 15122.1182 - val_loss: 367829056.0000 - val_rmse: 19178.8691\n",
      "Epoch 376/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280370720.0000 - rmse: 16744.2734 - val_loss: 371799968.0000 - val_rmse: 19282.1152\n",
      "Epoch 377/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 270861920.0000 - rmse: 16457.8828 - val_loss: 387645312.0000 - val_rmse: 19688.7109\n",
      "Epoch 378/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 243612096.0000 - rmse: 15608.0781 - val_loss: 352494016.0000 - val_rmse: 18774.8242\n",
      "Epoch 379/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 245585424.0000 - rmse: 15671.1650 - val_loss: 416395840.0000 - val_rmse: 20405.7793\n",
      "Epoch 380/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 232557184.0000 - rmse: 15249.8262 - val_loss: 349816640.0000 - val_rmse: 18703.3867\n",
      "Epoch 381/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 257605392.0000 - rmse: 16050.0898 - val_loss: 389938464.0000 - val_rmse: 19746.8594\n",
      "Epoch 382/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260375856.0000 - rmse: 16136.1660 - val_loss: 367979776.0000 - val_rmse: 19182.7988\n",
      "Epoch 383/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260009504.0000 - rmse: 16124.8105 - val_loss: 333677088.0000 - val_rmse: 18266.8301\n",
      "Epoch 384/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 242515568.0000 - rmse: 15572.9111 - val_loss: 343918048.0000 - val_rmse: 18545.0273\n",
      "Epoch 385/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268980864.0000 - rmse: 16400.6367 - val_loss: 387762336.0000 - val_rmse: 19691.6816\n",
      "Epoch 386/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 277717728.0000 - rmse: 16664.8652 - val_loss: 356812160.0000 - val_rmse: 18889.4727\n",
      "Epoch 387/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 283343104.0000 - rmse: 16832.7988 - val_loss: 404533664.0000 - val_rmse: 20113.0215\n",
      "Epoch 388/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 270084064.0000 - rmse: 16434.2344 - val_loss: 361348160.0000 - val_rmse: 19009.1602\n",
      "Epoch 389/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 234909968.0000 - rmse: 15326.7725 - val_loss: 341228512.0000 - val_rmse: 18472.3711\n",
      "Epoch 390/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 233371200.0000 - rmse: 15276.4922 - val_loss: 333007232.0000 - val_rmse: 18248.4863\n",
      "Epoch 391/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 232402336.0000 - rmse: 15244.7480 - val_loss: 336349472.0000 - val_rmse: 18339.8320\n",
      "Epoch 392/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 251177168.0000 - rmse: 15848.5703 - val_loss: 363743136.0000 - val_rmse: 19072.0508\n",
      "Epoch 393/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268138080.0000 - rmse: 16374.9219 - val_loss: 355683008.0000 - val_rmse: 18859.5605\n",
      "Epoch 394/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 255505456.0000 - rmse: 15984.5381 - val_loss: 332805312.0000 - val_rmse: 18242.9531\n",
      "Epoch 395/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 228548480.0000 - rmse: 15117.8203 - val_loss: 383130624.0000 - val_rmse: 19573.7227\n",
      "Epoch 396/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 239285392.0000 - rmse: 15468.8525 - val_loss: 344514720.0000 - val_rmse: 18561.1074\n",
      "Epoch 397/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 247530160.0000 - rmse: 15733.0908 - val_loss: 303380128.0000 - val_rmse: 17417.8105\n",
      "Epoch 398/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 237695424.0000 - rmse: 15417.3740 - val_loss: 333244896.0000 - val_rmse: 18254.9961\n",
      "Epoch 399/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 245535136.0000 - rmse: 15669.5605 - val_loss: 373708352.0000 - val_rmse: 19331.5371\n",
      "Epoch 400/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 248831104.0000 - rmse: 15774.3809 - val_loss: 352353536.0000 - val_rmse: 18771.0820\n",
      "104/104 [==============================] - 0s 851us/step - loss: 378631232.0000 - rmse: 19458.4492\n",
      "[378631232.0, 19458.44921875]\n",
      "<src.model.emb_model object at 0x7f846770d650>\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 20018059264.0000 - rmse: 141485.1875 - val_loss: 2780025344.0000 - val_rmse: 52725.9453\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2532239616.0000 - rmse: 50321.3633 - val_loss: 1446901376.0000 - val_rmse: 38038.1562\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1789557888.0000 - rmse: 42303.1680 - val_loss: 1322458880.0000 - val_rmse: 36365.6289\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1778725248.0000 - rmse: 42174.9375 - val_loss: 1280145280.0000 - val_rmse: 35779.1172\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1630091776.0000 - rmse: 40374.3945 - val_loss: 1266680448.0000 - val_rmse: 35590.4531\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1643917312.0000 - rmse: 40545.2500 - val_loss: 1231878784.0000 - val_rmse: 35098.1289\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1551894656.0000 - rmse: 39394.0938 - val_loss: 1201922816.0000 - val_rmse: 34668.7578\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1594030464.0000 - rmse: 39925.3125 - val_loss: 1210429184.0000 - val_rmse: 34791.2227\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1562743808.0000 - rmse: 39531.5547 - val_loss: 1139835648.0000 - val_rmse: 33761.4531\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1499862272.0000 - rmse: 38728.0547 - val_loss: 1144389632.0000 - val_rmse: 33828.8281\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1490424704.0000 - rmse: 38606.0195 - val_loss: 1073654848.0000 - val_rmse: 32766.6719\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1385897600.0000 - rmse: 37227.6445 - val_loss: 1063664256.0000 - val_rmse: 32613.8652\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1381999488.0000 - rmse: 37175.2539 - val_loss: 1140645376.0000 - val_rmse: 33773.4414\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1499097472.0000 - rmse: 38718.1797 - val_loss: 978890304.0000 - val_rmse: 31287.2227\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1373088384.0000 - rmse: 37055.2070 - val_loss: 1032050880.0000 - val_rmse: 32125.5488\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1303763456.0000 - rmse: 36107.6641 - val_loss: 939447488.0000 - val_rmse: 30650.4082\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1269163776.0000 - rmse: 35625.3242 - val_loss: 923589376.0000 - val_rmse: 30390.6133\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1299700608.0000 - rmse: 36051.3594 - val_loss: 948270848.0000 - val_rmse: 30794.0059\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1225731456.0000 - rmse: 35010.4492 - val_loss: 961456384.0000 - val_rmse: 31007.3594\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1282370560.0000 - rmse: 35810.2031 - val_loss: 871258624.0000 - val_rmse: 29517.0898\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1160923136.0000 - rmse: 34072.3203 - val_loss: 831835072.0000 - val_rmse: 28841.5508\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1135102464.0000 - rmse: 33691.2812 - val_loss: 813295040.0000 - val_rmse: 28518.3281\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1087337856.0000 - rmse: 32974.8047 - val_loss: 808189824.0000 - val_rmse: 28428.6797\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1091007872.0000 - rmse: 33030.4102 - val_loss: 823638016.0000 - val_rmse: 28699.0938\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1145693568.0000 - rmse: 33848.0938 - val_loss: 819299648.0000 - val_rmse: 28623.4102\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1112528128.0000 - rmse: 33354.5820 - val_loss: 778911360.0000 - val_rmse: 27908.9844\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1090486272.0000 - rmse: 33022.5117 - val_loss: 765385408.0000 - val_rmse: 27665.5996\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1066082880.0000 - rmse: 32650.9238 - val_loss: 720592896.0000 - val_rmse: 26843.8613\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1050237248.0000 - rmse: 32407.3633 - val_loss: 758110848.0000 - val_rmse: 27533.8125\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1085426304.0000 - rmse: 32945.8086 - val_loss: 720904384.0000 - val_rmse: 26849.6621\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1033675584.0000 - rmse: 32150.8262 - val_loss: 739565824.0000 - val_rmse: 27194.9590\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 993237824.0000 - rmse: 31515.6758 - val_loss: 711792896.0000 - val_rmse: 26679.4473\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 973378112.0000 - rmse: 31199.0078 - val_loss: 713498112.0000 - val_rmse: 26711.3848\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 983880832.0000 - rmse: 31366.8750 - val_loss: 758177920.0000 - val_rmse: 27535.0312\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 973451904.0000 - rmse: 31200.1914 - val_loss: 727638208.0000 - val_rmse: 26974.7695\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 957343296.0000 - rmse: 30940.9648 - val_loss: 677622784.0000 - val_rmse: 26031.1895\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 936023488.0000 - rmse: 30594.5000 - val_loss: 673872192.0000 - val_rmse: 25959.0488\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 967687808.0000 - rmse: 31107.6816 - val_loss: 708196672.0000 - val_rmse: 26611.9648\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 921151104.0000 - rmse: 30350.4707 - val_loss: 713236544.0000 - val_rmse: 26706.4883\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 903766848.0000 - rmse: 30062.7148 - val_loss: 672452224.0000 - val_rmse: 25931.6836\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 855050240.0000 - rmse: 29241.2422 - val_loss: 652710336.0000 - val_rmse: 25548.1973\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1042774336.0000 - rmse: 32292.0156 - val_loss: 651197312.0000 - val_rmse: 25518.5684\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 913626560.0000 - rmse: 30226.2559 - val_loss: 659552512.0000 - val_rmse: 25681.7539\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 881173952.0000 - rmse: 29684.5742 - val_loss: 646875840.0000 - val_rmse: 25433.7539\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 931325440.0000 - rmse: 30517.6250 - val_loss: 669200000.0000 - val_rmse: 25868.9004\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 904677440.0000 - rmse: 30077.8555 - val_loss: 647682432.0000 - val_rmse: 25449.6055\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 886284864.0000 - rmse: 29770.5371 - val_loss: 660621248.0000 - val_rmse: 25702.5527\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 902612480.0000 - rmse: 30043.5098 - val_loss: 671688576.0000 - val_rmse: 25916.9551\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 874045568.0000 - rmse: 29564.2617 - val_loss: 699246528.0000 - val_rmse: 26443.2695\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 858149696.0000 - rmse: 29294.1914 - val_loss: 614660800.0000 - val_rmse: 24792.3535\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 774047104.0000 - rmse: 27821.7012 - val_loss: 609113280.0000 - val_rmse: 24680.2207\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 866885440.0000 - rmse: 29442.9180 - val_loss: 740847168.0000 - val_rmse: 27218.5078\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 865269888.0000 - rmse: 29415.4707 - val_loss: 628750080.0000 - val_rmse: 25074.8887\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 831723520.0000 - rmse: 28839.6172 - val_loss: 603580864.0000 - val_rmse: 24567.8828\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 852641344.0000 - rmse: 29200.0234 - val_loss: 609154368.0000 - val_rmse: 24681.0527\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 893433344.0000 - rmse: 29890.3555 - val_loss: 610649536.0000 - val_rmse: 24711.3242\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 876632128.0000 - rmse: 29607.9746 - val_loss: 613887936.0000 - val_rmse: 24776.7617\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 896494272.0000 - rmse: 29941.5137 - val_loss: 615484480.0000 - val_rmse: 24808.9590\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 837664256.0000 - rmse: 28942.4297 - val_loss: 608717760.0000 - val_rmse: 24672.2070\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 849749376.0000 - rmse: 29150.4609 - val_loss: 598294784.0000 - val_rmse: 24460.0645\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 872992384.0000 - rmse: 29546.4453 - val_loss: 591790336.0000 - val_rmse: 24326.7402\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 771164096.0000 - rmse: 27769.8418 - val_loss: 595570816.0000 - val_rmse: 24404.3203\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 804909120.0000 - rmse: 28370.9199 - val_loss: 591850752.0000 - val_rmse: 24327.9824\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 885282304.0000 - rmse: 29753.6934 - val_loss: 614130304.0000 - val_rmse: 24781.6523\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 793672704.0000 - rmse: 28172.1973 - val_loss: 582256384.0000 - val_rmse: 24129.9902\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 808778816.0000 - rmse: 28439.0371 - val_loss: 590210688.0000 - val_rmse: 24294.2520\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 826418752.0000 - rmse: 28747.5000 - val_loss: 570355392.0000 - val_rmse: 23882.1133\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 848938624.0000 - rmse: 29136.5508 - val_loss: 557023168.0000 - val_rmse: 23601.3379\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 774697920.0000 - rmse: 27833.3965 - val_loss: 576210944.0000 - val_rmse: 24004.3945\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 752211328.0000 - rmse: 27426.4707 - val_loss: 563062464.0000 - val_rmse: 23728.9375\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 767558848.0000 - rmse: 27704.8516 - val_loss: 605008192.0000 - val_rmse: 24596.9141\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 797531648.0000 - rmse: 28240.6035 - val_loss: 564851648.0000 - val_rmse: 23766.6074\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 796035200.0000 - rmse: 28214.0957 - val_loss: 616007936.0000 - val_rmse: 24819.5078\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 763345472.0000 - rmse: 27628.7070 - val_loss: 543145472.0000 - val_rmse: 23305.4824\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 849143296.0000 - rmse: 29140.0625 - val_loss: 525493952.0000 - val_rmse: 22923.6543\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 690511168.0000 - rmse: 26277.5801 - val_loss: 534253376.0000 - val_rmse: 23113.9219\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 717750272.0000 - rmse: 26790.8613 - val_loss: 524383936.0000 - val_rmse: 22899.4316\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 780142976.0000 - rmse: 27931.0391 - val_loss: 526366528.0000 - val_rmse: 22942.6797\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 743574528.0000 - rmse: 27268.5625 - val_loss: 533034144.0000 - val_rmse: 23087.5312\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 778759872.0000 - rmse: 27906.2695 - val_loss: 588384000.0000 - val_rmse: 24256.6289\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 756220160.0000 - rmse: 27499.4570 - val_loss: 564639808.0000 - val_rmse: 23762.1504\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 790840960.0000 - rmse: 28121.8945 - val_loss: 566468416.0000 - val_rmse: 23800.5977\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 713958272.0000 - rmse: 26719.9980 - val_loss: 607574784.0000 - val_rmse: 24649.0312\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 724234560.0000 - rmse: 26911.6055 - val_loss: 524845376.0000 - val_rmse: 22909.5039\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 665553088.0000 - rmse: 25798.3164 - val_loss: 501372224.0000 - val_rmse: 22391.3418\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 706948096.0000 - rmse: 26588.4961 - val_loss: 529965280.0000 - val_rmse: 23020.9746\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 737234240.0000 - rmse: 27152.0586 - val_loss: 523380672.0000 - val_rmse: 22877.5137\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 711691456.0000 - rmse: 26677.5469 - val_loss: 499141024.0000 - val_rmse: 22341.4648\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 744207040.0000 - rmse: 27280.1582 - val_loss: 497593728.0000 - val_rmse: 22306.8086\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 724280448.0000 - rmse: 26912.4590 - val_loss: 524451168.0000 - val_rmse: 22900.8984\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 648037504.0000 - rmse: 25456.5801 - val_loss: 506584352.0000 - val_rmse: 22507.4297\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 742259136.0000 - rmse: 27244.4336 - val_loss: 506520064.0000 - val_rmse: 22506.0000\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 696727296.0000 - rmse: 26395.5918 - val_loss: 509251040.0000 - val_rmse: 22566.5918\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 761502784.0000 - rmse: 27595.3398 - val_loss: 482068608.0000 - val_rmse: 21956.0605\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 669990208.0000 - rmse: 25884.1699 - val_loss: 476257088.0000 - val_rmse: 21823.3145\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 643582848.0000 - rmse: 25368.9355 - val_loss: 504998016.0000 - val_rmse: 22472.1602\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 671243328.0000 - rmse: 25908.3633 - val_loss: 486769216.0000 - val_rmse: 22062.8477\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 787356032.0000 - rmse: 28059.8652 - val_loss: 483230528.0000 - val_rmse: 21982.5059\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 688504576.0000 - rmse: 26239.3711 - val_loss: 685278976.0000 - val_rmse: 26177.8340\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 678373696.0000 - rmse: 26045.6074 - val_loss: 495275680.0000 - val_rmse: 22254.7910\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 667120768.0000 - rmse: 25828.6816 - val_loss: 479113440.0000 - val_rmse: 21888.6602\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 687576576.0000 - rmse: 26221.6816 - val_loss: 542748288.0000 - val_rmse: 23296.9590\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 668722048.0000 - rmse: 25859.6602 - val_loss: 475537152.0000 - val_rmse: 21806.8145\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 621703808.0000 - rmse: 24933.9883 - val_loss: 474159648.0000 - val_rmse: 21775.2070\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 588799808.0000 - rmse: 24265.1973 - val_loss: 484067168.0000 - val_rmse: 22001.5273\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 635192704.0000 - rmse: 25203.0293 - val_loss: 466597312.0000 - val_rmse: 21600.8633\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 655334080.0000 - rmse: 25599.4941 - val_loss: 513429120.0000 - val_rmse: 22658.9746\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 726392256.0000 - rmse: 26951.6660 - val_loss: 468588672.0000 - val_rmse: 21646.9082\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 596408768.0000 - rmse: 24421.4824 - val_loss: 537011264.0000 - val_rmse: 23173.5039\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 657502912.0000 - rmse: 25641.8203 - val_loss: 479118752.0000 - val_rmse: 21888.7812\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 617869824.0000 - rmse: 24856.9883 - val_loss: 452500992.0000 - val_rmse: 21272.0703\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 681156480.0000 - rmse: 26098.9746 - val_loss: 456615360.0000 - val_rmse: 21368.5605\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 582606912.0000 - rmse: 24137.2520 - val_loss: 443760224.0000 - val_rmse: 21065.6172\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 641969920.0000 - rmse: 25337.1250 - val_loss: 467647264.0000 - val_rmse: 21625.1543\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 609005568.0000 - rmse: 24678.0391 - val_loss: 480424928.0000 - val_rmse: 21918.5977\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 632544832.0000 - rmse: 25150.4434 - val_loss: 447092256.0000 - val_rmse: 21144.5566\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 637644736.0000 - rmse: 25251.6289 - val_loss: 472359040.0000 - val_rmse: 21733.8223\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 581948096.0000 - rmse: 24123.5996 - val_loss: 460260384.0000 - val_rmse: 21453.6797\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 628004416.0000 - rmse: 25060.0156 - val_loss: 510344640.0000 - val_rmse: 22590.8086\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 653559552.0000 - rmse: 25564.8105 - val_loss: 432030240.0000 - val_rmse: 20785.3379\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 630750080.0000 - rmse: 25114.7383 - val_loss: 444111488.0000 - val_rmse: 21073.9531\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 584810432.0000 - rmse: 24182.8535 - val_loss: 470119904.0000 - val_rmse: 21682.2480\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 626099968.0000 - rmse: 25021.9902 - val_loss: 434291456.0000 - val_rmse: 20839.6602\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 569298816.0000 - rmse: 23859.9844 - val_loss: 445398368.0000 - val_rmse: 21104.4629\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 585889792.0000 - rmse: 24205.1602 - val_loss: 447133088.0000 - val_rmse: 21145.5215\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 622717184.0000 - rmse: 24954.3027 - val_loss: 460573472.0000 - val_rmse: 21460.9746\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 539141568.0000 - rmse: 23219.4219 - val_loss: 446664992.0000 - val_rmse: 21134.4512\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 605511680.0000 - rmse: 24607.1465 - val_loss: 427455072.0000 - val_rmse: 20674.9863\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 592634816.0000 - rmse: 24344.0918 - val_loss: 437244928.0000 - val_rmse: 20910.4023\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 630551936.0000 - rmse: 25110.7930 - val_loss: 443778688.0000 - val_rmse: 21066.0547\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 574655808.0000 - rmse: 23971.9805 - val_loss: 451634336.0000 - val_rmse: 21251.6895\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 567983104.0000 - rmse: 23832.3965 - val_loss: 448342400.0000 - val_rmse: 21174.0977\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 603881024.0000 - rmse: 24573.9902 - val_loss: 446265056.0000 - val_rmse: 21124.9863\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 597266560.0000 - rmse: 24439.0371 - val_loss: 442724000.0000 - val_rmse: 21041.0078\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 575208640.0000 - rmse: 23983.5078 - val_loss: 456241184.0000 - val_rmse: 21359.8027\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 602072320.0000 - rmse: 24537.1621 - val_loss: 456137280.0000 - val_rmse: 21357.3711\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 560328128.0000 - rmse: 23671.2520 - val_loss: 431040768.0000 - val_rmse: 20761.5215\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 555855232.0000 - rmse: 23576.5820 - val_loss: 415739008.0000 - val_rmse: 20389.6797\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 553810048.0000 - rmse: 23533.1699 - val_loss: 409095712.0000 - val_rmse: 20226.1152\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 537260672.0000 - rmse: 23178.8848 - val_loss: 402512608.0000 - val_rmse: 20062.7168\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 551077568.0000 - rmse: 23475.0410 - val_loss: 448677312.0000 - val_rmse: 21182.0039\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 539093824.0000 - rmse: 23218.3945 - val_loss: 401518112.0000 - val_rmse: 20037.9160\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 601839680.0000 - rmse: 24532.4219 - val_loss: 424906496.0000 - val_rmse: 20613.2598\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 526816800.0000 - rmse: 22952.4902 - val_loss: 416987424.0000 - val_rmse: 20420.2695\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 577493632.0000 - rmse: 24031.0977 - val_loss: 476524576.0000 - val_rmse: 21829.4434\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 566923904.0000 - rmse: 23810.1641 - val_loss: 410286720.0000 - val_rmse: 20255.5352\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 556478720.0000 - rmse: 23589.8008 - val_loss: 465300704.0000 - val_rmse: 21570.8301\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 561452672.0000 - rmse: 23694.9922 - val_loss: 412647840.0000 - val_rmse: 20313.7344\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 521005728.0000 - rmse: 22825.5508 - val_loss: 423172928.0000 - val_rmse: 20571.1680\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 584094976.0000 - rmse: 24168.0566 - val_loss: 429008736.0000 - val_rmse: 20712.5254\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 545637888.0000 - rmse: 23358.8926 - val_loss: 406488192.0000 - val_rmse: 20161.5527\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 556405504.0000 - rmse: 23588.2500 - val_loss: 496749344.0000 - val_rmse: 22287.8750\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 533453856.0000 - rmse: 23096.6191 - val_loss: 440261856.0000 - val_rmse: 20982.4180\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 567831616.0000 - rmse: 23829.2168 - val_loss: 441096192.0000 - val_rmse: 21002.2910\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 591322048.0000 - rmse: 24317.1152 - val_loss: 408584288.0000 - val_rmse: 20213.4688\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 500608864.0000 - rmse: 22374.2910 - val_loss: 418628224.0000 - val_rmse: 20460.4062\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 495073952.0000 - rmse: 22250.2578 - val_loss: 418327424.0000 - val_rmse: 20453.0547\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 521925504.0000 - rmse: 22845.6895 - val_loss: 404943904.0000 - val_rmse: 20123.2188\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 552274816.0000 - rmse: 23500.5273 - val_loss: 474453536.0000 - val_rmse: 21781.9551\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 548085248.0000 - rmse: 23411.2207 - val_loss: 475605920.0000 - val_rmse: 21808.3906\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 516550208.0000 - rmse: 22727.7402 - val_loss: 424346400.0000 - val_rmse: 20599.6699\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 537055424.0000 - rmse: 23174.4570 - val_loss: 397031360.0000 - val_rmse: 19925.6465\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 501466016.0000 - rmse: 22393.4375 - val_loss: 398977728.0000 - val_rmse: 19974.4277\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 511701568.0000 - rmse: 22620.8223 - val_loss: 427998624.0000 - val_rmse: 20688.1270\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 505326464.0000 - rmse: 22479.4668 - val_loss: 400955520.0000 - val_rmse: 20023.8730\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 493426816.0000 - rmse: 22213.2129 - val_loss: 412045344.0000 - val_rmse: 20298.9004\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 456654240.0000 - rmse: 21369.4707 - val_loss: 432879776.0000 - val_rmse: 20805.7637\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 501076608.0000 - rmse: 22384.7402 - val_loss: 405541312.0000 - val_rmse: 20138.0566\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 480258912.0000 - rmse: 21914.8105 - val_loss: 425896288.0000 - val_rmse: 20637.2539\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 473149280.0000 - rmse: 21751.9941 - val_loss: 418277920.0000 - val_rmse: 20451.8438\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 488593152.0000 - rmse: 22104.1426 - val_loss: 419610944.0000 - val_rmse: 20484.4082\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 523804960.0000 - rmse: 22886.7852 - val_loss: 412503168.0000 - val_rmse: 20310.1738\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 432826944.0000 - rmse: 20804.4941 - val_loss: 388297568.0000 - val_rmse: 19705.2676\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 531214464.0000 - rmse: 23048.0898 - val_loss: 450107072.0000 - val_rmse: 21215.7266\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 467706912.0000 - rmse: 21626.5332 - val_loss: 395831552.0000 - val_rmse: 19895.5156\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 460054016.0000 - rmse: 21448.8691 - val_loss: 381845728.0000 - val_rmse: 19540.8730\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 453984416.0000 - rmse: 21306.9102 - val_loss: 408545312.0000 - val_rmse: 20212.5039\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 472998880.0000 - rmse: 21748.5371 - val_loss: 387067200.0000 - val_rmse: 19674.0234\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 490689408.0000 - rmse: 22151.5098 - val_loss: 408094208.0000 - val_rmse: 20201.3418\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 507961440.0000 - rmse: 22538.0000 - val_loss: 408514112.0000 - val_rmse: 20211.7324\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 437496608.0000 - rmse: 20916.4199 - val_loss: 449439968.0000 - val_rmse: 21200.0000\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 465792000.0000 - rmse: 21582.2148 - val_loss: 387647968.0000 - val_rmse: 19688.7773\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 500441792.0000 - rmse: 22370.5566 - val_loss: 412374816.0000 - val_rmse: 20307.0137\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 457394368.0000 - rmse: 21386.7793 - val_loss: 407010784.0000 - val_rmse: 20174.5078\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 462978752.0000 - rmse: 21516.9414 - val_loss: 444711552.0000 - val_rmse: 21088.1855\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 456589120.0000 - rmse: 21367.9453 - val_loss: 375962144.0000 - val_rmse: 19389.7441\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 462385216.0000 - rmse: 21503.1445 - val_loss: 381966528.0000 - val_rmse: 19543.9648\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 474486624.0000 - rmse: 21782.7148 - val_loss: 375554208.0000 - val_rmse: 19379.2207\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 485661184.0000 - rmse: 22037.7227 - val_loss: 419125152.0000 - val_rmse: 20472.5469\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 484015008.0000 - rmse: 22000.3418 - val_loss: 419430656.0000 - val_rmse: 20480.0059\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 476500800.0000 - rmse: 21828.8984 - val_loss: 389857536.0000 - val_rmse: 19744.8105\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 440323904.0000 - rmse: 20983.8965 - val_loss: 357437120.0000 - val_rmse: 18906.0078\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 477595488.0000 - rmse: 21853.9590 - val_loss: 361774080.0000 - val_rmse: 19020.3594\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 433986592.0000 - rmse: 20832.3457 - val_loss: 377945888.0000 - val_rmse: 19440.8301\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 443883616.0000 - rmse: 21068.5449 - val_loss: 362777280.0000 - val_rmse: 19046.7129\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 432246848.0000 - rmse: 20790.5469 - val_loss: 368882784.0000 - val_rmse: 19206.3223\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 416140992.0000 - rmse: 20399.5332 - val_loss: 375042272.0000 - val_rmse: 19366.0078\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 437905888.0000 - rmse: 20926.2012 - val_loss: 363337696.0000 - val_rmse: 19061.4199\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 452956576.0000 - rmse: 21282.7773 - val_loss: 355004896.0000 - val_rmse: 18841.5742\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 456424768.0000 - rmse: 21364.0996 - val_loss: 499751360.0000 - val_rmse: 22355.1191\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 443644288.0000 - rmse: 21062.8652 - val_loss: 352592384.0000 - val_rmse: 18777.4434\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 418524960.0000 - rmse: 20457.8828 - val_loss: 394489088.0000 - val_rmse: 19861.7500\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 400010048.0000 - rmse: 20000.2520 - val_loss: 365195520.0000 - val_rmse: 19110.0898\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 413360288.0000 - rmse: 20331.2637 - val_loss: 361925792.0000 - val_rmse: 19024.3477\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 420850720.0000 - rmse: 20514.6465 - val_loss: 371899648.0000 - val_rmse: 19284.6992\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 432157760.0000 - rmse: 20788.4043 - val_loss: 374273952.0000 - val_rmse: 19346.1621\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 439450720.0000 - rmse: 20963.0801 - val_loss: 360135168.0000 - val_rmse: 18977.2285\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 421717824.0000 - rmse: 20535.7695 - val_loss: 364353760.0000 - val_rmse: 19088.0527\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 405239392.0000 - rmse: 20130.5586 - val_loss: 395779616.0000 - val_rmse: 19894.2109\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 427738912.0000 - rmse: 20681.8496 - val_loss: 388922496.0000 - val_rmse: 19721.1172\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 387696736.0000 - rmse: 19690.0156 - val_loss: 400219456.0000 - val_rmse: 20005.4863\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 434835072.0000 - rmse: 20852.6992 - val_loss: 373585024.0000 - val_rmse: 19328.3477\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 391201760.0000 - rmse: 19778.8203 - val_loss: 348549824.0000 - val_rmse: 18669.4883\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 436034176.0000 - rmse: 20881.4316 - val_loss: 396581664.0000 - val_rmse: 19914.3574\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 372924640.0000 - rmse: 19311.2559 - val_loss: 368912480.0000 - val_rmse: 19207.0938\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 423723872.0000 - rmse: 20584.5547 - val_loss: 369265376.0000 - val_rmse: 19216.2793\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 389707776.0000 - rmse: 19741.0176 - val_loss: 348445600.0000 - val_rmse: 18666.6973\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 386747328.0000 - rmse: 19665.8926 - val_loss: 402336032.0000 - val_rmse: 20058.3164\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 415308736.0000 - rmse: 20379.1250 - val_loss: 407211904.0000 - val_rmse: 20179.4922\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 389102048.0000 - rmse: 19725.6699 - val_loss: 358333856.0000 - val_rmse: 18929.7090\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 436838496.0000 - rmse: 20900.6816 - val_loss: 348285408.0000 - val_rmse: 18662.4062\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 393370816.0000 - rmse: 19833.5781 - val_loss: 343365504.0000 - val_rmse: 18530.1250\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 351400832.0000 - rmse: 18745.6875 - val_loss: 371106944.0000 - val_rmse: 19264.1367\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 380990016.0000 - rmse: 19518.9648 - val_loss: 338427456.0000 - val_rmse: 18396.3984\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 382853024.0000 - rmse: 19566.6309 - val_loss: 335866336.0000 - val_rmse: 18326.6562\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 356196960.0000 - rmse: 18873.1816 - val_loss: 361436832.0000 - val_rmse: 19011.4922\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 342466784.0000 - rmse: 18505.8574 - val_loss: 332216224.0000 - val_rmse: 18226.7988\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 392820448.0000 - rmse: 19819.6992 - val_loss: 379115456.0000 - val_rmse: 19470.8867\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 398434496.0000 - rmse: 19960.8242 - val_loss: 444493280.0000 - val_rmse: 21083.0098\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 353892672.0000 - rmse: 18812.0352 - val_loss: 344008352.0000 - val_rmse: 18547.4629\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 348203872.0000 - rmse: 18660.2207 - val_loss: 349481376.0000 - val_rmse: 18694.4219\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 366447808.0000 - rmse: 19142.8262 - val_loss: 338508256.0000 - val_rmse: 18398.5938\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 332328064.0000 - rmse: 18229.8672 - val_loss: 333609216.0000 - val_rmse: 18264.9727\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 364736736.0000 - rmse: 19098.0820 - val_loss: 343421312.0000 - val_rmse: 18531.6309\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 349005632.0000 - rmse: 18681.6934 - val_loss: 334031648.0000 - val_rmse: 18276.5332\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 346991104.0000 - rmse: 18627.6973 - val_loss: 334537344.0000 - val_rmse: 18290.3613\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 386060288.0000 - rmse: 19648.4160 - val_loss: 330456960.0000 - val_rmse: 18178.4746\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 366904064.0000 - rmse: 19154.7402 - val_loss: 354316896.0000 - val_rmse: 18823.3066\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 368434784.0000 - rmse: 19194.6543 - val_loss: 346066752.0000 - val_rmse: 18602.8691\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 362856768.0000 - rmse: 19048.7988 - val_loss: 352532768.0000 - val_rmse: 18775.8555\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 357105248.0000 - rmse: 18897.2285 - val_loss: 330963328.0000 - val_rmse: 18192.3984\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 349311040.0000 - rmse: 18689.8652 - val_loss: 332274048.0000 - val_rmse: 18228.3867\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 378351648.0000 - rmse: 19451.2637 - val_loss: 328407648.0000 - val_rmse: 18122.0215\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 340239168.0000 - rmse: 18445.5723 - val_loss: 334966560.0000 - val_rmse: 18302.0918\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 301253344.0000 - rmse: 17356.6504 - val_loss: 327039168.0000 - val_rmse: 18084.2246\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 368471520.0000 - rmse: 19195.6113 - val_loss: 339426432.0000 - val_rmse: 18423.5293\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 327820736.0000 - rmse: 18105.8203 - val_loss: 330530400.0000 - val_rmse: 18180.4941\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 341798336.0000 - rmse: 18487.7891 - val_loss: 335390624.0000 - val_rmse: 18313.6738\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 337538976.0000 - rmse: 18372.2344 - val_loss: 342052512.0000 - val_rmse: 18494.6621\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 323629920.0000 - rmse: 17989.7168 - val_loss: 360945248.0000 - val_rmse: 18998.5586\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 344536000.0000 - rmse: 18561.6816 - val_loss: 385929728.0000 - val_rmse: 19645.0938\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 365478592.0000 - rmse: 19117.4941 - val_loss: 380150432.0000 - val_rmse: 19497.4473\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 348570240.0000 - rmse: 18670.0352 - val_loss: 312991328.0000 - val_rmse: 17691.5605\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 295790848.0000 - rmse: 17198.5703 - val_loss: 324978272.0000 - val_rmse: 18027.1543\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 329732864.0000 - rmse: 18158.5488 - val_loss: 308876480.0000 - val_rmse: 17574.8828\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 298821856.0000 - rmse: 17286.4648 - val_loss: 303203232.0000 - val_rmse: 17412.7324\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 327629216.0000 - rmse: 18100.5312 - val_loss: 302893440.0000 - val_rmse: 17403.8340\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 330117536.0000 - rmse: 18169.1367 - val_loss: 305420256.0000 - val_rmse: 17476.2773\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 330497216.0000 - rmse: 18179.5820 - val_loss: 344856864.0000 - val_rmse: 18570.3223\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 300349824.0000 - rmse: 17330.6035 - val_loss: 317057088.0000 - val_rmse: 17806.0977\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 304918208.0000 - rmse: 17461.9082 - val_loss: 310444480.0000 - val_rmse: 17619.4355\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 294161280.0000 - rmse: 17151.1309 - val_loss: 305206240.0000 - val_rmse: 17470.1523\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 326539360.0000 - rmse: 18070.4004 - val_loss: 312473408.0000 - val_rmse: 17676.9180\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 316286656.0000 - rmse: 17784.4492 - val_loss: 314814240.0000 - val_rmse: 17743.0059\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 357020448.0000 - rmse: 18894.9844 - val_loss: 332565376.0000 - val_rmse: 18236.3750\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 336995680.0000 - rmse: 18357.4414 - val_loss: 330874784.0000 - val_rmse: 18189.9629\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 284585952.0000 - rmse: 16869.6758 - val_loss: 321515040.0000 - val_rmse: 17930.8398\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 282158048.0000 - rmse: 16797.5605 - val_loss: 305703904.0000 - val_rmse: 17484.3906\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286137984.0000 - rmse: 16915.6133 - val_loss: 293727872.0000 - val_rmse: 17138.4902\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299913568.0000 - rmse: 17318.0137 - val_loss: 297593376.0000 - val_rmse: 17250.8945\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 336138304.0000 - rmse: 18334.0742 - val_loss: 301502016.0000 - val_rmse: 17363.8125\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 311984896.0000 - rmse: 17663.0938 - val_loss: 298728320.0000 - val_rmse: 17283.7598\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280747648.0000 - rmse: 16755.5254 - val_loss: 301451712.0000 - val_rmse: 17362.3652\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 335513344.0000 - rmse: 18317.0234 - val_loss: 322304480.0000 - val_rmse: 17952.8398\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 305808224.0000 - rmse: 17487.3730 - val_loss: 289107296.0000 - val_rmse: 17003.1562\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 327796288.0000 - rmse: 18105.1445 - val_loss: 305249152.0000 - val_rmse: 17471.3809\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 323553984.0000 - rmse: 17987.6055 - val_loss: 320299936.0000 - val_rmse: 17896.9258\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299527616.0000 - rmse: 17306.8652 - val_loss: 331258528.0000 - val_rmse: 18200.5098\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 288848800.0000 - rmse: 16995.5527 - val_loss: 321127296.0000 - val_rmse: 17920.0254\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 281816672.0000 - rmse: 16787.3965 - val_loss: 314166720.0000 - val_rmse: 17724.7480\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 312965728.0000 - rmse: 17690.8379 - val_loss: 308546432.0000 - val_rmse: 17565.4902\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286461600.0000 - rmse: 16925.1758 - val_loss: 311561664.0000 - val_rmse: 17651.1094\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 285435072.0000 - rmse: 16894.8242 - val_loss: 297898368.0000 - val_rmse: 17259.7324\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 315552928.0000 - rmse: 17763.8086 - val_loss: 312658048.0000 - val_rmse: 17682.1387\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 294620288.0000 - rmse: 17164.5059 - val_loss: 351889280.0000 - val_rmse: 18758.7129\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 312956128.0000 - rmse: 17690.5664 - val_loss: 320861664.0000 - val_rmse: 17912.6113\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260243360.0000 - rmse: 16132.0596 - val_loss: 316299552.0000 - val_rmse: 17784.8125\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 279744736.0000 - rmse: 16725.5723 - val_loss: 296144672.0000 - val_rmse: 17208.8535\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 274325280.0000 - rmse: 16562.7676 - val_loss: 318946336.0000 - val_rmse: 17859.0684\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 283268224.0000 - rmse: 16830.5742 - val_loss: 291556992.0000 - val_rmse: 17075.0391\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286190880.0000 - rmse: 16917.1777 - val_loss: 322767936.0000 - val_rmse: 17965.7441\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 295340288.0000 - rmse: 17185.4668 - val_loss: 313652352.0000 - val_rmse: 17710.2324\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 296596480.0000 - rmse: 17221.9766 - val_loss: 327033600.0000 - val_rmse: 18084.0703\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 272466816.0000 - rmse: 16506.5684 - val_loss: 302087200.0000 - val_rmse: 17380.6562\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 263674544.0000 - rmse: 16238.0586 - val_loss: 287142912.0000 - val_rmse: 16945.2910\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299744512.0000 - rmse: 17313.1309 - val_loss: 367939008.0000 - val_rmse: 19181.7363\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260380432.0000 - rmse: 16136.3076 - val_loss: 289101376.0000 - val_rmse: 17002.9805\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 250387776.0000 - rmse: 15823.6465 - val_loss: 301938432.0000 - val_rmse: 17376.3750\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 253821824.0000 - rmse: 15931.7861 - val_loss: 298018688.0000 - val_rmse: 17263.2188\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 254265904.0000 - rmse: 15945.7178 - val_loss: 304723200.0000 - val_rmse: 17456.3223\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 254935408.0000 - rmse: 15966.6973 - val_loss: 292552960.0000 - val_rmse: 17104.1797\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 290811264.0000 - rmse: 17053.1895 - val_loss: 303658496.0000 - val_rmse: 17425.7988\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 292987296.0000 - rmse: 17116.8711 - val_loss: 287958560.0000 - val_rmse: 16969.3418\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 245146912.0000 - rmse: 15657.1680 - val_loss: 303633472.0000 - val_rmse: 17425.0820\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 292927360.0000 - rmse: 17115.1211 - val_loss: 274511104.0000 - val_rmse: 16568.3770\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264454944.0000 - rmse: 16262.0703 - val_loss: 332794752.0000 - val_rmse: 18242.6621\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 295624416.0000 - rmse: 17193.7324 - val_loss: 289439904.0000 - val_rmse: 17012.9336\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 288794912.0000 - rmse: 16993.9668 - val_loss: 296149952.0000 - val_rmse: 17209.0078\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 251953232.0000 - rmse: 15873.0352 - val_loss: 297171360.0000 - val_rmse: 17238.6582\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280561408.0000 - rmse: 16749.9668 - val_loss: 311271712.0000 - val_rmse: 17642.8945\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 279506336.0000 - rmse: 16718.4434 - val_loss: 298036672.0000 - val_rmse: 17263.7383\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 252354128.0000 - rmse: 15885.6582 - val_loss: 293496288.0000 - val_rmse: 17131.7324\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268608096.0000 - rmse: 16389.2676 - val_loss: 306465824.0000 - val_rmse: 17506.1660\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 281144064.0000 - rmse: 16767.3516 - val_loss: 283254752.0000 - val_rmse: 16830.1738\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 274949152.0000 - rmse: 16581.5898 - val_loss: 269712544.0000 - val_rmse: 16422.9277\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309511104.0000 - rmse: 17592.9277 - val_loss: 281209376.0000 - val_rmse: 16769.2988\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260184656.0000 - rmse: 16130.2402 - val_loss: 278162272.0000 - val_rmse: 16678.1973\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 246771536.0000 - rmse: 15708.9639 - val_loss: 297098688.0000 - val_rmse: 17236.5508\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 230513568.0000 - rmse: 15182.6729 - val_loss: 282942272.0000 - val_rmse: 16820.8887\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 327559712.0000 - rmse: 18098.6113 - val_loss: 289085952.0000 - val_rmse: 17002.5273\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 252022880.0000 - rmse: 15875.2285 - val_loss: 285228480.0000 - val_rmse: 16888.7090\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280867456.0000 - rmse: 16759.1016 - val_loss: 358966784.0000 - val_rmse: 18946.4180\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 297803200.0000 - rmse: 17256.9746 - val_loss: 300371648.0000 - val_rmse: 17331.2324\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 258871392.0000 - rmse: 16089.4805 - val_loss: 294004800.0000 - val_rmse: 17146.5684\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264184464.0000 - rmse: 16253.7520 - val_loss: 294340096.0000 - val_rmse: 17156.3418\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 271559776.0000 - rmse: 16479.0703 - val_loss: 306635968.0000 - val_rmse: 17511.0234\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264489488.0000 - rmse: 16263.1328 - val_loss: 366025024.0000 - val_rmse: 19131.7812\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 263077456.0000 - rmse: 16219.6631 - val_loss: 286301856.0000 - val_rmse: 16920.4570\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 265188192.0000 - rmse: 16284.5996 - val_loss: 349908096.0000 - val_rmse: 18705.8301\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 281340416.0000 - rmse: 16773.2051 - val_loss: 279015552.0000 - val_rmse: 16703.7578\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 246207376.0000 - rmse: 15690.9971 - val_loss: 285536160.0000 - val_rmse: 16897.8145\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 244460656.0000 - rmse: 15635.2373 - val_loss: 288307904.0000 - val_rmse: 16979.6328\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 244594576.0000 - rmse: 15639.5195 - val_loss: 335608224.0000 - val_rmse: 18319.6133\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 243207200.0000 - rmse: 15595.1016 - val_loss: 289680256.0000 - val_rmse: 17019.9961\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 237462192.0000 - rmse: 15409.8086 - val_loss: 307080928.0000 - val_rmse: 17523.7246\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 245291808.0000 - rmse: 15661.7949 - val_loss: 282848832.0000 - val_rmse: 16818.1094\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 296135680.0000 - rmse: 17208.5938 - val_loss: 305768096.0000 - val_rmse: 17486.2266\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 289306400.0000 - rmse: 17009.0098 - val_loss: 284701280.0000 - val_rmse: 16873.0938\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 253576016.0000 - rmse: 15924.0703 - val_loss: 304811776.0000 - val_rmse: 17458.8594\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 246930800.0000 - rmse: 15714.0322 - val_loss: 296959424.0000 - val_rmse: 17232.5098\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 232484928.0000 - rmse: 15247.4561 - val_loss: 290432000.0000 - val_rmse: 17042.0664\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 243096896.0000 - rmse: 15591.5645 - val_loss: 311795680.0000 - val_rmse: 17657.7363\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 247171696.0000 - rmse: 15721.6953 - val_loss: 294022048.0000 - val_rmse: 17147.0703\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 296851008.0000 - rmse: 17229.3652 - val_loss: 330059200.0000 - val_rmse: 18167.5312\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 230913520.0000 - rmse: 15195.8389 - val_loss: 297446720.0000 - val_rmse: 17246.6445\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 279488768.0000 - rmse: 16717.9180 - val_loss: 281476960.0000 - val_rmse: 16777.2754\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 261648208.0000 - rmse: 16175.5439 - val_loss: 287141984.0000 - val_rmse: 16945.2637\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 271165184.0000 - rmse: 16467.0938 - val_loss: 288799072.0000 - val_rmse: 16994.0898\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 225435152.0000 - rmse: 15014.4980 - val_loss: 285451008.0000 - val_rmse: 16895.2949\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 277231296.0000 - rmse: 16650.2637 - val_loss: 285769312.0000 - val_rmse: 16904.7129\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260329312.0000 - rmse: 16134.7236 - val_loss: 303851104.0000 - val_rmse: 17431.3262\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 234919824.0000 - rmse: 15327.0947 - val_loss: 291121984.0000 - val_rmse: 17062.2969\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264673104.0000 - rmse: 16268.7773 - val_loss: 294741728.0000 - val_rmse: 17168.0430\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 227107280.0000 - rmse: 15070.0791 - val_loss: 311839360.0000 - val_rmse: 17658.9746\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260247328.0000 - rmse: 16132.1826 - val_loss: 291545856.0000 - val_rmse: 17074.7148\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 246817792.0000 - rmse: 15710.4355 - val_loss: 295655104.0000 - val_rmse: 17194.6250\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 285128544.0000 - rmse: 16885.7500 - val_loss: 315686816.0000 - val_rmse: 17767.5781\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264250752.0000 - rmse: 16255.7910 - val_loss: 292485760.0000 - val_rmse: 17102.2148\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 226736368.0000 - rmse: 15057.7676 - val_loss: 282421952.0000 - val_rmse: 16805.4141\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 261260048.0000 - rmse: 16163.5410 - val_loss: 287286112.0000 - val_rmse: 16949.5156\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 227343568.0000 - rmse: 15077.9170 - val_loss: 281894368.0000 - val_rmse: 16789.7109\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 216753040.0000 - rmse: 14722.5352 - val_loss: 280140928.0000 - val_rmse: 16737.4102\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 215177360.0000 - rmse: 14668.9248 - val_loss: 302492384.0000 - val_rmse: 17392.3086\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 258958016.0000 - rmse: 16092.1729 - val_loss: 282405920.0000 - val_rmse: 16804.9375\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 244511264.0000 - rmse: 15636.8555 - val_loss: 290222816.0000 - val_rmse: 17035.9277\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 249963568.0000 - rmse: 15810.2363 - val_loss: 281404128.0000 - val_rmse: 16775.1035\n",
      "Epoch 367/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 225536768.0000 - rmse: 15017.8818 - val_loss: 284518912.0000 - val_rmse: 16867.6875\n",
      "Epoch 368/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 228560128.0000 - rmse: 15118.2051 - val_loss: 320386880.0000 - val_rmse: 17899.3535\n",
      "Epoch 369/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 238669248.0000 - rmse: 15448.9238 - val_loss: 293039616.0000 - val_rmse: 17118.4004\n",
      "Epoch 370/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 257486944.0000 - rmse: 16046.3994 - val_loss: 327801600.0000 - val_rmse: 18105.2930\n",
      "Epoch 371/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260200928.0000 - rmse: 16130.7451 - val_loss: 296458400.0000 - val_rmse: 17217.9668\n",
      "Epoch 372/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 228554560.0000 - rmse: 15118.0215 - val_loss: 288431584.0000 - val_rmse: 16983.2734\n",
      "Epoch 373/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 258778752.0000 - rmse: 16086.6016 - val_loss: 293352320.0000 - val_rmse: 17127.5312\n",
      "Epoch 374/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 240730720.0000 - rmse: 15515.4990 - val_loss: 294880832.0000 - val_rmse: 17172.0938\n",
      "Epoch 375/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 236148896.0000 - rmse: 15367.1367 - val_loss: 294931264.0000 - val_rmse: 17173.5625\n",
      "Epoch 376/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 235160720.0000 - rmse: 15334.9512 - val_loss: 297464224.0000 - val_rmse: 17247.1504\n",
      "Epoch 377/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 222521632.0000 - rmse: 14917.1592 - val_loss: 283530816.0000 - val_rmse: 16838.3730\n",
      "Epoch 378/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 228319600.0000 - rmse: 15110.2480 - val_loss: 298328128.0000 - val_rmse: 17272.1777\n",
      "Epoch 379/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 218636992.0000 - rmse: 14786.3789 - val_loss: 281250624.0000 - val_rmse: 16770.5293\n",
      "Epoch 380/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 231096624.0000 - rmse: 15201.8623 - val_loss: 317589536.0000 - val_rmse: 17821.0410\n",
      "Epoch 381/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 265361488.0000 - rmse: 16289.9199 - val_loss: 306749504.0000 - val_rmse: 17514.2656\n",
      "Epoch 382/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 237888464.0000 - rmse: 15423.6328 - val_loss: 296356576.0000 - val_rmse: 17215.0098\n",
      "Epoch 383/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 234304160.0000 - rmse: 15306.9971 - val_loss: 295759616.0000 - val_rmse: 17197.6641\n",
      "Epoch 384/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 214804336.0000 - rmse: 14656.2051 - val_loss: 335768064.0000 - val_rmse: 18323.9746\n",
      "Epoch 385/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 247622496.0000 - rmse: 15736.0254 - val_loss: 318606496.0000 - val_rmse: 17849.5508\n",
      "Epoch 386/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 244628608.0000 - rmse: 15640.6074 - val_loss: 298881888.0000 - val_rmse: 17288.2012\n",
      "Epoch 387/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 222775088.0000 - rmse: 14925.6523 - val_loss: 292364384.0000 - val_rmse: 17098.6660\n",
      "Epoch 388/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 266197632.0000 - rmse: 16315.5645 - val_loss: 289572480.0000 - val_rmse: 17016.8301\n",
      "Epoch 389/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 237577456.0000 - rmse: 15413.5479 - val_loss: 293500832.0000 - val_rmse: 17131.8652\n",
      "Epoch 390/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 206978096.0000 - rmse: 14386.7334 - val_loss: 287080992.0000 - val_rmse: 16943.4648\n",
      "Epoch 391/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 238805152.0000 - rmse: 15453.3213 - val_loss: 294633632.0000 - val_rmse: 17164.8945\n",
      "Epoch 392/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 236686160.0000 - rmse: 15384.6074 - val_loss: 294846880.0000 - val_rmse: 17171.1055\n",
      "Epoch 393/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 236817232.0000 - rmse: 15388.8672 - val_loss: 295857312.0000 - val_rmse: 17200.5039\n",
      "Epoch 394/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 187721008.0000 - rmse: 13701.1318 - val_loss: 287593184.0000 - val_rmse: 16958.5723\n",
      "Epoch 395/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 220821136.0000 - rmse: 14860.0518 - val_loss: 286607136.0000 - val_rmse: 16929.4746\n",
      "Epoch 396/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268720576.0000 - rmse: 16392.6992 - val_loss: 279087488.0000 - val_rmse: 16705.9121\n",
      "Epoch 397/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 205888832.0000 - rmse: 14348.8271 - val_loss: 299760640.0000 - val_rmse: 17313.5977\n",
      "Epoch 398/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 235486656.0000 - rmse: 15345.5742 - val_loss: 308793312.0000 - val_rmse: 17572.5156\n",
      "Epoch 399/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 228560064.0000 - rmse: 15118.2031 - val_loss: 291694976.0000 - val_rmse: 17079.0801\n",
      "Epoch 400/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 266450752.0000 - rmse: 16323.3193 - val_loss: 343250016.0000 - val_rmse: 18527.0078\n",
      "104/104 [==============================] - 0s 883us/step - loss: 576125824.0000 - rmse: 24002.6211\n",
      "[576125824.0, 24002.62109375]\n",
      "<src.model.emb_model object at 0x7f8467747450>\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 19702681600.0000 - rmse: 140366.2344 - val_loss: 3524039936.0000 - val_rmse: 59363.6250\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2724598784.0000 - rmse: 52197.6875 - val_loss: 1752733056.0000 - val_rmse: 41865.6562\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1841881088.0000 - rmse: 42917.1406 - val_loss: 1438911744.0000 - val_rmse: 37932.9883\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1731552768.0000 - rmse: 41611.9297 - val_loss: 1276355328.0000 - val_rmse: 35726.1172\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1518239232.0000 - rmse: 38964.5898 - val_loss: 1310935552.0000 - val_rmse: 36206.8438\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1512014592.0000 - rmse: 38884.6328 - val_loss: 1117136128.0000 - val_rmse: 33423.5859\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1410434688.0000 - rmse: 37555.7539 - val_loss: 1153401088.0000 - val_rmse: 33961.7578\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1360626688.0000 - rmse: 36886.6719 - val_loss: 1045568576.0000 - val_rmse: 32335.2520\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1291666944.0000 - rmse: 35939.7695 - val_loss: 1016595008.0000 - val_rmse: 31884.0879\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1279122688.0000 - rmse: 35764.8242 - val_loss: 974447232.0000 - val_rmse: 31216.1367\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1197921792.0000 - rmse: 34611.0078 - val_loss: 935613248.0000 - val_rmse: 30587.7949\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1167585536.0000 - rmse: 34169.9492 - val_loss: 899649152.0000 - val_rmse: 29994.1523\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1176693632.0000 - rmse: 34302.9688 - val_loss: 914435584.0000 - val_rmse: 30239.6367\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1199561984.0000 - rmse: 34634.6914 - val_loss: 842338112.0000 - val_rmse: 29023.0625\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1141520512.0000 - rmse: 33786.3945 - val_loss: 810302400.0000 - val_rmse: 28465.8105\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1073508608.0000 - rmse: 32764.4414 - val_loss: 823287552.0000 - val_rmse: 28692.9883\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1079857024.0000 - rmse: 32861.1797 - val_loss: 1074582400.0000 - val_rmse: 32780.8242\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1062192384.0000 - rmse: 32591.2930 - val_loss: 747248960.0000 - val_rmse: 27335.8555\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 964842624.0000 - rmse: 31061.9160 - val_loss: 756856064.0000 - val_rmse: 27511.0176\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1017541568.0000 - rmse: 31898.9277 - val_loss: 732382016.0000 - val_rmse: 27062.5566\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 944662592.0000 - rmse: 30735.3633 - val_loss: 719750592.0000 - val_rmse: 26828.1680\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1049326976.0000 - rmse: 32393.3164 - val_loss: 737102336.0000 - val_rmse: 27149.6289\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1001191360.0000 - rmse: 31641.6074 - val_loss: 703752896.0000 - val_rmse: 26528.3418\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 975875072.0000 - rmse: 31239.0000 - val_loss: 683572608.0000 - val_rmse: 26145.2207\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 904550080.0000 - rmse: 30075.7383 - val_loss: 971618816.0000 - val_rmse: 31170.8008\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 884890560.0000 - rmse: 29747.1094 - val_loss: 731615296.0000 - val_rmse: 27048.3887\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 907475392.0000 - rmse: 30124.3320 - val_loss: 679742336.0000 - val_rmse: 26071.8691\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 948197440.0000 - rmse: 30792.8145 - val_loss: 750915904.0000 - val_rmse: 27402.8457\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 925576320.0000 - rmse: 30423.2852 - val_loss: 742840192.0000 - val_rmse: 27255.0957\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 928290816.0000 - rmse: 30467.8652 - val_loss: 675312192.0000 - val_rmse: 25986.7695\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 878799936.0000 - rmse: 29644.5605 - val_loss: 656150592.0000 - val_rmse: 25615.4375\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 880287168.0000 - rmse: 29669.6328 - val_loss: 633715264.0000 - val_rmse: 25173.7012\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 825897088.0000 - rmse: 28738.4258 - val_loss: 696340416.0000 - val_rmse: 26388.2637\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 881534144.0000 - rmse: 29690.6406 - val_loss: 627847488.0000 - val_rmse: 25056.8848\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 823540160.0000 - rmse: 28697.3887 - val_loss: 718922560.0000 - val_rmse: 26812.7305\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 761776384.0000 - rmse: 27600.2969 - val_loss: 656085440.0000 - val_rmse: 25614.1641\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 813948352.0000 - rmse: 28529.7793 - val_loss: 771783616.0000 - val_rmse: 27780.9941\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 885164288.0000 - rmse: 29751.7109 - val_loss: 594010816.0000 - val_rmse: 24372.3379\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 803928704.0000 - rmse: 28353.6367 - val_loss: 654370752.0000 - val_rmse: 25580.6719\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 832174336.0000 - rmse: 28847.4316 - val_loss: 797914432.0000 - val_rmse: 28247.3789\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 827499392.0000 - rmse: 28766.2891 - val_loss: 603204864.0000 - val_rmse: 24560.2285\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 885626112.0000 - rmse: 29759.4707 - val_loss: 644301952.0000 - val_rmse: 25383.1035\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 790898560.0000 - rmse: 28122.9180 - val_loss: 655779456.0000 - val_rmse: 25608.1914\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 805729728.0000 - rmse: 28385.3789 - val_loss: 565967808.0000 - val_rmse: 23790.0781\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 743843712.0000 - rmse: 27273.4980 - val_loss: 567762816.0000 - val_rmse: 23827.7734\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 718059456.0000 - rmse: 26796.6309 - val_loss: 609614272.0000 - val_rmse: 24690.3672\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 786970688.0000 - rmse: 28052.9980 - val_loss: 574164224.0000 - val_rmse: 23961.7246\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 789885056.0000 - rmse: 28104.8945 - val_loss: 564569344.0000 - val_rmse: 23760.6680\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 728500032.0000 - rmse: 26990.7402 - val_loss: 558903296.0000 - val_rmse: 23641.1348\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 770153856.0000 - rmse: 27751.6465 - val_loss: 539787264.0000 - val_rmse: 23233.3223\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 713188096.0000 - rmse: 26705.5820 - val_loss: 547479424.0000 - val_rmse: 23398.2773\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 718874880.0000 - rmse: 26811.8418 - val_loss: 707252480.0000 - val_rmse: 26594.2188\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 742205888.0000 - rmse: 27243.4551 - val_loss: 535258144.0000 - val_rmse: 23135.6465\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 695510016.0000 - rmse: 26372.5234 - val_loss: 626197824.0000 - val_rmse: 25023.9453\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 752651072.0000 - rmse: 27434.4863 - val_loss: 612495232.0000 - val_rmse: 24748.6406\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 734208320.0000 - rmse: 27096.2793 - val_loss: 516875680.0000 - val_rmse: 22734.9004\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 630192832.0000 - rmse: 25103.6426 - val_loss: 493217408.0000 - val_rmse: 22208.4980\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 701142528.0000 - rmse: 26479.0957 - val_loss: 513154720.0000 - val_rmse: 22652.9180\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 721331904.0000 - rmse: 26857.6230 - val_loss: 505205984.0000 - val_rmse: 22476.7871\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 767136384.0000 - rmse: 27697.2266 - val_loss: 501249408.0000 - val_rmse: 22388.5996\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 654385920.0000 - rmse: 25580.9688 - val_loss: 500282624.0000 - val_rmse: 22366.9980\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 699405888.0000 - rmse: 26446.2832 - val_loss: 531116768.0000 - val_rmse: 23045.9707\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 686478400.0000 - rmse: 26200.7324 - val_loss: 480571328.0000 - val_rmse: 21921.9375\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 672985664.0000 - rmse: 25941.9668 - val_loss: 472664384.0000 - val_rmse: 21740.8457\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 660844672.0000 - rmse: 25706.8984 - val_loss: 481829568.0000 - val_rmse: 21950.6172\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 645516736.0000 - rmse: 25407.0215 - val_loss: 508076704.0000 - val_rmse: 22540.5566\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 602715840.0000 - rmse: 24550.2715 - val_loss: 478113632.0000 - val_rmse: 21865.8105\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 675999488.0000 - rmse: 25999.9902 - val_loss: 488224736.0000 - val_rmse: 22095.8086\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 628236736.0000 - rmse: 25064.6504 - val_loss: 878606464.0000 - val_rmse: 29641.2969\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 648953344.0000 - rmse: 25474.5625 - val_loss: 532793120.0000 - val_rmse: 23082.3125\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 703037632.0000 - rmse: 26514.8574 - val_loss: 494678848.0000 - val_rmse: 22241.3770\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 650547968.0000 - rmse: 25505.8418 - val_loss: 481308864.0000 - val_rmse: 21938.7520\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 596176384.0000 - rmse: 24416.7227 - val_loss: 434536032.0000 - val_rmse: 20845.5273\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 617712768.0000 - rmse: 24853.8281 - val_loss: 519115040.0000 - val_rmse: 22784.0957\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 674766336.0000 - rmse: 25976.2656 - val_loss: 472911392.0000 - val_rmse: 21746.5254\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 620302656.0000 - rmse: 24905.8750 - val_loss: 451616832.0000 - val_rmse: 21251.2793\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 703887872.0000 - rmse: 26530.8848 - val_loss: 431480544.0000 - val_rmse: 20772.1094\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 628871744.0000 - rmse: 25077.3145 - val_loss: 664650432.0000 - val_rmse: 25780.8145\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 650966912.0000 - rmse: 25514.0527 - val_loss: 426623104.0000 - val_rmse: 20654.8574\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 670251072.0000 - rmse: 25889.2070 - val_loss: 436620672.0000 - val_rmse: 20895.4707\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 677603200.0000 - rmse: 26030.8125 - val_loss: 445478688.0000 - val_rmse: 21106.3652\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 620013312.0000 - rmse: 24900.0664 - val_loss: 445929952.0000 - val_rmse: 21117.0527\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 659711680.0000 - rmse: 25684.8535 - val_loss: 433754336.0000 - val_rmse: 20826.7695\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 640928832.0000 - rmse: 25316.5723 - val_loss: 489787936.0000 - val_rmse: 22131.1523\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 633435200.0000 - rmse: 25168.1387 - val_loss: 407109824.0000 - val_rmse: 20176.9629\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 697162112.0000 - rmse: 26403.8281 - val_loss: 468886912.0000 - val_rmse: 21653.7969\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 593214592.0000 - rmse: 24355.9961 - val_loss: 456715904.0000 - val_rmse: 21370.9121\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 601585920.0000 - rmse: 24527.2480 - val_loss: 437158112.0000 - val_rmse: 20908.3262\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 582460288.0000 - rmse: 24134.2148 - val_loss: 453840928.0000 - val_rmse: 21303.5430\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 568633088.0000 - rmse: 23846.0293 - val_loss: 459067104.0000 - val_rmse: 21425.8516\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 656229568.0000 - rmse: 25616.9785 - val_loss: 546039296.0000 - val_rmse: 23367.4844\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 559136192.0000 - rmse: 23646.0605 - val_loss: 420814176.0000 - val_rmse: 20513.7559\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 587360320.0000 - rmse: 24235.5176 - val_loss: 478903328.0000 - val_rmse: 21883.8594\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 620622336.0000 - rmse: 24912.2930 - val_loss: 418762208.0000 - val_rmse: 20463.6797\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 613336832.0000 - rmse: 24765.6387 - val_loss: 620916864.0000 - val_rmse: 24918.2031\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 607891968.0000 - rmse: 24655.4648 - val_loss: 457703168.0000 - val_rmse: 21393.9980\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 599582656.0000 - rmse: 24486.3770 - val_loss: 411479072.0000 - val_rmse: 20284.9473\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 569189120.0000 - rmse: 23857.6855 - val_loss: 446928576.0000 - val_rmse: 21140.6855\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 580302528.0000 - rmse: 24089.4688 - val_loss: 408067136.0000 - val_rmse: 20200.6719\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 577086016.0000 - rmse: 24022.6152 - val_loss: 411085088.0000 - val_rmse: 20275.2324\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 645011200.0000 - rmse: 25397.0703 - val_loss: 414077536.0000 - val_rmse: 20348.8945\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 535730336.0000 - rmse: 23145.8496 - val_loss: 445120224.0000 - val_rmse: 21097.8730\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 607102784.0000 - rmse: 24639.4551 - val_loss: 391314400.0000 - val_rmse: 19781.6680\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 643289600.0000 - rmse: 25363.1543 - val_loss: 527826784.0000 - val_rmse: 22974.4805\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 570727232.0000 - rmse: 23889.8984 - val_loss: 393408800.0000 - val_rmse: 19834.5352\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 680767808.0000 - rmse: 26091.5273 - val_loss: 566655104.0000 - val_rmse: 23804.5195\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 504931424.0000 - rmse: 22470.6797 - val_loss: 406609248.0000 - val_rmse: 20164.5547\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 552788032.0000 - rmse: 23511.4453 - val_loss: 406704384.0000 - val_rmse: 20166.9141\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 599059264.0000 - rmse: 24475.6875 - val_loss: 413829344.0000 - val_rmse: 20342.7949\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 577067520.0000 - rmse: 24022.2305 - val_loss: 418895840.0000 - val_rmse: 20466.9453\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 562739712.0000 - rmse: 23722.1348 - val_loss: 494294272.0000 - val_rmse: 22232.7305\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 541910656.0000 - rmse: 23278.9746 - val_loss: 401388512.0000 - val_rmse: 20034.6836\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 558481792.0000 - rmse: 23632.2188 - val_loss: 456283488.0000 - val_rmse: 21360.7930\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 533204352.0000 - rmse: 23091.2188 - val_loss: 436548640.0000 - val_rmse: 20893.7461\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 568296128.0000 - rmse: 23838.9629 - val_loss: 396371776.0000 - val_rmse: 19909.0879\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 608803584.0000 - rmse: 24673.9453 - val_loss: 405290816.0000 - val_rmse: 20131.8359\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 564376576.0000 - rmse: 23756.6113 - val_loss: 419054752.0000 - val_rmse: 20470.8262\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 605252352.0000 - rmse: 24601.8770 - val_loss: 395137440.0000 - val_rmse: 19878.0645\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 586263616.0000 - rmse: 24212.8809 - val_loss: 405232064.0000 - val_rmse: 20130.3770\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 525902688.0000 - rmse: 22932.5684 - val_loss: 392402720.0000 - val_rmse: 19809.1582\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 542956160.0000 - rmse: 23301.4199 - val_loss: 387212352.0000 - val_rmse: 19677.7129\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 550143616.0000 - rmse: 23455.1406 - val_loss: 386654496.0000 - val_rmse: 19663.5312\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 509628832.0000 - rmse: 22574.9609 - val_loss: 404871680.0000 - val_rmse: 20121.4238\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 604367488.0000 - rmse: 24583.8867 - val_loss: 391626496.0000 - val_rmse: 19789.5547\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 502168832.0000 - rmse: 22409.1230 - val_loss: 420236416.0000 - val_rmse: 20499.6680\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 622254016.0000 - rmse: 24945.0195 - val_loss: 392757088.0000 - val_rmse: 19818.0996\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 568171392.0000 - rmse: 23836.3457 - val_loss: 444330368.0000 - val_rmse: 21079.1445\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 590339904.0000 - rmse: 24296.9121 - val_loss: 388824288.0000 - val_rmse: 19718.6289\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 609994048.0000 - rmse: 24698.0566 - val_loss: 384048896.0000 - val_rmse: 19597.1660\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 583754560.0000 - rmse: 24161.0137 - val_loss: 389877280.0000 - val_rmse: 19745.3105\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 547825280.0000 - rmse: 23405.6680 - val_loss: 417021920.0000 - val_rmse: 20421.1152\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 565975680.0000 - rmse: 23790.2441 - val_loss: 410331840.0000 - val_rmse: 20256.6484\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 517153184.0000 - rmse: 22741.0020 - val_loss: 377031648.0000 - val_rmse: 19417.3027\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 539612736.0000 - rmse: 23229.5664 - val_loss: 393936512.0000 - val_rmse: 19847.8340\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 574013504.0000 - rmse: 23958.5781 - val_loss: 417332992.0000 - val_rmse: 20428.7305\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 509843904.0000 - rmse: 22579.7227 - val_loss: 382100704.0000 - val_rmse: 19547.3965\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 510173280.0000 - rmse: 22587.0156 - val_loss: 378859456.0000 - val_rmse: 19464.3125\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 534877408.0000 - rmse: 23127.4160 - val_loss: 389824064.0000 - val_rmse: 19743.9629\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 468804000.0000 - rmse: 21651.8828 - val_loss: 431027776.0000 - val_rmse: 20761.2090\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 490791552.0000 - rmse: 22153.8164 - val_loss: 394649440.0000 - val_rmse: 19865.7852\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 481799136.0000 - rmse: 21949.9238 - val_loss: 388145504.0000 - val_rmse: 19701.4082\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 495107008.0000 - rmse: 22251.0000 - val_loss: 393723776.0000 - val_rmse: 19842.4746\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 501694816.0000 - rmse: 22398.5449 - val_loss: 392395456.0000 - val_rmse: 19808.9746\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 561446144.0000 - rmse: 23694.8555 - val_loss: 379043744.0000 - val_rmse: 19469.0449\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 617449152.0000 - rmse: 24848.5234 - val_loss: 362450816.0000 - val_rmse: 19038.1406\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 473367904.0000 - rmse: 21757.0195 - val_loss: 372533184.0000 - val_rmse: 19301.1191\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 512499808.0000 - rmse: 22638.4590 - val_loss: 386418112.0000 - val_rmse: 19657.5195\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 509597728.0000 - rmse: 22574.2715 - val_loss: 380783456.0000 - val_rmse: 19513.6738\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 444298528.0000 - rmse: 21078.3906 - val_loss: 407619392.0000 - val_rmse: 20189.5859\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 515911936.0000 - rmse: 22713.6953 - val_loss: 365516224.0000 - val_rmse: 19118.4785\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 523094080.0000 - rmse: 22871.2500 - val_loss: 425632608.0000 - val_rmse: 20630.8652\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 482778336.0000 - rmse: 21972.2168 - val_loss: 390651520.0000 - val_rmse: 19764.9062\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 492443872.0000 - rmse: 22191.0762 - val_loss: 395589280.0000 - val_rmse: 19889.4258\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 490498720.0000 - rmse: 22147.2051 - val_loss: 388520672.0000 - val_rmse: 19710.9277\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 515181952.0000 - rmse: 22697.6191 - val_loss: 412101568.0000 - val_rmse: 20300.2852\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 548750912.0000 - rmse: 23425.4336 - val_loss: 398419488.0000 - val_rmse: 19960.4473\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 529140544.0000 - rmse: 23003.0547 - val_loss: 416348608.0000 - val_rmse: 20404.6230\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 574548160.0000 - rmse: 23969.7344 - val_loss: 418327360.0000 - val_rmse: 20453.0527\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 479862656.0000 - rmse: 21905.7676 - val_loss: 399238528.0000 - val_rmse: 19980.9551\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 561688768.0000 - rmse: 23699.9746 - val_loss: 482594016.0000 - val_rmse: 21968.0234\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 520232832.0000 - rmse: 22808.6133 - val_loss: 411709728.0000 - val_rmse: 20290.6309\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 522875744.0000 - rmse: 22866.4766 - val_loss: 477678400.0000 - val_rmse: 21855.8555\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 500174496.0000 - rmse: 22364.5820 - val_loss: 416371072.0000 - val_rmse: 20405.1719\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 500621312.0000 - rmse: 22374.5684 - val_loss: 481792128.0000 - val_rmse: 21949.7637\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 468205440.0000 - rmse: 21638.0547 - val_loss: 398531040.0000 - val_rmse: 19963.2422\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 529686848.0000 - rmse: 23014.9258 - val_loss: 375037120.0000 - val_rmse: 19365.8750\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 512320480.0000 - rmse: 22634.4980 - val_loss: 355404160.0000 - val_rmse: 18852.1660\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 490191296.0000 - rmse: 22140.2637 - val_loss: 361333600.0000 - val_rmse: 19008.7773\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 482008352.0000 - rmse: 21954.6895 - val_loss: 358922784.0000 - val_rmse: 18945.2578\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 492824192.0000 - rmse: 22199.6445 - val_loss: 349379872.0000 - val_rmse: 18691.7051\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 510531168.0000 - rmse: 22594.9375 - val_loss: 372749984.0000 - val_rmse: 19306.7344\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 524245024.0000 - rmse: 22896.3984 - val_loss: 424495264.0000 - val_rmse: 20603.2832\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 510301632.0000 - rmse: 22589.8574 - val_loss: 411829952.0000 - val_rmse: 20293.5938\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 457446720.0000 - rmse: 21388.0039 - val_loss: 367825120.0000 - val_rmse: 19178.7676\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 455453376.0000 - rmse: 21341.3535 - val_loss: 389524864.0000 - val_rmse: 19736.3848\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 541674304.0000 - rmse: 23273.8984 - val_loss: 364885792.0000 - val_rmse: 19101.9844\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 536671136.0000 - rmse: 23166.1641 - val_loss: 351571328.0000 - val_rmse: 18750.2363\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 461574656.0000 - rmse: 21484.2891 - val_loss: 360422656.0000 - val_rmse: 18984.8008\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 481363200.0000 - rmse: 21939.9902 - val_loss: 361630592.0000 - val_rmse: 19016.5879\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 539187264.0000 - rmse: 23220.4062 - val_loss: 404981888.0000 - val_rmse: 20124.1621\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 440265184.0000 - rmse: 20982.4980 - val_loss: 353228032.0000 - val_rmse: 18794.3613\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 500324928.0000 - rmse: 22367.9434 - val_loss: 346529056.0000 - val_rmse: 18615.2910\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 496933920.0000 - rmse: 22292.0156 - val_loss: 354864960.0000 - val_rmse: 18837.8594\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 470149088.0000 - rmse: 21682.9219 - val_loss: 355024832.0000 - val_rmse: 18842.1035\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 446503680.0000 - rmse: 21130.6328 - val_loss: 368504160.0000 - val_rmse: 19196.4629\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 471974112.0000 - rmse: 21724.9648 - val_loss: 361599520.0000 - val_rmse: 19015.7695\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 521523392.0000 - rmse: 22836.8867 - val_loss: 429340736.0000 - val_rmse: 20720.5391\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 469841696.0000 - rmse: 21675.8320 - val_loss: 357935328.0000 - val_rmse: 18919.1797\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 447154944.0000 - rmse: 21146.0391 - val_loss: 338838464.0000 - val_rmse: 18407.5645\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 491343008.0000 - rmse: 22166.2578 - val_loss: 359144864.0000 - val_rmse: 18951.1172\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 476111872.0000 - rmse: 21819.9883 - val_loss: 343927456.0000 - val_rmse: 18545.2812\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 490613376.0000 - rmse: 22149.7949 - val_loss: 357640928.0000 - val_rmse: 18911.3965\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 437502400.0000 - rmse: 20916.5586 - val_loss: 368382368.0000 - val_rmse: 19193.2891\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 507762208.0000 - rmse: 22533.5801 - val_loss: 379911488.0000 - val_rmse: 19491.3184\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 557815808.0000 - rmse: 23618.1250 - val_loss: 388072928.0000 - val_rmse: 19699.5664\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 456021376.0000 - rmse: 21354.6562 - val_loss: 358944800.0000 - val_rmse: 18945.8379\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 463329696.0000 - rmse: 21525.0938 - val_loss: 337673408.0000 - val_rmse: 18375.8926\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 470262368.0000 - rmse: 21685.5332 - val_loss: 339617824.0000 - val_rmse: 18428.7227\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 500230080.0000 - rmse: 22365.8242 - val_loss: 351896032.0000 - val_rmse: 18758.8926\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 480146208.0000 - rmse: 21912.2383 - val_loss: 455357760.0000 - val_rmse: 21339.1133\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 501613152.0000 - rmse: 22396.7227 - val_loss: 338241056.0000 - val_rmse: 18391.3301\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 486278304.0000 - rmse: 22051.7188 - val_loss: 365102624.0000 - val_rmse: 19107.6582\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 473713504.0000 - rmse: 21764.9609 - val_loss: 354946816.0000 - val_rmse: 18840.0332\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 444851488.0000 - rmse: 21091.5020 - val_loss: 395143936.0000 - val_rmse: 19878.2285\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 413945760.0000 - rmse: 20345.6562 - val_loss: 351245312.0000 - val_rmse: 18741.5391\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 440754400.0000 - rmse: 20994.1523 - val_loss: 428881792.0000 - val_rmse: 20709.4609\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 468196832.0000 - rmse: 21637.8574 - val_loss: 367860736.0000 - val_rmse: 19179.6953\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 485090752.0000 - rmse: 22024.7754 - val_loss: 438496224.0000 - val_rmse: 20940.3008\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 450084224.0000 - rmse: 21215.1895 - val_loss: 398741664.0000 - val_rmse: 19968.5176\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 444284704.0000 - rmse: 21078.0625 - val_loss: 392736256.0000 - val_rmse: 19817.5742\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 456286112.0000 - rmse: 21360.8555 - val_loss: 403244384.0000 - val_rmse: 20080.9453\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 500035872.0000 - rmse: 22361.4824 - val_loss: 385330880.0000 - val_rmse: 19629.8457\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 493401024.0000 - rmse: 22212.6328 - val_loss: 369043424.0000 - val_rmse: 19210.5039\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 469101408.0000 - rmse: 21658.7480 - val_loss: 423550016.0000 - val_rmse: 20580.3301\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 491749344.0000 - rmse: 22175.4219 - val_loss: 340030304.0000 - val_rmse: 18439.9102\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 499948032.0000 - rmse: 22359.5176 - val_loss: 389293920.0000 - val_rmse: 19730.5332\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 466067232.0000 - rmse: 21588.5898 - val_loss: 407027872.0000 - val_rmse: 20174.9316\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 428966848.0000 - rmse: 20711.5156 - val_loss: 338171520.0000 - val_rmse: 18389.4414\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 461940544.0000 - rmse: 21492.8027 - val_loss: 333162816.0000 - val_rmse: 18252.7480\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 450070496.0000 - rmse: 21214.8652 - val_loss: 335876576.0000 - val_rmse: 18326.9355\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 448285216.0000 - rmse: 21172.7461 - val_loss: 407851040.0000 - val_rmse: 20195.3223\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 443189440.0000 - rmse: 21052.0645 - val_loss: 362646048.0000 - val_rmse: 19043.2676\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 505457920.0000 - rmse: 22482.3906 - val_loss: 354510464.0000 - val_rmse: 18828.4492\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 457761664.0000 - rmse: 21395.3652 - val_loss: 339126816.0000 - val_rmse: 18415.3965\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 416354592.0000 - rmse: 20404.7695 - val_loss: 351130880.0000 - val_rmse: 18738.4863\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 455737984.0000 - rmse: 21348.0215 - val_loss: 377707040.0000 - val_rmse: 19434.6855\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 436253792.0000 - rmse: 20886.6895 - val_loss: 341647936.0000 - val_rmse: 18483.7207\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 416645440.0000 - rmse: 20411.8945 - val_loss: 335227552.0000 - val_rmse: 18309.2207\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 426393760.0000 - rmse: 20649.3047 - val_loss: 344579008.0000 - val_rmse: 18562.8398\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 420886848.0000 - rmse: 20515.5273 - val_loss: 327811552.0000 - val_rmse: 18105.5664\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 412512704.0000 - rmse: 20310.4082 - val_loss: 354190528.0000 - val_rmse: 18819.9512\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 458050304.0000 - rmse: 21402.1094 - val_loss: 327451040.0000 - val_rmse: 18095.6074\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 433093056.0000 - rmse: 20810.8887 - val_loss: 336079264.0000 - val_rmse: 18332.4648\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 446866464.0000 - rmse: 21139.2168 - val_loss: 341931328.0000 - val_rmse: 18491.3848\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 423045792.0000 - rmse: 20568.0762 - val_loss: 348622592.0000 - val_rmse: 18671.4375\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 394140992.0000 - rmse: 19852.9844 - val_loss: 353221184.0000 - val_rmse: 18794.1797\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 452552608.0000 - rmse: 21273.2832 - val_loss: 347235168.0000 - val_rmse: 18634.2480\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 428346752.0000 - rmse: 20696.5391 - val_loss: 343811392.0000 - val_rmse: 18542.1523\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 420818464.0000 - rmse: 20513.8594 - val_loss: 337788736.0000 - val_rmse: 18379.0293\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 438977056.0000 - rmse: 20951.7793 - val_loss: 315469024.0000 - val_rmse: 17761.4473\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 464534912.0000 - rmse: 21553.0723 - val_loss: 365022720.0000 - val_rmse: 19105.5684\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 443012160.0000 - rmse: 21047.8535 - val_loss: 334116096.0000 - val_rmse: 18278.8438\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 391051904.0000 - rmse: 19775.0332 - val_loss: 326258944.0000 - val_rmse: 18062.6387\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 426457024.0000 - rmse: 20650.8359 - val_loss: 382606208.0000 - val_rmse: 19560.3223\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 384223136.0000 - rmse: 19601.6113 - val_loss: 338970912.0000 - val_rmse: 18411.1621\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 411272416.0000 - rmse: 20279.8516 - val_loss: 331975648.0000 - val_rmse: 18220.1992\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 434648576.0000 - rmse: 20848.2266 - val_loss: 338838208.0000 - val_rmse: 18407.5586\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 410460320.0000 - rmse: 20259.8203 - val_loss: 323058880.0000 - val_rmse: 17973.8379\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 416630720.0000 - rmse: 20411.5332 - val_loss: 326921152.0000 - val_rmse: 18080.9609\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 423883840.0000 - rmse: 20588.4395 - val_loss: 330611456.0000 - val_rmse: 18182.7246\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 424964800.0000 - rmse: 20614.6738 - val_loss: 321765728.0000 - val_rmse: 17937.8301\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 427533600.0000 - rmse: 20676.8848 - val_loss: 334605280.0000 - val_rmse: 18292.2188\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 374272256.0000 - rmse: 19346.1172 - val_loss: 316588832.0000 - val_rmse: 17792.9434\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 373847808.0000 - rmse: 19335.1445 - val_loss: 445833248.0000 - val_rmse: 21114.7637\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 419470688.0000 - rmse: 20480.9844 - val_loss: 346068448.0000 - val_rmse: 18602.9160\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 367322976.0000 - rmse: 19165.6719 - val_loss: 303390496.0000 - val_rmse: 17418.1074\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 447752864.0000 - rmse: 21160.1719 - val_loss: 334861248.0000 - val_rmse: 18299.2148\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 404766208.0000 - rmse: 20118.8027 - val_loss: 314769248.0000 - val_rmse: 17741.7383\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 476486080.0000 - rmse: 21828.5605 - val_loss: 308978720.0000 - val_rmse: 17577.7910\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 417952960.0000 - rmse: 20443.8984 - val_loss: 296979392.0000 - val_rmse: 17233.0898\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 399090944.0000 - rmse: 19977.2598 - val_loss: 310774432.0000 - val_rmse: 17628.7949\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 387001056.0000 - rmse: 19672.3418 - val_loss: 304325536.0000 - val_rmse: 17444.9277\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 423432352.0000 - rmse: 20577.4727 - val_loss: 294379008.0000 - val_rmse: 17157.4766\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 466702304.0000 - rmse: 21603.2930 - val_loss: 307178208.0000 - val_rmse: 17526.5000\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 364103232.0000 - rmse: 19081.4883 - val_loss: 296139520.0000 - val_rmse: 17208.7051\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 379566624.0000 - rmse: 19482.4688 - val_loss: 318325024.0000 - val_rmse: 17841.6660\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 423034624.0000 - rmse: 20567.8047 - val_loss: 292635264.0000 - val_rmse: 17106.5859\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 446247072.0000 - rmse: 21124.5605 - val_loss: 339535040.0000 - val_rmse: 18426.4766\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 377973536.0000 - rmse: 19441.5410 - val_loss: 289438976.0000 - val_rmse: 17012.9062\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 380764544.0000 - rmse: 19513.1895 - val_loss: 290619712.0000 - val_rmse: 17047.5723\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 349750592.0000 - rmse: 18701.6191 - val_loss: 302962784.0000 - val_rmse: 17405.8262\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 390436832.0000 - rmse: 19759.4746 - val_loss: 281402592.0000 - val_rmse: 16775.0586\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 391864448.0000 - rmse: 19795.5664 - val_loss: 352484160.0000 - val_rmse: 18774.5605\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 471604128.0000 - rmse: 21716.4492 - val_loss: 343734336.0000 - val_rmse: 18540.0742\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 377865312.0000 - rmse: 19438.7578 - val_loss: 314406496.0000 - val_rmse: 17731.5117\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 391185632.0000 - rmse: 19778.4141 - val_loss: 440321760.0000 - val_rmse: 20983.8457\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 401073344.0000 - rmse: 20026.8164 - val_loss: 320566752.0000 - val_rmse: 17904.3789\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 384401728.0000 - rmse: 19606.1660 - val_loss: 382476736.0000 - val_rmse: 19557.0117\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 386854176.0000 - rmse: 19668.6094 - val_loss: 323583360.0000 - val_rmse: 17988.4238\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 411180192.0000 - rmse: 20277.5781 - val_loss: 338840384.0000 - val_rmse: 18407.6172\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 369401088.0000 - rmse: 19219.8105 - val_loss: 290264960.0000 - val_rmse: 17037.1641\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 408949120.0000 - rmse: 20222.4902 - val_loss: 317641824.0000 - val_rmse: 17822.5098\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 407034016.0000 - rmse: 20175.0840 - val_loss: 342085984.0000 - val_rmse: 18495.5664\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 381994304.0000 - rmse: 19544.6738 - val_loss: 353849440.0000 - val_rmse: 18810.8867\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 377744224.0000 - rmse: 19435.6426 - val_loss: 295422176.0000 - val_rmse: 17187.8496\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 364053408.0000 - rmse: 19080.1836 - val_loss: 285320448.0000 - val_rmse: 16891.4316\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 367046048.0000 - rmse: 19158.4453 - val_loss: 330016736.0000 - val_rmse: 18166.3633\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 413937760.0000 - rmse: 20345.4609 - val_loss: 383984704.0000 - val_rmse: 19595.5273\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 441348800.0000 - rmse: 21008.3027 - val_loss: 439156352.0000 - val_rmse: 20956.0586\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 374431680.0000 - rmse: 19350.2363 - val_loss: 308850944.0000 - val_rmse: 17574.1562\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 385912608.0000 - rmse: 19644.6582 - val_loss: 324294848.0000 - val_rmse: 18008.1875\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 369003744.0000 - rmse: 19209.4707 - val_loss: 396459808.0000 - val_rmse: 19911.2988\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 444086592.0000 - rmse: 21073.3613 - val_loss: 389808608.0000 - val_rmse: 19743.5723\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 363879232.0000 - rmse: 19075.6191 - val_loss: 386881504.0000 - val_rmse: 19669.3027\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 360812192.0000 - rmse: 18995.0566 - val_loss: 314275680.0000 - val_rmse: 17727.8223\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 357707872.0000 - rmse: 18913.1660 - val_loss: 305547744.0000 - val_rmse: 17479.9238\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 366918688.0000 - rmse: 19155.1211 - val_loss: 431809952.0000 - val_rmse: 20780.0371\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 416953280.0000 - rmse: 20419.4336 - val_loss: 406740160.0000 - val_rmse: 20167.8008\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 362817632.0000 - rmse: 19047.7715 - val_loss: 341377248.0000 - val_rmse: 18476.3965\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 378211776.0000 - rmse: 19447.6680 - val_loss: 316939808.0000 - val_rmse: 17802.8027\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 362194144.0000 - rmse: 19031.3984 - val_loss: 349143648.0000 - val_rmse: 18685.3867\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 359402400.0000 - rmse: 18957.9121 - val_loss: 483751200.0000 - val_rmse: 21994.3457\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 364088448.0000 - rmse: 19081.1016 - val_loss: 429848576.0000 - val_rmse: 20732.7891\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 356256224.0000 - rmse: 18874.7500 - val_loss: 374227072.0000 - val_rmse: 19344.9492\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 367651040.0000 - rmse: 19174.2285 - val_loss: 333069888.0000 - val_rmse: 18250.2031\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 380022080.0000 - rmse: 19494.1543 - val_loss: 342634368.0000 - val_rmse: 18510.3848\n",
      "104/104 [==============================] - 0s 872us/step - loss: 1060295744.0000 - rmse: 32562.1836\n",
      "[1060295744.0, 32562.18359375]\n",
      "<src.model.emb_model object at 0x7f8469604b50>\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 19793625088.0000 - rmse: 140689.8125 - val_loss: 2321650432.0000 - val_rmse: 48183.5078\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2351005696.0000 - rmse: 48487.1719 - val_loss: 1336475520.0000 - val_rmse: 36557.8398\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1914397184.0000 - rmse: 43753.8242 - val_loss: 1290578816.0000 - val_rmse: 35924.6250\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1871391488.0000 - rmse: 43259.5820 - val_loss: 1115189504.0000 - val_rmse: 33394.4531\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1779947904.0000 - rmse: 42189.4297 - val_loss: 1063829248.0000 - val_rmse: 32616.3945\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1650932736.0000 - rmse: 40631.6719 - val_loss: 1075968384.0000 - val_rmse: 32801.9570\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1650567040.0000 - rmse: 40627.1719 - val_loss: 976356160.0000 - val_rmse: 31246.6992\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1624233984.0000 - rmse: 40301.7852 - val_loss: 942697600.0000 - val_rmse: 30703.3809\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1593886976.0000 - rmse: 39923.5156 - val_loss: 924922624.0000 - val_rmse: 30412.5410\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1510804608.0000 - rmse: 38869.0703 - val_loss: 899693312.0000 - val_rmse: 29994.8887\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1535778176.0000 - rmse: 39189.0039 - val_loss: 885529920.0000 - val_rmse: 29757.8555\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1439748096.0000 - rmse: 37944.0117 - val_loss: 859400768.0000 - val_rmse: 29315.5371\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1445089536.0000 - rmse: 38014.3320 - val_loss: 982792384.0000 - val_rmse: 31349.5195\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1422622336.0000 - rmse: 37717.6680 - val_loss: 830977920.0000 - val_rmse: 28826.6875\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1466271104.0000 - rmse: 38291.9180 - val_loss: 827657280.0000 - val_rmse: 28769.0332\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1343443328.0000 - rmse: 36653.0117 - val_loss: 899085184.0000 - val_rmse: 29984.7500\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1444050176.0000 - rmse: 38000.6602 - val_loss: 791144896.0000 - val_rmse: 28127.2988\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1284896000.0000 - rmse: 35845.4453 - val_loss: 800544832.0000 - val_rmse: 28293.9004\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1284115584.0000 - rmse: 35834.5586 - val_loss: 768072064.0000 - val_rmse: 27714.1133\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1249441280.0000 - rmse: 35347.4375 - val_loss: 764910720.0000 - val_rmse: 27657.0195\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1196288384.0000 - rmse: 34587.4023 - val_loss: 755735424.0000 - val_rmse: 27490.6426\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1222547200.0000 - rmse: 34964.9414 - val_loss: 724336384.0000 - val_rmse: 26913.4980\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1273992960.0000 - rmse: 35693.0391 - val_loss: 759579648.0000 - val_rmse: 27560.4727\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1233924992.0000 - rmse: 35127.2695 - val_loss: 722257152.0000 - val_rmse: 26874.8418\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1102367872.0000 - rmse: 33201.9258 - val_loss: 784095936.0000 - val_rmse: 28001.7129\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1164108160.0000 - rmse: 34119.0273 - val_loss: 710615104.0000 - val_rmse: 26657.3652\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1111363456.0000 - rmse: 33337.1172 - val_loss: 729945024.0000 - val_rmse: 27017.4941\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1132359040.0000 - rmse: 33650.5430 - val_loss: 702881408.0000 - val_rmse: 26511.9102\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1061699776.0000 - rmse: 32583.7344 - val_loss: 727440256.0000 - val_rmse: 26971.0996\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1099759104.0000 - rmse: 33162.6172 - val_loss: 696214016.0000 - val_rmse: 26385.8672\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1105783296.0000 - rmse: 33253.3203 - val_loss: 780753280.0000 - val_rmse: 27941.9629\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1095190912.0000 - rmse: 33093.6680 - val_loss: 670875200.0000 - val_rmse: 25901.2578\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1032408128.0000 - rmse: 32131.1094 - val_loss: 654334016.0000 - val_rmse: 25579.9531\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1021735296.0000 - rmse: 31964.5938 - val_loss: 662826176.0000 - val_rmse: 25745.4102\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1042601216.0000 - rmse: 32289.3359 - val_loss: 654329216.0000 - val_rmse: 25579.8594\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1015691712.0000 - rmse: 31869.9180 - val_loss: 690160768.0000 - val_rmse: 26270.9102\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1016068672.0000 - rmse: 31875.8320 - val_loss: 684763840.0000 - val_rmse: 26167.9922\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 990901248.0000 - rmse: 31478.5840 - val_loss: 689866176.0000 - val_rmse: 26265.3027\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 994575936.0000 - rmse: 31536.8984 - val_loss: 623914688.0000 - val_rmse: 24978.2852\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1006785984.0000 - rmse: 31729.8906 - val_loss: 643660864.0000 - val_rmse: 25370.4727\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 996843776.0000 - rmse: 31572.8320 - val_loss: 621881344.0000 - val_rmse: 24937.5488\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 994100352.0000 - rmse: 31529.3574 - val_loss: 622869696.0000 - val_rmse: 24957.3574\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1001472640.0000 - rmse: 31646.0527 - val_loss: 632635456.0000 - val_rmse: 25152.2461\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 934134464.0000 - rmse: 30563.6133 - val_loss: 633908032.0000 - val_rmse: 25177.5312\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 899611648.0000 - rmse: 29993.5273 - val_loss: 643351360.0000 - val_rmse: 25364.3711\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 945953408.0000 - rmse: 30756.3555 - val_loss: 624388672.0000 - val_rmse: 24987.7695\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 959563584.0000 - rmse: 30976.8242 - val_loss: 597957632.0000 - val_rmse: 24453.1719\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 881161984.0000 - rmse: 29684.3730 - val_loss: 589343488.0000 - val_rmse: 24276.3984\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 943654208.0000 - rmse: 30718.9551 - val_loss: 588711040.0000 - val_rmse: 24263.3691\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 959051328.0000 - rmse: 30968.5547 - val_loss: 648084416.0000 - val_rmse: 25457.5020\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 857764672.0000 - rmse: 29287.6191 - val_loss: 582686656.0000 - val_rmse: 24138.9043\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 888760128.0000 - rmse: 29812.0801 - val_loss: 624410688.0000 - val_rmse: 24988.2109\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 946876672.0000 - rmse: 30771.3613 - val_loss: 555362176.0000 - val_rmse: 23566.1230\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 865453376.0000 - rmse: 29418.5898 - val_loss: 580485760.0000 - val_rmse: 24093.2715\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 877809024.0000 - rmse: 29627.8418 - val_loss: 553913600.0000 - val_rmse: 23535.3691\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 878737280.0000 - rmse: 29643.5039 - val_loss: 548395200.0000 - val_rmse: 23417.8398\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 868066752.0000 - rmse: 29462.9727 - val_loss: 522717152.0000 - val_rmse: 22863.0078\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 821685248.0000 - rmse: 28665.0527 - val_loss: 517397664.0000 - val_rmse: 22746.3770\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 905960960.0000 - rmse: 30099.1855 - val_loss: 516689824.0000 - val_rmse: 22730.8125\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 869419456.0000 - rmse: 29485.9199 - val_loss: 613557504.0000 - val_rmse: 24770.0938\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 819202496.0000 - rmse: 28621.7129 - val_loss: 608381952.0000 - val_rmse: 24665.4004\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 872616640.0000 - rmse: 29540.0859 - val_loss: 500856512.0000 - val_rmse: 22379.8242\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 879038784.0000 - rmse: 29648.5879 - val_loss: 584572160.0000 - val_rmse: 24177.9277\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 814259136.0000 - rmse: 28535.2266 - val_loss: 516791904.0000 - val_rmse: 22733.0566\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 915318464.0000 - rmse: 30254.2305 - val_loss: 497863136.0000 - val_rmse: 22312.8477\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 822748288.0000 - rmse: 28683.5898 - val_loss: 524875328.0000 - val_rmse: 22910.1582\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 781082560.0000 - rmse: 27947.8535 - val_loss: 530403200.0000 - val_rmse: 23030.4844\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 738787072.0000 - rmse: 27180.6387 - val_loss: 584243776.0000 - val_rmse: 24171.1348\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 783172224.0000 - rmse: 27985.2148 - val_loss: 543418624.0000 - val_rmse: 23311.3418\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 746625024.0000 - rmse: 27324.4395 - val_loss: 510157248.0000 - val_rmse: 22586.6602\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 754303232.0000 - rmse: 27464.5820 - val_loss: 483927456.0000 - val_rmse: 21998.3516\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 742188672.0000 - rmse: 27243.1406 - val_loss: 457264064.0000 - val_rmse: 21383.7344\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 743289280.0000 - rmse: 27263.3320 - val_loss: 465362880.0000 - val_rmse: 21572.2715\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 774598784.0000 - rmse: 27831.6152 - val_loss: 463658528.0000 - val_rmse: 21532.7324\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 801259264.0000 - rmse: 28306.5234 - val_loss: 475642528.0000 - val_rmse: 21809.2305\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 762382720.0000 - rmse: 27611.2793 - val_loss: 462176608.0000 - val_rmse: 21498.2930\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 776792256.0000 - rmse: 27870.9922 - val_loss: 463050176.0000 - val_rmse: 21518.6016\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 695776192.0000 - rmse: 26377.5703 - val_loss: 481167840.0000 - val_rmse: 21935.5391\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 697239104.0000 - rmse: 26405.2852 - val_loss: 512608512.0000 - val_rmse: 22640.8594\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 704898560.0000 - rmse: 26549.9258 - val_loss: 498220032.0000 - val_rmse: 22320.8438\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 725841920.0000 - rmse: 26941.4531 - val_loss: 473962816.0000 - val_rmse: 21770.6875\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 755841472.0000 - rmse: 27492.5703 - val_loss: 515332928.0000 - val_rmse: 22700.9453\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 683825792.0000 - rmse: 26150.0625 - val_loss: 455650176.0000 - val_rmse: 21345.9648\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 646751680.0000 - rmse: 25431.3125 - val_loss: 474120096.0000 - val_rmse: 21774.2988\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 680693184.0000 - rmse: 26090.0977 - val_loss: 426207136.0000 - val_rmse: 20644.7852\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 651679232.0000 - rmse: 25528.0078 - val_loss: 473346176.0000 - val_rmse: 21756.5195\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 677350592.0000 - rmse: 26025.9609 - val_loss: 481826144.0000 - val_rmse: 21950.5391\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 709383488.0000 - rmse: 26634.2539 - val_loss: 474510848.0000 - val_rmse: 21783.2695\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 663388992.0000 - rmse: 25756.3398 - val_loss: 476933728.0000 - val_rmse: 21838.8125\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 708003328.0000 - rmse: 26608.3320 - val_loss: 444420128.0000 - val_rmse: 21081.2734\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 676243904.0000 - rmse: 26004.6895 - val_loss: 438530272.0000 - val_rmse: 20941.1152\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 654781824.0000 - rmse: 25588.7051 - val_loss: 446772160.0000 - val_rmse: 21136.9863\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 693929280.0000 - rmse: 26342.5371 - val_loss: 476653120.0000 - val_rmse: 21832.3867\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 692197632.0000 - rmse: 26309.6484 - val_loss: 444571776.0000 - val_rmse: 21084.8711\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 690941120.0000 - rmse: 26285.7598 - val_loss: 469643808.0000 - val_rmse: 21671.2676\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 616023872.0000 - rmse: 24819.8281 - val_loss: 471595488.0000 - val_rmse: 21716.2500\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 659111680.0000 - rmse: 25673.1699 - val_loss: 437808928.0000 - val_rmse: 20923.8848\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 624745728.0000 - rmse: 24994.9141 - val_loss: 429577312.0000 - val_rmse: 20726.2461\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 709962624.0000 - rmse: 26645.1230 - val_loss: 485359904.0000 - val_rmse: 22030.8848\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 640724160.0000 - rmse: 25312.5293 - val_loss: 439190912.0000 - val_rmse: 20956.8828\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 628078848.0000 - rmse: 25061.5020 - val_loss: 481654016.0000 - val_rmse: 21946.6172\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 620359488.0000 - rmse: 24907.0176 - val_loss: 432696256.0000 - val_rmse: 20801.3516\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 666245120.0000 - rmse: 25811.7246 - val_loss: 442591296.0000 - val_rmse: 21037.8535\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 612616512.0000 - rmse: 24751.0918 - val_loss: 445211840.0000 - val_rmse: 21100.0430\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 608428352.0000 - rmse: 24666.3398 - val_loss: 420251680.0000 - val_rmse: 20500.0410\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 629353152.0000 - rmse: 25086.9121 - val_loss: 406743168.0000 - val_rmse: 20167.8750\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 622381376.0000 - rmse: 24947.5723 - val_loss: 411668448.0000 - val_rmse: 20289.6152\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 611736256.0000 - rmse: 24733.3027 - val_loss: 476732256.0000 - val_rmse: 21834.1992\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 590601600.0000 - rmse: 24302.2969 - val_loss: 467433856.0000 - val_rmse: 21620.2188\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 597937024.0000 - rmse: 24452.7500 - val_loss: 412308960.0000 - val_rmse: 20305.3926\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 614478976.0000 - rmse: 24788.6855 - val_loss: 602498432.0000 - val_rmse: 24545.8438\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 615402944.0000 - rmse: 24807.3164 - val_loss: 403529824.0000 - val_rmse: 20088.0527\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 592496256.0000 - rmse: 24341.2461 - val_loss: 491684800.0000 - val_rmse: 22173.9668\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 583265216.0000 - rmse: 24150.8848 - val_loss: 506435936.0000 - val_rmse: 22504.1309\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 564279552.0000 - rmse: 23754.5684 - val_loss: 405379392.0000 - val_rmse: 20134.0352\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 624977792.0000 - rmse: 24999.5566 - val_loss: 459871072.0000 - val_rmse: 21444.6055\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 636600448.0000 - rmse: 25230.9414 - val_loss: 395846144.0000 - val_rmse: 19895.8828\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 633072256.0000 - rmse: 25160.9277 - val_loss: 465482720.0000 - val_rmse: 21575.0488\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 578369536.0000 - rmse: 24049.3145 - val_loss: 428830560.0000 - val_rmse: 20708.2246\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 618472064.0000 - rmse: 24869.0977 - val_loss: 410916896.0000 - val_rmse: 20271.0859\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 605617344.0000 - rmse: 24609.2930 - val_loss: 427270368.0000 - val_rmse: 20670.5195\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 578715264.0000 - rmse: 24056.5020 - val_loss: 395261312.0000 - val_rmse: 19881.1797\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 520705248.0000 - rmse: 22818.9668 - val_loss: 435074240.0000 - val_rmse: 20858.4336\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 555268480.0000 - rmse: 23564.1348 - val_loss: 443400960.0000 - val_rmse: 21057.0879\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 578812864.0000 - rmse: 24058.5293 - val_loss: 401097632.0000 - val_rmse: 20027.4219\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 554042624.0000 - rmse: 23538.1094 - val_loss: 412626080.0000 - val_rmse: 20313.1992\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 532767040.0000 - rmse: 23081.7461 - val_loss: 390261952.0000 - val_rmse: 19755.0488\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 579556160.0000 - rmse: 24073.9727 - val_loss: 406352384.0000 - val_rmse: 20158.1836\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 587427840.0000 - rmse: 24236.9102 - val_loss: 382455040.0000 - val_rmse: 19556.4570\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 527593536.0000 - rmse: 22969.4043 - val_loss: 410969568.0000 - val_rmse: 20272.3848\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 540240000.0000 - rmse: 23243.0645 - val_loss: 405438176.0000 - val_rmse: 20135.4961\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 609731456.0000 - rmse: 24692.7402 - val_loss: 432364064.0000 - val_rmse: 20793.3652\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 535356416.0000 - rmse: 23137.7695 - val_loss: 402947424.0000 - val_rmse: 20073.5508\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 615435264.0000 - rmse: 24807.9668 - val_loss: 407452672.0000 - val_rmse: 20185.4570\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 579250816.0000 - rmse: 24067.6309 - val_loss: 429479712.0000 - val_rmse: 20723.8926\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 548424896.0000 - rmse: 23418.4727 - val_loss: 390314080.0000 - val_rmse: 19756.3672\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 541175680.0000 - rmse: 23263.1836 - val_loss: 513110560.0000 - val_rmse: 22651.9434\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 544357248.0000 - rmse: 23331.4648 - val_loss: 444917664.0000 - val_rmse: 21093.0723\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 570709888.0000 - rmse: 23889.5352 - val_loss: 429486816.0000 - val_rmse: 20724.0645\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 572506112.0000 - rmse: 23927.0996 - val_loss: 388009568.0000 - val_rmse: 19697.9590\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 524770464.0000 - rmse: 22907.8691 - val_loss: 510648608.0000 - val_rmse: 22597.5352\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 539854336.0000 - rmse: 23234.7656 - val_loss: 450850656.0000 - val_rmse: 21233.2441\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 512945184.0000 - rmse: 22648.2930 - val_loss: 465286336.0000 - val_rmse: 21570.4961\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 522613088.0000 - rmse: 22860.7324 - val_loss: 388713696.0000 - val_rmse: 19715.8242\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 538755584.0000 - rmse: 23211.1094 - val_loss: 391193504.0000 - val_rmse: 19778.6113\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 518215232.0000 - rmse: 22764.3418 - val_loss: 415580032.0000 - val_rmse: 20385.7793\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 542376832.0000 - rmse: 23288.9844 - val_loss: 611320768.0000 - val_rmse: 24724.9023\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 520941376.0000 - rmse: 22824.1406 - val_loss: 413537152.0000 - val_rmse: 20335.6133\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 532096352.0000 - rmse: 23067.2129 - val_loss: 394503840.0000 - val_rmse: 19862.1211\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 492807744.0000 - rmse: 22199.2734 - val_loss: 396079456.0000 - val_rmse: 19901.7441\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 535258400.0000 - rmse: 23135.6523 - val_loss: 367801600.0000 - val_rmse: 19178.1543\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 529988288.0000 - rmse: 23021.4746 - val_loss: 444095232.0000 - val_rmse: 21073.5664\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 544724416.0000 - rmse: 23339.3320 - val_loss: 388311712.0000 - val_rmse: 19705.6270\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 519470240.0000 - rmse: 22791.8906 - val_loss: 469030624.0000 - val_rmse: 21657.1152\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 512120320.0000 - rmse: 22630.0762 - val_loss: 395500096.0000 - val_rmse: 19887.1836\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 463648032.0000 - rmse: 21532.4883 - val_loss: 437497216.0000 - val_rmse: 20916.4336\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 488746656.0000 - rmse: 22107.6152 - val_loss: 376649536.0000 - val_rmse: 19407.4609\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 461181216.0000 - rmse: 21475.1309 - val_loss: 397189888.0000 - val_rmse: 19929.6230\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 514414624.0000 - rmse: 22680.7109 - val_loss: 368696448.0000 - val_rmse: 19201.4707\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 484519488.0000 - rmse: 22011.8027 - val_loss: 469769344.0000 - val_rmse: 21674.1621\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 522094048.0000 - rmse: 22849.3770 - val_loss: 379285184.0000 - val_rmse: 19475.2461\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 455350592.0000 - rmse: 21338.9453 - val_loss: 422484608.0000 - val_rmse: 20554.4297\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 491714944.0000 - rmse: 22174.6465 - val_loss: 402175872.0000 - val_rmse: 20054.3223\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 485416000.0000 - rmse: 22032.1582 - val_loss: 456064992.0000 - val_rmse: 21355.6777\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 485582208.0000 - rmse: 22035.9297 - val_loss: 409457888.0000 - val_rmse: 20235.0664\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 443973120.0000 - rmse: 21070.6699 - val_loss: 382042624.0000 - val_rmse: 19545.9102\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 487321056.0000 - rmse: 22075.3496 - val_loss: 379101376.0000 - val_rmse: 19470.5254\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 497025760.0000 - rmse: 22294.0742 - val_loss: 408077344.0000 - val_rmse: 20200.9238\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 476827296.0000 - rmse: 21836.3750 - val_loss: 416379360.0000 - val_rmse: 20405.3750\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 428149216.0000 - rmse: 20691.7676 - val_loss: 383503360.0000 - val_rmse: 19583.2422\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 455701888.0000 - rmse: 21347.1758 - val_loss: 410986944.0000 - val_rmse: 20272.8125\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 443517120.0000 - rmse: 21059.8457 - val_loss: 416804384.0000 - val_rmse: 20415.7871\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 498585248.0000 - rmse: 22329.0234 - val_loss: 377586048.0000 - val_rmse: 19431.5742\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 487936992.0000 - rmse: 22089.2949 - val_loss: 367560384.0000 - val_rmse: 19171.8652\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 493023392.0000 - rmse: 22204.1309 - val_loss: 388258656.0000 - val_rmse: 19704.2793\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 473749920.0000 - rmse: 21765.7969 - val_loss: 393686144.0000 - val_rmse: 19841.5254\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 440660960.0000 - rmse: 20991.9258 - val_loss: 396006240.0000 - val_rmse: 19899.9062\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 484719520.0000 - rmse: 22016.3457 - val_loss: 357914592.0000 - val_rmse: 18918.6309\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 477138688.0000 - rmse: 21843.5039 - val_loss: 363408352.0000 - val_rmse: 19063.2715\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 453786272.0000 - rmse: 21302.2598 - val_loss: 359938720.0000 - val_rmse: 18972.0508\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 471455360.0000 - rmse: 21713.0234 - val_loss: 373646112.0000 - val_rmse: 19329.9277\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 481434112.0000 - rmse: 21941.6074 - val_loss: 355649344.0000 - val_rmse: 18858.6680\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 430833856.0000 - rmse: 20756.5371 - val_loss: 409526976.0000 - val_rmse: 20236.7734\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 455585440.0000 - rmse: 21344.4473 - val_loss: 359492384.0000 - val_rmse: 18960.2852\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 439760192.0000 - rmse: 20970.4609 - val_loss: 372948256.0000 - val_rmse: 19311.8691\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 420222368.0000 - rmse: 20499.3262 - val_loss: 352539968.0000 - val_rmse: 18776.0469\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 458574592.0000 - rmse: 21414.3555 - val_loss: 383873472.0000 - val_rmse: 19592.6895\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 446474752.0000 - rmse: 21129.9492 - val_loss: 394584000.0000 - val_rmse: 19864.1387\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 418557696.0000 - rmse: 20458.6836 - val_loss: 391177856.0000 - val_rmse: 19778.2168\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 471572800.0000 - rmse: 21715.7266 - val_loss: 388396480.0000 - val_rmse: 19707.7773\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 432067872.0000 - rmse: 20786.2422 - val_loss: 367175808.0000 - val_rmse: 19161.8320\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 433445632.0000 - rmse: 20819.3574 - val_loss: 367124800.0000 - val_rmse: 19160.5020\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 415661216.0000 - rmse: 20387.7715 - val_loss: 375204736.0000 - val_rmse: 19370.2031\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 408448800.0000 - rmse: 20210.1172 - val_loss: 364691040.0000 - val_rmse: 19096.8848\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 507451680.0000 - rmse: 22526.6875 - val_loss: 390320032.0000 - val_rmse: 19756.5195\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 427063936.0000 - rmse: 20665.5254 - val_loss: 388559744.0000 - val_rmse: 19711.9180\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 464399168.0000 - rmse: 21549.9219 - val_loss: 347500448.0000 - val_rmse: 18641.3633\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 431086080.0000 - rmse: 20762.6133 - val_loss: 417139328.0000 - val_rmse: 20423.9883\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 463725632.0000 - rmse: 21534.2891 - val_loss: 529516864.0000 - val_rmse: 23011.2344\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 415152672.0000 - rmse: 20375.2949 - val_loss: 404246400.0000 - val_rmse: 20105.8789\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 416088896.0000 - rmse: 20398.2578 - val_loss: 401250720.0000 - val_rmse: 20031.2441\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 453184160.0000 - rmse: 21288.1230 - val_loss: 395456352.0000 - val_rmse: 19886.0840\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 399587552.0000 - rmse: 19989.6855 - val_loss: 385819936.0000 - val_rmse: 19642.2988\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 403727488.0000 - rmse: 20092.9707 - val_loss: 367910624.0000 - val_rmse: 19180.9961\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 421259424.0000 - rmse: 20524.6055 - val_loss: 489734112.0000 - val_rmse: 22129.9375\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 443534080.0000 - rmse: 21060.2480 - val_loss: 365924192.0000 - val_rmse: 19129.1445\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 447245824.0000 - rmse: 21148.1875 - val_loss: 366838176.0000 - val_rmse: 19153.0195\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 416348192.0000 - rmse: 20404.6113 - val_loss: 349561824.0000 - val_rmse: 18696.5723\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 384217568.0000 - rmse: 19601.4688 - val_loss: 383329504.0000 - val_rmse: 19578.8027\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 432614496.0000 - rmse: 20799.3867 - val_loss: 364011584.0000 - val_rmse: 19079.0879\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 397720576.0000 - rmse: 19942.9336 - val_loss: 367402560.0000 - val_rmse: 19167.7480\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 410855488.0000 - rmse: 20269.5703 - val_loss: 376241088.0000 - val_rmse: 19396.9355\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 449486976.0000 - rmse: 21201.1074 - val_loss: 357441120.0000 - val_rmse: 18906.1133\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 412855168.0000 - rmse: 20318.8379 - val_loss: 380654592.0000 - val_rmse: 19510.3711\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 383154848.0000 - rmse: 19574.3418 - val_loss: 371438560.0000 - val_rmse: 19272.7422\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 423987264.0000 - rmse: 20590.9512 - val_loss: 423262208.0000 - val_rmse: 20573.3379\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 403998624.0000 - rmse: 20099.7168 - val_loss: 441459584.0000 - val_rmse: 21010.9395\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 375631232.0000 - rmse: 19381.2090 - val_loss: 396377568.0000 - val_rmse: 19909.2324\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 395860992.0000 - rmse: 19896.2559 - val_loss: 379089728.0000 - val_rmse: 19470.2266\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 417514048.0000 - rmse: 20433.1602 - val_loss: 374414048.0000 - val_rmse: 19349.7812\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 389035168.0000 - rmse: 19723.9746 - val_loss: 399892992.0000 - val_rmse: 19997.3242\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 376434624.0000 - rmse: 19401.9238 - val_loss: 403344384.0000 - val_rmse: 20083.4355\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 364421856.0000 - rmse: 19089.8359 - val_loss: 375587520.0000 - val_rmse: 19380.0801\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 367041984.0000 - rmse: 19158.3398 - val_loss: 351050656.0000 - val_rmse: 18736.3457\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 439154080.0000 - rmse: 20956.0039 - val_loss: 369398656.0000 - val_rmse: 19219.7461\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 404352672.0000 - rmse: 20108.5215 - val_loss: 365669792.0000 - val_rmse: 19122.4941\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 378765632.0000 - rmse: 19461.9023 - val_loss: 394200896.0000 - val_rmse: 19854.4922\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 374029312.0000 - rmse: 19339.8379 - val_loss: 507937120.0000 - val_rmse: 22537.4609\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 379644640.0000 - rmse: 19484.4727 - val_loss: 433860800.0000 - val_rmse: 20829.3262\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 384984320.0000 - rmse: 19621.0176 - val_loss: 364383072.0000 - val_rmse: 19088.8203\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 385932896.0000 - rmse: 19645.1758 - val_loss: 379167712.0000 - val_rmse: 19472.2285\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 357223072.0000 - rmse: 18900.3457 - val_loss: 359247648.0000 - val_rmse: 18953.8301\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 350522912.0000 - rmse: 18722.2578 - val_loss: 416582752.0000 - val_rmse: 20410.3594\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 375785952.0000 - rmse: 19385.1992 - val_loss: 368980864.0000 - val_rmse: 19208.8750\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 403046080.0000 - rmse: 20076.0078 - val_loss: 349993472.0000 - val_rmse: 18708.1133\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 406231904.0000 - rmse: 20155.1953 - val_loss: 478821824.0000 - val_rmse: 21881.9980\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 369875040.0000 - rmse: 19232.1348 - val_loss: 379284480.0000 - val_rmse: 19475.2266\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 367438784.0000 - rmse: 19168.6934 - val_loss: 394292704.0000 - val_rmse: 19856.8047\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 387928416.0000 - rmse: 19695.8984 - val_loss: 401261824.0000 - val_rmse: 20031.5215\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 343341952.0000 - rmse: 18529.4883 - val_loss: 350029280.0000 - val_rmse: 18709.0703\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 379611680.0000 - rmse: 19483.6250 - val_loss: 381772192.0000 - val_rmse: 19538.9922\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 370048960.0000 - rmse: 19236.6562 - val_loss: 394576768.0000 - val_rmse: 19863.9570\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 426387488.0000 - rmse: 20649.1523 - val_loss: 377051648.0000 - val_rmse: 19417.8184\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 395583776.0000 - rmse: 19889.2871 - val_loss: 400278912.0000 - val_rmse: 20006.9707\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 366699616.0000 - rmse: 19149.4023 - val_loss: 358850784.0000 - val_rmse: 18943.3574\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 422382880.0000 - rmse: 20551.9551 - val_loss: 384861632.0000 - val_rmse: 19617.8906\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 409797536.0000 - rmse: 20243.4570 - val_loss: 429111264.0000 - val_rmse: 20715.0000\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 405636480.0000 - rmse: 20140.4199 - val_loss: 407289344.0000 - val_rmse: 20181.4102\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 438103136.0000 - rmse: 20930.9141 - val_loss: 347650880.0000 - val_rmse: 18645.3984\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 354744160.0000 - rmse: 18834.6523 - val_loss: 365464480.0000 - val_rmse: 19117.1250\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 353896608.0000 - rmse: 18812.1406 - val_loss: 421912416.0000 - val_rmse: 20540.5059\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 353804448.0000 - rmse: 18809.6895 - val_loss: 361715424.0000 - val_rmse: 19018.8184\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 383622944.0000 - rmse: 19586.2949 - val_loss: 382683520.0000 - val_rmse: 19562.2988\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 317558752.0000 - rmse: 17820.1777 - val_loss: 365658208.0000 - val_rmse: 19122.1914\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 334386112.0000 - rmse: 18286.2266 - val_loss: 373590208.0000 - val_rmse: 19328.4824\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 336377152.0000 - rmse: 18340.5879 - val_loss: 359345664.0000 - val_rmse: 18956.4141\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 381490336.0000 - rmse: 19531.7773 - val_loss: 364217088.0000 - val_rmse: 19084.4727\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 340061056.0000 - rmse: 18440.7441 - val_loss: 361978304.0000 - val_rmse: 19025.7266\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 321671872.0000 - rmse: 17935.2129 - val_loss: 382283712.0000 - val_rmse: 19552.0762\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 352005824.0000 - rmse: 18761.8184 - val_loss: 357882528.0000 - val_rmse: 18917.7832\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 351982784.0000 - rmse: 18761.2051 - val_loss: 372718080.0000 - val_rmse: 19305.9082\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 344037888.0000 - rmse: 18548.2578 - val_loss: 345471072.0000 - val_rmse: 18586.8516\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 369914720.0000 - rmse: 19233.1680 - val_loss: 357948512.0000 - val_rmse: 18919.5273\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 359901600.0000 - rmse: 18971.0723 - val_loss: 354215424.0000 - val_rmse: 18820.6113\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 320730528.0000 - rmse: 17908.9512 - val_loss: 345398432.0000 - val_rmse: 18584.8984\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 375561408.0000 - rmse: 19379.4062 - val_loss: 474695104.0000 - val_rmse: 21787.4980\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 333083072.0000 - rmse: 18250.5645 - val_loss: 364105408.0000 - val_rmse: 19081.5469\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 345217088.0000 - rmse: 18580.0176 - val_loss: 380003584.0000 - val_rmse: 19493.6797\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 427426816.0000 - rmse: 20674.3027 - val_loss: 362971968.0000 - val_rmse: 19051.8223\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 301109216.0000 - rmse: 17352.4980 - val_loss: 370299392.0000 - val_rmse: 19243.1641\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 341562560.0000 - rmse: 18481.4121 - val_loss: 378771360.0000 - val_rmse: 19462.0488\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 335742592.0000 - rmse: 18323.2793 - val_loss: 361395168.0000 - val_rmse: 19010.3965\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 332223200.0000 - rmse: 18226.9902 - val_loss: 367826432.0000 - val_rmse: 19178.8008\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 332042944.0000 - rmse: 18222.0449 - val_loss: 352488832.0000 - val_rmse: 18774.6855\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 278394112.0000 - rmse: 16685.1465 - val_loss: 384028672.0000 - val_rmse: 19596.6504\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 332192832.0000 - rmse: 18226.1582 - val_loss: 358558336.0000 - val_rmse: 18935.6367\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 336381088.0000 - rmse: 18340.6953 - val_loss: 380638336.0000 - val_rmse: 19509.9551\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 332596288.0000 - rmse: 18237.2227 - val_loss: 373362848.0000 - val_rmse: 19322.5996\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 330781024.0000 - rmse: 18187.3867 - val_loss: 377505792.0000 - val_rmse: 19429.5078\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 369137312.0000 - rmse: 19212.9473 - val_loss: 340262656.0000 - val_rmse: 18446.2090\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 321173920.0000 - rmse: 17921.3262 - val_loss: 381308960.0000 - val_rmse: 19527.1348\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 393541344.0000 - rmse: 19837.8770 - val_loss: 394801344.0000 - val_rmse: 19869.6094\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 341503776.0000 - rmse: 18479.8203 - val_loss: 360971424.0000 - val_rmse: 18999.2480\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 316637760.0000 - rmse: 17794.3184 - val_loss: 356772864.0000 - val_rmse: 18888.4316\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 338276896.0000 - rmse: 18392.3047 - val_loss: 388853248.0000 - val_rmse: 19719.3613\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 311493568.0000 - rmse: 17649.1797 - val_loss: 404778848.0000 - val_rmse: 20119.1172\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 331999200.0000 - rmse: 18220.8457 - val_loss: 353631264.0000 - val_rmse: 18805.0859\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 327816928.0000 - rmse: 18105.7148 - val_loss: 383251200.0000 - val_rmse: 19576.8027\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 321710880.0000 - rmse: 17936.3008 - val_loss: 366587456.0000 - val_rmse: 19146.4746\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 332045024.0000 - rmse: 18222.1035 - val_loss: 375701120.0000 - val_rmse: 19383.0117\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 345647104.0000 - rmse: 18591.5879 - val_loss: 426045152.0000 - val_rmse: 20640.8613\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 345084832.0000 - rmse: 18576.4590 - val_loss: 350743328.0000 - val_rmse: 18728.1426\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 319119264.0000 - rmse: 17863.9102 - val_loss: 357618048.0000 - val_rmse: 18910.7910\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 317608800.0000 - rmse: 17821.5820 - val_loss: 374150144.0000 - val_rmse: 19342.9609\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309974880.0000 - rmse: 17606.1035 - val_loss: 363361312.0000 - val_rmse: 19062.0391\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 292532960.0000 - rmse: 17103.5957 - val_loss: 381903360.0000 - val_rmse: 19542.3477\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 322047200.0000 - rmse: 17945.6738 - val_loss: 354427424.0000 - val_rmse: 18826.2422\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 326074848.0000 - rmse: 18057.5430 - val_loss: 352918624.0000 - val_rmse: 18786.1289\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 319882496.0000 - rmse: 17885.2598 - val_loss: 415654880.0000 - val_rmse: 20387.6152\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 319220480.0000 - rmse: 17866.7422 - val_loss: 364472128.0000 - val_rmse: 19091.1523\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 359524800.0000 - rmse: 18961.1387 - val_loss: 421428768.0000 - val_rmse: 20528.7305\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 318300288.0000 - rmse: 17840.9727 - val_loss: 362938112.0000 - val_rmse: 19050.9355\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 307588960.0000 - rmse: 17538.2148 - val_loss: 370338880.0000 - val_rmse: 19244.1914\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 325699040.0000 - rmse: 18047.1348 - val_loss: 410895968.0000 - val_rmse: 20270.5684\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 328436512.0000 - rmse: 18122.8184 - val_loss: 436901472.0000 - val_rmse: 20902.1875\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 313462208.0000 - rmse: 17704.8633 - val_loss: 431430464.0000 - val_rmse: 20770.9043\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 335079360.0000 - rmse: 18305.1738 - val_loss: 421539296.0000 - val_rmse: 20531.4219\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 285156480.0000 - rmse: 16886.5762 - val_loss: 361233184.0000 - val_rmse: 19006.1348\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 338976160.0000 - rmse: 18411.3047 - val_loss: 400411712.0000 - val_rmse: 20010.2910\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268663712.0000 - rmse: 16390.9648 - val_loss: 343176320.0000 - val_rmse: 18525.0195\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 308221152.0000 - rmse: 17556.2285 - val_loss: 357888160.0000 - val_rmse: 18917.9316\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 308172416.0000 - rmse: 17554.8398 - val_loss: 365428416.0000 - val_rmse: 19116.1816\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 291536288.0000 - rmse: 17074.4336 - val_loss: 362452256.0000 - val_rmse: 19038.1797\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 317826496.0000 - rmse: 17827.6895 - val_loss: 345615008.0000 - val_rmse: 18590.7246\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 314399872.0000 - rmse: 17731.3242 - val_loss: 365811264.0000 - val_rmse: 19126.1934\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286377600.0000 - rmse: 16922.6953 - val_loss: 358152128.0000 - val_rmse: 18924.9082\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 320230400.0000 - rmse: 17894.9824 - val_loss: 384167328.0000 - val_rmse: 19600.1875\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 291439392.0000 - rmse: 17071.5957 - val_loss: 400072832.0000 - val_rmse: 20001.8203\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 319334912.0000 - rmse: 17869.9453 - val_loss: 392283552.0000 - val_rmse: 19806.1484\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 315730304.0000 - rmse: 17768.8008 - val_loss: 391855520.0000 - val_rmse: 19795.3418\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 295021600.0000 - rmse: 17176.1934 - val_loss: 355655232.0000 - val_rmse: 18858.8242\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 267257120.0000 - rmse: 16348.0010 - val_loss: 359202496.0000 - val_rmse: 18952.6387\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 284078816.0000 - rmse: 16854.6387 - val_loss: 367839200.0000 - val_rmse: 19179.1348\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 267621088.0000 - rmse: 16359.1289 - val_loss: 420253120.0000 - val_rmse: 20500.0762\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 302857408.0000 - rmse: 17402.7988 - val_loss: 369979392.0000 - val_rmse: 19234.8477\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280584512.0000 - rmse: 16750.6562 - val_loss: 358955520.0000 - val_rmse: 18946.1211\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 284907872.0000 - rmse: 16879.2148 - val_loss: 357703776.0000 - val_rmse: 18913.0586\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 285461472.0000 - rmse: 16895.6055 - val_loss: 364321568.0000 - val_rmse: 19087.2090\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 319656384.0000 - rmse: 17878.9375 - val_loss: 359703616.0000 - val_rmse: 18965.8535\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309115104.0000 - rmse: 17581.6699 - val_loss: 360144672.0000 - val_rmse: 18977.4785\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264164112.0000 - rmse: 16253.1260 - val_loss: 345602592.0000 - val_rmse: 18590.3906\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 288626496.0000 - rmse: 16989.0117 - val_loss: 358936832.0000 - val_rmse: 18945.6289\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 271454240.0000 - rmse: 16475.8691 - val_loss: 358800224.0000 - val_rmse: 18942.0234\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309560512.0000 - rmse: 17594.3320 - val_loss: 367816224.0000 - val_rmse: 19178.5352\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299110304.0000 - rmse: 17294.8066 - val_loss: 355544032.0000 - val_rmse: 18855.8750\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 272268800.0000 - rmse: 16500.5703 - val_loss: 360949440.0000 - val_rmse: 18998.6699\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 274710304.0000 - rmse: 16574.3867 - val_loss: 371561824.0000 - val_rmse: 19275.9395\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 262289376.0000 - rmse: 16195.3506 - val_loss: 453648448.0000 - val_rmse: 21299.0254\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 296341600.0000 - rmse: 17214.5762 - val_loss: 356777920.0000 - val_rmse: 18888.5664\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 291050944.0000 - rmse: 17060.2148 - val_loss: 361494304.0000 - val_rmse: 19013.0039\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 277302848.0000 - rmse: 16652.4121 - val_loss: 382793664.0000 - val_rmse: 19565.1133\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 274197568.0000 - rmse: 16558.9121 - val_loss: 366607360.0000 - val_rmse: 19146.9941\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 307801120.0000 - rmse: 17544.2617 - val_loss: 368902944.0000 - val_rmse: 19206.8457\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 292530912.0000 - rmse: 17103.5352 - val_loss: 358152800.0000 - val_rmse: 18924.9258\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 315474464.0000 - rmse: 17761.6016 - val_loss: 348501824.0000 - val_rmse: 18668.2031\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 273881888.0000 - rmse: 16549.3770 - val_loss: 382200256.0000 - val_rmse: 19549.9434\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268645696.0000 - rmse: 16390.4141 - val_loss: 351332224.0000 - val_rmse: 18743.8574\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 296646048.0000 - rmse: 17223.4160 - val_loss: 353459424.0000 - val_rmse: 18800.5156\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 326526560.0000 - rmse: 18070.0469 - val_loss: 346580704.0000 - val_rmse: 18616.6777\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 274690912.0000 - rmse: 16573.8027 - val_loss: 365672768.0000 - val_rmse: 19122.5723\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 307800928.0000 - rmse: 17544.2559 - val_loss: 338940384.0000 - val_rmse: 18410.3340\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 265914768.0000 - rmse: 16306.8936 - val_loss: 346953088.0000 - val_rmse: 18626.6777\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 294258752.0000 - rmse: 17153.9727 - val_loss: 338462432.0000 - val_rmse: 18397.3477\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 277649600.0000 - rmse: 16662.8203 - val_loss: 455406816.0000 - val_rmse: 21340.2637\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 238730912.0000 - rmse: 15450.9199 - val_loss: 341034752.0000 - val_rmse: 18467.1270\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264505152.0000 - rmse: 16263.6143 - val_loss: 333520960.0000 - val_rmse: 18262.5566\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 294406112.0000 - rmse: 17158.2656 - val_loss: 411711136.0000 - val_rmse: 20290.6660\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 275043936.0000 - rmse: 16584.4492 - val_loss: 339191488.0000 - val_rmse: 18417.1523\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 263957008.0000 - rmse: 16246.7539 - val_loss: 384684832.0000 - val_rmse: 19613.3848\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 283780000.0000 - rmse: 16845.7715 - val_loss: 339867264.0000 - val_rmse: 18435.4883\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 273014336.0000 - rmse: 16523.1445 - val_loss: 409428640.0000 - val_rmse: 20234.3438\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 310098240.0000 - rmse: 17609.6074 - val_loss: 346602496.0000 - val_rmse: 18617.2637\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 246793952.0000 - rmse: 15709.6768 - val_loss: 444678624.0000 - val_rmse: 21087.4043\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 266727440.0000 - rmse: 16331.7920 - val_loss: 353517312.0000 - val_rmse: 18802.0566\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 265898128.0000 - rmse: 16306.3828 - val_loss: 338243552.0000 - val_rmse: 18391.3984\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 274973536.0000 - rmse: 16582.3262 - val_loss: 365723552.0000 - val_rmse: 19123.9004\n",
      "Epoch 367/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 301152672.0000 - rmse: 17353.7500 - val_loss: 334773920.0000 - val_rmse: 18296.8281\n",
      "Epoch 368/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260792480.0000 - rmse: 16149.0703 - val_loss: 336802080.0000 - val_rmse: 18352.1680\n",
      "Epoch 369/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 215626976.0000 - rmse: 14684.2422 - val_loss: 339286496.0000 - val_rmse: 18419.7305\n",
      "Epoch 370/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 300548448.0000 - rmse: 17336.3340 - val_loss: 356773408.0000 - val_rmse: 18888.4473\n",
      "Epoch 371/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264435888.0000 - rmse: 16261.4844 - val_loss: 335928736.0000 - val_rmse: 18328.3594\n",
      "Epoch 372/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 266798688.0000 - rmse: 16333.9736 - val_loss: 354728256.0000 - val_rmse: 18834.2305\n",
      "Epoch 373/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 281192544.0000 - rmse: 16768.7969 - val_loss: 340333472.0000 - val_rmse: 18448.1289\n",
      "Epoch 374/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280513440.0000 - rmse: 16748.5352 - val_loss: 329749664.0000 - val_rmse: 18159.0098\n",
      "Epoch 375/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 255798064.0000 - rmse: 15993.6885 - val_loss: 342986784.0000 - val_rmse: 18519.9023\n",
      "Epoch 376/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 246596848.0000 - rmse: 15703.4023 - val_loss: 322257088.0000 - val_rmse: 17951.5195\n",
      "Epoch 377/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 263541200.0000 - rmse: 16233.9521 - val_loss: 342912096.0000 - val_rmse: 18517.8867\n",
      "Epoch 378/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 243184608.0000 - rmse: 15594.3779 - val_loss: 324853728.0000 - val_rmse: 18023.6992\n",
      "Epoch 379/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 238822000.0000 - rmse: 15453.8672 - val_loss: 335429824.0000 - val_rmse: 18314.7441\n",
      "Epoch 380/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 243408752.0000 - rmse: 15601.5625 - val_loss: 327930752.0000 - val_rmse: 18108.8574\n",
      "Epoch 381/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268203632.0000 - rmse: 16376.9238 - val_loss: 337829600.0000 - val_rmse: 18380.1406\n",
      "Epoch 382/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 259661632.0000 - rmse: 16114.0195 - val_loss: 318294400.0000 - val_rmse: 17840.8066\n",
      "Epoch 383/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 238118560.0000 - rmse: 15431.0908 - val_loss: 326278624.0000 - val_rmse: 18063.1836\n",
      "Epoch 384/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 233883344.0000 - rmse: 15293.2451 - val_loss: 347538752.0000 - val_rmse: 18642.3906\n",
      "Epoch 385/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280859968.0000 - rmse: 16758.8770 - val_loss: 325037184.0000 - val_rmse: 18028.7871\n",
      "Epoch 386/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 251965072.0000 - rmse: 15873.4072 - val_loss: 348299488.0000 - val_rmse: 18662.7832\n",
      "Epoch 387/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264353376.0000 - rmse: 16258.9473 - val_loss: 338995712.0000 - val_rmse: 18411.8359\n",
      "Epoch 388/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 274643904.0000 - rmse: 16572.3828 - val_loss: 328451616.0000 - val_rmse: 18123.2344\n",
      "Epoch 389/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 243252688.0000 - rmse: 15596.5605 - val_loss: 330579936.0000 - val_rmse: 18181.8574\n",
      "Epoch 390/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 258964768.0000 - rmse: 16092.3818 - val_loss: 333182496.0000 - val_rmse: 18253.2871\n",
      "Epoch 391/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 271753408.0000 - rmse: 16484.9453 - val_loss: 344020448.0000 - val_rmse: 18547.7891\n",
      "Epoch 392/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 276475200.0000 - rmse: 16627.5430 - val_loss: 330796544.0000 - val_rmse: 18187.8125\n",
      "Epoch 393/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 292589856.0000 - rmse: 17105.2578 - val_loss: 341699520.0000 - val_rmse: 18485.1152\n",
      "Epoch 394/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 241437616.0000 - rmse: 15538.2627 - val_loss: 326957440.0000 - val_rmse: 18081.9648\n",
      "Epoch 395/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 282008288.0000 - rmse: 16793.1016 - val_loss: 328008512.0000 - val_rmse: 18111.0059\n",
      "Epoch 396/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 250112144.0000 - rmse: 15814.9346 - val_loss: 319712992.0000 - val_rmse: 17880.5195\n",
      "Epoch 397/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 287224096.0000 - rmse: 16947.6875 - val_loss: 335700128.0000 - val_rmse: 18322.1211\n",
      "Epoch 398/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 225813936.0000 - rmse: 15027.1064 - val_loss: 323963488.0000 - val_rmse: 17998.9863\n",
      "Epoch 399/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 265597648.0000 - rmse: 16297.1670 - val_loss: 319253600.0000 - val_rmse: 17867.6699\n",
      "Epoch 400/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 234746768.0000 - rmse: 15321.4482 - val_loss: 336835424.0000 - val_rmse: 18353.0762\n",
      "104/104 [==============================] - 0s 810us/step - loss: 599811200.0000 - rmse: 24491.0430\n",
      "[599811200.0, 24491.04296875]\n",
      "<src.model.emb_model object at 0x7f846442f390>\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 20526061568.0000 - rmse: 143269.1875 - val_loss: 2668288768.0000 - val_rmse: 51655.4805\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2843836160.0000 - rmse: 53327.6289 - val_loss: 1490602624.0000 - val_rmse: 38608.3242\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2108875136.0000 - rmse: 45922.4922 - val_loss: 1402694272.0000 - val_rmse: 37452.5586\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1961682304.0000 - rmse: 44290.8828 - val_loss: 1307876608.0000 - val_rmse: 36164.5781\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1970409344.0000 - rmse: 44389.2930 - val_loss: 1229192448.0000 - val_rmse: 35059.8398\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1952961920.0000 - rmse: 44192.3281 - val_loss: 1205680000.0000 - val_rmse: 34722.9023\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1860113152.0000 - rmse: 43129.0273 - val_loss: 1225210112.0000 - val_rmse: 35003.0000\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1738839424.0000 - rmse: 41699.3945 - val_loss: 1101552256.0000 - val_rmse: 33189.6406\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1727559040.0000 - rmse: 41563.9141 - val_loss: 1044053376.0000 - val_rmse: 32311.8145\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1684716032.0000 - rmse: 41045.2930 - val_loss: 1274547968.0000 - val_rmse: 35700.8125\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1578761472.0000 - rmse: 39733.6328 - val_loss: 1010322048.0000 - val_rmse: 31785.5645\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1562897920.0000 - rmse: 39533.5039 - val_loss: 1030029760.0000 - val_rmse: 32094.0762\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1554840064.0000 - rmse: 39431.4609 - val_loss: 918797760.0000 - val_rmse: 30311.6777\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1494144128.0000 - rmse: 38654.1602 - val_loss: 936469120.0000 - val_rmse: 30601.7832\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1464290816.0000 - rmse: 38266.0547 - val_loss: 929496000.0000 - val_rmse: 30487.6367\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1401536896.0000 - rmse: 37437.1055 - val_loss: 899054720.0000 - val_rmse: 29984.2402\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1342855552.0000 - rmse: 36644.9922 - val_loss: 824741952.0000 - val_rmse: 28718.3203\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1482835328.0000 - rmse: 38507.6016 - val_loss: 928893696.0000 - val_rmse: 30477.7578\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1301018368.0000 - rmse: 36069.6328 - val_loss: 865999680.0000 - val_rmse: 29427.8730\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1323252352.0000 - rmse: 36376.5352 - val_loss: 800803008.0000 - val_rmse: 28298.4629\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1264738944.0000 - rmse: 35563.1680 - val_loss: 781938944.0000 - val_rmse: 27963.1719\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1306047360.0000 - rmse: 36139.2773 - val_loss: 1022755776.0000 - val_rmse: 31980.5527\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1283744512.0000 - rmse: 35829.3789 - val_loss: 1263451776.0000 - val_rmse: 35545.0664\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1234758656.0000 - rmse: 35139.1328 - val_loss: 826005504.0000 - val_rmse: 28740.3105\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1141442048.0000 - rmse: 33785.2344 - val_loss: 914717184.0000 - val_rmse: 30244.2910\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1207120768.0000 - rmse: 34743.6445 - val_loss: 767173184.0000 - val_rmse: 27697.8906\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1174222720.0000 - rmse: 34266.9336 - val_loss: 825251200.0000 - val_rmse: 28727.1855\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1215010816.0000 - rmse: 34857.0039 - val_loss: 718246528.0000 - val_rmse: 26800.1211\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1087150976.0000 - rmse: 32971.9727 - val_loss: 800057472.0000 - val_rmse: 28285.2871\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1185476352.0000 - rmse: 34430.7461 - val_loss: 726382080.0000 - val_rmse: 26951.4766\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1028589184.0000 - rmse: 32071.6250 - val_loss: 835196288.0000 - val_rmse: 28899.7637\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1107655296.0000 - rmse: 33281.4570 - val_loss: 735272768.0000 - val_rmse: 27115.9141\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1081249664.0000 - rmse: 32882.3594 - val_loss: 728621376.0000 - val_rmse: 26992.9883\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1039495872.0000 - rmse: 32241.2148 - val_loss: 911852736.0000 - val_rmse: 30196.9004\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1095944832.0000 - rmse: 33105.0586 - val_loss: 689962240.0000 - val_rmse: 26267.1328\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1085550208.0000 - rmse: 32947.6875 - val_loss: 809477440.0000 - val_rmse: 28451.3164\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1030945472.0000 - rmse: 32108.3398 - val_loss: 798343680.0000 - val_rmse: 28254.9766\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1085605248.0000 - rmse: 32948.5234 - val_loss: 695362880.0000 - val_rmse: 26369.7344\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 999694912.0000 - rmse: 31617.9531 - val_loss: 701141568.0000 - val_rmse: 26479.0781\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1035787968.0000 - rmse: 32183.6602 - val_loss: 809146752.0000 - val_rmse: 28445.5059\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1051794304.0000 - rmse: 32431.3789 - val_loss: 655769536.0000 - val_rmse: 25607.9980\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1033371200.0000 - rmse: 32146.0918 - val_loss: 670878592.0000 - val_rmse: 25901.3242\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 995323328.0000 - rmse: 31548.7461 - val_loss: 633174848.0000 - val_rmse: 25162.9648\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1010418496.0000 - rmse: 31787.0801 - val_loss: 641574208.0000 - val_rmse: 25329.3145\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 973027072.0000 - rmse: 31193.3828 - val_loss: 609888448.0000 - val_rmse: 24695.9199\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 957802112.0000 - rmse: 30948.3789 - val_loss: 645028672.0000 - val_rmse: 25397.4141\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 926283712.0000 - rmse: 30434.9102 - val_loss: 648416000.0000 - val_rmse: 25464.0137\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 893345984.0000 - rmse: 29888.8945 - val_loss: 642852416.0000 - val_rmse: 25354.5352\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1013504448.0000 - rmse: 31835.5840 - val_loss: 727533824.0000 - val_rmse: 26972.8340\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 979875456.0000 - rmse: 31302.9629 - val_loss: 617543552.0000 - val_rmse: 24850.4238\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 933420416.0000 - rmse: 30551.9297 - val_loss: 659627456.0000 - val_rmse: 25683.2129\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 992716544.0000 - rmse: 31507.4043 - val_loss: 678473536.0000 - val_rmse: 26047.5254\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1011804416.0000 - rmse: 31808.8730 - val_loss: 748273920.0000 - val_rmse: 27354.5957\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 974022976.0000 - rmse: 31209.3418 - val_loss: 598526336.0000 - val_rmse: 24464.7988\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 937797888.0000 - rmse: 30623.4863 - val_loss: 621312320.0000 - val_rmse: 24926.1367\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 893387328.0000 - rmse: 29889.5859 - val_loss: 607380544.0000 - val_rmse: 24645.0918\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 931267264.0000 - rmse: 30516.6719 - val_loss: 588011968.0000 - val_rmse: 24248.9590\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 862630208.0000 - rmse: 29370.5664 - val_loss: 656623424.0000 - val_rmse: 25624.6641\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 945571328.0000 - rmse: 30750.1426 - val_loss: 573464192.0000 - val_rmse: 23947.1133\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 881724800.0000 - rmse: 29693.8516 - val_loss: 599310656.0000 - val_rmse: 24480.8223\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 923517760.0000 - rmse: 30389.4355 - val_loss: 591595904.0000 - val_rmse: 24322.7441\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 907532608.0000 - rmse: 30125.2812 - val_loss: 601360448.0000 - val_rmse: 24522.6523\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 994077696.0000 - rmse: 31528.9980 - val_loss: 606744320.0000 - val_rmse: 24632.1797\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 898198336.0000 - rmse: 29969.9570 - val_loss: 627811456.0000 - val_rmse: 25056.1660\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 829827968.0000 - rmse: 28806.7344 - val_loss: 630928064.0000 - val_rmse: 25118.2812\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 903489920.0000 - rmse: 30058.1094 - val_loss: 581598528.0000 - val_rmse: 24116.3535\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 807458752.0000 - rmse: 28415.8184 - val_loss: 1033203136.0000 - val_rmse: 32143.4766\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 914270464.0000 - rmse: 30236.9062 - val_loss: 568723904.0000 - val_rmse: 23847.9336\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 837590784.0000 - rmse: 28941.1602 - val_loss: 551242112.0000 - val_rmse: 23478.5449\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 823320128.0000 - rmse: 28693.5547 - val_loss: 574321408.0000 - val_rmse: 23965.0039\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 861480128.0000 - rmse: 29350.9824 - val_loss: 622175360.0000 - val_rmse: 24943.4434\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 803850368.0000 - rmse: 28352.2559 - val_loss: 553093184.0000 - val_rmse: 23517.9336\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 898227968.0000 - rmse: 29970.4512 - val_loss: 587750592.0000 - val_rmse: 24243.5684\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 822988544.0000 - rmse: 28687.7773 - val_loss: 594429632.0000 - val_rmse: 24380.9277\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 833370112.0000 - rmse: 28868.1504 - val_loss: 615549632.0000 - val_rmse: 24810.2734\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 779742656.0000 - rmse: 27923.8730 - val_loss: 546833600.0000 - val_rmse: 23384.4727\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 830752576.0000 - rmse: 28822.7793 - val_loss: 552650752.0000 - val_rmse: 23508.5254\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 784413056.0000 - rmse: 28007.3750 - val_loss: 545335680.0000 - val_rmse: 23352.4238\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 783203584.0000 - rmse: 27985.7754 - val_loss: 543255552.0000 - val_rmse: 23307.8438\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 753154560.0000 - rmse: 27443.6621 - val_loss: 635390784.0000 - val_rmse: 25206.9590\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 804534656.0000 - rmse: 28364.3203 - val_loss: 568950208.0000 - val_rmse: 23852.6777\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 828779392.0000 - rmse: 28788.5293 - val_loss: 560423424.0000 - val_rmse: 23673.2637\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 765250944.0000 - rmse: 27663.1699 - val_loss: 534954432.0000 - val_rmse: 23129.0820\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 787839680.0000 - rmse: 28068.4824 - val_loss: 555695616.0000 - val_rmse: 23573.1973\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 730576384.0000 - rmse: 27029.1758 - val_loss: 568847872.0000 - val_rmse: 23850.5312\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 778863616.0000 - rmse: 27908.1289 - val_loss: 539489216.0000 - val_rmse: 23226.9062\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 768548992.0000 - rmse: 27722.7168 - val_loss: 551599680.0000 - val_rmse: 23486.1602\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 825541696.0000 - rmse: 28732.2422 - val_loss: 613516096.0000 - val_rmse: 24769.2578\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 736898432.0000 - rmse: 27145.8730 - val_loss: 538327104.0000 - val_rmse: 23201.8770\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 746912896.0000 - rmse: 27329.7070 - val_loss: 547264192.0000 - val_rmse: 23393.6777\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 759975488.0000 - rmse: 27567.6523 - val_loss: 588155072.0000 - val_rmse: 24251.9082\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 788025344.0000 - rmse: 28071.7891 - val_loss: 523971744.0000 - val_rmse: 22890.4297\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 812657664.0000 - rmse: 28507.1504 - val_loss: 597847104.0000 - val_rmse: 24450.9121\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 731928512.0000 - rmse: 27054.1777 - val_loss: 722617152.0000 - val_rmse: 26881.5391\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 757267328.0000 - rmse: 27518.4902 - val_loss: 601569152.0000 - val_rmse: 24526.9062\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 757718016.0000 - rmse: 27526.6777 - val_loss: 525725440.0000 - val_rmse: 22928.7031\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 741514624.0000 - rmse: 27230.7656 - val_loss: 583438144.0000 - val_rmse: 24154.4648\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 721896000.0000 - rmse: 26868.1230 - val_loss: 551136832.0000 - val_rmse: 23476.3027\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 761116864.0000 - rmse: 27588.3457 - val_loss: 528854240.0000 - val_rmse: 22996.8320\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 719312000.0000 - rmse: 26819.9922 - val_loss: 519143456.0000 - val_rmse: 22784.7207\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 719675136.0000 - rmse: 26826.7617 - val_loss: 531056288.0000 - val_rmse: 23044.6582\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 701592512.0000 - rmse: 26487.5918 - val_loss: 511736992.0000 - val_rmse: 22621.6055\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 695072128.0000 - rmse: 26364.2207 - val_loss: 558692224.0000 - val_rmse: 23636.6719\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 728170304.0000 - rmse: 26984.6309 - val_loss: 524590592.0000 - val_rmse: 22903.9434\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 754146112.0000 - rmse: 27461.7207 - val_loss: 514899104.0000 - val_rmse: 22691.3887\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 677012288.0000 - rmse: 26019.4590 - val_loss: 527506432.0000 - val_rmse: 22967.5078\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 659638848.0000 - rmse: 25683.4355 - val_loss: 507907008.0000 - val_rmse: 22536.7930\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 716477632.0000 - rmse: 26767.0996 - val_loss: 532869088.0000 - val_rmse: 23083.9570\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 734045440.0000 - rmse: 27093.2734 - val_loss: 521756256.0000 - val_rmse: 22841.9844\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 687635264.0000 - rmse: 26222.8008 - val_loss: 514655072.0000 - val_rmse: 22686.0098\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 640242880.0000 - rmse: 25303.0215 - val_loss: 519888032.0000 - val_rmse: 22801.0527\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 696310656.0000 - rmse: 26387.6992 - val_loss: 530604192.0000 - val_rmse: 23034.8477\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 639478720.0000 - rmse: 25287.9160 - val_loss: 495264032.0000 - val_rmse: 22254.5293\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 694656640.0000 - rmse: 26356.3398 - val_loss: 515049120.0000 - val_rmse: 22694.6934\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 702586304.0000 - rmse: 26506.3438 - val_loss: 512515872.0000 - val_rmse: 22638.8125\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 633561280.0000 - rmse: 25170.6426 - val_loss: 505354048.0000 - val_rmse: 22480.0820\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 602997440.0000 - rmse: 24556.0059 - val_loss: 524587520.0000 - val_rmse: 22903.8750\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 653533184.0000 - rmse: 25564.2949 - val_loss: 516704768.0000 - val_rmse: 22731.1406\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 652055616.0000 - rmse: 25535.3789 - val_loss: 530504672.0000 - val_rmse: 23032.6875\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 633189760.0000 - rmse: 25163.2617 - val_loss: 505357664.0000 - val_rmse: 22480.1621\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 637506752.0000 - rmse: 25248.8965 - val_loss: 516665920.0000 - val_rmse: 22730.2871\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 666722496.0000 - rmse: 25820.9707 - val_loss: 497075424.0000 - val_rmse: 22295.1875\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 594913792.0000 - rmse: 24390.8555 - val_loss: 501088512.0000 - val_rmse: 22385.0059\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 650068736.0000 - rmse: 25496.4453 - val_loss: 522595008.0000 - val_rmse: 22860.3379\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 664480000.0000 - rmse: 25777.5098 - val_loss: 773506752.0000 - val_rmse: 27811.9902\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 593105600.0000 - rmse: 24353.7598 - val_loss: 501403968.0000 - val_rmse: 22392.0508\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 617653184.0000 - rmse: 24852.6289 - val_loss: 578659008.0000 - val_rmse: 24055.3320\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 621500992.0000 - rmse: 24929.9219 - val_loss: 523756960.0000 - val_rmse: 22885.7363\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 600394752.0000 - rmse: 24502.9531 - val_loss: 495076128.0000 - val_rmse: 22250.3066\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 629296384.0000 - rmse: 25085.7812 - val_loss: 587482752.0000 - val_rmse: 24238.0430\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 596876736.0000 - rmse: 24431.0605 - val_loss: 517451712.0000 - val_rmse: 22747.5645\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 610658112.0000 - rmse: 24711.4980 - val_loss: 521275520.0000 - val_rmse: 22831.4590\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 632531840.0000 - rmse: 25150.1855 - val_loss: 520422176.0000 - val_rmse: 22812.7637\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 618517120.0000 - rmse: 24870.0039 - val_loss: 546824320.0000 - val_rmse: 23384.2754\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 677987200.0000 - rmse: 26038.1875 - val_loss: 508763456.0000 - val_rmse: 22555.7852\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 586620224.0000 - rmse: 24220.2441 - val_loss: 509505440.0000 - val_rmse: 22572.2266\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 615726080.0000 - rmse: 24813.8281 - val_loss: 502370976.0000 - val_rmse: 22413.6328\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 584297344.0000 - rmse: 24172.2441 - val_loss: 487396352.0000 - val_rmse: 22077.0547\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 574972672.0000 - rmse: 23978.5879 - val_loss: 509600640.0000 - val_rmse: 22574.3359\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 584094336.0000 - rmse: 24168.0430 - val_loss: 525169728.0000 - val_rmse: 22916.5820\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 542423744.0000 - rmse: 23289.9922 - val_loss: 516908960.0000 - val_rmse: 22735.6328\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 597597440.0000 - rmse: 24445.8066 - val_loss: 507830624.0000 - val_rmse: 22535.0977\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 544115968.0000 - rmse: 23326.2930 - val_loss: 517874016.0000 - val_rmse: 22756.8457\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 579259712.0000 - rmse: 24067.8145 - val_loss: 582418816.0000 - val_rmse: 24133.3555\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 619401728.0000 - rmse: 24887.7832 - val_loss: 478472128.0000 - val_rmse: 21874.0059\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 597299584.0000 - rmse: 24439.7129 - val_loss: 485282272.0000 - val_rmse: 22029.1230\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 583839680.0000 - rmse: 24162.7754 - val_loss: 507766848.0000 - val_rmse: 22533.6816\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 630532032.0000 - rmse: 25110.3965 - val_loss: 490391680.0000 - val_rmse: 22144.7891\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 567374464.0000 - rmse: 23819.6230 - val_loss: 481003040.0000 - val_rmse: 21931.7812\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 613690304.0000 - rmse: 24772.7734 - val_loss: 492209408.0000 - val_rmse: 22185.7930\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 593505792.0000 - rmse: 24361.9746 - val_loss: 483490400.0000 - val_rmse: 21988.4160\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 595410176.0000 - rmse: 24401.0273 - val_loss: 536181248.0000 - val_rmse: 23155.5879\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 563274496.0000 - rmse: 23733.4043 - val_loss: 539358016.0000 - val_rmse: 23224.0820\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 604192704.0000 - rmse: 24580.3320 - val_loss: 494391488.0000 - val_rmse: 22234.9160\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 592811776.0000 - rmse: 24347.7266 - val_loss: 518412960.0000 - val_rmse: 22768.6836\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 529425152.0000 - rmse: 23009.2402 - val_loss: 465443296.0000 - val_rmse: 21574.1348\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 552725632.0000 - rmse: 23510.1172 - val_loss: 477289920.0000 - val_rmse: 21846.9668\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 541788224.0000 - rmse: 23276.3457 - val_loss: 502516704.0000 - val_rmse: 22416.8848\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 606649792.0000 - rmse: 24630.2617 - val_loss: 480059168.0000 - val_rmse: 21910.2520\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 543737664.0000 - rmse: 23318.1836 - val_loss: 486456320.0000 - val_rmse: 22055.7539\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 531502048.0000 - rmse: 23054.3281 - val_loss: 529203872.0000 - val_rmse: 23004.4316\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 584319360.0000 - rmse: 24172.6992 - val_loss: 498089088.0000 - val_rmse: 22317.9102\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 561174208.0000 - rmse: 23689.1152 - val_loss: 494697024.0000 - val_rmse: 22241.7852\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 528881312.0000 - rmse: 22997.4199 - val_loss: 480002880.0000 - val_rmse: 21908.9688\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 593247488.0000 - rmse: 24356.6719 - val_loss: 528169600.0000 - val_rmse: 22981.9414\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 484292480.0000 - rmse: 22006.6465 - val_loss: 494378272.0000 - val_rmse: 22234.6191\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 520277696.0000 - rmse: 22809.5957 - val_loss: 492327328.0000 - val_rmse: 22188.4512\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 532722592.0000 - rmse: 23080.7832 - val_loss: 472861632.0000 - val_rmse: 21745.3828\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 544801728.0000 - rmse: 23340.9883 - val_loss: 475333184.0000 - val_rmse: 21802.1367\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 543662464.0000 - rmse: 23316.5703 - val_loss: 509605408.0000 - val_rmse: 22574.4414\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 506282816.0000 - rmse: 22500.7285 - val_loss: 502352576.0000 - val_rmse: 22413.2227\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 559182848.0000 - rmse: 23647.0469 - val_loss: 499376704.0000 - val_rmse: 22346.7383\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 533678752.0000 - rmse: 23101.4883 - val_loss: 479515968.0000 - val_rmse: 21897.8535\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 559317888.0000 - rmse: 23649.9023 - val_loss: 484455840.0000 - val_rmse: 22010.3574\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 514302368.0000 - rmse: 22678.2363 - val_loss: 492853280.0000 - val_rmse: 22200.2988\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 475959520.0000 - rmse: 21816.4961 - val_loss: 501158560.0000 - val_rmse: 22386.5703\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 533109248.0000 - rmse: 23089.1582 - val_loss: 486488704.0000 - val_rmse: 22056.4883\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 518374720.0000 - rmse: 22767.8438 - val_loss: 530861888.0000 - val_rmse: 23040.4395\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 514037728.0000 - rmse: 22672.4004 - val_loss: 494912768.0000 - val_rmse: 22246.6348\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 515661408.0000 - rmse: 22708.1797 - val_loss: 495878304.0000 - val_rmse: 22268.3242\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 542271936.0000 - rmse: 23286.7324 - val_loss: 497833664.0000 - val_rmse: 22312.1855\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 532126656.0000 - rmse: 23067.8711 - val_loss: 507758400.0000 - val_rmse: 22533.4941\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 489722464.0000 - rmse: 22129.6738 - val_loss: 565437504.0000 - val_rmse: 23778.9297\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 486960736.0000 - rmse: 22067.1875 - val_loss: 489515456.0000 - val_rmse: 22124.9961\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 474917952.0000 - rmse: 21792.6133 - val_loss: 545040640.0000 - val_rmse: 23346.1055\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 503788960.0000 - rmse: 22445.2441 - val_loss: 548321472.0000 - val_rmse: 23416.2656\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 483487328.0000 - rmse: 21988.3457 - val_loss: 460043456.0000 - val_rmse: 21448.6230\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 535371424.0000 - rmse: 23138.0938 - val_loss: 490416640.0000 - val_rmse: 22145.3535\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 463148416.0000 - rmse: 21520.8828 - val_loss: 522562848.0000 - val_rmse: 22859.6328\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 448143776.0000 - rmse: 21169.4062 - val_loss: 486174720.0000 - val_rmse: 22049.3691\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 496335520.0000 - rmse: 22278.5879 - val_loss: 478870432.0000 - val_rmse: 21883.1074\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 535523616.0000 - rmse: 23141.3828 - val_loss: 495384096.0000 - val_rmse: 22257.2266\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 522064032.0000 - rmse: 22848.7207 - val_loss: 470087136.0000 - val_rmse: 21681.4922\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 468549984.0000 - rmse: 21646.0156 - val_loss: 447699008.0000 - val_rmse: 21158.8984\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 467541696.0000 - rmse: 21622.7129 - val_loss: 501812192.0000 - val_rmse: 22401.1641\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 436745024.0000 - rmse: 20898.4453 - val_loss: 472441248.0000 - val_rmse: 21735.7129\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 486622976.0000 - rmse: 22059.5332 - val_loss: 471018912.0000 - val_rmse: 21702.9707\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 446046784.0000 - rmse: 21119.8203 - val_loss: 462597024.0000 - val_rmse: 21508.0684\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 551281984.0000 - rmse: 23479.3945 - val_loss: 473714304.0000 - val_rmse: 21764.9785\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 446799840.0000 - rmse: 21137.6406 - val_loss: 483349696.0000 - val_rmse: 21985.2148\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 485345984.0000 - rmse: 22030.5684 - val_loss: 502683936.0000 - val_rmse: 22420.6133\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 487938048.0000 - rmse: 22089.3203 - val_loss: 455380576.0000 - val_rmse: 21339.6484\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 456025920.0000 - rmse: 21354.7637 - val_loss: 479347872.0000 - val_rmse: 21894.0137\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 435977952.0000 - rmse: 20880.0859 - val_loss: 515767968.0000 - val_rmse: 22710.5254\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 482194400.0000 - rmse: 21958.9258 - val_loss: 485179168.0000 - val_rmse: 22026.7832\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 431037440.0000 - rmse: 20761.4414 - val_loss: 488998432.0000 - val_rmse: 22113.3086\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 451056896.0000 - rmse: 21238.0996 - val_loss: 452883616.0000 - val_rmse: 21281.0625\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 472259584.0000 - rmse: 21731.5352 - val_loss: 594213888.0000 - val_rmse: 24376.5020\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 448493056.0000 - rmse: 21177.6543 - val_loss: 516269152.0000 - val_rmse: 22721.5566\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 465236480.0000 - rmse: 21569.3418 - val_loss: 578288064.0000 - val_rmse: 24047.6211\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 484129632.0000 - rmse: 22002.9453 - val_loss: 468724096.0000 - val_rmse: 21650.0371\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 481887520.0000 - rmse: 21951.9375 - val_loss: 468259328.0000 - val_rmse: 21639.3008\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 465224928.0000 - rmse: 21569.0742 - val_loss: 508457760.0000 - val_rmse: 22549.0078\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 471223200.0000 - rmse: 21707.6758 - val_loss: 501476768.0000 - val_rmse: 22393.6777\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 437582048.0000 - rmse: 20918.4609 - val_loss: 461163520.0000 - val_rmse: 21474.7188\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 461029088.0000 - rmse: 21471.5879 - val_loss: 491850656.0000 - val_rmse: 22177.7070\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 447176640.0000 - rmse: 21146.5508 - val_loss: 441744224.0000 - val_rmse: 21017.7129\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 473658816.0000 - rmse: 21763.7031 - val_loss: 451552032.0000 - val_rmse: 21249.7539\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 455067360.0000 - rmse: 21332.3086 - val_loss: 433292832.0000 - val_rmse: 20815.6875\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 474967488.0000 - rmse: 21793.7480 - val_loss: 512991328.0000 - val_rmse: 22649.3125\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 441389792.0000 - rmse: 21009.2793 - val_loss: 491406144.0000 - val_rmse: 22167.6816\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 471116640.0000 - rmse: 21705.2207 - val_loss: 450145280.0000 - val_rmse: 21216.6270\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 422193760.0000 - rmse: 20547.3535 - val_loss: 445638624.0000 - val_rmse: 21110.1543\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 437261824.0000 - rmse: 20910.8066 - val_loss: 433945376.0000 - val_rmse: 20831.3555\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 399559232.0000 - rmse: 19988.9785 - val_loss: 458456480.0000 - val_rmse: 21411.5977\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 404094176.0000 - rmse: 20102.0938 - val_loss: 438908800.0000 - val_rmse: 20950.1504\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 436723904.0000 - rmse: 20897.9395 - val_loss: 535167456.0000 - val_rmse: 23133.6875\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 421157632.0000 - rmse: 20522.1250 - val_loss: 427073248.0000 - val_rmse: 20665.7500\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 418303648.0000 - rmse: 20452.4727 - val_loss: 435639584.0000 - val_rmse: 20871.9805\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 417370240.0000 - rmse: 20429.6406 - val_loss: 521789888.0000 - val_rmse: 22842.7207\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 409539136.0000 - rmse: 20237.0742 - val_loss: 465869888.0000 - val_rmse: 21584.0195\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 442859200.0000 - rmse: 21044.2207 - val_loss: 490119296.0000 - val_rmse: 22138.6387\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 447021376.0000 - rmse: 21142.8809 - val_loss: 426901696.0000 - val_rmse: 20661.5996\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 414877248.0000 - rmse: 20368.5352 - val_loss: 408730432.0000 - val_rmse: 20217.0820\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 470307872.0000 - rmse: 21686.5820 - val_loss: 556328256.0000 - val_rmse: 23586.6113\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 428536096.0000 - rmse: 20701.1133 - val_loss: 461757472.0000 - val_rmse: 21488.5430\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 434092128.0000 - rmse: 20834.8770 - val_loss: 446059296.0000 - val_rmse: 21120.1152\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 428990688.0000 - rmse: 20712.0898 - val_loss: 455035424.0000 - val_rmse: 21331.5586\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 363619712.0000 - rmse: 19068.8145 - val_loss: 469368736.0000 - val_rmse: 21664.9199\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 390365632.0000 - rmse: 19757.6719 - val_loss: 491070016.0000 - val_rmse: 22160.0996\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 427861760.0000 - rmse: 20684.8203 - val_loss: 469035008.0000 - val_rmse: 21657.2168\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 411380736.0000 - rmse: 20282.5234 - val_loss: 454107296.0000 - val_rmse: 21309.7930\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 386340192.0000 - rmse: 19655.5391 - val_loss: 461714144.0000 - val_rmse: 21487.5352\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 384888032.0000 - rmse: 19618.5625 - val_loss: 430015008.0000 - val_rmse: 20736.8027\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 403408064.0000 - rmse: 20085.0215 - val_loss: 414905184.0000 - val_rmse: 20369.2207\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 363788032.0000 - rmse: 19073.2285 - val_loss: 424814176.0000 - val_rmse: 20611.0215\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 383664736.0000 - rmse: 19587.3613 - val_loss: 452948448.0000 - val_rmse: 21282.5859\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 404663680.0000 - rmse: 20116.2539 - val_loss: 431062720.0000 - val_rmse: 20762.0508\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 402853312.0000 - rmse: 20071.2051 - val_loss: 434730976.0000 - val_rmse: 20850.2031\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 403214496.0000 - rmse: 20080.2012 - val_loss: 478795456.0000 - val_rmse: 21881.3945\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 397350688.0000 - rmse: 19933.6562 - val_loss: 475488000.0000 - val_rmse: 21805.6875\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 390853792.0000 - rmse: 19770.0234 - val_loss: 527889888.0000 - val_rmse: 22975.8535\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 406355136.0000 - rmse: 20158.2520 - val_loss: 411401920.0000 - val_rmse: 20283.0449\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 387204192.0000 - rmse: 19677.5039 - val_loss: 403786176.0000 - val_rmse: 20094.4316\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 414667616.0000 - rmse: 20363.3887 - val_loss: 433707680.0000 - val_rmse: 20825.6504\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 403703264.0000 - rmse: 20092.3691 - val_loss: 419090048.0000 - val_rmse: 20471.6895\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 374724384.0000 - rmse: 19357.7988 - val_loss: 399246560.0000 - val_rmse: 19981.1543\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 398562592.0000 - rmse: 19964.0332 - val_loss: 408581920.0000 - val_rmse: 20213.4102\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 391008416.0000 - rmse: 19773.9336 - val_loss: 416868928.0000 - val_rmse: 20417.3691\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 388882880.0000 - rmse: 19720.1133 - val_loss: 422183104.0000 - val_rmse: 20547.0957\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 398811136.0000 - rmse: 19970.2559 - val_loss: 420986528.0000 - val_rmse: 20517.9570\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 387583680.0000 - rmse: 19687.1445 - val_loss: 426049120.0000 - val_rmse: 20640.9570\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 374857120.0000 - rmse: 19361.2266 - val_loss: 411248832.0000 - val_rmse: 20279.2715\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 366895616.0000 - rmse: 19154.5195 - val_loss: 399674688.0000 - val_rmse: 19991.8652\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 396566272.0000 - rmse: 19913.9727 - val_loss: 410763456.0000 - val_rmse: 20267.3008\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 383306592.0000 - rmse: 19578.2168 - val_loss: 423770304.0000 - val_rmse: 20585.6816\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 415994368.0000 - rmse: 20395.9395 - val_loss: 457324032.0000 - val_rmse: 21385.1367\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 375925248.0000 - rmse: 19388.7910 - val_loss: 393393952.0000 - val_rmse: 19834.1621\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 343440576.0000 - rmse: 18532.1504 - val_loss: 381182528.0000 - val_rmse: 19523.8965\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 375898976.0000 - rmse: 19388.1152 - val_loss: 428296192.0000 - val_rmse: 20695.3184\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 367825504.0000 - rmse: 19178.7773 - val_loss: 396083232.0000 - val_rmse: 19901.8398\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 369223808.0000 - rmse: 19215.1973 - val_loss: 397409568.0000 - val_rmse: 19935.1348\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 357917312.0000 - rmse: 18918.7031 - val_loss: 446647520.0000 - val_rmse: 21134.0371\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 400257408.0000 - rmse: 20006.4336 - val_loss: 420557024.0000 - val_rmse: 20507.4863\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 364495424.0000 - rmse: 19091.7637 - val_loss: 406227808.0000 - val_rmse: 20155.0938\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 348282080.0000 - rmse: 18662.3164 - val_loss: 475184416.0000 - val_rmse: 21798.7246\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 354685312.0000 - rmse: 18833.0918 - val_loss: 480801024.0000 - val_rmse: 21927.1758\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 379225632.0000 - rmse: 19473.7168 - val_loss: 450230976.0000 - val_rmse: 21218.6465\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 416019680.0000 - rmse: 20396.5605 - val_loss: 415980256.0000 - val_rmse: 20395.5938\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 376326176.0000 - rmse: 19399.1289 - val_loss: 408664288.0000 - val_rmse: 20215.4473\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 341608928.0000 - rmse: 18482.6660 - val_loss: 422327904.0000 - val_rmse: 20550.6172\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 381307040.0000 - rmse: 19527.0840 - val_loss: 426076352.0000 - val_rmse: 20641.6172\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 305699680.0000 - rmse: 17484.2695 - val_loss: 429857600.0000 - val_rmse: 20733.0078\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 354212352.0000 - rmse: 18820.5293 - val_loss: 443570112.0000 - val_rmse: 21061.1035\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 363603936.0000 - rmse: 19068.4023 - val_loss: 389557280.0000 - val_rmse: 19737.2051\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 331336416.0000 - rmse: 18202.6484 - val_loss: 389781024.0000 - val_rmse: 19742.8730\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 321561600.0000 - rmse: 17932.1387 - val_loss: 430196864.0000 - val_rmse: 20741.1875\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 387971488.0000 - rmse: 19696.9922 - val_loss: 397607968.0000 - val_rmse: 19940.1094\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 373989184.0000 - rmse: 19338.8008 - val_loss: 446250688.0000 - val_rmse: 21124.6465\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 357379968.0000 - rmse: 18904.4961 - val_loss: 393288064.0000 - val_rmse: 19831.4922\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 350997504.0000 - rmse: 18734.9277 - val_loss: 399488320.0000 - val_rmse: 19987.2031\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 360819744.0000 - rmse: 18995.2559 - val_loss: 394508448.0000 - val_rmse: 19862.2363\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 348232096.0000 - rmse: 18660.9785 - val_loss: 380707360.0000 - val_rmse: 19511.7246\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 348720256.0000 - rmse: 18674.0527 - val_loss: 374637248.0000 - val_rmse: 19355.5488\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 365317056.0000 - rmse: 19113.2695 - val_loss: 370533152.0000 - val_rmse: 19249.2383\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 346408832.0000 - rmse: 18612.0605 - val_loss: 398483360.0000 - val_rmse: 19962.0488\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 322743968.0000 - rmse: 17965.0762 - val_loss: 396086208.0000 - val_rmse: 19901.9141\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 327872352.0000 - rmse: 18107.2461 - val_loss: 423630784.0000 - val_rmse: 20582.2930\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 336115264.0000 - rmse: 18333.4473 - val_loss: 404862464.0000 - val_rmse: 20121.1953\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 346734944.0000 - rmse: 18620.8203 - val_loss: 398090496.0000 - val_rmse: 19952.2051\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 366449888.0000 - rmse: 19142.8809 - val_loss: 401084384.0000 - val_rmse: 20027.0918\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 346390304.0000 - rmse: 18611.5645 - val_loss: 398402656.0000 - val_rmse: 19960.0273\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 328661056.0000 - rmse: 18129.0117 - val_loss: 398567392.0000 - val_rmse: 19964.1523\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 327074080.0000 - rmse: 18085.1895 - val_loss: 396357760.0000 - val_rmse: 19908.7363\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 329872480.0000 - rmse: 18162.3926 - val_loss: 414161888.0000 - val_rmse: 20350.9668\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 346071392.0000 - rmse: 18602.9941 - val_loss: 388957088.0000 - val_rmse: 19721.9941\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 314296160.0000 - rmse: 17728.4004 - val_loss: 406191872.0000 - val_rmse: 20154.2031\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 315132224.0000 - rmse: 17751.9648 - val_loss: 414317664.0000 - val_rmse: 20354.7949\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299425216.0000 - rmse: 17303.9082 - val_loss: 422539840.0000 - val_rmse: 20555.7734\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 334527264.0000 - rmse: 18290.0859 - val_loss: 423636352.0000 - val_rmse: 20582.4277\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 391626176.0000 - rmse: 19789.5469 - val_loss: 415066848.0000 - val_rmse: 20373.1895\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 334898048.0000 - rmse: 18300.2207 - val_loss: 424861568.0000 - val_rmse: 20612.1699\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 321557728.0000 - rmse: 17932.0312 - val_loss: 464371200.0000 - val_rmse: 21549.2734\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 317070080.0000 - rmse: 17806.4609 - val_loss: 410948960.0000 - val_rmse: 20271.8770\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 316828512.0000 - rmse: 17799.6777 - val_loss: 444950784.0000 - val_rmse: 21093.8574\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 313562496.0000 - rmse: 17707.6953 - val_loss: 402897760.0000 - val_rmse: 20072.3125\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 300839712.0000 - rmse: 17344.7324 - val_loss: 383606432.0000 - val_rmse: 19585.8730\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 322046176.0000 - rmse: 17945.6445 - val_loss: 377386944.0000 - val_rmse: 19426.4492\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299021120.0000 - rmse: 17292.2266 - val_loss: 391494560.0000 - val_rmse: 19786.2207\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 326545856.0000 - rmse: 18070.5801 - val_loss: 412556960.0000 - val_rmse: 20311.4980\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309274304.0000 - rmse: 17586.1973 - val_loss: 434146592.0000 - val_rmse: 20836.1855\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 324146464.0000 - rmse: 18004.0684 - val_loss: 430343040.0000 - val_rmse: 20744.7109\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 308659744.0000 - rmse: 17568.7148 - val_loss: 437153888.0000 - val_rmse: 20908.2246\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286563968.0000 - rmse: 16928.2012 - val_loss: 508511616.0000 - val_rmse: 22550.2012\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 304859808.0000 - rmse: 17460.2344 - val_loss: 429152032.0000 - val_rmse: 20715.9844\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 331625088.0000 - rmse: 18210.5762 - val_loss: 404072832.0000 - val_rmse: 20101.5625\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 328771360.0000 - rmse: 18132.0527 - val_loss: 406293056.0000 - val_rmse: 20156.7129\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 320416800.0000 - rmse: 17900.1895 - val_loss: 396120576.0000 - val_rmse: 19902.7773\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 322704736.0000 - rmse: 17963.9844 - val_loss: 428742528.0000 - val_rmse: 20706.0996\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 300803552.0000 - rmse: 17343.6895 - val_loss: 447680096.0000 - val_rmse: 21158.4512\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 289223168.0000 - rmse: 17006.5625 - val_loss: 407432352.0000 - val_rmse: 20184.9531\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 316178784.0000 - rmse: 17781.4160 - val_loss: 398932416.0000 - val_rmse: 19973.2930\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 349643808.0000 - rmse: 18698.7656 - val_loss: 437830976.0000 - val_rmse: 20924.4102\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 293175520.0000 - rmse: 17122.3691 - val_loss: 378717728.0000 - val_rmse: 19460.6719\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 348741504.0000 - rmse: 18674.6211 - val_loss: 378058112.0000 - val_rmse: 19443.7168\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 311837888.0000 - rmse: 17658.9316 - val_loss: 386492800.0000 - val_rmse: 19659.4199\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 307191616.0000 - rmse: 17526.8828 - val_loss: 375977792.0000 - val_rmse: 19390.1465\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 319662432.0000 - rmse: 17879.1055 - val_loss: 370875616.0000 - val_rmse: 19258.1309\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 287199424.0000 - rmse: 16946.9590 - val_loss: 437294944.0000 - val_rmse: 20911.5977\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 278052896.0000 - rmse: 16674.9180 - val_loss: 388377088.0000 - val_rmse: 19707.2852\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 300262464.0000 - rmse: 17328.0840 - val_loss: 397078016.0000 - val_rmse: 19926.8164\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 279872544.0000 - rmse: 16729.3926 - val_loss: 387946976.0000 - val_rmse: 19696.3691\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 279970592.0000 - rmse: 16732.3223 - val_loss: 444570784.0000 - val_rmse: 21084.8477\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 296377088.0000 - rmse: 17215.6055 - val_loss: 424454432.0000 - val_rmse: 20602.2910\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 358710016.0000 - rmse: 18939.6406 - val_loss: 338496192.0000 - val_rmse: 18398.2656\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 313026112.0000 - rmse: 17692.5449 - val_loss: 382484576.0000 - val_rmse: 19557.2129\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 300399488.0000 - rmse: 17332.0371 - val_loss: 366118496.0000 - val_rmse: 19134.2227\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 301469184.0000 - rmse: 17362.8672 - val_loss: 366202880.0000 - val_rmse: 19136.4277\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 295374624.0000 - rmse: 17186.4668 - val_loss: 394372064.0000 - val_rmse: 19858.8027\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 289872608.0000 - rmse: 17025.6465 - val_loss: 506225248.0000 - val_rmse: 22499.4492\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 288730944.0000 - rmse: 16992.0840 - val_loss: 428226432.0000 - val_rmse: 20693.6328\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 267948224.0000 - rmse: 16369.1240 - val_loss: 394572992.0000 - val_rmse: 19863.8613\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268253136.0000 - rmse: 16378.4355 - val_loss: 352094688.0000 - val_rmse: 18764.1855\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299499072.0000 - rmse: 17306.0410 - val_loss: 412975392.0000 - val_rmse: 20321.7969\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 273704288.0000 - rmse: 16544.0098 - val_loss: 340791360.0000 - val_rmse: 18460.5352\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 279109568.0000 - rmse: 16706.5723 - val_loss: 360863584.0000 - val_rmse: 18996.4102\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 278784960.0000 - rmse: 16696.8555 - val_loss: 366947104.0000 - val_rmse: 19155.8633\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 262827216.0000 - rmse: 16211.9463 - val_loss: 405566400.0000 - val_rmse: 20138.6797\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 298862720.0000 - rmse: 17287.6465 - val_loss: 389212736.0000 - val_rmse: 19728.4746\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 282203520.0000 - rmse: 16798.9141 - val_loss: 392724992.0000 - val_rmse: 19817.2910\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 295602464.0000 - rmse: 17193.0938 - val_loss: 439056352.0000 - val_rmse: 20953.6719\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 303071072.0000 - rmse: 17408.9375 - val_loss: 415396064.0000 - val_rmse: 20381.2676\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268247120.0000 - rmse: 16378.2510 - val_loss: 408866080.0000 - val_rmse: 20220.4375\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 288386240.0000 - rmse: 16981.9395 - val_loss: 361550720.0000 - val_rmse: 19014.4863\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 274598976.0000 - rmse: 16571.0273 - val_loss: 399526848.0000 - val_rmse: 19988.1680\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 300856640.0000 - rmse: 17345.2188 - val_loss: 391700896.0000 - val_rmse: 19791.4355\n",
      "Epoch 367/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 291890496.0000 - rmse: 17084.8027 - val_loss: 353328800.0000 - val_rmse: 18797.0430\n",
      "Epoch 368/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 273383872.0000 - rmse: 16534.3242 - val_loss: 372737088.0000 - val_rmse: 19306.4004\n",
      "Epoch 369/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 283450688.0000 - rmse: 16835.9941 - val_loss: 539723968.0000 - val_rmse: 23231.9609\n",
      "Epoch 370/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268014752.0000 - rmse: 16371.1562 - val_loss: 399969056.0000 - val_rmse: 19999.2266\n",
      "Epoch 371/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 287854112.0000 - rmse: 16966.2637 - val_loss: 398612224.0000 - val_rmse: 19965.2754\n",
      "Epoch 372/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 262864160.0000 - rmse: 16213.0859 - val_loss: 386212096.0000 - val_rmse: 19652.2793\n",
      "Epoch 373/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 272537440.0000 - rmse: 16508.7070 - val_loss: 373044992.0000 - val_rmse: 19314.3730\n",
      "Epoch 374/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 279109088.0000 - rmse: 16706.5586 - val_loss: 406466080.0000 - val_rmse: 20161.0039\n",
      "Epoch 375/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 261708592.0000 - rmse: 16177.4102 - val_loss: 501391776.0000 - val_rmse: 22391.7793\n",
      "Epoch 376/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 283154240.0000 - rmse: 16827.1875 - val_loss: 399469088.0000 - val_rmse: 19986.7227\n",
      "Epoch 377/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 283064864.0000 - rmse: 16824.5312 - val_loss: 400854976.0000 - val_rmse: 20021.3633\n",
      "Epoch 378/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 267646880.0000 - rmse: 16359.9170 - val_loss: 371984992.0000 - val_rmse: 19286.9121\n",
      "Epoch 379/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 273734848.0000 - rmse: 16544.9336 - val_loss: 468807072.0000 - val_rmse: 21651.9531\n",
      "Epoch 380/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 267951248.0000 - rmse: 16369.2168 - val_loss: 455406784.0000 - val_rmse: 21340.2617\n",
      "Epoch 381/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 272514784.0000 - rmse: 16508.0215 - val_loss: 349800064.0000 - val_rmse: 18702.9434\n",
      "Epoch 382/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309711840.0000 - rmse: 17598.6309 - val_loss: 397613536.0000 - val_rmse: 19940.2500\n",
      "Epoch 383/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 246948144.0000 - rmse: 15714.5840 - val_loss: 404906304.0000 - val_rmse: 20122.2832\n",
      "Epoch 384/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 314616096.0000 - rmse: 17737.4199 - val_loss: 372667808.0000 - val_rmse: 19304.6055\n",
      "Epoch 385/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 250710208.0000 - rmse: 15833.8311 - val_loss: 382232704.0000 - val_rmse: 19550.7734\n",
      "Epoch 386/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 237522432.0000 - rmse: 15411.7627 - val_loss: 366304416.0000 - val_rmse: 19139.0801\n",
      "Epoch 387/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264238784.0000 - rmse: 16255.4229 - val_loss: 374370080.0000 - val_rmse: 19348.6445\n",
      "Epoch 388/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 263229520.0000 - rmse: 16224.3496 - val_loss: 366347200.0000 - val_rmse: 19140.1992\n",
      "Epoch 389/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 287966624.0000 - rmse: 16969.5801 - val_loss: 491073408.0000 - val_rmse: 22160.1758\n",
      "Epoch 390/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264952320.0000 - rmse: 16277.3564 - val_loss: 543785728.0000 - val_rmse: 23319.2129\n",
      "Epoch 391/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 271211872.0000 - rmse: 16468.5117 - val_loss: 389433280.0000 - val_rmse: 19734.0645\n",
      "Epoch 392/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299654272.0000 - rmse: 17310.5254 - val_loss: 368231712.0000 - val_rmse: 19189.3652\n",
      "Epoch 393/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 259495568.0000 - rmse: 16108.8662 - val_loss: 365946048.0000 - val_rmse: 19129.7168\n",
      "Epoch 394/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268151888.0000 - rmse: 16375.3438 - val_loss: 365306976.0000 - val_rmse: 19113.0059\n",
      "Epoch 395/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264923680.0000 - rmse: 16276.4766 - val_loss: 454375712.0000 - val_rmse: 21316.0898\n",
      "Epoch 396/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286550368.0000 - rmse: 16927.7988 - val_loss: 382416800.0000 - val_rmse: 19555.4805\n",
      "Epoch 397/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309683584.0000 - rmse: 17597.8281 - val_loss: 368464288.0000 - val_rmse: 19195.4238\n",
      "Epoch 398/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 256407696.0000 - rmse: 16012.7354 - val_loss: 346103872.0000 - val_rmse: 18603.8672\n",
      "Epoch 399/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 294019104.0000 - rmse: 17146.9844 - val_loss: 405866880.0000 - val_rmse: 20146.1387\n",
      "Epoch 400/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 247307616.0000 - rmse: 15726.0176 - val_loss: 344357696.0000 - val_rmse: 18556.8770\n",
      "104/104 [==============================] - 0s 825us/step - loss: 331810080.0000 - rmse: 18215.6543\n",
      "[331810080.0, 18215.654296875]\n",
      "[19458.44921875, 24002.62109375, 32562.18359375, 24491.04296875, 18215.654296875]\n",
      "23745.990234375\n"
     ]
    }
   ],
   "source": [
    "k_fold(\"emb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Result of Embedding Model and Generate the CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[157251.16  248304.06  211173.38  160709.7    70034.586  95465.78\n",
      "  44185.54   48945.723  77350.85  118380.88 ] (5000,)\n"
     ]
    }
   ],
   "source": [
    "predict(emb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the tree, forest and gbr model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16589, 123) (16589,)\n",
      "(13271, 123) (13271,) (3318, 123) (3318,)\n",
      "26756.44249876791\n",
      "[245900. 220000. 568700.  72400.  94600.  57800.  55000.  77300. 146100.\n",
      " 193500. 153800.  85600.  62500. 150500.  21800.]\n",
      "[241800. 212500. 600600.  71300.  94400.  59400.  55900.  77600. 151800.\n",
      " 183500. 152900.  83000.  59200. 150500.  13100.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_get_tree_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16589, 123) (16589,)\n",
      "(13271, 123) (13271,) (3318, 123) (3318,)\n",
      "20067.248762992844\n",
      "[249884.5 217825.5 573435.   72138.   95572.5  58415.   55594.   77332.5\n",
      " 149681.  189431.5 162305.5  83155.5  64260.5 149858.   26892.5]\n",
      "[241800. 212500. 600600.  71300.  94400.  59400.  55900.  77600. 151800.\n",
      " 183500. 152900.  83000.  59200. 150500.  13100.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=200, random_state=2021)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_get_forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16589, 16) (16589,)\n",
      "(13271, 16) (13271,) (3318, 16) (3318,)\n",
      "19469.488896465802\n",
      "[239583.76920316 222004.89179006 592907.7816322   72684.21467127\n",
      "  92124.27032083  56445.28929688  60161.47767012  77568.97335866\n",
      " 156096.07412775 188863.3778439  161407.15030768  81823.85263659\n",
      "  57713.88842893 144417.81857786  60444.92828361]\n",
      "[241800. 212500. 600600.  71300.  94400.  59400.  55900.  77600. 151800.\n",
      " 183500. 152900.  83000.  59200. 150500.  13100.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.05, max_depth=8, max_features='sqrt',\n",
       "                          min_samples_leaf=16, min_samples_split=8,\n",
       "                          n_estimators=200, random_state=2021)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_get_gbr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the result when combine different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16589, 123) (16589,)\n",
      "(13271, 123) (13271,) (3318, 123) (3318,)\n",
      "(16589, 16) (16589,)\n",
      "(13271, 16) (13271,) (3318, 16) (3318,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_55 (Dense)             (None, 128)               15872     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,873\n",
      "Trainable params: 20,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "19517.8371547344 20740.037688226887 21457.881819561913 18640.43221127609\n",
      "[248892.60986731 218237.01425062 584946.97940544  71929.05231169\n",
      "  94113.40043441  59369.88566951  56870.13951577  77918.49880804\n",
      " 154239.52272792 189435.20819867] [241800. 212500. 600600.  71300.  94400.  59400.  55900.  77600. 151800.\n",
      " 183500.]\n",
      "[254468.53289103 220751.3964604  600089.84801814  72294.78635145\n",
      "  91956.62384387  59051.77905461  59449.02364631  77442.27727678\n",
      " 155753.50805139 189767.63149558]\n",
      "[247183.5 218035.5 574298.   72156.   95581.   58244.5  55501.5  77388.5\n",
      " 149463.  188536.5]\n",
      "[245992.5   216502.36  581576.56   71484.54   94630.28   60452.504\n",
      "  55962.453  78673.164 156686.42  189859.92 ]\n"
     ]
    }
   ],
   "source": [
    "get_blend_model(baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16589, 123) (16589,)\n",
      "(13271, 123) (13271,) (3318, 123) (3318,)\n",
      "(16589, 16) (16589,)\n",
      "(13271, 16) (13271,) (3318, 16) (3318,)\n",
      "Model: \"emb_model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_61 (Dense)             multiple                  860       \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             multiple                  39        \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             multiple                  4         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             multiple                  36        \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             multiple                  156       \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             multiple                  3840      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             multiple                  4128      \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             multiple                  528       \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             multiple                  272       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             multiple                  68        \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             multiple                  5         \n",
      "=================================================================\n",
      "Total params: 9,936\n",
      "Trainable params: 9,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "19517.8371547344 20740.037688226887 20119.658126584123 18171.255540167906\n",
      "[248092.34424231 214637.60800062 598989.63565544  72619.20856169\n",
      "  92967.23637191  58887.31926326  57627.33287514  79022.77419866\n",
      " 155064.29616542 187270.87226117] [241800. 212500. 600600.  71300.  94400.  59400.  55900.  77600. 151800.\n",
      " 183500.]\n",
      "[254468.53289103 220751.3964604  600089.84801814  72294.78635145\n",
      "  91956.62384387  59051.77905461  59449.02364631  77442.27727678\n",
      " 155753.50805139 189767.63149558]\n",
      "[247183.5 218035.5 574298.   72156.   95581.   58244.5  55501.5  77388.5\n",
      " 149463.  188536.5]\n",
      "[243991.83  207503.84  616683.2    73209.93   91764.875  59246.086\n",
      "  57855.438  81433.85  158748.36  184449.08 ]\n"
     ]
    }
   ],
   "source": [
    "get_blend_model(emb_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c468ecac6db731323e72de7cfd0c8c9743bf341e8fe46d77b9f6f4da3ba35254"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('media': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
