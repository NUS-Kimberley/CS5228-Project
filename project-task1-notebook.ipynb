{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import train_and_get_deep_learning_model, predict, check_result, train_and_get_tree_model, train_and_get_forest, train_and_get_gbr, k_fold, get_ensemble_model, grid_search\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run k-fold for baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 26000998400.0000 - rmse: 161248.2500 - val_loss: 22052696064.0000 - val_rmse: 148501.5000\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 19512244224.0000 - rmse: 139686.2344 - val_loss: 7671927808.0000 - val_rmse: 87589.5391\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 4290976512.0000 - rmse: 65505.5469 - val_loss: 2159055616.0000 - val_rmse: 46465.6406\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 2670384896.0000 - rmse: 51675.7656 - val_loss: 1653729792.0000 - val_rmse: 40666.0781\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 2255241216.0000 - rmse: 47489.3789 - val_loss: 1396337920.0000 - val_rmse: 37367.6055\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2087958400.0000 - rmse: 45694.1836 - val_loss: 1248311296.0000 - val_rmse: 35331.4492\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1913190656.0000 - rmse: 43740.0352 - val_loss: 1159184128.0000 - val_rmse: 34046.7930\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1888238848.0000 - rmse: 43453.8711 - val_loss: 1125417344.0000 - val_rmse: 33547.2422\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1797387264.0000 - rmse: 42395.6055 - val_loss: 1051706816.0000 - val_rmse: 32430.0293\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1753002368.0000 - rmse: 41868.8711 - val_loss: 994212992.0000 - val_rmse: 31531.1426\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1702148352.0000 - rmse: 41257.1016 - val_loss: 952890368.0000 - val_rmse: 30868.9219\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1690540928.0000 - rmse: 41116.1875 - val_loss: 922839616.0000 - val_rmse: 30378.2754\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1628062336.0000 - rmse: 40349.2539 - val_loss: 877108480.0000 - val_rmse: 29616.0176\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1625552128.0000 - rmse: 40318.1367 - val_loss: 856874752.0000 - val_rmse: 29272.4238\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1590369024.0000 - rmse: 39879.4297 - val_loss: 826463424.0000 - val_rmse: 28748.2773\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1546546816.0000 - rmse: 39326.1602 - val_loss: 800299008.0000 - val_rmse: 28289.5566\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1538352256.0000 - rmse: 39221.8320 - val_loss: 785504896.0000 - val_rmse: 28026.8594\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1492876160.0000 - rmse: 38637.7539 - val_loss: 763333504.0000 - val_rmse: 27628.4902\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1517807232.0000 - rmse: 38959.0469 - val_loss: 768848704.0000 - val_rmse: 27728.1211\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1471700864.0000 - rmse: 38362.7539 - val_loss: 739321856.0000 - val_rmse: 27190.4727\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1429421696.0000 - rmse: 37807.6953 - val_loss: 724819392.0000 - val_rmse: 26922.4707\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1449544064.0000 - rmse: 38072.8789 - val_loss: 719989248.0000 - val_rmse: 26832.6152\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1413225728.0000 - rmse: 37592.8945 - val_loss: 715138496.0000 - val_rmse: 26742.0742\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1377198336.0000 - rmse: 37110.6211 - val_loss: 713394560.0000 - val_rmse: 26709.4473\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1410719232.0000 - rmse: 37559.5430 - val_loss: 703249152.0000 - val_rmse: 26518.8457\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1289320832.0000 - rmse: 35907.1133 - val_loss: 741174400.0000 - val_rmse: 27224.5176\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1377488384.0000 - rmse: 37114.5312 - val_loss: 715794240.0000 - val_rmse: 26754.3320\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1388412672.0000 - rmse: 37261.4102 - val_loss: 689838912.0000 - val_rmse: 26264.7852\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1297475456.0000 - rmse: 36020.4883 - val_loss: 684563904.0000 - val_rmse: 26164.1719\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1306564864.0000 - rmse: 36146.4336 - val_loss: 682972032.0000 - val_rmse: 26133.7344\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1319457408.0000 - rmse: 36324.3359 - val_loss: 681299904.0000 - val_rmse: 26101.7227\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1324471040.0000 - rmse: 36393.2812 - val_loss: 680483584.0000 - val_rmse: 26086.0801\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1314732416.0000 - rmse: 36259.2383 - val_loss: 682085824.0000 - val_rmse: 26116.7734\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1350714112.0000 - rmse: 36752.0625 - val_loss: 682694592.0000 - val_rmse: 26128.4258\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1279516160.0000 - rmse: 35770.3242 - val_loss: 676891776.0000 - val_rmse: 26017.1445\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1264819968.0000 - rmse: 35564.3086 - val_loss: 718009472.0000 - val_rmse: 26795.6992\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1349039360.0000 - rmse: 36729.2695 - val_loss: 679189824.0000 - val_rmse: 26061.2715\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1291887616.0000 - rmse: 35942.8398 - val_loss: 677375424.0000 - val_rmse: 26026.4375\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1239040768.0000 - rmse: 35200.0117 - val_loss: 685294656.0000 - val_rmse: 26178.1328\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1259066752.0000 - rmse: 35483.3320 - val_loss: 685495040.0000 - val_rmse: 26181.9609\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1236797696.0000 - rmse: 35168.1328 - val_loss: 706247232.0000 - val_rmse: 26575.3125\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1229070848.0000 - rmse: 35058.1055 - val_loss: 696034240.0000 - val_rmse: 26382.4609\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1265413888.0000 - rmse: 35572.6562 - val_loss: 702380352.0000 - val_rmse: 26502.4590\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1224176640.0000 - rmse: 34988.2344 - val_loss: 685127808.0000 - val_rmse: 26174.9453\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1252217856.0000 - rmse: 35386.6914 - val_loss: 717191616.0000 - val_rmse: 26780.4336\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1196401792.0000 - rmse: 34589.0430 - val_loss: 681839872.0000 - val_rmse: 26112.0645\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1244347904.0000 - rmse: 35275.3164 - val_loss: 691817792.0000 - val_rmse: 26302.4297\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1188812928.0000 - rmse: 34479.1680 - val_loss: 692764480.0000 - val_rmse: 26320.4199\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1220822144.0000 - rmse: 34940.2656 - val_loss: 684206336.0000 - val_rmse: 26157.3379\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1220007680.0000 - rmse: 34928.6094 - val_loss: 683197248.0000 - val_rmse: 26138.0430\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1157673088.0000 - rmse: 34024.5938 - val_loss: 693229248.0000 - val_rmse: 26329.2461\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1155597824.0000 - rmse: 33994.0859 - val_loss: 688559552.0000 - val_rmse: 26240.4180\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1189374720.0000 - rmse: 34487.3125 - val_loss: 690324992.0000 - val_rmse: 26274.0371\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1163972480.0000 - rmse: 34117.0391 - val_loss: 704458752.0000 - val_rmse: 26541.6426\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1130281728.0000 - rmse: 33619.6641 - val_loss: 696593856.0000 - val_rmse: 26393.0645\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1132939904.0000 - rmse: 33659.1719 - val_loss: 701142656.0000 - val_rmse: 26479.0977\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1166272128.0000 - rmse: 34150.7266 - val_loss: 700907008.0000 - val_rmse: 26474.6484\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1127747712.0000 - rmse: 33581.9570 - val_loss: 679583680.0000 - val_rmse: 26068.8262\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1118562560.0000 - rmse: 33444.9180 - val_loss: 686012096.0000 - val_rmse: 26191.8320\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1164084352.0000 - rmse: 34118.6797 - val_loss: 682657408.0000 - val_rmse: 26127.7129\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1068496960.0000 - rmse: 32687.8711 - val_loss: 688290432.0000 - val_rmse: 26235.2891\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1113002752.0000 - rmse: 33361.6953 - val_loss: 683474944.0000 - val_rmse: 26143.3535\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1138057088.0000 - rmse: 33735.1016 - val_loss: 681140672.0000 - val_rmse: 26098.6719\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1131953920.0000 - rmse: 33644.5234 - val_loss: 678803392.0000 - val_rmse: 26053.8555\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1160939264.0000 - rmse: 34072.5586 - val_loss: 699917376.0000 - val_rmse: 26455.9512\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1056844288.0000 - rmse: 32509.1426 - val_loss: 688836352.0000 - val_rmse: 26245.6914\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1093405952.0000 - rmse: 33066.6914 - val_loss: 689883520.0000 - val_rmse: 26265.6348\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1109236736.0000 - rmse: 33305.2070 - val_loss: 714202752.0000 - val_rmse: 26724.5723\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1097482752.0000 - rmse: 33128.2773 - val_loss: 773181952.0000 - val_rmse: 27806.1504\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1085912064.0000 - rmse: 32953.1797 - val_loss: 730918272.0000 - val_rmse: 27035.5000\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1071112576.0000 - rmse: 32727.8555 - val_loss: 707191616.0000 - val_rmse: 26593.0742\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1013812608.0000 - rmse: 31840.4238 - val_loss: 692741312.0000 - val_rmse: 26319.9785\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1043962496.0000 - rmse: 32310.4082 - val_loss: 703849856.0000 - val_rmse: 26530.1660\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1061890816.0000 - rmse: 32586.6660 - val_loss: 709073280.0000 - val_rmse: 26628.4297\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1078951936.0000 - rmse: 32847.4023 - val_loss: 720058624.0000 - val_rmse: 26833.9082\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1062333440.0000 - rmse: 32593.4570 - val_loss: 721172736.0000 - val_rmse: 26854.6582\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1044724544.0000 - rmse: 32322.1992 - val_loss: 702886528.0000 - val_rmse: 26512.0059\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1008713408.0000 - rmse: 31760.2480 - val_loss: 712386240.0000 - val_rmse: 26690.5645\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1022790464.0000 - rmse: 31981.0957 - val_loss: 726429248.0000 - val_rmse: 26952.3516\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1050156544.0000 - rmse: 32406.1191 - val_loss: 716805568.0000 - val_rmse: 26773.2246\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 981781056.0000 - rmse: 31333.3828 - val_loss: 720164416.0000 - val_rmse: 26835.8789\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 948132672.0000 - rmse: 30791.7637 - val_loss: 702649344.0000 - val_rmse: 26507.5332\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 974242304.0000 - rmse: 31212.8555 - val_loss: 727635456.0000 - val_rmse: 26974.7188\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1018754432.0000 - rmse: 31917.9336 - val_loss: 714486592.0000 - val_rmse: 26729.8809\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 977267520.0000 - rmse: 31261.2773 - val_loss: 720458688.0000 - val_rmse: 26841.3613\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1058366592.0000 - rmse: 32532.5469 - val_loss: 739872000.0000 - val_rmse: 27200.5879\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 969045888.0000 - rmse: 31129.5020 - val_loss: 764254144.0000 - val_rmse: 27645.1465\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 962730752.0000 - rmse: 31027.9023 - val_loss: 763536704.0000 - val_rmse: 27632.1680\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 861678976.0000 - rmse: 29354.3691 - val_loss: 849282112.0000 - val_rmse: 29142.4453\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 954613376.0000 - rmse: 30896.8184 - val_loss: 744750144.0000 - val_rmse: 27290.1113\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 923032512.0000 - rmse: 30381.4492 - val_loss: 731623296.0000 - val_rmse: 27048.5352\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 944369728.0000 - rmse: 30730.5996 - val_loss: 779028736.0000 - val_rmse: 27911.0859\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 975744320.0000 - rmse: 31236.9062 - val_loss: 743667840.0000 - val_rmse: 27270.2734\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 928129216.0000 - rmse: 30465.2129 - val_loss: 735569024.0000 - val_rmse: 27121.3750\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 929723712.0000 - rmse: 30491.3711 - val_loss: 774459264.0000 - val_rmse: 27829.1074\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 928018176.0000 - rmse: 30463.3906 - val_loss: 757681344.0000 - val_rmse: 27526.0117\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 934560640.0000 - rmse: 30570.5840 - val_loss: 735607232.0000 - val_rmse: 27122.0801\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 912984064.0000 - rmse: 30215.6270 - val_loss: 848083008.0000 - val_rmse: 29121.8652\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 915895552.0000 - rmse: 30263.7656 - val_loss: 826659776.0000 - val_rmse: 28751.6914\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 899012800.0000 - rmse: 29983.5430 - val_loss: 731927168.0000 - val_rmse: 27054.1523\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 871953088.0000 - rmse: 29528.8516 - val_loss: 779142400.0000 - val_rmse: 27913.1230\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 846260864.0000 - rmse: 29090.5625 - val_loss: 758027008.0000 - val_rmse: 27532.2910\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 886501376.0000 - rmse: 29774.1738 - val_loss: 794599744.0000 - val_rmse: 28188.6465\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 885473344.0000 - rmse: 29756.9043 - val_loss: 732219456.0000 - val_rmse: 27059.5547\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 834751744.0000 - rmse: 28892.0684 - val_loss: 782537472.0000 - val_rmse: 27973.8691\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800805440.0000 - rmse: 28298.5039 - val_loss: 811302848.0000 - val_rmse: 28483.3789\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 797089408.0000 - rmse: 28232.7715 - val_loss: 778336960.0000 - val_rmse: 27898.6914\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 891438080.0000 - rmse: 29856.9609 - val_loss: 788478720.0000 - val_rmse: 28079.8613\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 817090752.0000 - rmse: 28584.7988 - val_loss: 783874880.0000 - val_rmse: 27997.7656\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 884981824.0000 - rmse: 29748.6445 - val_loss: 886035072.0000 - val_rmse: 29766.3398\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 808316160.0000 - rmse: 28430.9023 - val_loss: 834252864.0000 - val_rmse: 28883.4355\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 843561536.0000 - rmse: 29044.1289 - val_loss: 789197760.0000 - val_rmse: 28092.6641\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 817523456.0000 - rmse: 28592.3672 - val_loss: 711044096.0000 - val_rmse: 26665.4082\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 752686464.0000 - rmse: 27435.1328 - val_loss: 696327488.0000 - val_rmse: 26388.0176\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 728896768.0000 - rmse: 26998.0879 - val_loss: 735769792.0000 - val_rmse: 27125.0762\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 750760832.0000 - rmse: 27400.0156 - val_loss: 865604992.0000 - val_rmse: 29421.1660\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 755353344.0000 - rmse: 27483.6914 - val_loss: 827800640.0000 - val_rmse: 28771.5254\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 789770880.0000 - rmse: 28102.8613 - val_loss: 767605120.0000 - val_rmse: 27705.6875\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 706192832.0000 - rmse: 26574.2871 - val_loss: 696610176.0000 - val_rmse: 26393.3730\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 751523456.0000 - rmse: 27413.9277 - val_loss: 711982144.0000 - val_rmse: 26682.9941\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 735804480.0000 - rmse: 27125.7168 - val_loss: 767912000.0000 - val_rmse: 27711.2246\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 738904960.0000 - rmse: 27182.8066 - val_loss: 813176000.0000 - val_rmse: 28516.2402\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 764176832.0000 - rmse: 27643.7480 - val_loss: 753768064.0000 - val_rmse: 27454.8359\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730461440.0000 - rmse: 27027.0508 - val_loss: 770020992.0000 - val_rmse: 27749.2500\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 729297472.0000 - rmse: 27005.5078 - val_loss: 772528640.0000 - val_rmse: 27794.4004\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 688430208.0000 - rmse: 26237.9531 - val_loss: 852471424.0000 - val_rmse: 29197.1113\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 711751360.0000 - rmse: 26678.6680 - val_loss: 1004238592.0000 - val_rmse: 31689.7246\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 678585856.0000 - rmse: 26049.6797 - val_loss: 847638528.0000 - val_rmse: 29114.2324\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 658121472.0000 - rmse: 25653.8770 - val_loss: 884095488.0000 - val_rmse: 29733.7441\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692596096.0000 - rmse: 26317.2188 - val_loss: 864382656.0000 - val_rmse: 29400.3848\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 681023872.0000 - rmse: 26096.4336 - val_loss: 795444224.0000 - val_rmse: 28203.6211\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 682477952.0000 - rmse: 26124.2773 - val_loss: 727686144.0000 - val_rmse: 26975.6582\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 698670656.0000 - rmse: 26432.3770 - val_loss: 674376960.0000 - val_rmse: 25968.7676\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 639462784.0000 - rmse: 25287.6016 - val_loss: 771588352.0000 - val_rmse: 27777.4785\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 672725696.0000 - rmse: 25936.9570 - val_loss: 687913216.0000 - val_rmse: 26228.0996\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 698110144.0000 - rmse: 26421.7715 - val_loss: 823965760.0000 - val_rmse: 28704.8008\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 734722560.0000 - rmse: 27105.7637 - val_loss: 679329920.0000 - val_rmse: 26063.9590\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 634045824.0000 - rmse: 25180.2656 - val_loss: 963948288.0000 - val_rmse: 31047.5176\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 658012992.0000 - rmse: 25651.7617 - val_loss: 678410816.0000 - val_rmse: 26046.3203\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 641217536.0000 - rmse: 25322.2734 - val_loss: 706933504.0000 - val_rmse: 26588.2207\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 582738624.0000 - rmse: 24139.9785 - val_loss: 629813952.0000 - val_rmse: 25096.0918\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 587576192.0000 - rmse: 24239.9707 - val_loss: 758117184.0000 - val_rmse: 27533.9258\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 585463744.0000 - rmse: 24196.3574 - val_loss: 627824896.0000 - val_rmse: 25056.4336\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 619609472.0000 - rmse: 24891.9551 - val_loss: 625020864.0000 - val_rmse: 25000.4160\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564858240.0000 - rmse: 23766.7461 - val_loss: 708968768.0000 - val_rmse: 26626.4668\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 659742336.0000 - rmse: 25685.4492 - val_loss: 710244096.0000 - val_rmse: 26650.4043\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 618278208.0000 - rmse: 24865.1973 - val_loss: 714724096.0000 - val_rmse: 26734.3223\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 514795936.0000 - rmse: 22689.1152 - val_loss: 825099072.0000 - val_rmse: 28724.5371\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 550876288.0000 - rmse: 23470.7539 - val_loss: 782823552.0000 - val_rmse: 27978.9844\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 577096320.0000 - rmse: 24022.8301 - val_loss: 818242880.0000 - val_rmse: 28604.9453\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 552847296.0000 - rmse: 23512.7031 - val_loss: 601957632.0000 - val_rmse: 24534.8242\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 545485440.0000 - rmse: 23355.6289 - val_loss: 811321728.0000 - val_rmse: 28483.7090\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 527236032.0000 - rmse: 22961.6211 - val_loss: 703820224.0000 - val_rmse: 26529.6094\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 552000000.0000 - rmse: 23494.6797 - val_loss: 840541760.0000 - val_rmse: 28992.0977\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 554821952.0000 - rmse: 23554.6582 - val_loss: 601457088.0000 - val_rmse: 24524.6211\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 504783968.0000 - rmse: 22467.3945 - val_loss: 834433664.0000 - val_rmse: 28886.5645\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 467432640.0000 - rmse: 21620.1895 - val_loss: 841316928.0000 - val_rmse: 29005.4629\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465592832.0000 - rmse: 21577.5996 - val_loss: 788513472.0000 - val_rmse: 28080.4824\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490365152.0000 - rmse: 22144.1895 - val_loss: 929483712.0000 - val_rmse: 30487.4355\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 521525952.0000 - rmse: 22836.9434 - val_loss: 857746944.0000 - val_rmse: 29287.3145\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470307712.0000 - rmse: 21686.5781 - val_loss: 642787008.0000 - val_rmse: 25353.2441\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467247936.0000 - rmse: 21615.9141 - val_loss: 832695936.0000 - val_rmse: 28856.4707\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 461921440.0000 - rmse: 21492.3574 - val_loss: 712827264.0000 - val_rmse: 26698.8242\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 471155008.0000 - rmse: 21706.1055 - val_loss: 652643968.0000 - val_rmse: 25546.8965\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412044736.0000 - rmse: 20298.8848 - val_loss: 886724352.0000 - val_rmse: 29777.9180\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 510384928.0000 - rmse: 22591.6992 - val_loss: 1021842432.0000 - val_rmse: 31966.2695\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510474464.0000 - rmse: 22593.6816 - val_loss: 792562944.0000 - val_rmse: 28152.4941\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446466048.0000 - rmse: 21129.7441 - val_loss: 731028672.0000 - val_rmse: 27037.5410\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470612640.0000 - rmse: 21693.6074 - val_loss: 1058289152.0000 - val_rmse: 32531.3555\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479190976.0000 - rmse: 21890.4316 - val_loss: 733856768.0000 - val_rmse: 27089.7891\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428382208.0000 - rmse: 20697.3945 - val_loss: 767614528.0000 - val_rmse: 27705.8574\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420285632.0000 - rmse: 20500.8672 - val_loss: 716740032.0000 - val_rmse: 26772.0000\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 444996000.0000 - rmse: 21094.9258 - val_loss: 645350016.0000 - val_rmse: 25403.7383\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438819872.0000 - rmse: 20948.0273 - val_loss: 1029431872.0000 - val_rmse: 32084.7617\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461414816.0000 - rmse: 21480.5684 - val_loss: 794886400.0000 - val_rmse: 28193.7305\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479038368.0000 - rmse: 21886.9453 - val_loss: 707357248.0000 - val_rmse: 26596.1895\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470021376.0000 - rmse: 21679.9746 - val_loss: 840679808.0000 - val_rmse: 28994.4785\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 449904832.0000 - rmse: 21210.9609 - val_loss: 1124710144.0000 - val_rmse: 33536.6992\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486604640.0000 - rmse: 22059.1172 - val_loss: 750964096.0000 - val_rmse: 27403.7246\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 467757088.0000 - rmse: 21627.6914 - val_loss: 805070144.0000 - val_rmse: 28373.7578\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 425618304.0000 - rmse: 20630.5176 - val_loss: 680283008.0000 - val_rmse: 26082.2363\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 436902720.0000 - rmse: 20902.2168 - val_loss: 920360320.0000 - val_rmse: 30337.4414\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 400980480.0000 - rmse: 20024.4961 - val_loss: 678080768.0000 - val_rmse: 26039.9844\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455597472.0000 - rmse: 21344.7266 - val_loss: 764287104.0000 - val_rmse: 27645.7422\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385810112.0000 - rmse: 19642.0488 - val_loss: 742018112.0000 - val_rmse: 27240.0098\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456208736.0000 - rmse: 21359.0430 - val_loss: 783261952.0000 - val_rmse: 27986.8184\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407169280.0000 - rmse: 20178.4355 - val_loss: 1203372416.0000 - val_rmse: 34689.6602\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401296448.0000 - rmse: 20032.3848 - val_loss: 982707136.0000 - val_rmse: 31348.1602\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440921120.0000 - rmse: 20998.1211 - val_loss: 741800256.0000 - val_rmse: 27236.0098\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448524736.0000 - rmse: 21178.4023 - val_loss: 987409984.0000 - val_rmse: 31423.0801\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 419329024.0000 - rmse: 20477.5234 - val_loss: 998280704.0000 - val_rmse: 31595.5801\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 389770144.0000 - rmse: 19742.5977 - val_loss: 1023072512.0000 - val_rmse: 31985.5039\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 396990784.0000 - rmse: 19924.6270 - val_loss: 788255872.0000 - val_rmse: 28075.8945\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375476448.0000 - rmse: 19377.2148 - val_loss: 960838912.0000 - val_rmse: 30997.4004\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 397091840.0000 - rmse: 19927.1641 - val_loss: 752487040.0000 - val_rmse: 27431.4980\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 348554432.0000 - rmse: 18669.6133 - val_loss: 1047943424.0000 - val_rmse: 32371.9551\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362135008.0000 - rmse: 19029.8438 - val_loss: 1091924736.0000 - val_rmse: 33044.2812\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323759424.0000 - rmse: 17993.3145 - val_loss: 963804544.0000 - val_rmse: 31045.2012\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373053536.0000 - rmse: 19314.5918 - val_loss: 769601216.0000 - val_rmse: 27741.6875\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 368878528.0000 - rmse: 19206.2090 - val_loss: 540773248.0000 - val_rmse: 23254.5312\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 377376800.0000 - rmse: 19426.1895 - val_loss: 704861888.0000 - val_rmse: 26549.2344\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 357240480.0000 - rmse: 18900.8066 - val_loss: 674484928.0000 - val_rmse: 25970.8477\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 371966624.0000 - rmse: 19286.4355 - val_loss: 942530688.0000 - val_rmse: 30700.6621\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383755712.0000 - rmse: 19589.6836 - val_loss: 543503488.0000 - val_rmse: 23313.1621\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398831808.0000 - rmse: 19970.7734 - val_loss: 905461376.0000 - val_rmse: 30090.8848\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341746560.0000 - rmse: 18486.3887 - val_loss: 1133173120.0000 - val_rmse: 33662.6367\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 382240096.0000 - rmse: 19550.9609 - val_loss: 588183872.0000 - val_rmse: 24252.5020\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365528512.0000 - rmse: 19118.8008 - val_loss: 672815296.0000 - val_rmse: 25938.6816\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 378659552.0000 - rmse: 19459.1758 - val_loss: 903684928.0000 - val_rmse: 30061.3535\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340025760.0000 - rmse: 18439.7871 - val_loss: 810300160.0000 - val_rmse: 28465.7715\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357280160.0000 - rmse: 18901.8555 - val_loss: 638105920.0000 - val_rmse: 25260.7578\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367910816.0000 - rmse: 19181.0000 - val_loss: 787457600.0000 - val_rmse: 28061.6758\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332272736.0000 - rmse: 18228.3496 - val_loss: 1193971072.0000 - val_rmse: 34553.8867\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 318560384.0000 - rmse: 17848.2598 - val_loss: 630938624.0000 - val_rmse: 25118.4902\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 406735584.0000 - rmse: 20167.6855 - val_loss: 795257856.0000 - val_rmse: 28200.3164\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 308800896.0000 - rmse: 17572.7305 - val_loss: 854180352.0000 - val_rmse: 29226.3633\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 332077760.0000 - rmse: 18223.0000 - val_loss: 716849216.0000 - val_rmse: 26774.0391\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352269824.0000 - rmse: 18768.8516 - val_loss: 911295680.0000 - val_rmse: 30187.6738\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318955360.0000 - rmse: 17859.3203 - val_loss: 853186816.0000 - val_rmse: 29209.3613\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335407360.0000 - rmse: 18314.1309 - val_loss: 614551872.0000 - val_rmse: 24790.1562\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325103200.0000 - rmse: 18030.6191 - val_loss: 1003153920.0000 - val_rmse: 31672.6055\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291149440.0000 - rmse: 17063.0996 - val_loss: 831522752.0000 - val_rmse: 28836.1348\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333447232.0000 - rmse: 18260.5371 - val_loss: 975180352.0000 - val_rmse: 31227.8770\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 332715456.0000 - rmse: 18240.4883 - val_loss: 773627968.0000 - val_rmse: 27814.1680\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317513344.0000 - rmse: 17818.9023 - val_loss: 733280064.0000 - val_rmse: 27079.1426\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297109536.0000 - rmse: 17236.8633 - val_loss: 1144565376.0000 - val_rmse: 33831.4258\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 382721184.0000 - rmse: 19563.2598 - val_loss: 996593280.0000 - val_rmse: 31568.8652\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 324329984.0000 - rmse: 18009.1621 - val_loss: 911575808.0000 - val_rmse: 30192.3145\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295494720.0000 - rmse: 17189.9570 - val_loss: 962953024.0000 - val_rmse: 31031.4844\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 328406048.0000 - rmse: 18121.9766 - val_loss: 1239930496.0000 - val_rmse: 35212.6484\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317497472.0000 - rmse: 17818.4590 - val_loss: 747568512.0000 - val_rmse: 27341.6992\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 317443232.0000 - rmse: 17816.9355 - val_loss: 1448827136.0000 - val_rmse: 38063.4609\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 309295104.0000 - rmse: 17586.7852 - val_loss: 1107166720.0000 - val_rmse: 33274.1133\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 342588032.0000 - rmse: 18509.1309 - val_loss: 1167901312.0000 - val_rmse: 34174.5703\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 292209312.0000 - rmse: 17094.1289 - val_loss: 984642752.0000 - val_rmse: 31379.0176\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328027392.0000 - rmse: 18111.5234 - val_loss: 744845184.0000 - val_rmse: 27291.8516\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 286892640.0000 - rmse: 16937.9062 - val_loss: 1217098112.0000 - val_rmse: 34886.9336\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 343846464.0000 - rmse: 18543.0977 - val_loss: 1505932416.0000 - val_rmse: 38806.3438\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339427104.0000 - rmse: 18423.5469 - val_loss: 919417408.0000 - val_rmse: 30321.8965\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 319962912.0000 - rmse: 17887.5039 - val_loss: 922308288.0000 - val_rmse: 30369.5293\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304262944.0000 - rmse: 17443.1328 - val_loss: 1535390976.0000 - val_rmse: 39184.0664\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307531744.0000 - rmse: 17536.5820 - val_loss: 1477797248.0000 - val_rmse: 38442.1250\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 262101408.0000 - rmse: 16189.5449 - val_loss: 1026242688.0000 - val_rmse: 32035.0234\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 314345216.0000 - rmse: 17729.7812 - val_loss: 1423034368.0000 - val_rmse: 37723.1250\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295287232.0000 - rmse: 17183.9238 - val_loss: 1124455936.0000 - val_rmse: 33532.9102\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281560992.0000 - rmse: 16779.7773 - val_loss: 1217841280.0000 - val_rmse: 34897.5820\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330054720.0000 - rmse: 18167.4082 - val_loss: 1274154368.0000 - val_rmse: 35695.3008\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267824128.0000 - rmse: 16365.3320 - val_loss: 1023625408.0000 - val_rmse: 31994.1465\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288827040.0000 - rmse: 16994.9121 - val_loss: 1169409536.0000 - val_rmse: 34196.6289\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273596288.0000 - rmse: 16540.7461 - val_loss: 1177846784.0000 - val_rmse: 34319.7695\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300661856.0000 - rmse: 17339.6035 - val_loss: 1340293760.0000 - val_rmse: 36610.0234\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292590336.0000 - rmse: 17105.2715 - val_loss: 1244681984.0000 - val_rmse: 35280.0508\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 271305888.0000 - rmse: 16471.3652 - val_loss: 1506003072.0000 - val_rmse: 38807.2539\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273717120.0000 - rmse: 16544.3984 - val_loss: 1170770688.0000 - val_rmse: 34216.5234\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254133248.0000 - rmse: 15941.5566 - val_loss: 821072128.0000 - val_rmse: 28654.3555\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246209984.0000 - rmse: 15691.0781 - val_loss: 1509118336.0000 - val_rmse: 38847.3711\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287168672.0000 - rmse: 16946.0508 - val_loss: 813624768.0000 - val_rmse: 28524.1074\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228516752.0000 - rmse: 15116.7705 - val_loss: 1561031424.0000 - val_rmse: 39509.8906\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262748048.0000 - rmse: 16209.5039 - val_loss: 1275563648.0000 - val_rmse: 35715.0352\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254930144.0000 - rmse: 15966.5312 - val_loss: 1321356032.0000 - val_rmse: 36350.4570\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265473152.0000 - rmse: 16293.3457 - val_loss: 1411082112.0000 - val_rmse: 37564.3711\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263081696.0000 - rmse: 16219.7930 - val_loss: 1213309568.0000 - val_rmse: 34832.5898\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293820480.0000 - rmse: 17141.1914 - val_loss: 882382016.0000 - val_rmse: 29704.9160\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302673472.0000 - rmse: 17397.5117 - val_loss: 1147681280.0000 - val_rmse: 33877.4453\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268567232.0000 - rmse: 16388.0195 - val_loss: 1240008704.0000 - val_rmse: 35213.7578\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286481248.0000 - rmse: 16925.7559 - val_loss: 1764661760.0000 - val_rmse: 42007.8789\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 340022112.0000 - rmse: 18439.6875 - val_loss: 1734165760.0000 - val_rmse: 41643.3164\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249193600.0000 - rmse: 15785.8652 - val_loss: 1481014912.0000 - val_rmse: 38483.9570\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 271021824.0000 - rmse: 16462.7402 - val_loss: 1190044544.0000 - val_rmse: 34497.0234\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 274036352.0000 - rmse: 16554.0430 - val_loss: 1007937984.0000 - val_rmse: 31748.0391\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 242692480.0000 - rmse: 15578.5908 - val_loss: 1481158272.0000 - val_rmse: 38485.8203\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267205168.0000 - rmse: 16346.4111 - val_loss: 982620864.0000 - val_rmse: 31346.7832\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246002560.0000 - rmse: 15684.4678 - val_loss: 1404023168.0000 - val_rmse: 37470.2930\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304276544.0000 - rmse: 17443.5234 - val_loss: 1339248000.0000 - val_rmse: 36595.7383\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278044224.0000 - rmse: 16674.6562 - val_loss: 1029464640.0000 - val_rmse: 32085.2695\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246061376.0000 - rmse: 15686.3418 - val_loss: 1112812288.0000 - val_rmse: 33358.8398\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 235411456.0000 - rmse: 15343.1221 - val_loss: 1716662528.0000 - val_rmse: 41432.6250\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266219104.0000 - rmse: 16316.2207 - val_loss: 1352754432.0000 - val_rmse: 36779.8086\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237991952.0000 - rmse: 15426.9863 - val_loss: 1819508224.0000 - val_rmse: 42655.6953\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266852144.0000 - rmse: 16335.6074 - val_loss: 1457922176.0000 - val_rmse: 38182.7461\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271404608.0000 - rmse: 16474.3613 - val_loss: 1925940224.0000 - val_rmse: 43885.5352\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 271039744.0000 - rmse: 16463.2852 - val_loss: 1327158784.0000 - val_rmse: 36430.1914\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261062960.0000 - rmse: 16157.4404 - val_loss: 1389059840.0000 - val_rmse: 37270.0938\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 270458368.0000 - rmse: 16445.6152 - val_loss: 1446557568.0000 - val_rmse: 38033.6367\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 259258832.0000 - rmse: 16101.5146 - val_loss: 1316533376.0000 - val_rmse: 36284.0664\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234326512.0000 - rmse: 15307.7266 - val_loss: 1862621824.0000 - val_rmse: 43158.1016\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303050976.0000 - rmse: 17408.3594 - val_loss: 1658195712.0000 - val_rmse: 40720.9492\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249141008.0000 - rmse: 15784.1982 - val_loss: 1459187712.0000 - val_rmse: 38199.3164\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 213526240.0000 - rmse: 14612.5352 - val_loss: 1349227520.0000 - val_rmse: 36731.8320\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244550320.0000 - rmse: 15638.1035 - val_loss: 1442771712.0000 - val_rmse: 37983.8359\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253618176.0000 - rmse: 15925.3926 - val_loss: 1719003904.0000 - val_rmse: 41460.8672\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248795392.0000 - rmse: 15773.2461 - val_loss: 917354048.0000 - val_rmse: 30287.8535\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 225828912.0000 - rmse: 15027.6035 - val_loss: 1518933760.0000 - val_rmse: 38973.5000\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304606912.0000 - rmse: 17452.9922 - val_loss: 1296617728.0000 - val_rmse: 36008.5781\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244891440.0000 - rmse: 15649.0059 - val_loss: 1164231680.0000 - val_rmse: 34120.8359\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312629472.0000 - rmse: 17681.3301 - val_loss: 1460559360.0000 - val_rmse: 38217.2656\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270382592.0000 - rmse: 16443.3145 - val_loss: 1255924992.0000 - val_rmse: 35439.0312\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267320560.0000 - rmse: 16349.9395 - val_loss: 1132362752.0000 - val_rmse: 33650.5977\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 225083072.0000 - rmse: 15002.7676 - val_loss: 2137958144.0000 - val_rmse: 46238.0586\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253645360.0000 - rmse: 15926.2461 - val_loss: 1185445632.0000 - val_rmse: 34430.3008\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260896240.0000 - rmse: 16152.2812 - val_loss: 1048297856.0000 - val_rmse: 32377.4277\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241555520.0000 - rmse: 15542.0547 - val_loss: 1302411520.0000 - val_rmse: 36088.9375\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 228744464.0000 - rmse: 15124.2988 - val_loss: 1612423168.0000 - val_rmse: 40154.9883\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 258022992.0000 - rmse: 16063.0928 - val_loss: 927311424.0000 - val_rmse: 30451.7891\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224690848.0000 - rmse: 14989.6904 - val_loss: 2158621952.0000 - val_rmse: 46460.9688\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256420176.0000 - rmse: 16013.1230 - val_loss: 1875021184.0000 - val_rmse: 43301.5156\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 225648416.0000 - rmse: 15021.5977 - val_loss: 2041412992.0000 - val_rmse: 45182.0000\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252358000.0000 - rmse: 15885.7783 - val_loss: 1724724992.0000 - val_rmse: 41529.8086\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252244192.0000 - rmse: 15882.1963 - val_loss: 1716274048.0000 - val_rmse: 41427.9375\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242088720.0000 - rmse: 15559.1973 - val_loss: 1534815616.0000 - val_rmse: 39176.7227\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206712864.0000 - rmse: 14377.5117 - val_loss: 1275393792.0000 - val_rmse: 35712.6562\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230608416.0000 - rmse: 15185.7959 - val_loss: 1343252608.0000 - val_rmse: 36650.4102\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 195891456.0000 - rmse: 13996.1211 - val_loss: 1048384128.0000 - val_rmse: 32378.7598\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247302016.0000 - rmse: 15725.8389 - val_loss: 1748104320.0000 - val_rmse: 41810.3359\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288725472.0000 - rmse: 16991.9219 - val_loss: 1072264512.0000 - val_rmse: 32745.4492\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235889664.0000 - rmse: 15358.7002 - val_loss: 1909670784.0000 - val_rmse: 43699.7812\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236202416.0000 - rmse: 15368.8770 - val_loss: 1465481984.0000 - val_rmse: 38281.6133\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243897136.0000 - rmse: 15617.2051 - val_loss: 1283096576.0000 - val_rmse: 35820.3359\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 260627664.0000 - rmse: 16143.9648 - val_loss: 1191356288.0000 - val_rmse: 34516.0273\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 219966256.0000 - rmse: 14831.2578 - val_loss: 1348215680.0000 - val_rmse: 36718.0547\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 248550992.0000 - rmse: 15765.5000 - val_loss: 1437043456.0000 - val_rmse: 37908.3555\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 220566736.0000 - rmse: 14851.4873 - val_loss: 1491500800.0000 - val_rmse: 38619.9531\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 202738624.0000 - rmse: 14238.6299 - val_loss: 1026553728.0000 - val_rmse: 32039.8750\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 217610800.0000 - rmse: 14751.6367 - val_loss: 1670931072.0000 - val_rmse: 40877.0234\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219389760.0000 - rmse: 14811.8115 - val_loss: 1218926720.0000 - val_rmse: 34913.1289\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202142368.0000 - rmse: 14217.6748 - val_loss: 1568538240.0000 - val_rmse: 39604.7734\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 220537376.0000 - rmse: 14850.4980 - val_loss: 1048527616.0000 - val_rmse: 32380.9766\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 223839152.0000 - rmse: 14961.2539 - val_loss: 1550483072.0000 - val_rmse: 39376.1758\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 234881392.0000 - rmse: 15325.8389 - val_loss: 1432785536.0000 - val_rmse: 37852.1523\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211839792.0000 - rmse: 14554.7158 - val_loss: 956289344.0000 - val_rmse: 30923.9277\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 179726544.0000 - rmse: 13406.2119 - val_loss: 1568977536.0000 - val_rmse: 39610.3203\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 209557632.0000 - rmse: 14476.1025 - val_loss: 1241899904.0000 - val_rmse: 35240.6016\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243249312.0000 - rmse: 15596.4521 - val_loss: 1354067072.0000 - val_rmse: 36797.6484\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216210672.0000 - rmse: 14704.1025 - val_loss: 1108685312.0000 - val_rmse: 33296.9258\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267568304.0000 - rmse: 16357.5137 - val_loss: 1288861568.0000 - val_rmse: 35900.7188\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251859216.0000 - rmse: 15870.0713 - val_loss: 1388191872.0000 - val_rmse: 37258.4453\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240655680.0000 - rmse: 15513.0801 - val_loss: 1771207168.0000 - val_rmse: 42085.7109\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 197802656.0000 - rmse: 14064.2305 - val_loss: 1419733504.0000 - val_rmse: 37679.3516\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238470512.0000 - rmse: 15442.4902 - val_loss: 1200312704.0000 - val_rmse: 34645.5312\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 200465136.0000 - rmse: 14158.5693 - val_loss: 1170570496.0000 - val_rmse: 34213.6016\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224583472.0000 - rmse: 14986.1074 - val_loss: 1852860032.0000 - val_rmse: 43044.8594\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268787424.0000 - rmse: 16394.7383 - val_loss: 1151248000.0000 - val_rmse: 33930.0469\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 225635616.0000 - rmse: 15021.1699 - val_loss: 1969496320.0000 - val_rmse: 44379.0078\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 200502976.0000 - rmse: 14159.9053 - val_loss: 1674791680.0000 - val_rmse: 40924.2188\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 254959840.0000 - rmse: 15967.4609 - val_loss: 1367020544.0000 - val_rmse: 36973.2422\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 222196128.0000 - rmse: 14906.2441 - val_loss: 1070361728.0000 - val_rmse: 32716.3828\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 201914976.0000 - rmse: 14209.6777 - val_loss: 1047489408.0000 - val_rmse: 32364.9414\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265356256.0000 - rmse: 16289.7568 - val_loss: 1404074752.0000 - val_rmse: 37470.9844\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219094624.0000 - rmse: 14801.8447 - val_loss: 1270925696.0000 - val_rmse: 35650.0391\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228126848.0000 - rmse: 15103.8662 - val_loss: 1570027776.0000 - val_rmse: 39623.5742\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227387392.0000 - rmse: 15079.3691 - val_loss: 971652672.0000 - val_rmse: 31171.3398\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 195137808.0000 - rmse: 13969.1719 - val_loss: 1253593088.0000 - val_rmse: 35406.1172\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 203155312.0000 - rmse: 14253.2559 - val_loss: 2098739840.0000 - val_rmse: 45812.0039\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 181212000.0000 - rmse: 13461.4990 - val_loss: 876149632.0000 - val_rmse: 29599.8223\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224854288.0000 - rmse: 14995.1406 - val_loss: 1913582720.0000 - val_rmse: 43744.5156\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207812752.0000 - rmse: 14415.7100 - val_loss: 2188209920.0000 - val_rmse: 46778.3008\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217331248.0000 - rmse: 14742.1572 - val_loss: 1360375680.0000 - val_rmse: 36883.2695\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 198131120.0000 - rmse: 14075.9033 - val_loss: 1547237248.0000 - val_rmse: 39334.9336\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234515168.0000 - rmse: 15313.8857 - val_loss: 1469253760.0000 - val_rmse: 38330.8477\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 200163568.0000 - rmse: 14147.9160 - val_loss: 1327309056.0000 - val_rmse: 36432.2500\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217065696.0000 - rmse: 14733.1475 - val_loss: 1785427328.0000 - val_rmse: 42254.3164\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 194885840.0000 - rmse: 13960.1504 - val_loss: 1370930816.0000 - val_rmse: 37026.0820\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 233270848.0000 - rmse: 15273.2061 - val_loss: 1814247424.0000 - val_rmse: 42593.9844\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 193558368.0000 - rmse: 13912.5234 - val_loss: 1564003200.0000 - val_rmse: 39547.4805\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 186921328.0000 - rmse: 13671.9170 - val_loss: 1977474560.0000 - val_rmse: 44468.8047\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 208758432.0000 - rmse: 14448.4736 - val_loss: 1501763200.0000 - val_rmse: 38752.5898\n",
      "104/104 [==============================] - 0s 658us/step - loss: 400149056.0000 - rmse: 20003.7246\n",
      "[400149056.0, 20003.724609375]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 25328785408.0000 - rmse: 159150.2031 - val_loss: 18157766656.0000 - val_rmse: 134750.7500\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 10388385792.0000 - rmse: 101923.4297 - val_loss: 2453396224.0000 - val_rmse: 49531.7695\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 2691952896.0000 - rmse: 51884.0312 - val_loss: 1760165120.0000 - val_rmse: 41954.3203\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 2125508992.0000 - rmse: 46103.2422 - val_loss: 1410764544.0000 - val_rmse: 37560.1445\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1764407936.0000 - rmse: 42004.8555 - val_loss: 1285136768.0000 - val_rmse: 35848.8047\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1741526144.0000 - rmse: 41731.5977 - val_loss: 1204125312.0000 - val_rmse: 34700.5078\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1653108864.0000 - rmse: 40658.4414 - val_loss: 1153605760.0000 - val_rmse: 33964.7734\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1501168256.0000 - rmse: 38744.9141 - val_loss: 1083964032.0000 - val_rmse: 32923.6094\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1466973824.0000 - rmse: 38301.0938 - val_loss: 1047853760.0000 - val_rmse: 32370.5703\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1403409792.0000 - rmse: 37462.1133 - val_loss: 1012974464.0000 - val_rmse: 31827.2598\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1366596096.0000 - rmse: 36967.5000 - val_loss: 991297472.0000 - val_rmse: 31484.8770\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1301240448.0000 - rmse: 36072.7109 - val_loss: 970676864.0000 - val_rmse: 31155.6875\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1278742144.0000 - rmse: 35759.5039 - val_loss: 956785024.0000 - val_rmse: 30931.9414\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1261480704.0000 - rmse: 35517.3281 - val_loss: 940640896.0000 - val_rmse: 30669.8691\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1195901568.0000 - rmse: 34581.8086 - val_loss: 913283840.0000 - val_rmse: 30220.5859\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1207949312.0000 - rmse: 34755.5664 - val_loss: 961740864.0000 - val_rmse: 31011.9473\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1158703744.0000 - rmse: 34039.7383 - val_loss: 892494400.0000 - val_rmse: 29874.6445\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1160255744.0000 - rmse: 34062.5273 - val_loss: 883180416.0000 - val_rmse: 29718.3516\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1143883392.0000 - rmse: 33821.3438 - val_loss: 883392512.0000 - val_rmse: 29721.9199\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1091285760.0000 - rmse: 33034.6133 - val_loss: 863389312.0000 - val_rmse: 29383.4863\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1121443968.0000 - rmse: 33487.9688 - val_loss: 860911872.0000 - val_rmse: 29341.2988\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1077069184.0000 - rmse: 32818.7305 - val_loss: 854198464.0000 - val_rmse: 29226.6719\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1153373824.0000 - rmse: 33961.3594 - val_loss: 855775872.0000 - val_rmse: 29253.6465\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1092035712.0000 - rmse: 33045.9648 - val_loss: 841943552.0000 - val_rmse: 29016.2637\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1050048832.0000 - rmse: 32404.4570 - val_loss: 868020864.0000 - val_rmse: 29462.1934\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1056454080.0000 - rmse: 32503.1387 - val_loss: 831321792.0000 - val_rmse: 28832.6523\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1014462016.0000 - rmse: 31850.6211 - val_loss: 829144064.0000 - val_rmse: 28794.8613\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1069924928.0000 - rmse: 32709.7070 - val_loss: 842222656.0000 - val_rmse: 29021.0723\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1046730048.0000 - rmse: 32353.2070 - val_loss: 820225152.0000 - val_rmse: 28639.5723\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1069760448.0000 - rmse: 32707.1934 - val_loss: 817305856.0000 - val_rmse: 28588.5625\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1075098112.0000 - rmse: 32788.6875 - val_loss: 844636864.0000 - val_rmse: 29062.6367\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1048771968.0000 - rmse: 32384.7480 - val_loss: 807896704.0000 - val_rmse: 28423.5234\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1030956224.0000 - rmse: 32108.5078 - val_loss: 805320192.0000 - val_rmse: 28378.1641\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1011430976.0000 - rmse: 31803.0020 - val_loss: 828116928.0000 - val_rmse: 28777.0215\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 980828672.0000 - rmse: 31318.1836 - val_loss: 818811648.0000 - val_rmse: 28614.8848\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 979146944.0000 - rmse: 31291.3242 - val_loss: 797724736.0000 - val_rmse: 28244.0215\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 954193216.0000 - rmse: 30890.0176 - val_loss: 813844736.0000 - val_rmse: 28527.9648\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 984046400.0000 - rmse: 31369.5137 - val_loss: 791454272.0000 - val_rmse: 28132.7969\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 976869312.0000 - rmse: 31254.9082 - val_loss: 795062272.0000 - val_rmse: 28196.8477\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 974813376.0000 - rmse: 31222.0020 - val_loss: 792844800.0000 - val_rmse: 28157.5000\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 968581760.0000 - rmse: 31122.0469 - val_loss: 782675776.0000 - val_rmse: 27976.3438\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 930893504.0000 - rmse: 30510.5469 - val_loss: 801279552.0000 - val_rmse: 28306.8809\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 896674112.0000 - rmse: 29944.5176 - val_loss: 789004544.0000 - val_rmse: 28089.2246\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 954332672.0000 - rmse: 30892.2754 - val_loss: 776215680.0000 - val_rmse: 27860.6465\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 946833536.0000 - rmse: 30770.6602 - val_loss: 773199232.0000 - val_rmse: 27806.4609\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 876865088.0000 - rmse: 29611.9082 - val_loss: 782246720.0000 - val_rmse: 27968.6738\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 907233984.0000 - rmse: 30120.3242 - val_loss: 771150848.0000 - val_rmse: 27769.6016\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 947057792.0000 - rmse: 30774.3047 - val_loss: 764995712.0000 - val_rmse: 27658.5566\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 913625280.0000 - rmse: 30226.2344 - val_loss: 764280448.0000 - val_rmse: 27645.6230\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 889770368.0000 - rmse: 29829.0195 - val_loss: 819331200.0000 - val_rmse: 28623.9629\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 898928320.0000 - rmse: 29982.1328 - val_loss: 758977920.0000 - val_rmse: 27549.5547\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 845703104.0000 - rmse: 29080.9746 - val_loss: 757830400.0000 - val_rmse: 27528.7188\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 889453376.0000 - rmse: 29823.7051 - val_loss: 762682624.0000 - val_rmse: 27616.7090\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 888403456.0000 - rmse: 29806.0977 - val_loss: 746570944.0000 - val_rmse: 27323.4512\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 898725952.0000 - rmse: 29978.7578 - val_loss: 758306560.0000 - val_rmse: 27537.3672\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 890902016.0000 - rmse: 29847.9824 - val_loss: 736200192.0000 - val_rmse: 27133.0078\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 866779008.0000 - rmse: 29441.1113 - val_loss: 798963584.0000 - val_rmse: 28265.9434\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 915340864.0000 - rmse: 30254.6016 - val_loss: 741422016.0000 - val_rmse: 27229.0664\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 895066560.0000 - rmse: 29917.6621 - val_loss: 732802496.0000 - val_rmse: 27070.3223\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 879143360.0000 - rmse: 29650.3516 - val_loss: 729050880.0000 - val_rmse: 27000.9414\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 879454272.0000 - rmse: 29655.5938 - val_loss: 726232512.0000 - val_rmse: 26948.7012\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 838170368.0000 - rmse: 28951.1719 - val_loss: 729739136.0000 - val_rmse: 27013.6836\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 860753984.0000 - rmse: 29338.6094 - val_loss: 718614528.0000 - val_rmse: 26806.9863\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 851332608.0000 - rmse: 29177.6055 - val_loss: 715256128.0000 - val_rmse: 26744.2734\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 847869184.0000 - rmse: 29118.1934 - val_loss: 721795712.0000 - val_rmse: 26866.2559\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 876845312.0000 - rmse: 29611.5723 - val_loss: 710636928.0000 - val_rmse: 26657.7734\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 892781888.0000 - rmse: 29879.4551 - val_loss: 708090432.0000 - val_rmse: 26609.9688\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 837824384.0000 - rmse: 28945.1953 - val_loss: 700608064.0000 - val_rmse: 26469.0000\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 838241792.0000 - rmse: 28952.4062 - val_loss: 706121472.0000 - val_rmse: 26572.9453\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 841594240.0000 - rmse: 29010.2441 - val_loss: 700191168.0000 - val_rmse: 26461.1250\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 815921472.0000 - rmse: 28564.3398 - val_loss: 698933248.0000 - val_rmse: 26437.3457\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 774562368.0000 - rmse: 27830.9609 - val_loss: 708409344.0000 - val_rmse: 26615.9609\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 811475328.0000 - rmse: 28486.4062 - val_loss: 739970752.0000 - val_rmse: 27202.4043\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 886052032.0000 - rmse: 29766.6270 - val_loss: 685648384.0000 - val_rmse: 26184.8867\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 789809280.0000 - rmse: 28103.5449 - val_loss: 710710784.0000 - val_rmse: 26659.1602\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 809285504.0000 - rmse: 28447.9434 - val_loss: 692214272.0000 - val_rmse: 26309.9648\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 778210624.0000 - rmse: 27896.4258 - val_loss: 690347584.0000 - val_rmse: 26274.4668\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 786731456.0000 - rmse: 28048.7344 - val_loss: 699739840.0000 - val_rmse: 26452.5957\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 778971200.0000 - rmse: 27910.0547 - val_loss: 685766528.0000 - val_rmse: 26187.1426\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 782866048.0000 - rmse: 27979.7441 - val_loss: 682866048.0000 - val_rmse: 26131.7051\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 758984576.0000 - rmse: 27549.6738 - val_loss: 699625920.0000 - val_rmse: 26450.4434\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 770732864.0000 - rmse: 27762.0762 - val_loss: 673738368.0000 - val_rmse: 25956.4707\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 769813760.0000 - rmse: 27745.5176 - val_loss: 677952192.0000 - val_rmse: 26037.5156\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 718733952.0000 - rmse: 26809.2129 - val_loss: 674418304.0000 - val_rmse: 25969.5645\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 715856960.0000 - rmse: 26755.5039 - val_loss: 684528064.0000 - val_rmse: 26163.4863\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 723788480.0000 - rmse: 26903.3164 - val_loss: 673805440.0000 - val_rmse: 25957.7617\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 755387968.0000 - rmse: 27484.3223 - val_loss: 741467264.0000 - val_rmse: 27229.8965\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 729721024.0000 - rmse: 27013.3496 - val_loss: 724730304.0000 - val_rmse: 26920.8145\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 734038208.0000 - rmse: 27093.1387 - val_loss: 686836224.0000 - val_rmse: 26207.5605\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 685170560.0000 - rmse: 26175.7637 - val_loss: 670436224.0000 - val_rmse: 25892.7832\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 707666112.0000 - rmse: 26601.9941 - val_loss: 675757056.0000 - val_rmse: 25995.3281\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 703074048.0000 - rmse: 26515.5430 - val_loss: 669993664.0000 - val_rmse: 25884.2363\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 746632960.0000 - rmse: 27324.5859 - val_loss: 678968960.0000 - val_rmse: 26057.0332\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 675431616.0000 - rmse: 25989.0664 - val_loss: 665772416.0000 - val_rmse: 25802.5664\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 741742336.0000 - rmse: 27234.9473 - val_loss: 668445312.0000 - val_rmse: 25854.3086\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 694489792.0000 - rmse: 26353.1738 - val_loss: 672375168.0000 - val_rmse: 25930.1973\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 717173248.0000 - rmse: 26780.0898 - val_loss: 671666176.0000 - val_rmse: 25916.5234\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 686915200.0000 - rmse: 26209.0664 - val_loss: 670958848.0000 - val_rmse: 25902.8730\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 702107712.0000 - rmse: 26497.3145 - val_loss: 677821632.0000 - val_rmse: 26035.0059\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 696869440.0000 - rmse: 26398.2852 - val_loss: 694573184.0000 - val_rmse: 26354.7559\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 722416512.0000 - rmse: 26877.8066 - val_loss: 664945216.0000 - val_rmse: 25786.5312\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 715662912.0000 - rmse: 26751.8750 - val_loss: 656454400.0000 - val_rmse: 25621.3633\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 711712384.0000 - rmse: 26677.9375 - val_loss: 670553088.0000 - val_rmse: 25895.0391\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 654493248.0000 - rmse: 25583.0664 - val_loss: 661264960.0000 - val_rmse: 25715.0723\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 648086016.0000 - rmse: 25457.5332 - val_loss: 683687360.0000 - val_rmse: 26147.4160\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 696062912.0000 - rmse: 26383.0039 - val_loss: 661631360.0000 - val_rmse: 25722.1953\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 652474368.0000 - rmse: 25543.5762 - val_loss: 689873536.0000 - val_rmse: 26265.4434\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 619310144.0000 - rmse: 24885.9414 - val_loss: 700673024.0000 - val_rmse: 26470.2285\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 644462976.0000 - rmse: 25386.2754 - val_loss: 681806144.0000 - val_rmse: 26111.4180\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 706893632.0000 - rmse: 26587.4707 - val_loss: 660337472.0000 - val_rmse: 25697.0332\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 664076736.0000 - rmse: 25769.6855 - val_loss: 677915712.0000 - val_rmse: 26036.8145\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 668825664.0000 - rmse: 25861.6641 - val_loss: 657643648.0000 - val_rmse: 25644.5645\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 656967680.0000 - rmse: 25631.3809 - val_loss: 672958464.0000 - val_rmse: 25941.4414\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 633578624.0000 - rmse: 25170.9883 - val_loss: 687457856.0000 - val_rmse: 26219.4180\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 666556608.0000 - rmse: 25817.7578 - val_loss: 727711424.0000 - val_rmse: 26976.1270\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 630677888.0000 - rmse: 25113.3008 - val_loss: 657729856.0000 - val_rmse: 25646.2441\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 630801728.0000 - rmse: 25115.7656 - val_loss: 662196544.0000 - val_rmse: 25733.1777\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 658156480.0000 - rmse: 25654.5586 - val_loss: 671289856.0000 - val_rmse: 25909.2617\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 640464192.0000 - rmse: 25307.3945 - val_loss: 655323136.0000 - val_rmse: 25599.2793\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 660447168.0000 - rmse: 25699.1660 - val_loss: 669569472.0000 - val_rmse: 25876.0410\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 674433216.0000 - rmse: 25969.8516 - val_loss: 672749248.0000 - val_rmse: 25937.4082\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 598866304.0000 - rmse: 24471.7441 - val_loss: 677648704.0000 - val_rmse: 26031.6875\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 590758080.0000 - rmse: 24305.5156 - val_loss: 675698752.0000 - val_rmse: 25994.2051\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 649624704.0000 - rmse: 25487.7363 - val_loss: 656501184.0000 - val_rmse: 25622.2773\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 669895424.0000 - rmse: 25882.3379 - val_loss: 672586112.0000 - val_rmse: 25934.2637\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 669364672.0000 - rmse: 25872.0801 - val_loss: 664647872.0000 - val_rmse: 25780.7637\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 665251712.0000 - rmse: 25792.4746 - val_loss: 682537664.0000 - val_rmse: 26125.4219\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 672038720.0000 - rmse: 25923.7090 - val_loss: 662192576.0000 - val_rmse: 25733.1035\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 631356096.0000 - rmse: 25126.7988 - val_loss: 648870848.0000 - val_rmse: 25472.9414\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605734016.0000 - rmse: 24611.6641 - val_loss: 653890816.0000 - val_rmse: 25571.2871\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 597561856.0000 - rmse: 24445.0762 - val_loss: 690367872.0000 - val_rmse: 26274.8516\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 623566080.0000 - rmse: 24971.3047 - val_loss: 654679744.0000 - val_rmse: 25586.7090\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 546682304.0000 - rmse: 23381.2383 - val_loss: 667570304.0000 - val_rmse: 25837.3828\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 573442944.0000 - rmse: 23946.6680 - val_loss: 672999744.0000 - val_rmse: 25942.2383\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 604439552.0000 - rmse: 24585.3516 - val_loss: 643999616.0000 - val_rmse: 25377.1484\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 604768448.0000 - rmse: 24592.0410 - val_loss: 653499520.0000 - val_rmse: 25563.6367\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 590648064.0000 - rmse: 24303.2520 - val_loss: 656203136.0000 - val_rmse: 25616.4629\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574945664.0000 - rmse: 23978.0234 - val_loss: 649623040.0000 - val_rmse: 25487.7031\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610416448.0000 - rmse: 24706.6055 - val_loss: 690943936.0000 - val_rmse: 26285.8105\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 571673280.0000 - rmse: 23909.6895 - val_loss: 664284992.0000 - val_rmse: 25773.7266\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 596073984.0000 - rmse: 24414.6270 - val_loss: 673418944.0000 - val_rmse: 25950.3164\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582226688.0000 - rmse: 24129.3730 - val_loss: 659820416.0000 - val_rmse: 25686.9707\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 605925952.0000 - rmse: 24615.5625 - val_loss: 639462976.0000 - val_rmse: 25287.6035\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 537164992.0000 - rmse: 23176.8203 - val_loss: 663507584.0000 - val_rmse: 25758.6406\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 593390656.0000 - rmse: 24359.6113 - val_loss: 693556288.0000 - val_rmse: 26335.4570\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 566843776.0000 - rmse: 23808.4805 - val_loss: 664740736.0000 - val_rmse: 25782.5645\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 596563456.0000 - rmse: 24424.6465 - val_loss: 668394112.0000 - val_rmse: 25853.3184\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 552785024.0000 - rmse: 23511.3809 - val_loss: 664042560.0000 - val_rmse: 25769.0234\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 549206144.0000 - rmse: 23435.1484 - val_loss: 691048640.0000 - val_rmse: 26287.8027\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584571648.0000 - rmse: 24177.9160 - val_loss: 671142528.0000 - val_rmse: 25906.4180\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594200640.0000 - rmse: 24376.2305 - val_loss: 665737728.0000 - val_rmse: 25801.8945\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574968192.0000 - rmse: 23978.4941 - val_loss: 686642432.0000 - val_rmse: 26203.8633\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 576326336.0000 - rmse: 24006.7969 - val_loss: 751127104.0000 - val_rmse: 27406.6973\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509462048.0000 - rmse: 22571.2656 - val_loss: 639111040.0000 - val_rmse: 25280.6465\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 540924160.0000 - rmse: 23257.7754 - val_loss: 657028672.0000 - val_rmse: 25632.5703\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 565244416.0000 - rmse: 23774.8672 - val_loss: 684569344.0000 - val_rmse: 26164.2754\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560029824.0000 - rmse: 23664.9492 - val_loss: 694129856.0000 - val_rmse: 26346.3438\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544619264.0000 - rmse: 23337.0801 - val_loss: 675777792.0000 - val_rmse: 25995.7266\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578956352.0000 - rmse: 24061.5098 - val_loss: 690655424.0000 - val_rmse: 26280.3223\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 570886592.0000 - rmse: 23893.2324 - val_loss: 699227072.0000 - val_rmse: 26442.9004\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561434304.0000 - rmse: 23694.6035 - val_loss: 680308288.0000 - val_rmse: 26082.7207\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 518740000.0000 - rmse: 22775.8633 - val_loss: 734930432.0000 - val_rmse: 27109.5996\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512644704.0000 - rmse: 22641.6582 - val_loss: 673604608.0000 - val_rmse: 25953.8945\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 616978368.0000 - rmse: 24839.0488 - val_loss: 649907136.0000 - val_rmse: 25493.2754\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 530139360.0000 - rmse: 23024.7559 - val_loss: 655191232.0000 - val_rmse: 25596.7031\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530939488.0000 - rmse: 23042.1230 - val_loss: 663150656.0000 - val_rmse: 25751.7109\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508161312.0000 - rmse: 22542.4336 - val_loss: 689248576.0000 - val_rmse: 26253.5449\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542578496.0000 - rmse: 23293.3145 - val_loss: 703060288.0000 - val_rmse: 26515.2832\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473008096.0000 - rmse: 21748.7500 - val_loss: 677013568.0000 - val_rmse: 26019.4824\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516486144.0000 - rmse: 22726.3320 - val_loss: 693726656.0000 - val_rmse: 26338.6914\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532581280.0000 - rmse: 23077.7227 - val_loss: 694301568.0000 - val_rmse: 26349.6035\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 493530528.0000 - rmse: 22215.5449 - val_loss: 677556352.0000 - val_rmse: 26029.9121\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 527756160.0000 - rmse: 22972.9434 - val_loss: 667434624.0000 - val_rmse: 25834.7539\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 568033728.0000 - rmse: 23833.4590 - val_loss: 704813760.0000 - val_rmse: 26548.3281\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 562176448.0000 - rmse: 23710.2598 - val_loss: 738526976.0000 - val_rmse: 27175.8516\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558671104.0000 - rmse: 23636.2246 - val_loss: 690711104.0000 - val_rmse: 26281.3828\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 471941760.0000 - rmse: 21724.2207 - val_loss: 683040896.0000 - val_rmse: 26135.0508\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510361760.0000 - rmse: 22591.1875 - val_loss: 703043520.0000 - val_rmse: 26514.9688\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 532230944.0000 - rmse: 23070.1289 - val_loss: 676072384.0000 - val_rmse: 26001.3926\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510035680.0000 - rmse: 22583.9688 - val_loss: 696708416.0000 - val_rmse: 26395.2344\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 471500064.0000 - rmse: 21714.0527 - val_loss: 700741120.0000 - val_rmse: 26471.5156\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582334784.0000 - rmse: 24131.6133 - val_loss: 725498176.0000 - val_rmse: 26935.0742\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 511592736.0000 - rmse: 22618.4160 - val_loss: 721457472.0000 - val_rmse: 26859.9609\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 556685632.0000 - rmse: 23594.1855 - val_loss: 698762432.0000 - val_rmse: 26434.1152\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500849952.0000 - rmse: 22379.6777 - val_loss: 701660224.0000 - val_rmse: 26488.8691\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492534880.0000 - rmse: 22193.1270 - val_loss: 717540224.0000 - val_rmse: 26786.9395\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 505555072.0000 - rmse: 22484.5488 - val_loss: 683622784.0000 - val_rmse: 26146.1816\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 502950976.0000 - rmse: 22426.5684 - val_loss: 696471488.0000 - val_rmse: 26390.7461\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 499588672.0000 - rmse: 22351.4805 - val_loss: 697584832.0000 - val_rmse: 26411.8320\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 490427520.0000 - rmse: 22145.5977 - val_loss: 706418944.0000 - val_rmse: 26578.5430\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 485427584.0000 - rmse: 22032.4219 - val_loss: 707094912.0000 - val_rmse: 26591.2559\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 456148032.0000 - rmse: 21357.6230 - val_loss: 694404352.0000 - val_rmse: 26351.5527\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 467288128.0000 - rmse: 21616.8477 - val_loss: 689635008.0000 - val_rmse: 26260.9004\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495408992.0000 - rmse: 22257.7852 - val_loss: 707324032.0000 - val_rmse: 26595.5645\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441355872.0000 - rmse: 21008.4707 - val_loss: 710207488.0000 - val_rmse: 26649.7168\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 480180288.0000 - rmse: 21913.0156 - val_loss: 690682688.0000 - val_rmse: 26280.8418\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483946432.0000 - rmse: 21998.7812 - val_loss: 691137344.0000 - val_rmse: 26289.4902\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423290752.0000 - rmse: 20574.0312 - val_loss: 734281216.0000 - val_rmse: 27097.6230\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 460643008.0000 - rmse: 21462.5957 - val_loss: 750023744.0000 - val_rmse: 27386.5605\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517218208.0000 - rmse: 22742.4297 - val_loss: 693166272.0000 - val_rmse: 26328.0508\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 538666368.0000 - rmse: 23209.1875 - val_loss: 695271232.0000 - val_rmse: 26367.9961\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 488502880.0000 - rmse: 22102.0996 - val_loss: 738888192.0000 - val_rmse: 27182.4980\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446048448.0000 - rmse: 21119.8574 - val_loss: 698090688.0000 - val_rmse: 26421.4062\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473726304.0000 - rmse: 21765.2539 - val_loss: 682541696.0000 - val_rmse: 26125.4980\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 513776896.0000 - rmse: 22666.6465 - val_loss: 733287168.0000 - val_rmse: 27079.2754\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 475232640.0000 - rmse: 21799.8301 - val_loss: 699490944.0000 - val_rmse: 26447.8887\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447340896.0000 - rmse: 21150.4336 - val_loss: 692523776.0000 - val_rmse: 26315.8457\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430028960.0000 - rmse: 20737.1387 - val_loss: 700493568.0000 - val_rmse: 26466.8398\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505711040.0000 - rmse: 22488.0195 - val_loss: 721784000.0000 - val_rmse: 26866.0371\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 430284544.0000 - rmse: 20743.2988 - val_loss: 792915008.0000 - val_rmse: 28158.7461\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 496650816.0000 - rmse: 22285.6621 - val_loss: 696559296.0000 - val_rmse: 26392.4102\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419153152.0000 - rmse: 20473.2305 - val_loss: 743019392.0000 - val_rmse: 27258.3828\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418935136.0000 - rmse: 20467.9043 - val_loss: 733482688.0000 - val_rmse: 27082.8848\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512278080.0000 - rmse: 22633.5605 - val_loss: 719361280.0000 - val_rmse: 26820.9082\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451763488.0000 - rmse: 21254.7285 - val_loss: 701139904.0000 - val_rmse: 26479.0449\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 478318624.0000 - rmse: 21870.4961 - val_loss: 681699072.0000 - val_rmse: 26109.3672\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446639104.0000 - rmse: 21133.8359 - val_loss: 714665984.0000 - val_rmse: 26733.2383\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473324096.0000 - rmse: 21756.0137 - val_loss: 779915712.0000 - val_rmse: 27926.9707\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456359616.0000 - rmse: 21362.5742 - val_loss: 720323456.0000 - val_rmse: 26838.8398\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423050816.0000 - rmse: 20568.1992 - val_loss: 705274752.0000 - val_rmse: 26557.0098\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 444684576.0000 - rmse: 21087.5449 - val_loss: 697754240.0000 - val_rmse: 26415.0391\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489021824.0000 - rmse: 22113.8379 - val_loss: 709257088.0000 - val_rmse: 26631.8809\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 470334144.0000 - rmse: 21687.1855 - val_loss: 693246464.0000 - val_rmse: 26329.5723\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428279584.0000 - rmse: 20694.9160 - val_loss: 695172544.0000 - val_rmse: 26366.1250\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 408091136.0000 - rmse: 20201.2656 - val_loss: 720451520.0000 - val_rmse: 26841.2285\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 408662624.0000 - rmse: 20215.4043 - val_loss: 731853824.0000 - val_rmse: 27052.7949\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 454127328.0000 - rmse: 21310.2617 - val_loss: 728711232.0000 - val_rmse: 26994.6504\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405267872.0000 - rmse: 20131.2637 - val_loss: 739829888.0000 - val_rmse: 27199.8145\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 444904448.0000 - rmse: 21092.7559 - val_loss: 726802624.0000 - val_rmse: 26959.2773\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 483470816.0000 - rmse: 21987.9688 - val_loss: 739888832.0000 - val_rmse: 27200.8984\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439625696.0000 - rmse: 20967.2520 - val_loss: 742148800.0000 - val_rmse: 27242.4082\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 472772640.0000 - rmse: 21743.3359 - val_loss: 783876032.0000 - val_rmse: 27997.7852\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 462855584.0000 - rmse: 21514.0781 - val_loss: 735840320.0000 - val_rmse: 27126.3770\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363494144.0000 - rmse: 19065.5215 - val_loss: 739907264.0000 - val_rmse: 27201.2363\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 404355584.0000 - rmse: 20108.5938 - val_loss: 742803136.0000 - val_rmse: 27254.4141\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385143008.0000 - rmse: 19625.0605 - val_loss: 745062144.0000 - val_rmse: 27295.8262\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402226144.0000 - rmse: 20055.5762 - val_loss: 725783360.0000 - val_rmse: 26940.3652\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411532608.0000 - rmse: 20286.2656 - val_loss: 694249600.0000 - val_rmse: 26348.6172\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 448093472.0000 - rmse: 21168.2168 - val_loss: 709239936.0000 - val_rmse: 26631.5586\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 451314912.0000 - rmse: 21244.1719 - val_loss: 695435072.0000 - val_rmse: 26371.1035\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 391911744.0000 - rmse: 19796.7598 - val_loss: 689571136.0000 - val_rmse: 26259.6855\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 395165600.0000 - rmse: 19878.7715 - val_loss: 714608000.0000 - val_rmse: 26732.1523\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 399266784.0000 - rmse: 19981.6621 - val_loss: 727350592.0000 - val_rmse: 26969.4375\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420194112.0000 - rmse: 20498.6367 - val_loss: 715984960.0000 - val_rmse: 26757.8945\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410650176.0000 - rmse: 20264.5039 - val_loss: 723039360.0000 - val_rmse: 26889.3906\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428606816.0000 - rmse: 20702.8223 - val_loss: 697713088.0000 - val_rmse: 26414.2578\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447948448.0000 - rmse: 21164.7910 - val_loss: 717590016.0000 - val_rmse: 26787.8711\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427638304.0000 - rmse: 20679.4160 - val_loss: 741141632.0000 - val_rmse: 27223.9160\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353889920.0000 - rmse: 18811.9609 - val_loss: 725444672.0000 - val_rmse: 26934.0801\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 375430752.0000 - rmse: 19376.0352 - val_loss: 719542656.0000 - val_rmse: 26824.2930\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450245504.0000 - rmse: 21218.9883 - val_loss: 751396352.0000 - val_rmse: 27411.6094\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355430432.0000 - rmse: 18852.8633 - val_loss: 705983744.0000 - val_rmse: 26570.3555\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416663168.0000 - rmse: 20412.3281 - val_loss: 704756096.0000 - val_rmse: 26547.2422\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358329696.0000 - rmse: 18929.5977 - val_loss: 766928960.0000 - val_rmse: 27693.4824\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403299328.0000 - rmse: 20082.3125 - val_loss: 711652864.0000 - val_rmse: 26676.8223\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 423215136.0000 - rmse: 20572.1934 - val_loss: 732299904.0000 - val_rmse: 27061.0410\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402457984.0000 - rmse: 20061.3535 - val_loss: 732789632.0000 - val_rmse: 27070.0879\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386901728.0000 - rmse: 19669.8184 - val_loss: 755978944.0000 - val_rmse: 27495.0703\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405957920.0000 - rmse: 20148.3965 - val_loss: 692356864.0000 - val_rmse: 26312.6738\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413293536.0000 - rmse: 20329.6230 - val_loss: 738302080.0000 - val_rmse: 27171.7129\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388530688.0000 - rmse: 19711.1816 - val_loss: 767207040.0000 - val_rmse: 27698.5000\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 370298304.0000 - rmse: 19243.1348 - val_loss: 685560640.0000 - val_rmse: 26183.2129\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 355900064.0000 - rmse: 18865.3145 - val_loss: 703283136.0000 - val_rmse: 26519.4844\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 387562880.0000 - rmse: 19686.6172 - val_loss: 706303424.0000 - val_rmse: 26576.3672\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 421303776.0000 - rmse: 20525.6855 - val_loss: 707651712.0000 - val_rmse: 26601.7227\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 405427616.0000 - rmse: 20135.2324 - val_loss: 694592640.0000 - val_rmse: 26355.1230\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 353577920.0000 - rmse: 18803.6660 - val_loss: 738093760.0000 - val_rmse: 27167.8809\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 389141024.0000 - rmse: 19726.6582 - val_loss: 698593344.0000 - val_rmse: 26430.9160\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 366478208.0000 - rmse: 19143.6172 - val_loss: 711943680.0000 - val_rmse: 26682.2734\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377700768.0000 - rmse: 19434.5254 - val_loss: 680706560.0000 - val_rmse: 26090.3535\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 366973312.0000 - rmse: 19156.5469 - val_loss: 698374912.0000 - val_rmse: 26426.7832\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362264672.0000 - rmse: 19033.2500 - val_loss: 677023936.0000 - val_rmse: 26019.6816\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344686912.0000 - rmse: 18565.7441 - val_loss: 705794624.0000 - val_rmse: 26566.7949\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336872928.0000 - rmse: 18354.0977 - val_loss: 777239168.0000 - val_rmse: 27879.0098\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381674208.0000 - rmse: 19536.4824 - val_loss: 753741824.0000 - val_rmse: 27454.3594\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364201120.0000 - rmse: 19084.0527 - val_loss: 691353600.0000 - val_rmse: 26293.6035\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 380356736.0000 - rmse: 19502.7363 - val_loss: 679486016.0000 - val_rmse: 26066.9512\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388922208.0000 - rmse: 19721.1113 - val_loss: 719919232.0000 - val_rmse: 26831.3086\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 381564704.0000 - rmse: 19533.6797 - val_loss: 699519360.0000 - val_rmse: 26448.4277\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364364448.0000 - rmse: 19088.3320 - val_loss: 677176128.0000 - val_rmse: 26022.6074\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355461088.0000 - rmse: 18853.6758 - val_loss: 663614656.0000 - val_rmse: 25760.7188\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359970080.0000 - rmse: 18972.8770 - val_loss: 739662592.0000 - val_rmse: 27196.7383\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398015104.0000 - rmse: 19950.3164 - val_loss: 662488768.0000 - val_rmse: 25738.8555\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314929504.0000 - rmse: 17746.2500 - val_loss: 716073408.0000 - val_rmse: 26759.5488\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377225056.0000 - rmse: 19422.2812 - val_loss: 692779840.0000 - val_rmse: 26320.7109\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307227296.0000 - rmse: 17527.9004 - val_loss: 670597248.0000 - val_rmse: 25895.8926\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323379200.0000 - rmse: 17982.7461 - val_loss: 680813248.0000 - val_rmse: 26092.3965\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304668672.0000 - rmse: 17454.7598 - val_loss: 727358336.0000 - val_rmse: 26969.5820\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373431040.0000 - rmse: 19324.3633 - val_loss: 675918464.0000 - val_rmse: 25998.4316\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 435735072.0000 - rmse: 20874.2676 - val_loss: 683459840.0000 - val_rmse: 26143.0625\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353074048.0000 - rmse: 18790.2656 - val_loss: 663407744.0000 - val_rmse: 25756.7031\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 375621056.0000 - rmse: 19380.9434 - val_loss: 656299904.0000 - val_rmse: 25618.3516\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 285971072.0000 - rmse: 16910.6777 - val_loss: 649218688.0000 - val_rmse: 25479.7676\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299805760.0000 - rmse: 17314.9004 - val_loss: 669744768.0000 - val_rmse: 25879.4258\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 397630144.0000 - rmse: 19940.6641 - val_loss: 672756672.0000 - val_rmse: 25937.5527\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 401039200.0000 - rmse: 20025.9629 - val_loss: 688586880.0000 - val_rmse: 26240.9395\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304015648.0000 - rmse: 17436.0430 - val_loss: 713054016.0000 - val_rmse: 26703.0703\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341567104.0000 - rmse: 18481.5332 - val_loss: 741168000.0000 - val_rmse: 27224.4004\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326219136.0000 - rmse: 18061.5371 - val_loss: 685825984.0000 - val_rmse: 26188.2773\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288300640.0000 - rmse: 16979.4160 - val_loss: 667697472.0000 - val_rmse: 25839.8418\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305170176.0000 - rmse: 17469.1191 - val_loss: 723289984.0000 - val_rmse: 26894.0508\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342367360.0000 - rmse: 18503.1699 - val_loss: 798534144.0000 - val_rmse: 28258.3457\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258323952.0000 - rmse: 16072.4580 - val_loss: 656978880.0000 - val_rmse: 25631.5977\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356120096.0000 - rmse: 18871.1426 - val_loss: 686531008.0000 - val_rmse: 26201.7363\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323056512.0000 - rmse: 17973.7715 - val_loss: 702802560.0000 - val_rmse: 26510.4238\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345632128.0000 - rmse: 18591.1816 - val_loss: 741104320.0000 - val_rmse: 27223.2305\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302767328.0000 - rmse: 17400.2090 - val_loss: 699055424.0000 - val_rmse: 26439.6543\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485352864.0000 - rmse: 22030.7246 - val_loss: 687676096.0000 - val_rmse: 26223.5781\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335923552.0000 - rmse: 18328.2168 - val_loss: 644401536.0000 - val_rmse: 25385.0645\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320884576.0000 - rmse: 17913.2500 - val_loss: 647053248.0000 - val_rmse: 25437.2422\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269787232.0000 - rmse: 16425.1992 - val_loss: 665503808.0000 - val_rmse: 25797.3574\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303169792.0000 - rmse: 17411.7695 - val_loss: 645526464.0000 - val_rmse: 25407.2129\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372555552.0000 - rmse: 19301.6973 - val_loss: 652388224.0000 - val_rmse: 25541.8906\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307594112.0000 - rmse: 17538.3594 - val_loss: 695411200.0000 - val_rmse: 26370.6484\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335438112.0000 - rmse: 18314.9688 - val_loss: 654174912.0000 - val_rmse: 25576.8438\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344784640.0000 - rmse: 18568.3770 - val_loss: 679419072.0000 - val_rmse: 26065.6680\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 263671312.0000 - rmse: 16237.9590 - val_loss: 734520192.0000 - val_rmse: 27102.0332\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 289547872.0000 - rmse: 17016.1055 - val_loss: 678473216.0000 - val_rmse: 26047.5176\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297970592.0000 - rmse: 17261.8223 - val_loss: 666401408.0000 - val_rmse: 25814.7500\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356747072.0000 - rmse: 18887.7480 - val_loss: 674894784.0000 - val_rmse: 25978.7363\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 326722560.0000 - rmse: 18075.4668 - val_loss: 664406656.0000 - val_rmse: 25776.0859\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 314285024.0000 - rmse: 17728.0840 - val_loss: 657309120.0000 - val_rmse: 25638.0410\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 302241760.0000 - rmse: 17385.1016 - val_loss: 663163136.0000 - val_rmse: 25751.9531\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257349456.0000 - rmse: 16042.1133 - val_loss: 687433088.0000 - val_rmse: 26218.9434\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 307483328.0000 - rmse: 17535.2012 - val_loss: 636438400.0000 - val_rmse: 25227.7305\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286757920.0000 - rmse: 16933.9277 - val_loss: 663817728.0000 - val_rmse: 25764.6602\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275249984.0000 - rmse: 16590.6582 - val_loss: 679351872.0000 - val_rmse: 26064.3789\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312339648.0000 - rmse: 17673.1328 - val_loss: 633515328.0000 - val_rmse: 25169.7285\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258226160.0000 - rmse: 16069.4160 - val_loss: 725398720.0000 - val_rmse: 26933.2266\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364013216.0000 - rmse: 19079.1289 - val_loss: 699999296.0000 - val_rmse: 26457.4980\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299437024.0000 - rmse: 17304.2480 - val_loss: 697661568.0000 - val_rmse: 26413.2832\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 300304160.0000 - rmse: 17329.2852 - val_loss: 668217472.0000 - val_rmse: 25849.9023\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279020992.0000 - rmse: 16703.9180 - val_loss: 688487296.0000 - val_rmse: 26239.0410\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 273439488.0000 - rmse: 16536.0039 - val_loss: 651116928.0000 - val_rmse: 25516.9922\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268926752.0000 - rmse: 16398.9844 - val_loss: 661112384.0000 - val_rmse: 25712.1055\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 404710304.0000 - rmse: 20117.4121 - val_loss: 709229312.0000 - val_rmse: 26631.3594\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351724096.0000 - rmse: 18754.3066 - val_loss: 679537664.0000 - val_rmse: 26067.9414\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324588288.0000 - rmse: 18016.3340 - val_loss: 648425408.0000 - val_rmse: 25464.1992\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251808352.0000 - rmse: 15868.4688 - val_loss: 756100032.0000 - val_rmse: 27497.2715\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299220992.0000 - rmse: 17298.0039 - val_loss: 687801728.0000 - val_rmse: 26225.9746\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361824768.0000 - rmse: 19021.6914 - val_loss: 640595840.0000 - val_rmse: 25309.9941\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288968288.0000 - rmse: 16999.0664 - val_loss: 675434432.0000 - val_rmse: 25989.1191\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273994912.0000 - rmse: 16552.7910 - val_loss: 727515136.0000 - val_rmse: 26972.4883\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293828320.0000 - rmse: 17141.4199 - val_loss: 659844288.0000 - val_rmse: 25687.4336\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286753152.0000 - rmse: 16933.7871 - val_loss: 716612608.0000 - val_rmse: 26769.6191\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295426272.0000 - rmse: 17187.9688 - val_loss: 672724992.0000 - val_rmse: 25936.9414\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258859440.0000 - rmse: 16089.1084 - val_loss: 684461888.0000 - val_rmse: 26162.2227\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309667072.0000 - rmse: 17597.3574 - val_loss: 680933440.0000 - val_rmse: 26094.7012\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292770528.0000 - rmse: 17110.5371 - val_loss: 622426496.0000 - val_rmse: 24948.4766\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280686400.0000 - rmse: 16753.6953 - val_loss: 685272960.0000 - val_rmse: 26177.7168\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280402112.0000 - rmse: 16745.2109 - val_loss: 670820352.0000 - val_rmse: 25900.1992\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313589568.0000 - rmse: 17708.4609 - val_loss: 632460096.0000 - val_rmse: 25148.7598\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308348512.0000 - rmse: 17559.8535 - val_loss: 677707520.0000 - val_rmse: 26032.8145\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252707056.0000 - rmse: 15896.7627 - val_loss: 781758592.0000 - val_rmse: 27959.9453\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272912640.0000 - rmse: 16520.0664 - val_loss: 723233408.0000 - val_rmse: 26892.9980\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292290624.0000 - rmse: 17096.5098 - val_loss: 719289344.0000 - val_rmse: 26819.5703\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243226720.0000 - rmse: 15595.7275 - val_loss: 657647104.0000 - val_rmse: 25644.6309\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267918528.0000 - rmse: 16368.2158 - val_loss: 669715712.0000 - val_rmse: 25878.8652\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 326712992.0000 - rmse: 18075.2012 - val_loss: 688470720.0000 - val_rmse: 26238.7227\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279732672.0000 - rmse: 16725.2090 - val_loss: 641509888.0000 - val_rmse: 25328.0449\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265242928.0000 - rmse: 16286.2793 - val_loss: 692943232.0000 - val_rmse: 26323.8145\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250103776.0000 - rmse: 15814.6680 - val_loss: 710112256.0000 - val_rmse: 26647.9297\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256936976.0000 - rmse: 16029.2520 - val_loss: 733145088.0000 - val_rmse: 27076.6523\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298030496.0000 - rmse: 17263.5586 - val_loss: 666073152.0000 - val_rmse: 25808.3926\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270265536.0000 - rmse: 16439.7539 - val_loss: 677533312.0000 - val_rmse: 26029.4707\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287398656.0000 - rmse: 16952.8359 - val_loss: 630680768.0000 - val_rmse: 25113.3574\n",
      "Epoch 367/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269803072.0000 - rmse: 16425.6816 - val_loss: 692543104.0000 - val_rmse: 26316.2109\n",
      "Epoch 368/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240270736.0000 - rmse: 15500.6670 - val_loss: 698625088.0000 - val_rmse: 26431.5156\n",
      "Epoch 369/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245212048.0000 - rmse: 15659.2471 - val_loss: 661095296.0000 - val_rmse: 25711.7734\n",
      "Epoch 370/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266079376.0000 - rmse: 16311.9385 - val_loss: 741113408.0000 - val_rmse: 27223.3965\n",
      "Epoch 371/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270872320.0000 - rmse: 16458.1953 - val_loss: 627514112.0000 - val_rmse: 25050.2324\n",
      "Epoch 372/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277654560.0000 - rmse: 16662.9668 - val_loss: 670035904.0000 - val_rmse: 25885.0508\n",
      "Epoch 373/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286908768.0000 - rmse: 16938.3809 - val_loss: 694509440.0000 - val_rmse: 26353.5469\n",
      "Epoch 374/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303827552.0000 - rmse: 17430.6484 - val_loss: 666296128.0000 - val_rmse: 25812.7109\n",
      "Epoch 375/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309672768.0000 - rmse: 17597.5195 - val_loss: 679083712.0000 - val_rmse: 26059.2344\n",
      "Epoch 376/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318928864.0000 - rmse: 17858.5781 - val_loss: 703275264.0000 - val_rmse: 26519.3359\n",
      "Epoch 377/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257219888.0000 - rmse: 16038.0742 - val_loss: 643059776.0000 - val_rmse: 25358.6211\n",
      "Epoch 378/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245369952.0000 - rmse: 15664.2881 - val_loss: 661436928.0000 - val_rmse: 25718.4141\n",
      "Epoch 379/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 313016096.0000 - rmse: 17692.2598 - val_loss: 663040640.0000 - val_rmse: 25749.5742\n",
      "Epoch 380/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248797232.0000 - rmse: 15773.3037 - val_loss: 697535360.0000 - val_rmse: 26410.8945\n",
      "Epoch 381/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259220512.0000 - rmse: 16100.3252 - val_loss: 664084288.0000 - val_rmse: 25769.8320\n",
      "Epoch 382/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245897120.0000 - rmse: 15681.1045 - val_loss: 634570240.0000 - val_rmse: 25190.6777\n",
      "Epoch 383/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270578656.0000 - rmse: 16449.2734 - val_loss: 670753024.0000 - val_rmse: 25898.9004\n",
      "Epoch 384/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265497968.0000 - rmse: 16294.1055 - val_loss: 711288448.0000 - val_rmse: 26669.9883\n",
      "Epoch 385/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262832944.0000 - rmse: 16212.1221 - val_loss: 679513600.0000 - val_rmse: 26067.4785\n",
      "Epoch 386/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261597216.0000 - rmse: 16173.9648 - val_loss: 698993216.0000 - val_rmse: 26438.4805\n",
      "Epoch 387/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 231815744.0000 - rmse: 15225.4961 - val_loss: 707893504.0000 - val_rmse: 26606.2676\n",
      "Epoch 388/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217118096.0000 - rmse: 14734.9268 - val_loss: 734874240.0000 - val_rmse: 27108.5645\n",
      "Epoch 389/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 338903008.0000 - rmse: 18409.3164 - val_loss: 636666496.0000 - val_rmse: 25232.2500\n",
      "Epoch 390/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 243755968.0000 - rmse: 15612.6855 - val_loss: 674005824.0000 - val_rmse: 25961.6230\n",
      "Epoch 391/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231636400.0000 - rmse: 15219.6045 - val_loss: 726977152.0000 - val_rmse: 26962.5117\n",
      "Epoch 392/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 211292960.0000 - rmse: 14535.9170 - val_loss: 636924160.0000 - val_rmse: 25237.3555\n",
      "Epoch 393/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253784304.0000 - rmse: 15930.6074 - val_loss: 698400832.0000 - val_rmse: 26427.2734\n",
      "Epoch 394/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281243040.0000 - rmse: 16770.3008 - val_loss: 670567104.0000 - val_rmse: 25895.3086\n",
      "Epoch 395/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 254932176.0000 - rmse: 15966.5938 - val_loss: 724825024.0000 - val_rmse: 26922.5742\n",
      "Epoch 396/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 208852288.0000 - rmse: 14451.7217 - val_loss: 836191744.0000 - val_rmse: 28916.9805\n",
      "Epoch 397/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247490256.0000 - rmse: 15731.8223 - val_loss: 693707968.0000 - val_rmse: 26338.3340\n",
      "Epoch 398/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 269408096.0000 - rmse: 16413.6543 - val_loss: 655054592.0000 - val_rmse: 25594.0352\n",
      "Epoch 399/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240944608.0000 - rmse: 15522.3887 - val_loss: 704612160.0000 - val_rmse: 26544.5312\n",
      "Epoch 400/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217221072.0000 - rmse: 14738.4219 - val_loss: 679277440.0000 - val_rmse: 26062.9512\n",
      "104/104 [==============================] - 0s 634us/step - loss: 671667136.0000 - rmse: 25916.5371\n",
      "[671667136.0, 25916.537109375]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 22844540928.0000 - rmse: 151144.1094 - val_loss: 12772165632.0000 - val_rmse: 113014.0078\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 4534493696.0000 - rmse: 67338.6484 - val_loss: 2207866880.0000 - val_rmse: 46987.9453\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2145467136.0000 - rmse: 46319.1875 - val_loss: 1687322624.0000 - val_rmse: 41077.0312\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1801194240.0000 - rmse: 42440.4805 - val_loss: 1441167872.0000 - val_rmse: 37962.7188\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1616021376.0000 - rmse: 40199.7695 - val_loss: 1321228800.0000 - val_rmse: 36348.7109\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1454697088.0000 - rmse: 38140.4922 - val_loss: 1293755136.0000 - val_rmse: 35968.8086\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1442960896.0000 - rmse: 37986.3242 - val_loss: 1185501568.0000 - val_rmse: 34431.1133\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1366707840.0000 - rmse: 36969.0117 - val_loss: 1104337536.0000 - val_rmse: 33231.5781\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1330871296.0000 - rmse: 36481.1094 - val_loss: 1062021888.0000 - val_rmse: 32588.6777\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1255434624.0000 - rmse: 35432.1133 - val_loss: 1050071360.0000 - val_rmse: 32404.8047\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1208781696.0000 - rmse: 34767.5391 - val_loss: 1019460160.0000 - val_rmse: 31928.9863\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1220072704.0000 - rmse: 34929.5391 - val_loss: 1043651072.0000 - val_rmse: 32305.5879\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1184709760.0000 - rmse: 34419.6133 - val_loss: 967136512.0000 - val_rmse: 31098.8184\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1135544576.0000 - rmse: 33697.8438 - val_loss: 948580672.0000 - val_rmse: 30799.0371\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1126249216.0000 - rmse: 33559.6367 - val_loss: 943290752.0000 - val_rmse: 30713.0410\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1103388928.0000 - rmse: 33217.2969 - val_loss: 923870272.0000 - val_rmse: 30395.2344\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1088755072.0000 - rmse: 32996.2891 - val_loss: 937203008.0000 - val_rmse: 30613.7715\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1097273600.0000 - rmse: 33125.1211 - val_loss: 912970048.0000 - val_rmse: 30215.3945\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1060863104.0000 - rmse: 32570.8926 - val_loss: 910382592.0000 - val_rmse: 30172.5469\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1031605312.0000 - rmse: 32118.6133 - val_loss: 939475328.0000 - val_rmse: 30650.8613\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1020084096.0000 - rmse: 31938.7559 - val_loss: 895008768.0000 - val_rmse: 29916.6973\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1079179648.0000 - rmse: 32850.8672 - val_loss: 889652928.0000 - val_rmse: 29827.0508\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1004257344.0000 - rmse: 31690.0195 - val_loss: 901487552.0000 - val_rmse: 30024.7832\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1025144960.0000 - rmse: 32017.8848 - val_loss: 896738432.0000 - val_rmse: 29945.5918\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1021955328.0000 - rmse: 31968.0352 - val_loss: 920161984.0000 - val_rmse: 30334.1719\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1019150080.0000 - rmse: 31924.1309 - val_loss: 981077824.0000 - val_rmse: 31322.1621\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 972032832.0000 - rmse: 31177.4414 - val_loss: 890910848.0000 - val_rmse: 29848.1289\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 972354944.0000 - rmse: 31182.6055 - val_loss: 982055232.0000 - val_rmse: 31337.7598\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 942029760.0000 - rmse: 30692.5020 - val_loss: 910924416.0000 - val_rmse: 30181.5254\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 964428160.0000 - rmse: 31055.2441 - val_loss: 893744064.0000 - val_rmse: 29895.5527\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 943218496.0000 - rmse: 30711.8633 - val_loss: 919349504.0000 - val_rmse: 30320.7773\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 989637376.0000 - rmse: 31458.5020 - val_loss: 962814720.0000 - val_rmse: 31029.2559\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 982864960.0000 - rmse: 31350.6777 - val_loss: 877638144.0000 - val_rmse: 29624.9590\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 949871552.0000 - rmse: 30819.9863 - val_loss: 888637376.0000 - val_rmse: 29810.0215\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 987997696.0000 - rmse: 31432.4297 - val_loss: 865152128.0000 - val_rmse: 29413.4688\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 935567360.0000 - rmse: 30587.0449 - val_loss: 861897472.0000 - val_rmse: 29358.0898\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 934054336.0000 - rmse: 30562.3027 - val_loss: 904274048.0000 - val_rmse: 30071.1504\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 954562816.0000 - rmse: 30896.0000 - val_loss: 864072064.0000 - val_rmse: 29395.1035\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 888412928.0000 - rmse: 29806.2559 - val_loss: 912675840.0000 - val_rmse: 30210.5254\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 914738432.0000 - rmse: 30244.6426 - val_loss: 895388608.0000 - val_rmse: 29923.0449\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 909017280.0000 - rmse: 30149.9141 - val_loss: 839561152.0000 - val_rmse: 28975.1816\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 946438528.0000 - rmse: 30764.2402 - val_loss: 850494080.0000 - val_rmse: 29163.2324\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 955492032.0000 - rmse: 30911.0332 - val_loss: 857270912.0000 - val_rmse: 29279.1895\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 951823552.0000 - rmse: 30851.6387 - val_loss: 835179648.0000 - val_rmse: 28899.4746\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 892957120.0000 - rmse: 29882.3887 - val_loss: 835621376.0000 - val_rmse: 28907.1172\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 901402304.0000 - rmse: 30023.3633 - val_loss: 823478208.0000 - val_rmse: 28696.3105\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 852478976.0000 - rmse: 29197.2422 - val_loss: 824504384.0000 - val_rmse: 28714.1836\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 885761536.0000 - rmse: 29761.7461 - val_loss: 828140352.0000 - val_rmse: 28777.4277\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 928476224.0000 - rmse: 30470.9082 - val_loss: 849108096.0000 - val_rmse: 29139.4590\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 844841408.0000 - rmse: 29066.1562 - val_loss: 841885056.0000 - val_rmse: 29015.2559\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 833768640.0000 - rmse: 28875.0527 - val_loss: 863619456.0000 - val_rmse: 29387.4004\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 867264384.0000 - rmse: 29449.3535 - val_loss: 945521984.0000 - val_rmse: 30749.3418\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 822875968.0000 - rmse: 28685.8145 - val_loss: 846927360.0000 - val_rmse: 29102.0156\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 830275904.0000 - rmse: 28814.5078 - val_loss: 873239488.0000 - val_rmse: 29550.6230\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 809009088.0000 - rmse: 28443.0859 - val_loss: 826576256.0000 - val_rmse: 28750.2383\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 934254272.0000 - rmse: 30565.5742 - val_loss: 868601472.0000 - val_rmse: 29472.0449\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 831698368.0000 - rmse: 28839.1816 - val_loss: 838328704.0000 - val_rmse: 28953.9062\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 863302912.0000 - rmse: 29382.0176 - val_loss: 906267264.0000 - val_rmse: 30104.2734\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 866099904.0000 - rmse: 29429.5762 - val_loss: 911537472.0000 - val_rmse: 30191.6797\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 840321664.0000 - rmse: 28988.3027 - val_loss: 891856512.0000 - val_rmse: 29863.9668\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 817953216.0000 - rmse: 28599.8809 - val_loss: 854455936.0000 - val_rmse: 29231.0781\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 876608064.0000 - rmse: 29607.5684 - val_loss: 898126400.0000 - val_rmse: 29968.7578\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 835284800.0000 - rmse: 28901.2949 - val_loss: 838807872.0000 - val_rmse: 28962.1797\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 851957696.0000 - rmse: 29188.3125 - val_loss: 823343808.0000 - val_rmse: 28693.9688\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 827062656.0000 - rmse: 28758.6973 - val_loss: 864092992.0000 - val_rmse: 29395.4590\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 800511552.0000 - rmse: 28293.3125 - val_loss: 851945792.0000 - val_rmse: 29188.1113\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 848459776.0000 - rmse: 29128.3320 - val_loss: 837086912.0000 - val_rmse: 28932.4551\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 804101120.0000 - rmse: 28356.6777 - val_loss: 824297728.0000 - val_rmse: 28710.5859\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 790706752.0000 - rmse: 28119.5078 - val_loss: 877938432.0000 - val_rmse: 29630.0254\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 765606912.0000 - rmse: 27669.6035 - val_loss: 875396864.0000 - val_rmse: 29587.1035\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 772311360.0000 - rmse: 27790.4902 - val_loss: 892973760.0000 - val_rmse: 29882.6660\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 802425536.0000 - rmse: 28327.1172 - val_loss: 884933248.0000 - val_rmse: 29747.8281\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 732971968.0000 - rmse: 27073.4551 - val_loss: 867937536.0000 - val_rmse: 29460.7793\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 798790400.0000 - rmse: 28262.8809 - val_loss: 857770304.0000 - val_rmse: 29287.7168\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 822883520.0000 - rmse: 28685.9473 - val_loss: 915646848.0000 - val_rmse: 30259.6562\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 745718016.0000 - rmse: 27307.8379 - val_loss: 887241600.0000 - val_rmse: 29786.5996\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 824828416.0000 - rmse: 28719.8262 - val_loss: 858629888.0000 - val_rmse: 29302.3867\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 833036160.0000 - rmse: 28862.3652 - val_loss: 848938176.0000 - val_rmse: 29136.5430\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 763873728.0000 - rmse: 27638.2656 - val_loss: 894673152.0000 - val_rmse: 29911.0879\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 765165120.0000 - rmse: 27661.6191 - val_loss: 825654080.0000 - val_rmse: 28734.1973\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 798023936.0000 - rmse: 28249.3164 - val_loss: 904654080.0000 - val_rmse: 30077.4688\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 701196480.0000 - rmse: 26480.1152 - val_loss: 894151168.0000 - val_rmse: 29902.3613\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 720472000.0000 - rmse: 26841.6094 - val_loss: 851815232.0000 - val_rmse: 29185.8730\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 775612480.0000 - rmse: 27849.8203 - val_loss: 811679744.0000 - val_rmse: 28489.9941\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 731818752.0000 - rmse: 27052.1484 - val_loss: 867076864.0000 - val_rmse: 29446.1680\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 777326528.0000 - rmse: 27880.5762 - val_loss: 861352832.0000 - val_rmse: 29348.8125\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 779395584.0000 - rmse: 27917.6562 - val_loss: 844025216.0000 - val_rmse: 29052.1113\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 744432896.0000 - rmse: 27284.2969 - val_loss: 914950400.0000 - val_rmse: 30248.1465\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 727618624.0000 - rmse: 26974.4062 - val_loss: 824192704.0000 - val_rmse: 28708.7559\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 742393344.0000 - rmse: 27246.8945 - val_loss: 822377024.0000 - val_rmse: 28677.1152\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 795130944.0000 - rmse: 28198.0664 - val_loss: 833968512.0000 - val_rmse: 28878.5137\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 727800064.0000 - rmse: 26977.7695 - val_loss: 820914432.0000 - val_rmse: 28651.6035\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 745975872.0000 - rmse: 27312.5586 - val_loss: 832083584.0000 - val_rmse: 28845.8594\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 627633664.0000 - rmse: 25052.6172 - val_loss: 821956032.0000 - val_rmse: 28669.7754\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 736996096.0000 - rmse: 27147.6719 - val_loss: 825269248.0000 - val_rmse: 28727.4980\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751743040.0000 - rmse: 27417.9297 - val_loss: 865075712.0000 - val_rmse: 29412.1699\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 716742272.0000 - rmse: 26772.0430 - val_loss: 862920896.0000 - val_rmse: 29375.5156\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 659876608.0000 - rmse: 25688.0645 - val_loss: 849572992.0000 - val_rmse: 29147.4355\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 710260608.0000 - rmse: 26650.7148 - val_loss: 868732288.0000 - val_rmse: 29474.2656\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 670466432.0000 - rmse: 25893.3672 - val_loss: 866095744.0000 - val_rmse: 29429.5020\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 663321408.0000 - rmse: 25755.0254 - val_loss: 869593728.0000 - val_rmse: 29488.8730\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 689432000.0000 - rmse: 26257.0371 - val_loss: 918149056.0000 - val_rmse: 30300.9746\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656157312.0000 - rmse: 25615.5684 - val_loss: 885834752.0000 - val_rmse: 29762.9766\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 686794688.0000 - rmse: 26206.7676 - val_loss: 885334272.0000 - val_rmse: 29754.5664\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698429440.0000 - rmse: 26427.8164 - val_loss: 939255360.0000 - val_rmse: 30647.2734\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 698828672.0000 - rmse: 26435.3672 - val_loss: 908181760.0000 - val_rmse: 30136.0547\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649607232.0000 - rmse: 25487.3945 - val_loss: 862478464.0000 - val_rmse: 29367.9844\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 640041152.0000 - rmse: 25299.0352 - val_loss: 876621184.0000 - val_rmse: 29607.7891\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 684549376.0000 - rmse: 26163.8945 - val_loss: 912469184.0000 - val_rmse: 30207.1055\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 667491904.0000 - rmse: 25835.8652 - val_loss: 903270720.0000 - val_rmse: 30054.4629\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 629222272.0000 - rmse: 25084.3027 - val_loss: 932070720.0000 - val_rmse: 30529.8340\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 601145600.0000 - rmse: 24518.2715 - val_loss: 938004544.0000 - val_rmse: 30626.8594\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 600878144.0000 - rmse: 24512.8164 - val_loss: 927805120.0000 - val_rmse: 30459.8945\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614370880.0000 - rmse: 24786.5059 - val_loss: 1085557760.0000 - val_rmse: 32947.8047\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 651584320.0000 - rmse: 25526.1465 - val_loss: 912490496.0000 - val_rmse: 30207.4551\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585798656.0000 - rmse: 24203.2754 - val_loss: 962929856.0000 - val_rmse: 31031.1113\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586309376.0000 - rmse: 24213.8262 - val_loss: 946282880.0000 - val_rmse: 30761.7109\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589092288.0000 - rmse: 24271.2227 - val_loss: 1135047424.0000 - val_rmse: 33690.4648\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 620814784.0000 - rmse: 24916.1543 - val_loss: 1098785280.0000 - val_rmse: 33147.9297\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 630236288.0000 - rmse: 25104.5078 - val_loss: 1110889600.0000 - val_rmse: 33330.0117\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 638063168.0000 - rmse: 25259.9121 - val_loss: 1043175744.0000 - val_rmse: 32298.2305\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 593152896.0000 - rmse: 24354.7305 - val_loss: 969258624.0000 - val_rmse: 31132.9180\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 630802496.0000 - rmse: 25115.7812 - val_loss: 1082110208.0000 - val_rmse: 32895.4453\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588042496.0000 - rmse: 24249.5879 - val_loss: 952413376.0000 - val_rmse: 30861.1953\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594636352.0000 - rmse: 24385.1660 - val_loss: 1023082048.0000 - val_rmse: 31985.6543\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620828480.0000 - rmse: 24916.4297 - val_loss: 941942208.0000 - val_rmse: 30691.0762\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 565174144.0000 - rmse: 23773.3906 - val_loss: 968411840.0000 - val_rmse: 31119.3164\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573585600.0000 - rmse: 23949.6465 - val_loss: 1018855488.0000 - val_rmse: 31919.5156\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 552609728.0000 - rmse: 23507.6504 - val_loss: 1019805056.0000 - val_rmse: 31934.3867\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560632576.0000 - rmse: 23677.6797 - val_loss: 1105241088.0000 - val_rmse: 33245.1680\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586985152.0000 - rmse: 24227.7773 - val_loss: 993048064.0000 - val_rmse: 31512.6660\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572926592.0000 - rmse: 23935.8828 - val_loss: 1006289472.0000 - val_rmse: 31722.0664\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599170944.0000 - rmse: 24477.9688 - val_loss: 976254592.0000 - val_rmse: 31245.0723\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 619694912.0000 - rmse: 24893.6699 - val_loss: 934955776.0000 - val_rmse: 30577.0469\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 566370496.0000 - rmse: 23798.5391 - val_loss: 956146368.0000 - val_rmse: 30921.6172\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561097152.0000 - rmse: 23687.4902 - val_loss: 1041897728.0000 - val_rmse: 32278.4414\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520023168.0000 - rmse: 22804.0156 - val_loss: 945894144.0000 - val_rmse: 30755.3926\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 512429792.0000 - rmse: 22636.9102 - val_loss: 874967872.0000 - val_rmse: 29579.8555\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559538496.0000 - rmse: 23654.5664 - val_loss: 990998784.0000 - val_rmse: 31480.1328\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 584248512.0000 - rmse: 24171.2324 - val_loss: 910994752.0000 - val_rmse: 30182.6855\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525488832.0000 - rmse: 22923.5430 - val_loss: 894973440.0000 - val_rmse: 29916.1074\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533346592.0000 - rmse: 23094.2969 - val_loss: 947927616.0000 - val_rmse: 30788.4336\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516140992.0000 - rmse: 22718.7344 - val_loss: 906529792.0000 - val_rmse: 30108.6328\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 566004480.0000 - rmse: 23790.8496 - val_loss: 939731904.0000 - val_rmse: 30655.0469\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533680992.0000 - rmse: 23101.5371 - val_loss: 1020413760.0000 - val_rmse: 31943.9141\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507077664.0000 - rmse: 22518.3828 - val_loss: 960995328.0000 - val_rmse: 30999.9238\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542787264.0000 - rmse: 23297.7949 - val_loss: 944197696.0000 - val_rmse: 30727.7949\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 594497088.0000 - rmse: 24382.3105 - val_loss: 922217472.0000 - val_rmse: 30368.0332\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508266048.0000 - rmse: 22544.7539 - val_loss: 918195520.0000 - val_rmse: 30301.7402\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 554740224.0000 - rmse: 23552.9238 - val_loss: 964437248.0000 - val_rmse: 31055.3887\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 492233376.0000 - rmse: 22186.3340 - val_loss: 1049057344.0000 - val_rmse: 32389.1543\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508375008.0000 - rmse: 22547.1738 - val_loss: 857839360.0000 - val_rmse: 29288.8945\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 503018816.0000 - rmse: 22428.0801 - val_loss: 930570624.0000 - val_rmse: 30505.2559\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498795040.0000 - rmse: 22333.7188 - val_loss: 1003823232.0000 - val_rmse: 31683.1699\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 494965152.0000 - rmse: 22247.8125 - val_loss: 1000913792.0000 - val_rmse: 31637.2188\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569514624.0000 - rmse: 23864.5059 - val_loss: 979555328.0000 - val_rmse: 31297.8477\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 465455392.0000 - rmse: 21574.4141 - val_loss: 946646144.0000 - val_rmse: 30767.6152\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 573587840.0000 - rmse: 23949.6934 - val_loss: 966359872.0000 - val_rmse: 31086.3301\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 516145696.0000 - rmse: 22718.8379 - val_loss: 917536512.0000 - val_rmse: 30290.8652\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517366016.0000 - rmse: 22745.6797 - val_loss: 971818688.0000 - val_rmse: 31174.0059\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 519093472.0000 - rmse: 22783.6211 - val_loss: 913902080.0000 - val_rmse: 30230.8125\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 509151712.0000 - rmse: 22564.3887 - val_loss: 1001205760.0000 - val_rmse: 31641.8359\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467205216.0000 - rmse: 21614.9297 - val_loss: 1036833728.0000 - val_rmse: 32199.9023\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461052864.0000 - rmse: 21472.1406 - val_loss: 1039181440.0000 - val_rmse: 32236.3379\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 538017472.0000 - rmse: 23195.2012 - val_loss: 1009182720.0000 - val_rmse: 31767.6367\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487217888.0000 - rmse: 22073.0117 - val_loss: 922633216.0000 - val_rmse: 30374.8789\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466827040.0000 - rmse: 21606.1777 - val_loss: 1086071040.0000 - val_rmse: 32955.5938\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 467703232.0000 - rmse: 21626.4453 - val_loss: 1002965504.0000 - val_rmse: 31669.6309\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463013728.0000 - rmse: 21517.7520 - val_loss: 951259648.0000 - val_rmse: 30842.4980\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 479126528.0000 - rmse: 21888.9590 - val_loss: 924082944.0000 - val_rmse: 30398.7324\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 501885760.0000 - rmse: 22402.8066 - val_loss: 955016256.0000 - val_rmse: 30903.3359\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463607104.0000 - rmse: 21531.5352 - val_loss: 899438784.0000 - val_rmse: 29990.6445\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456991136.0000 - rmse: 21377.3516 - val_loss: 932543872.0000 - val_rmse: 30537.5820\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481729024.0000 - rmse: 21948.3262 - val_loss: 982034880.0000 - val_rmse: 31337.4355\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 463223104.0000 - rmse: 21522.6191 - val_loss: 1095779840.0000 - val_rmse: 33102.5664\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 480299424.0000 - rmse: 21915.7324 - val_loss: 1070498944.0000 - val_rmse: 32718.4785\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427664672.0000 - rmse: 20680.0547 - val_loss: 960239872.0000 - val_rmse: 30987.7363\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 466573600.0000 - rmse: 21600.3145 - val_loss: 983147008.0000 - val_rmse: 31355.1758\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 477488320.0000 - rmse: 21851.5059 - val_loss: 991505728.0000 - val_rmse: 31488.1836\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481427808.0000 - rmse: 21941.4609 - val_loss: 1084459392.0000 - val_rmse: 32931.1328\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452064608.0000 - rmse: 21261.8105 - val_loss: 985676416.0000 - val_rmse: 31395.4844\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 469584608.0000 - rmse: 21669.9004 - val_loss: 1007446720.0000 - val_rmse: 31740.3008\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 489042624.0000 - rmse: 22114.3066 - val_loss: 1021127296.0000 - val_rmse: 31955.0820\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 505765312.0000 - rmse: 22489.2266 - val_loss: 1060182976.0000 - val_rmse: 32560.4512\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438669184.0000 - rmse: 20944.4316 - val_loss: 948962176.0000 - val_rmse: 30805.2305\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 401097952.0000 - rmse: 20027.4277 - val_loss: 949706048.0000 - val_rmse: 30817.3008\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436877088.0000 - rmse: 20901.6055 - val_loss: 997426432.0000 - val_rmse: 31582.0586\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455763520.0000 - rmse: 21348.6172 - val_loss: 943855360.0000 - val_rmse: 30722.2285\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410865664.0000 - rmse: 20269.8223 - val_loss: 1007191936.0000 - val_rmse: 31736.2871\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457534176.0000 - rmse: 21390.0488 - val_loss: 960850752.0000 - val_rmse: 30997.5918\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 446248640.0000 - rmse: 21124.5977 - val_loss: 1002371200.0000 - val_rmse: 31660.2461\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447787584.0000 - rmse: 21160.9922 - val_loss: 1172523136.0000 - val_rmse: 34242.1250\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399913088.0000 - rmse: 19997.8262 - val_loss: 979079616.0000 - val_rmse: 31290.2480\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461978976.0000 - rmse: 21493.6953 - val_loss: 992717440.0000 - val_rmse: 31507.4180\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 390027968.0000 - rmse: 19749.1250 - val_loss: 1042155968.0000 - val_rmse: 32282.4414\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416464736.0000 - rmse: 20407.4668 - val_loss: 1074104448.0000 - val_rmse: 32773.5312\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448813344.0000 - rmse: 21185.2129 - val_loss: 959691008.0000 - val_rmse: 30978.8809\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496206784.0000 - rmse: 22275.6992 - val_loss: 981561024.0000 - val_rmse: 31329.8750\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392163136.0000 - rmse: 19803.1074 - val_loss: 998621696.0000 - val_rmse: 31600.9766\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437418816.0000 - rmse: 20914.5605 - val_loss: 1195140096.0000 - val_rmse: 34570.7969\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448815520.0000 - rmse: 21185.2656 - val_loss: 900976576.0000 - val_rmse: 30016.2715\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392563296.0000 - rmse: 19813.2090 - val_loss: 942391872.0000 - val_rmse: 30698.4023\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420890752.0000 - rmse: 20515.6211 - val_loss: 1095461504.0000 - val_rmse: 33097.7578\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 406353760.0000 - rmse: 20158.2188 - val_loss: 998923456.0000 - val_rmse: 31605.7500\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395477056.0000 - rmse: 19886.6035 - val_loss: 987831872.0000 - val_rmse: 31429.7930\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433335328.0000 - rmse: 20816.7070 - val_loss: 993161152.0000 - val_rmse: 31514.4590\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405958976.0000 - rmse: 20148.4238 - val_loss: 1092437120.0000 - val_rmse: 33052.0352\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423073152.0000 - rmse: 20568.7422 - val_loss: 922382720.0000 - val_rmse: 30370.7539\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439924992.0000 - rmse: 20974.3887 - val_loss: 1055483584.0000 - val_rmse: 32488.2070\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362955520.0000 - rmse: 19051.3906 - val_loss: 918590016.0000 - val_rmse: 30308.2500\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389450976.0000 - rmse: 19734.5117 - val_loss: 1003604224.0000 - val_rmse: 31679.7129\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415240160.0000 - rmse: 20377.4434 - val_loss: 940750016.0000 - val_rmse: 30671.6484\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379592608.0000 - rmse: 19483.1348 - val_loss: 998945280.0000 - val_rmse: 31606.0957\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391354976.0000 - rmse: 19782.6934 - val_loss: 1076502912.0000 - val_rmse: 32810.1055\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408917152.0000 - rmse: 20221.6992 - val_loss: 1088078336.0000 - val_rmse: 32986.0312\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387602976.0000 - rmse: 19687.6348 - val_loss: 1096174976.0000 - val_rmse: 33108.5312\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484863136.0000 - rmse: 22019.6074 - val_loss: 1074375424.0000 - val_rmse: 32777.6680\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428005600.0000 - rmse: 20688.2969 - val_loss: 1025948992.0000 - val_rmse: 32030.4375\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354248960.0000 - rmse: 18821.5020 - val_loss: 1193707264.0000 - val_rmse: 34550.0703\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 409658720.0000 - rmse: 20240.0254 - val_loss: 1003573824.0000 - val_rmse: 31679.2305\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391453856.0000 - rmse: 19785.1914 - val_loss: 909293184.0000 - val_rmse: 30154.4883\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 419837184.0000 - rmse: 20489.9297 - val_loss: 898866304.0000 - val_rmse: 29981.0996\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327548224.0000 - rmse: 18098.2910 - val_loss: 942788032.0000 - val_rmse: 30704.8535\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358767296.0000 - rmse: 18941.1543 - val_loss: 1022922816.0000 - val_rmse: 31983.1641\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403151904.0000 - rmse: 20078.6426 - val_loss: 1052193280.0000 - val_rmse: 32437.5293\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 411724992.0000 - rmse: 20291.0078 - val_loss: 1006166528.0000 - val_rmse: 31720.1289\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363745952.0000 - rmse: 19072.1230 - val_loss: 986359296.0000 - val_rmse: 31406.3574\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356325024.0000 - rmse: 18876.5723 - val_loss: 1027760000.0000 - val_rmse: 32058.6953\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385913760.0000 - rmse: 19644.6855 - val_loss: 1101652096.0000 - val_rmse: 33191.1445\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 358577024.0000 - rmse: 18936.1289 - val_loss: 1118628224.0000 - val_rmse: 33445.8984\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388857792.0000 - rmse: 19719.4766 - val_loss: 1000535232.0000 - val_rmse: 31631.2344\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 349316896.0000 - rmse: 18690.0195 - val_loss: 1026715968.0000 - val_rmse: 32042.4082\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 402526976.0000 - rmse: 20063.0742 - val_loss: 1008319360.0000 - val_rmse: 31754.0449\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324365408.0000 - rmse: 18010.1465 - val_loss: 978491136.0000 - val_rmse: 31280.8438\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 363933248.0000 - rmse: 19077.0352 - val_loss: 1071122176.0000 - val_rmse: 32728.0020\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337506016.0000 - rmse: 18371.3359 - val_loss: 1077296768.0000 - val_rmse: 32822.1992\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 393829472.0000 - rmse: 19845.1367 - val_loss: 1190191488.0000 - val_rmse: 34499.1523\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341840800.0000 - rmse: 18488.9375 - val_loss: 1182328064.0000 - val_rmse: 34384.9961\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364053952.0000 - rmse: 19080.1973 - val_loss: 1357135232.0000 - val_rmse: 36839.3164\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355714752.0000 - rmse: 18860.4004 - val_loss: 1099296384.0000 - val_rmse: 33155.6406\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350762784.0000 - rmse: 18728.6621 - val_loss: 1222226304.0000 - val_rmse: 34960.3516\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400938176.0000 - rmse: 20023.4395 - val_loss: 1174436864.0000 - val_rmse: 34270.0586\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353365472.0000 - rmse: 18798.0176 - val_loss: 1356983168.0000 - val_rmse: 36837.2539\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368587136.0000 - rmse: 19198.6230 - val_loss: 1124001152.0000 - val_rmse: 33526.1250\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399934048.0000 - rmse: 19998.3496 - val_loss: 1037242560.0000 - val_rmse: 32206.2500\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333443968.0000 - rmse: 18260.4473 - val_loss: 1149288320.0000 - val_rmse: 33901.1562\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324529120.0000 - rmse: 18014.6895 - val_loss: 1219558400.0000 - val_rmse: 34922.1719\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350852640.0000 - rmse: 18731.0586 - val_loss: 1291657472.0000 - val_rmse: 35939.6367\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 376994656.0000 - rmse: 19416.3496 - val_loss: 1195522432.0000 - val_rmse: 34576.3281\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 346605472.0000 - rmse: 18617.3438 - val_loss: 1236136832.0000 - val_rmse: 35158.7383\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388613536.0000 - rmse: 19713.2832 - val_loss: 1187669504.0000 - val_rmse: 34462.5820\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367701248.0000 - rmse: 19175.5352 - val_loss: 1243696128.0000 - val_rmse: 35266.0742\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353076224.0000 - rmse: 18790.3223 - val_loss: 1157187584.0000 - val_rmse: 34017.4609\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339497216.0000 - rmse: 18425.4492 - val_loss: 1243216256.0000 - val_rmse: 35259.2734\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330814432.0000 - rmse: 18188.3047 - val_loss: 1221794816.0000 - val_rmse: 34954.1836\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378657856.0000 - rmse: 19459.1328 - val_loss: 1390171520.0000 - val_rmse: 37285.0039\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350601760.0000 - rmse: 18724.3613 - val_loss: 1199561984.0000 - val_rmse: 34634.6914\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324582048.0000 - rmse: 18016.1602 - val_loss: 1062241344.0000 - val_rmse: 32592.0410\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330435360.0000 - rmse: 18177.8809 - val_loss: 1251437184.0000 - val_rmse: 35375.6562\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329118272.0000 - rmse: 18141.6172 - val_loss: 1496682240.0000 - val_rmse: 38686.9766\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 377880960.0000 - rmse: 19439.1582 - val_loss: 1180261504.0000 - val_rmse: 34354.9336\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 329962720.0000 - rmse: 18164.8770 - val_loss: 1157512448.0000 - val_rmse: 34022.2344\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298465440.0000 - rmse: 17276.1523 - val_loss: 1205650176.0000 - val_rmse: 34722.4727\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322478048.0000 - rmse: 17957.6719 - val_loss: 1481987072.0000 - val_rmse: 38496.5859\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 412058848.0000 - rmse: 20299.2324 - val_loss: 1178585216.0000 - val_rmse: 34330.5273\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333651296.0000 - rmse: 18266.1250 - val_loss: 1211961600.0000 - val_rmse: 34813.2383\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 342933696.0000 - rmse: 18518.4668 - val_loss: 1488471296.0000 - val_rmse: 38580.7109\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348224000.0000 - rmse: 18660.7598 - val_loss: 1311545984.0000 - val_rmse: 36215.2734\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320250240.0000 - rmse: 17895.5371 - val_loss: 1167625856.0000 - val_rmse: 34170.5391\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336021312.0000 - rmse: 18330.8828 - val_loss: 1179433344.0000 - val_rmse: 34342.8789\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 320553600.0000 - rmse: 17904.0098 - val_loss: 1313279232.0000 - val_rmse: 36239.1953\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354869056.0000 - rmse: 18837.9668 - val_loss: 1127022976.0000 - val_rmse: 33571.1641\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 356688608.0000 - rmse: 18886.2012 - val_loss: 1194390016.0000 - val_rmse: 34559.9492\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277118912.0000 - rmse: 16646.8887 - val_loss: 1308318976.0000 - val_rmse: 36170.6914\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321450496.0000 - rmse: 17929.0391 - val_loss: 1395288192.0000 - val_rmse: 37353.5547\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289983808.0000 - rmse: 17028.9082 - val_loss: 1285386240.0000 - val_rmse: 35852.2852\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307105632.0000 - rmse: 17524.4277 - val_loss: 1401116288.0000 - val_rmse: 37431.4883\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322743584.0000 - rmse: 17965.0645 - val_loss: 1215284608.0000 - val_rmse: 34860.9336\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281800000.0000 - rmse: 16786.8984 - val_loss: 1561015424.0000 - val_rmse: 39509.6836\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300949376.0000 - rmse: 17347.8926 - val_loss: 1085136768.0000 - val_rmse: 32941.4141\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290423456.0000 - rmse: 17041.8125 - val_loss: 1282295552.0000 - val_rmse: 35809.1562\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303899552.0000 - rmse: 17432.7129 - val_loss: 1327923456.0000 - val_rmse: 36440.6836\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260948432.0000 - rmse: 16153.8955 - val_loss: 1331857664.0000 - val_rmse: 36494.6250\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324663328.0000 - rmse: 18018.4160 - val_loss: 1362330624.0000 - val_rmse: 36909.7617\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298146688.0000 - rmse: 17266.9238 - val_loss: 1482154240.0000 - val_rmse: 38498.7578\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278475968.0000 - rmse: 16687.5977 - val_loss: 1409824000.0000 - val_rmse: 37547.6211\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299153216.0000 - rmse: 17296.0449 - val_loss: 1327121920.0000 - val_rmse: 36429.6836\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336232128.0000 - rmse: 18336.6309 - val_loss: 1487480832.0000 - val_rmse: 38567.8711\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350807584.0000 - rmse: 18729.8574 - val_loss: 1372419200.0000 - val_rmse: 37046.1758\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 347912128.0000 - rmse: 18652.4004 - val_loss: 1426510080.0000 - val_rmse: 37769.1680\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312836128.0000 - rmse: 17687.1738 - val_loss: 1403006720.0000 - val_rmse: 37456.7305\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275137952.0000 - rmse: 16587.2812 - val_loss: 1227411968.0000 - val_rmse: 35034.4414\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362751360.0000 - rmse: 19046.0332 - val_loss: 1121557632.0000 - val_rmse: 33489.6641\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328887392.0000 - rmse: 18135.2520 - val_loss: 1233209472.0000 - val_rmse: 35117.0820\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 283061824.0000 - rmse: 16824.4395 - val_loss: 1225610112.0000 - val_rmse: 35008.7148\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295762976.0000 - rmse: 17197.7617 - val_loss: 1091643008.0000 - val_rmse: 33040.0195\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335281120.0000 - rmse: 18310.6816 - val_loss: 1473471104.0000 - val_rmse: 38385.8203\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243793616.0000 - rmse: 15613.8906 - val_loss: 1513859456.0000 - val_rmse: 38908.3477\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270231488.0000 - rmse: 16438.7188 - val_loss: 1077084672.0000 - val_rmse: 32818.9688\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 298888000.0000 - rmse: 17288.3750 - val_loss: 1163071744.0000 - val_rmse: 34103.8359\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280138176.0000 - rmse: 16737.3281 - val_loss: 1304271488.0000 - val_rmse: 36114.6992\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259234912.0000 - rmse: 16100.7725 - val_loss: 1302790656.0000 - val_rmse: 36094.1914\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 308752480.0000 - rmse: 17571.3535 - val_loss: 1152193536.0000 - val_rmse: 33943.9766\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295742688.0000 - rmse: 17197.1699 - val_loss: 1264145920.0000 - val_rmse: 35554.8281\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255831280.0000 - rmse: 15994.7256 - val_loss: 1330219264.0000 - val_rmse: 36472.1680\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294327200.0000 - rmse: 17155.9648 - val_loss: 1113103104.0000 - val_rmse: 33363.1992\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266310112.0000 - rmse: 16319.0098 - val_loss: 1014105664.0000 - val_rmse: 31845.0254\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271683296.0000 - rmse: 16482.8164 - val_loss: 1077666176.0000 - val_rmse: 32827.8242\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327685760.0000 - rmse: 18102.0918 - val_loss: 1433644288.0000 - val_rmse: 37863.4961\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 233670752.0000 - rmse: 15286.2920 - val_loss: 1300714880.0000 - val_rmse: 36065.4258\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 366146944.0000 - rmse: 19134.9668 - val_loss: 1552284544.0000 - val_rmse: 39399.0391\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271611712.0000 - rmse: 16480.6445 - val_loss: 1472254592.0000 - val_rmse: 38369.9688\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278666464.0000 - rmse: 16693.3047 - val_loss: 1530393472.0000 - val_rmse: 39120.2422\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293547392.0000 - rmse: 17133.2246 - val_loss: 1252499072.0000 - val_rmse: 35390.6641\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264671776.0000 - rmse: 16268.7344 - val_loss: 1193835008.0000 - val_rmse: 34551.9180\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262591648.0000 - rmse: 16204.6797 - val_loss: 1275903616.0000 - val_rmse: 35719.7930\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259828736.0000 - rmse: 16119.2021 - val_loss: 1277376256.0000 - val_rmse: 35740.4023\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301611232.0000 - rmse: 17366.9570 - val_loss: 1260427136.0000 - val_rmse: 35502.4961\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275405312.0000 - rmse: 16595.3379 - val_loss: 1469532800.0000 - val_rmse: 38334.4844\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270348960.0000 - rmse: 16442.2910 - val_loss: 1404054912.0000 - val_rmse: 37470.7227\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278924832.0000 - rmse: 16701.0410 - val_loss: 1616764928.0000 - val_rmse: 40209.0156\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271982048.0000 - rmse: 16491.8770 - val_loss: 1550168704.0000 - val_rmse: 39372.1836\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272222880.0000 - rmse: 16499.1777 - val_loss: 1332619008.0000 - val_rmse: 36505.0547\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302960992.0000 - rmse: 17405.7754 - val_loss: 1444934784.0000 - val_rmse: 38012.2969\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299282976.0000 - rmse: 17299.7969 - val_loss: 1428507392.0000 - val_rmse: 37795.6016\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250114048.0000 - rmse: 15814.9922 - val_loss: 1579161472.0000 - val_rmse: 39738.6641\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238064736.0000 - rmse: 15429.3447 - val_loss: 1457024000.0000 - val_rmse: 38170.9844\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237515504.0000 - rmse: 15411.5361 - val_loss: 1611374208.0000 - val_rmse: 40141.9258\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280841376.0000 - rmse: 16758.3223 - val_loss: 1470519808.0000 - val_rmse: 38347.3555\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252076496.0000 - rmse: 15876.9160 - val_loss: 1409959680.0000 - val_rmse: 37549.4297\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273578464.0000 - rmse: 16540.2070 - val_loss: 1520476800.0000 - val_rmse: 38993.2930\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292303680.0000 - rmse: 17096.8887 - val_loss: 1489085952.0000 - val_rmse: 38588.6758\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 274570528.0000 - rmse: 16570.1680 - val_loss: 1469509760.0000 - val_rmse: 38334.1836\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291623872.0000 - rmse: 17076.9980 - val_loss: 1514914816.0000 - val_rmse: 38921.9062\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 269229120.0000 - rmse: 16408.2012 - val_loss: 1538254720.0000 - val_rmse: 39220.5898\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 316033056.0000 - rmse: 17777.3184 - val_loss: 1314341376.0000 - val_rmse: 36253.8477\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264053504.0000 - rmse: 16249.7217 - val_loss: 1610427776.0000 - val_rmse: 40130.1328\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217951104.0000 - rmse: 14763.1660 - val_loss: 1494195328.0000 - val_rmse: 38654.8242\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253243376.0000 - rmse: 15913.6211 - val_loss: 1341654528.0000 - val_rmse: 36628.6016\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232157584.0000 - rmse: 15236.7158 - val_loss: 1152615424.0000 - val_rmse: 33950.1914\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251608080.0000 - rmse: 15862.1572 - val_loss: 1490109696.0000 - val_rmse: 38601.9375\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276579456.0000 - rmse: 16630.6777 - val_loss: 1702329728.0000 - val_rmse: 41259.2969\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255171104.0000 - rmse: 15974.0752 - val_loss: 1459582336.0000 - val_rmse: 38204.4805\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307325888.0000 - rmse: 17530.7109 - val_loss: 1587959296.0000 - val_rmse: 39849.2070\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265714128.0000 - rmse: 16300.7383 - val_loss: 1745099008.0000 - val_rmse: 41774.3828\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256554272.0000 - rmse: 16017.3105 - val_loss: 1601843072.0000 - val_rmse: 40023.0312\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244576320.0000 - rmse: 15638.9346 - val_loss: 1696113152.0000 - val_rmse: 41183.8945\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229043680.0000 - rmse: 15134.1885 - val_loss: 1781311232.0000 - val_rmse: 42205.5820\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260400784.0000 - rmse: 16136.9365 - val_loss: 1354767360.0000 - val_rmse: 36807.1641\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243039264.0000 - rmse: 15589.7148 - val_loss: 1648624896.0000 - val_rmse: 40603.2617\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296351296.0000 - rmse: 17214.8555 - val_loss: 1584674688.0000 - val_rmse: 39807.9727\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271450112.0000 - rmse: 16475.7402 - val_loss: 1543575808.0000 - val_rmse: 39288.3672\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 224910592.0000 - rmse: 14997.0195 - val_loss: 1472744064.0000 - val_rmse: 38376.3477\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 243218048.0000 - rmse: 15595.4492 - val_loss: 1560745600.0000 - val_rmse: 39506.2695\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248016544.0000 - rmse: 15748.5391 - val_loss: 1926203520.0000 - val_rmse: 43888.5352\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250020928.0000 - rmse: 15812.0498 - val_loss: 1610482048.0000 - val_rmse: 40130.8125\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259167056.0000 - rmse: 16098.6650 - val_loss: 1868737024.0000 - val_rmse: 43228.8906\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284469888.0000 - rmse: 16866.2344 - val_loss: 1654145664.0000 - val_rmse: 40671.1875\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234542624.0000 - rmse: 15314.7842 - val_loss: 1883362304.0000 - val_rmse: 43397.7227\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219053408.0000 - rmse: 14800.4521 - val_loss: 1947352576.0000 - val_rmse: 44128.8164\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210626288.0000 - rmse: 14512.9688 - val_loss: 2237614336.0000 - val_rmse: 47303.4297\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300982592.0000 - rmse: 17348.8496 - val_loss: 1831249536.0000 - val_rmse: 42793.1016\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239378208.0000 - rmse: 15471.8525 - val_loss: 1958963840.0000 - val_rmse: 44260.1836\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247285360.0000 - rmse: 15725.3066 - val_loss: 1478702080.0000 - val_rmse: 38453.8945\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250923712.0000 - rmse: 15840.5713 - val_loss: 1966837504.0000 - val_rmse: 44349.0430\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254227584.0000 - rmse: 15944.5137 - val_loss: 1397483008.0000 - val_rmse: 37382.9258\n",
      "Epoch 367/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272744832.0000 - rmse: 16514.9863 - val_loss: 1508192768.0000 - val_rmse: 38835.4570\n",
      "Epoch 368/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232691440.0000 - rmse: 15254.2256 - val_loss: 1618032256.0000 - val_rmse: 40224.7734\n",
      "Epoch 369/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211296528.0000 - rmse: 14536.0410 - val_loss: 1405758336.0000 - val_rmse: 37493.4453\n",
      "Epoch 370/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244103456.0000 - rmse: 15623.8096 - val_loss: 1716571392.0000 - val_rmse: 41431.5273\n",
      "Epoch 371/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228420496.0000 - rmse: 15113.5850 - val_loss: 1717808768.0000 - val_rmse: 41446.4570\n",
      "Epoch 372/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217356176.0000 - rmse: 14743.0020 - val_loss: 1897815680.0000 - val_rmse: 43563.9258\n",
      "Epoch 373/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285729024.0000 - rmse: 16903.5195 - val_loss: 1479902592.0000 - val_rmse: 38469.5039\n",
      "Epoch 374/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 212998848.0000 - rmse: 14594.4775 - val_loss: 1658774144.0000 - val_rmse: 40728.0508\n",
      "Epoch 375/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252722624.0000 - rmse: 15897.2520 - val_loss: 1701492864.0000 - val_rmse: 41249.1562\n",
      "Epoch 376/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219735760.0000 - rmse: 14823.4863 - val_loss: 1493814144.0000 - val_rmse: 38649.8945\n",
      "Epoch 377/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 201369232.0000 - rmse: 14190.4609 - val_loss: 1505853440.0000 - val_rmse: 38805.3281\n",
      "Epoch 378/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240741280.0000 - rmse: 15515.8398 - val_loss: 1527695616.0000 - val_rmse: 39085.7461\n",
      "Epoch 379/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 194319456.0000 - rmse: 13939.8506 - val_loss: 1300375552.0000 - val_rmse: 36060.7188\n",
      "Epoch 380/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247622288.0000 - rmse: 15736.0176 - val_loss: 1284369408.0000 - val_rmse: 35838.1016\n",
      "Epoch 381/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259562608.0000 - rmse: 16110.9463 - val_loss: 1648447744.0000 - val_rmse: 40601.0820\n",
      "Epoch 382/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236087536.0000 - rmse: 15365.1387 - val_loss: 1666814592.0000 - val_rmse: 40826.6406\n",
      "Epoch 383/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250942448.0000 - rmse: 15841.1631 - val_loss: 1597000320.0000 - val_rmse: 39962.4883\n",
      "Epoch 384/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202347152.0000 - rmse: 14224.8760 - val_loss: 1682024320.0000 - val_rmse: 41012.4883\n",
      "Epoch 385/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278445280.0000 - rmse: 16686.6777 - val_loss: 1965373696.0000 - val_rmse: 44332.5352\n",
      "Epoch 386/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237879904.0000 - rmse: 15423.3545 - val_loss: 1794046592.0000 - val_rmse: 42356.1875\n",
      "Epoch 387/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255537808.0000 - rmse: 15985.5488 - val_loss: 1818875520.0000 - val_rmse: 42648.2773\n",
      "Epoch 388/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229741136.0000 - rmse: 15157.2139 - val_loss: 1528707968.0000 - val_rmse: 39098.6953\n",
      "Epoch 389/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210985968.0000 - rmse: 14525.3545 - val_loss: 1809003136.0000 - val_rmse: 42532.3750\n",
      "Epoch 390/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253350688.0000 - rmse: 15916.9932 - val_loss: 2340869888.0000 - val_rmse: 48382.5352\n",
      "Epoch 391/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266420608.0000 - rmse: 16322.3945 - val_loss: 1977399424.0000 - val_rmse: 44467.9609\n",
      "Epoch 392/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215727456.0000 - rmse: 14687.6611 - val_loss: 2050840704.0000 - val_rmse: 45286.2070\n",
      "Epoch 393/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 209571456.0000 - rmse: 14476.5811 - val_loss: 1623097728.0000 - val_rmse: 40287.6836\n",
      "Epoch 394/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218542912.0000 - rmse: 14783.1943 - val_loss: 1874828160.0000 - val_rmse: 43299.2852\n",
      "Epoch 395/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286503520.0000 - rmse: 16926.4121 - val_loss: 1686282624.0000 - val_rmse: 41064.3672\n",
      "Epoch 396/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214149152.0000 - rmse: 14633.8330 - val_loss: 1846517632.0000 - val_rmse: 42971.1250\n",
      "Epoch 397/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 207093616.0000 - rmse: 14390.7451 - val_loss: 1721049856.0000 - val_rmse: 41485.5391\n",
      "Epoch 398/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 205177424.0000 - rmse: 14324.0146 - val_loss: 1718098304.0000 - val_rmse: 41449.9492\n",
      "Epoch 399/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222496480.0000 - rmse: 14916.3145 - val_loss: 1666172416.0000 - val_rmse: 40818.7734\n",
      "Epoch 400/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 233555616.0000 - rmse: 15282.5254 - val_loss: 1839214976.0000 - val_rmse: 42886.0703\n",
      "104/104 [==============================] - 0s 696us/step - loss: 884871872.0000 - rmse: 29746.7930\n",
      "[884871872.0, 29746.79296875]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 23473182720.0000 - rmse: 153209.6094 - val_loss: 12384513024.0000 - val_rmse: 111285.7266\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 5129243648.0000 - rmse: 71618.7344 - val_loss: 1967848576.0000 - val_rmse: 44360.4375\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2342409216.0000 - rmse: 48398.4414 - val_loss: 1494126720.0000 - val_rmse: 38653.9336\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1950848128.0000 - rmse: 44168.4062 - val_loss: 1296668672.0000 - val_rmse: 36009.2852\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1870164352.0000 - rmse: 43245.3984 - val_loss: 1194167296.0000 - val_rmse: 34556.7266\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1756542336.0000 - rmse: 41911.1250 - val_loss: 1138920832.0000 - val_rmse: 33747.9023\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1648684800.0000 - rmse: 40604.0000 - val_loss: 1061180608.0000 - val_rmse: 32575.7676\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1586731008.0000 - rmse: 39833.7930 - val_loss: 1028436224.0000 - val_rmse: 32069.2402\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1525058048.0000 - rmse: 39051.9922 - val_loss: 985186304.0000 - val_rmse: 31387.6777\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1551741696.0000 - rmse: 39392.1523 - val_loss: 953484864.0000 - val_rmse: 30878.5508\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1491077504.0000 - rmse: 38614.4727 - val_loss: 916821248.0000 - val_rmse: 30279.0566\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1405670272.0000 - rmse: 37492.2695 - val_loss: 927677760.0000 - val_rmse: 30457.8027\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1332654592.0000 - rmse: 36505.5391 - val_loss: 892220288.0000 - val_rmse: 29870.0566\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1416122240.0000 - rmse: 37631.3984 - val_loss: 892322176.0000 - val_rmse: 29871.7617\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1364889344.0000 - rmse: 36944.4102 - val_loss: 857370624.0000 - val_rmse: 29280.8926\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1311572992.0000 - rmse: 36215.6445 - val_loss: 846064576.0000 - val_rmse: 29087.1895\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1257848448.0000 - rmse: 35466.1602 - val_loss: 830867136.0000 - val_rmse: 28824.7656\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1238807552.0000 - rmse: 35196.6992 - val_loss: 842756608.0000 - val_rmse: 29030.2695\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1242920064.0000 - rmse: 35255.0703 - val_loss: 824257856.0000 - val_rmse: 28709.8906\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1174212608.0000 - rmse: 34266.7852 - val_loss: 898540352.0000 - val_rmse: 29975.6621\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1228342144.0000 - rmse: 35047.7109 - val_loss: 850872128.0000 - val_rmse: 29169.7129\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1147384576.0000 - rmse: 33873.0664 - val_loss: 891298752.0000 - val_rmse: 29854.6270\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1180103680.0000 - rmse: 34352.6367 - val_loss: 906954048.0000 - val_rmse: 30115.6777\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1112332544.0000 - rmse: 33351.6484 - val_loss: 857143744.0000 - val_rmse: 29277.0176\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1140465408.0000 - rmse: 33770.7773 - val_loss: 856217088.0000 - val_rmse: 29261.1875\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1096025216.0000 - rmse: 33106.2734 - val_loss: 834359424.0000 - val_rmse: 28885.2812\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1120202240.0000 - rmse: 33469.4219 - val_loss: 834949696.0000 - val_rmse: 28895.4941\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1090692480.0000 - rmse: 33025.6328 - val_loss: 871607552.0000 - val_rmse: 29523.0000\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1043875200.0000 - rmse: 32309.0566 - val_loss: 853606208.0000 - val_rmse: 29216.5391\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1089548160.0000 - rmse: 33008.3047 - val_loss: 867202240.0000 - val_rmse: 29448.2969\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1031787328.0000 - rmse: 32121.4473 - val_loss: 874525504.0000 - val_rmse: 29572.3770\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1047511616.0000 - rmse: 32365.2832 - val_loss: 844051648.0000 - val_rmse: 29052.5664\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1024731264.0000 - rmse: 32011.4238 - val_loss: 845756224.0000 - val_rmse: 29081.8887\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1055313984.0000 - rmse: 32485.5957 - val_loss: 1045567936.0000 - val_rmse: 32335.2422\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1067580928.0000 - rmse: 32673.8574 - val_loss: 899371072.0000 - val_rmse: 29989.5156\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 981668160.0000 - rmse: 31331.5840 - val_loss: 856469888.0000 - val_rmse: 29265.5059\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 965953920.0000 - rmse: 31079.7988 - val_loss: 821747136.0000 - val_rmse: 28666.1328\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1005745408.0000 - rmse: 31713.4902 - val_loss: 827461952.0000 - val_rmse: 28765.6387\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 946865536.0000 - rmse: 30771.1797 - val_loss: 842828928.0000 - val_rmse: 29031.5156\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 945128320.0000 - rmse: 30742.9395 - val_loss: 838695808.0000 - val_rmse: 28960.2461\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 940533440.0000 - rmse: 30668.1172 - val_loss: 854726080.0000 - val_rmse: 29235.6992\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 903863232.0000 - rmse: 30064.3184 - val_loss: 880623936.0000 - val_rmse: 29675.3086\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1005354048.0000 - rmse: 31707.3184 - val_loss: 843271232.0000 - val_rmse: 29039.1328\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 877439040.0000 - rmse: 29621.5977 - val_loss: 876054016.0000 - val_rmse: 29598.2090\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 898025536.0000 - rmse: 29967.0742 - val_loss: 805603520.0000 - val_rmse: 28383.1562\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 890248640.0000 - rmse: 29837.0352 - val_loss: 869365376.0000 - val_rmse: 29485.0020\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 885563200.0000 - rmse: 29758.4141 - val_loss: 872795072.0000 - val_rmse: 29543.1055\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 902121984.0000 - rmse: 30035.3418 - val_loss: 815374272.0000 - val_rmse: 28554.7598\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 881891520.0000 - rmse: 29696.6582 - val_loss: 999985728.0000 - val_rmse: 31622.5508\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 826684800.0000 - rmse: 28752.1270 - val_loss: 846571392.0000 - val_rmse: 29095.9004\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 810857984.0000 - rmse: 28475.5684 - val_loss: 851454592.0000 - val_rmse: 29179.6953\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 813029120.0000 - rmse: 28513.6660 - val_loss: 875236992.0000 - val_rmse: 29584.4043\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 853035520.0000 - rmse: 29206.7715 - val_loss: 1107736064.0000 - val_rmse: 33282.6680\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 724364032.0000 - rmse: 26914.0117 - val_loss: 826025984.0000 - val_rmse: 28740.6680\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 790898944.0000 - rmse: 28122.9258 - val_loss: 846065344.0000 - val_rmse: 29087.2031\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 764308416.0000 - rmse: 27646.1289 - val_loss: 796780032.0000 - val_rmse: 28227.2930\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 814868032.0000 - rmse: 28545.8926 - val_loss: 923167616.0000 - val_rmse: 30383.6738\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 713346304.0000 - rmse: 26708.5430 - val_loss: 1019023232.0000 - val_rmse: 31922.1426\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 693245952.0000 - rmse: 26329.5645 - val_loss: 861783424.0000 - val_rmse: 29356.1484\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 699066368.0000 - rmse: 26439.8633 - val_loss: 863970560.0000 - val_rmse: 29393.3770\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 705962944.0000 - rmse: 26569.9629 - val_loss: 886177920.0000 - val_rmse: 29768.7402\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 781316672.0000 - rmse: 27952.0430 - val_loss: 766573888.0000 - val_rmse: 27687.0703\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649213696.0000 - rmse: 25479.6719 - val_loss: 1251971840.0000 - val_rmse: 35383.2148\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 638871744.0000 - rmse: 25275.9102 - val_loss: 844178496.0000 - val_rmse: 29054.7500\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630362752.0000 - rmse: 25107.0254 - val_loss: 858778624.0000 - val_rmse: 29304.9258\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 705049984.0000 - rmse: 26552.7773 - val_loss: 810210752.0000 - val_rmse: 28464.2012\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 627739264.0000 - rmse: 25054.7246 - val_loss: 813287488.0000 - val_rmse: 28518.1953\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 696741760.0000 - rmse: 26395.8672 - val_loss: 784793408.0000 - val_rmse: 28014.1641\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 621658496.0000 - rmse: 24933.0801 - val_loss: 880124160.0000 - val_rmse: 29666.8867\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 579508160.0000 - rmse: 24072.9766 - val_loss: 879889728.0000 - val_rmse: 29662.9336\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626830272.0000 - rmse: 25036.5781 - val_loss: 996189632.0000 - val_rmse: 31562.4727\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 725023360.0000 - rmse: 26926.2578 - val_loss: 932530624.0000 - val_rmse: 30537.3652\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661376256.0000 - rmse: 25717.2344 - val_loss: 823810944.0000 - val_rmse: 28702.1055\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 558835712.0000 - rmse: 23639.7070 - val_loss: 1505987200.0000 - val_rmse: 38807.0508\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 618689216.0000 - rmse: 24873.4648 - val_loss: 887759872.0000 - val_rmse: 29795.2988\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576137536.0000 - rmse: 24002.8652 - val_loss: 835409088.0000 - val_rmse: 28903.4434\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583413248.0000 - rmse: 24153.9473 - val_loss: 837066176.0000 - val_rmse: 28932.0957\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497180544.0000 - rmse: 22297.5449 - val_loss: 717992896.0000 - val_rmse: 26795.3887\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 515880032.0000 - rmse: 22712.9922 - val_loss: 1297340800.0000 - val_rmse: 36018.6172\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 588004352.0000 - rmse: 24248.8008 - val_loss: 781340928.0000 - val_rmse: 27952.4766\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 549876480.0000 - rmse: 23449.4453 - val_loss: 745555904.0000 - val_rmse: 27304.8691\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 555444480.0000 - rmse: 23567.8691 - val_loss: 880275904.0000 - val_rmse: 29669.4434\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 514479136.0000 - rmse: 22682.1328 - val_loss: 1043488640.0000 - val_rmse: 32303.0742\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 522714848.0000 - rmse: 22862.9570 - val_loss: 819272768.0000 - val_rmse: 28622.9414\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520363104.0000 - rmse: 22811.4688 - val_loss: 894160960.0000 - val_rmse: 29902.5234\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520803712.0000 - rmse: 22821.1250 - val_loss: 784332480.0000 - val_rmse: 28005.9355\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 526487776.0000 - rmse: 22945.3223 - val_loss: 752855744.0000 - val_rmse: 27438.2168\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 508388320.0000 - rmse: 22547.4668 - val_loss: 789658944.0000 - val_rmse: 28100.8711\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 512360256.0000 - rmse: 22635.3770 - val_loss: 797156032.0000 - val_rmse: 28233.9492\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495537696.0000 - rmse: 22260.6758 - val_loss: 937632192.0000 - val_rmse: 30620.7793\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502338528.0000 - rmse: 22412.9082 - val_loss: 950585088.0000 - val_rmse: 30831.5605\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 486318912.0000 - rmse: 22052.6387 - val_loss: 749453952.0000 - val_rmse: 27376.1562\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418751136.0000 - rmse: 20463.4102 - val_loss: 992777152.0000 - val_rmse: 31508.3672\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 473655104.0000 - rmse: 21763.6191 - val_loss: 777349952.0000 - val_rmse: 27880.9961\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482260704.0000 - rmse: 21960.4355 - val_loss: 1045895040.0000 - val_rmse: 32340.3008\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485153984.0000 - rmse: 22026.2109 - val_loss: 781137024.0000 - val_rmse: 27948.8281\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 427679648.0000 - rmse: 20680.4141 - val_loss: 1058980224.0000 - val_rmse: 32541.9766\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 481204512.0000 - rmse: 21936.3750 - val_loss: 906413952.0000 - val_rmse: 30106.7090\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374379776.0000 - rmse: 19348.8965 - val_loss: 1131764992.0000 - val_rmse: 33641.7148\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 448838080.0000 - rmse: 21185.7988 - val_loss: 1027402560.0000 - val_rmse: 32053.1211\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438577088.0000 - rmse: 20942.2324 - val_loss: 1108049792.0000 - val_rmse: 33287.3789\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425152288.0000 - rmse: 20619.2207 - val_loss: 1123441408.0000 - val_rmse: 33517.7773\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 425944000.0000 - rmse: 20638.4102 - val_loss: 1440085632.0000 - val_rmse: 37948.4570\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 530825408.0000 - rmse: 23039.6484 - val_loss: 707860992.0000 - val_rmse: 26605.6562\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435170848.0000 - rmse: 20860.7480 - val_loss: 733542080.0000 - val_rmse: 27083.9824\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464646080.0000 - rmse: 21555.6504 - val_loss: 834849280.0000 - val_rmse: 28893.7578\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 407098496.0000 - rmse: 20176.6816 - val_loss: 890900672.0000 - val_rmse: 29847.9590\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 378118496.0000 - rmse: 19445.2695 - val_loss: 1195057664.0000 - val_rmse: 34569.6055\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 418122048.0000 - rmse: 20448.0312 - val_loss: 981945216.0000 - val_rmse: 31336.0059\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 447088672.0000 - rmse: 21144.4707 - val_loss: 1022777408.0000 - val_rmse: 31980.8887\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420970240.0000 - rmse: 20517.5586 - val_loss: 811702656.0000 - val_rmse: 28490.3965\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367980256.0000 - rmse: 19182.8105 - val_loss: 639707712.0000 - val_rmse: 25292.4434\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348256224.0000 - rmse: 18661.6250 - val_loss: 1020411968.0000 - val_rmse: 31943.8887\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497639040.0000 - rmse: 22307.8242 - val_loss: 987955264.0000 - val_rmse: 31431.7559\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441410144.0000 - rmse: 21009.7617 - val_loss: 1639559296.0000 - val_rmse: 40491.4727\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420688192.0000 - rmse: 20510.6855 - val_loss: 1742144896.0000 - val_rmse: 41739.0078\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 439347456.0000 - rmse: 20960.6172 - val_loss: 1007896000.0000 - val_rmse: 31747.3750\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327728448.0000 - rmse: 18103.2715 - val_loss: 1913668224.0000 - val_rmse: 43745.4922\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 364824256.0000 - rmse: 19100.3730 - val_loss: 1718488832.0000 - val_rmse: 41454.6602\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424923680.0000 - rmse: 20613.6777 - val_loss: 1007096640.0000 - val_rmse: 31734.7852\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354946592.0000 - rmse: 18840.0254 - val_loss: 1148971776.0000 - val_rmse: 33896.4844\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379568448.0000 - rmse: 19482.5156 - val_loss: 1211358080.0000 - val_rmse: 34804.5703\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334150112.0000 - rmse: 18279.7734 - val_loss: 1240818560.0000 - val_rmse: 35225.2539\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 539149504.0000 - rmse: 23219.5938 - val_loss: 2115231488.0000 - val_rmse: 45991.6445\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403161344.0000 - rmse: 20078.8789 - val_loss: 1254797184.0000 - val_rmse: 35423.1133\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 428246144.0000 - rmse: 20694.1094 - val_loss: 1147463424.0000 - val_rmse: 33874.2266\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367987520.0000 - rmse: 19183.0000 - val_loss: 1361784576.0000 - val_rmse: 36902.3672\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353697504.0000 - rmse: 18806.8457 - val_loss: 794792832.0000 - val_rmse: 28192.0703\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 432327712.0000 - rmse: 20792.4922 - val_loss: 1258806912.0000 - val_rmse: 35479.6641\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436501952.0000 - rmse: 20892.6289 - val_loss: 1833100416.0000 - val_rmse: 42814.7227\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 434523968.0000 - rmse: 20845.2383 - val_loss: 1569620864.0000 - val_rmse: 39618.4414\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395795072.0000 - rmse: 19894.5996 - val_loss: 1341184128.0000 - val_rmse: 36622.1797\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 400008128.0000 - rmse: 20000.2031 - val_loss: 1187131648.0000 - val_rmse: 34454.7773\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362269408.0000 - rmse: 19033.3750 - val_loss: 801789504.0000 - val_rmse: 28315.8887\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352462400.0000 - rmse: 18773.9824 - val_loss: 1417986944.0000 - val_rmse: 37656.1680\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345865760.0000 - rmse: 18597.4648 - val_loss: 1248605312.0000 - val_rmse: 35335.6094\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337264448.0000 - rmse: 18364.7598 - val_loss: 1240826752.0000 - val_rmse: 35225.3711\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396857952.0000 - rmse: 19921.2930 - val_loss: 1582153472.0000 - val_rmse: 39776.2930\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341426624.0000 - rmse: 18477.7324 - val_loss: 904130240.0000 - val_rmse: 30068.7559\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350142848.0000 - rmse: 18712.1035 - val_loss: 1083243904.0000 - val_rmse: 32912.6719\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 386083104.0000 - rmse: 19648.9961 - val_loss: 1082246144.0000 - val_rmse: 32897.5078\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 362117088.0000 - rmse: 19029.3750 - val_loss: 1107311104.0000 - val_rmse: 33276.2852\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322503776.0000 - rmse: 17958.3887 - val_loss: 739220544.0000 - val_rmse: 27188.6113\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333512768.0000 - rmse: 18262.3320 - val_loss: 1256834432.0000 - val_rmse: 35451.8594\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279438560.0000 - rmse: 16716.4141 - val_loss: 1410840192.0000 - val_rmse: 37561.1523\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350089920.0000 - rmse: 18710.6895 - val_loss: 957640960.0000 - val_rmse: 30945.7754\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350540224.0000 - rmse: 18722.7188 - val_loss: 1087745024.0000 - val_rmse: 32980.9805\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312170624.0000 - rmse: 17668.3516 - val_loss: 1165833216.0000 - val_rmse: 34144.2969\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357639232.0000 - rmse: 18911.3516 - val_loss: 1352339328.0000 - val_rmse: 36774.1680\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309814944.0000 - rmse: 17601.5605 - val_loss: 1273218304.0000 - val_rmse: 35682.1836\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319758048.0000 - rmse: 17881.7793 - val_loss: 1387730688.0000 - val_rmse: 37252.2578\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 395600224.0000 - rmse: 19889.7012 - val_loss: 1212197632.0000 - val_rmse: 34816.6289\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 387741856.0000 - rmse: 19691.1621 - val_loss: 1138802944.0000 - val_rmse: 33746.1523\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296967040.0000 - rmse: 17232.7324 - val_loss: 972991424.0000 - val_rmse: 31192.8105\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306970016.0000 - rmse: 17520.5605 - val_loss: 1273294464.0000 - val_rmse: 35683.2500\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287805408.0000 - rmse: 16964.8281 - val_loss: 1359812608.0000 - val_rmse: 36875.6367\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249357248.0000 - rmse: 15791.0488 - val_loss: 839364672.0000 - val_rmse: 28971.7910\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319769120.0000 - rmse: 17882.0898 - val_loss: 1028548288.0000 - val_rmse: 32070.9863\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253280016.0000 - rmse: 15914.7734 - val_loss: 1273513856.0000 - val_rmse: 35686.3242\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289983168.0000 - rmse: 17028.8926 - val_loss: 1189757824.0000 - val_rmse: 34492.8672\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296556928.0000 - rmse: 17220.8281 - val_loss: 935067840.0000 - val_rmse: 30578.8789\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242734720.0000 - rmse: 15579.9453 - val_loss: 1482265728.0000 - val_rmse: 38500.2031\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305023872.0000 - rmse: 17464.9336 - val_loss: 1467542528.0000 - val_rmse: 38308.5195\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304254144.0000 - rmse: 17442.8828 - val_loss: 1939954688.0000 - val_rmse: 44044.9180\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264270384.0000 - rmse: 16256.3945 - val_loss: 1184352000.0000 - val_rmse: 34414.4141\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311852960.0000 - rmse: 17659.3594 - val_loss: 1552276864.0000 - val_rmse: 39398.9414\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323959520.0000 - rmse: 17998.8750 - val_loss: 1844886400.0000 - val_rmse: 42952.1406\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264052032.0000 - rmse: 16249.6768 - val_loss: 1878818688.0000 - val_rmse: 43345.3438\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281629824.0000 - rmse: 16781.8301 - val_loss: 1980205312.0000 - val_rmse: 44499.4961\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263429360.0000 - rmse: 16230.5068 - val_loss: 1649232512.0000 - val_rmse: 40610.7422\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323425888.0000 - rmse: 17984.0449 - val_loss: 1538075008.0000 - val_rmse: 39218.3008\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277465216.0000 - rmse: 16657.2871 - val_loss: 1377970560.0000 - val_rmse: 37121.0273\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301104480.0000 - rmse: 17352.3633 - val_loss: 1299624832.0000 - val_rmse: 36050.3086\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323550848.0000 - rmse: 17987.5195 - val_loss: 1655891712.0000 - val_rmse: 40692.6484\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350490784.0000 - rmse: 18721.3984 - val_loss: 1705551488.0000 - val_rmse: 41298.3242\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275168096.0000 - rmse: 16588.1914 - val_loss: 1700180864.0000 - val_rmse: 41233.2461\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 303562592.0000 - rmse: 17423.0469 - val_loss: 2014990208.0000 - val_rmse: 44888.6406\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273805248.0000 - rmse: 16547.0625 - val_loss: 1354489088.0000 - val_rmse: 36803.3828\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272548384.0000 - rmse: 16509.0391 - val_loss: 1827142912.0000 - val_rmse: 42745.0938\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 275218464.0000 - rmse: 16589.7090 - val_loss: 2133054720.0000 - val_rmse: 46185.0039\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254104112.0000 - rmse: 15940.6436 - val_loss: 2142510464.0000 - val_rmse: 46287.2617\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 309842112.0000 - rmse: 17602.3320 - val_loss: 1914876672.0000 - val_rmse: 43759.3047\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263319472.0000 - rmse: 16227.1211 - val_loss: 1946306560.0000 - val_rmse: 44116.9648\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420925920.0000 - rmse: 20516.4785 - val_loss: 1103533696.0000 - val_rmse: 33219.4766\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244989984.0000 - rmse: 15652.1553 - val_loss: 2701182976.0000 - val_rmse: 51972.9062\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 318124928.0000 - rmse: 17836.0566 - val_loss: 1591277824.0000 - val_rmse: 39890.8242\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319396224.0000 - rmse: 17871.6582 - val_loss: 1360270592.0000 - val_rmse: 36881.8477\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293434048.0000 - rmse: 17129.9160 - val_loss: 1392480896.0000 - val_rmse: 37315.9609\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258737344.0000 - rmse: 16085.3145 - val_loss: 1409232896.0000 - val_rmse: 37539.7500\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267273712.0000 - rmse: 16348.5078 - val_loss: 1524614656.0000 - val_rmse: 39046.3125\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257246848.0000 - rmse: 16038.9160 - val_loss: 1386576384.0000 - val_rmse: 37236.7617\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297113472.0000 - rmse: 17236.9766 - val_loss: 1543605888.0000 - val_rmse: 39288.7500\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323297344.0000 - rmse: 17980.4707 - val_loss: 1542517376.0000 - val_rmse: 39274.8945\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 271546432.0000 - rmse: 16478.6660 - val_loss: 1266398592.0000 - val_rmse: 35586.4961\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267474960.0000 - rmse: 16354.6611 - val_loss: 1887436032.0000 - val_rmse: 43444.6328\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338242080.0000 - rmse: 18391.3574 - val_loss: 1579480448.0000 - val_rmse: 39742.6797\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245549584.0000 - rmse: 15670.0215 - val_loss: 1424381824.0000 - val_rmse: 37740.9844\n",
      "104/104 [==============================] - 0s 754us/step - loss: 488868000.0000 - rmse: 22110.3574\n",
      "[488868000.0, 22110.357421875]\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 128)               15744     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,745\n",
      "Trainable params: 20,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 22289817600.0000 - rmse: 149297.7500 - val_loss: 7491725824.0000 - val_rmse: 86554.7578\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 3978954240.0000 - rmse: 63078.9531 - val_loss: 1853703424.0000 - val_rmse: 43054.6562\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2334997504.0000 - rmse: 48321.8125 - val_loss: 1428681344.0000 - val_rmse: 37797.9023\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2000231808.0000 - rmse: 44723.9492 - val_loss: 1311543808.0000 - val_rmse: 36215.2422\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1826005504.0000 - rmse: 42731.7852 - val_loss: 1184201984.0000 - val_rmse: 34412.2344\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1785039232.0000 - rmse: 42249.7266 - val_loss: 1091854336.0000 - val_rmse: 33043.2188\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1708462464.0000 - rmse: 41333.5508 - val_loss: 1050860032.0000 - val_rmse: 32416.9707\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1610120064.0000 - rmse: 40126.3008 - val_loss: 1021125632.0000 - val_rmse: 31955.0566\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1517356544.0000 - rmse: 38953.2617 - val_loss: 1095739392.0000 - val_rmse: 33101.9531\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1501670528.0000 - rmse: 38751.3945 - val_loss: 964270080.0000 - val_rmse: 31052.6992\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1466432128.0000 - rmse: 38294.0234 - val_loss: 930072000.0000 - val_rmse: 30497.0820\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1437570176.0000 - rmse: 37915.3008 - val_loss: 921628480.0000 - val_rmse: 30358.3340\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1372551680.0000 - rmse: 37047.9648 - val_loss: 912689024.0000 - val_rmse: 30210.7441\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1298998912.0000 - rmse: 36041.6289 - val_loss: 897204480.0000 - val_rmse: 29953.3711\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1354680448.0000 - rmse: 36805.9844 - val_loss: 910774080.0000 - val_rmse: 30179.0332\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1322107904.0000 - rmse: 36360.8008 - val_loss: 917427008.0000 - val_rmse: 30289.0566\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1350877056.0000 - rmse: 36754.2812 - val_loss: 864495808.0000 - val_rmse: 29402.3086\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1282138368.0000 - rmse: 35806.9609 - val_loss: 866773760.0000 - val_rmse: 29441.0215\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1245635584.0000 - rmse: 35293.5625 - val_loss: 988236288.0000 - val_rmse: 31436.2266\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1256200832.0000 - rmse: 35442.9219 - val_loss: 882053952.0000 - val_rmse: 29699.3926\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1139510912.0000 - rmse: 33756.6406 - val_loss: 865401216.0000 - val_rmse: 29417.7031\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1201258624.0000 - rmse: 34659.1797 - val_loss: 892351040.0000 - val_rmse: 29872.2461\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1166920704.0000 - rmse: 34160.2227 - val_loss: 1012244992.0000 - val_rmse: 31815.7969\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1176831232.0000 - rmse: 34304.9727 - val_loss: 866832320.0000 - val_rmse: 29442.0156\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1158219904.0000 - rmse: 34032.6289 - val_loss: 852633664.0000 - val_rmse: 29199.8906\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1093664128.0000 - rmse: 33070.5898 - val_loss: 865033024.0000 - val_rmse: 29411.4434\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1119183872.0000 - rmse: 33454.2070 - val_loss: 854072448.0000 - val_rmse: 29224.5176\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1082040960.0000 - rmse: 32894.3906 - val_loss: 862678080.0000 - val_rmse: 29371.3828\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1095596032.0000 - rmse: 33099.7891 - val_loss: 846603264.0000 - val_rmse: 29096.4473\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1127472384.0000 - rmse: 33577.8555 - val_loss: 890011584.0000 - val_rmse: 29833.0625\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1050936768.0000 - rmse: 32418.1543 - val_loss: 909254144.0000 - val_rmse: 30153.8418\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1059913600.0000 - rmse: 32556.3145 - val_loss: 852759744.0000 - val_rmse: 29202.0508\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1099400448.0000 - rmse: 33157.2070 - val_loss: 851842688.0000 - val_rmse: 29186.3438\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1051252608.0000 - rmse: 32423.0254 - val_loss: 862710848.0000 - val_rmse: 29371.9395\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1023148480.0000 - rmse: 31986.6914 - val_loss: 864760896.0000 - val_rmse: 29406.8164\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1072827264.0000 - rmse: 32754.0410 - val_loss: 867369408.0000 - val_rmse: 29451.1367\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1072281984.0000 - rmse: 32745.7168 - val_loss: 862331328.0000 - val_rmse: 29365.4785\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 999883904.0000 - rmse: 31620.9414 - val_loss: 912113792.0000 - val_rmse: 30201.2227\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1043636224.0000 - rmse: 32305.3594 - val_loss: 877558784.0000 - val_rmse: 29623.6191\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 972770560.0000 - rmse: 31189.2695 - val_loss: 855353664.0000 - val_rmse: 29246.4297\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 995218432.0000 - rmse: 31547.0820 - val_loss: 865198720.0000 - val_rmse: 29414.2598\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 937516352.0000 - rmse: 30618.8887 - val_loss: 860892544.0000 - val_rmse: 29340.9707\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 944826048.0000 - rmse: 30738.0234 - val_loss: 908267328.0000 - val_rmse: 30137.4746\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 994827520.0000 - rmse: 31540.8867 - val_loss: 870068800.0000 - val_rmse: 29496.9277\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 968673856.0000 - rmse: 31123.5254 - val_loss: 923383808.0000 - val_rmse: 30387.2305\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 978965056.0000 - rmse: 31288.4180 - val_loss: 883351616.0000 - val_rmse: 29721.2324\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 963381312.0000 - rmse: 31038.3848 - val_loss: 846811712.0000 - val_rmse: 29100.0293\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 931429184.0000 - rmse: 30519.3242 - val_loss: 864287488.0000 - val_rmse: 29398.7676\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 973491840.0000 - rmse: 31200.8301 - val_loss: 853704000.0000 - val_rmse: 29218.2129\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 995294400.0000 - rmse: 31548.2871 - val_loss: 864924992.0000 - val_rmse: 29409.6074\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 877745344.0000 - rmse: 29626.7676 - val_loss: 872182336.0000 - val_rmse: 29532.7324\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 905160128.0000 - rmse: 30085.8789 - val_loss: 833036928.0000 - val_rmse: 28862.3789\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 897004672.0000 - rmse: 29950.0371 - val_loss: 978771264.0000 - val_rmse: 31285.3203\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 941451008.0000 - rmse: 30683.0742 - val_loss: 864977344.0000 - val_rmse: 29410.4980\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 879828928.0000 - rmse: 29661.9102 - val_loss: 811987200.0000 - val_rmse: 28495.3887\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 922153024.0000 - rmse: 30366.9727 - val_loss: 920499136.0000 - val_rmse: 30339.7285\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 910094016.0000 - rmse: 30167.7637 - val_loss: 886353472.0000 - val_rmse: 29771.6895\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 878316288.0000 - rmse: 29636.4023 - val_loss: 938427520.0000 - val_rmse: 30633.7637\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 857914240.0000 - rmse: 29290.1738 - val_loss: 907702272.0000 - val_rmse: 30128.0977\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 903259200.0000 - rmse: 30054.2715 - val_loss: 856607424.0000 - val_rmse: 29267.8574\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 865990528.0000 - rmse: 29427.7168 - val_loss: 839796736.0000 - val_rmse: 28979.2461\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 815509440.0000 - rmse: 28557.1250 - val_loss: 854021632.0000 - val_rmse: 29223.6484\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 801658752.0000 - rmse: 28313.5781 - val_loss: 869075712.0000 - val_rmse: 29480.0898\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 824819072.0000 - rmse: 28719.6641 - val_loss: 857329664.0000 - val_rmse: 29280.1914\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 801665856.0000 - rmse: 28313.7051 - val_loss: 889678016.0000 - val_rmse: 29827.4707\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 813262848.0000 - rmse: 28517.7637 - val_loss: 870568384.0000 - val_rmse: 29505.3965\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 816765568.0000 - rmse: 28579.1113 - val_loss: 986331456.0000 - val_rmse: 31405.9141\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800677056.0000 - rmse: 28296.2383 - val_loss: 917848896.0000 - val_rmse: 30296.0215\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 821879872.0000 - rmse: 28668.4473 - val_loss: 934082688.0000 - val_rmse: 30562.7637\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 792165504.0000 - rmse: 28145.4355 - val_loss: 972074368.0000 - val_rmse: 31178.1074\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 784452352.0000 - rmse: 28008.0742 - val_loss: 966769408.0000 - val_rmse: 31092.9160\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 773388160.0000 - rmse: 27809.8574 - val_loss: 1080403968.0000 - val_rmse: 32869.5000\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 759127872.0000 - rmse: 27552.2734 - val_loss: 1004585984.0000 - val_rmse: 31695.2051\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 839071936.0000 - rmse: 28966.7383 - val_loss: 935960704.0000 - val_rmse: 30593.4746\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 742972352.0000 - rmse: 27257.5176 - val_loss: 960276096.0000 - val_rmse: 30988.3203\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 786311552.0000 - rmse: 28041.2480 - val_loss: 964714816.0000 - val_rmse: 31059.8574\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 728249024.0000 - rmse: 26986.0898 - val_loss: 944886272.0000 - val_rmse: 30739.0020\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 754599744.0000 - rmse: 27469.9785 - val_loss: 973049216.0000 - val_rmse: 31193.7363\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 721421312.0000 - rmse: 26859.2871 - val_loss: 943210496.0000 - val_rmse: 30711.7324\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691697088.0000 - rmse: 26300.1348 - val_loss: 969393792.0000 - val_rmse: 31135.0898\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 749808896.0000 - rmse: 27382.6367 - val_loss: 1128572544.0000 - val_rmse: 33594.2344\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 743746240.0000 - rmse: 27271.7109 - val_loss: 974320704.0000 - val_rmse: 31214.1113\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 713440832.0000 - rmse: 26710.3125 - val_loss: 1115239808.0000 - val_rmse: 33395.2070\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 674771456.0000 - rmse: 25976.3633 - val_loss: 1000129600.0000 - val_rmse: 31624.8242\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 713843648.0000 - rmse: 26717.8535 - val_loss: 983437696.0000 - val_rmse: 31359.8105\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 708948544.0000 - rmse: 26626.0859 - val_loss: 962364096.0000 - val_rmse: 31021.9941\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 725318720.0000 - rmse: 26931.7422 - val_loss: 1022576448.0000 - val_rmse: 31977.7500\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 685921920.0000 - rmse: 26190.1113 - val_loss: 999426240.0000 - val_rmse: 31613.7031\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676561024.0000 - rmse: 26010.7852 - val_loss: 1080604032.0000 - val_rmse: 32872.5430\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 660795840.0000 - rmse: 25705.9492 - val_loss: 1037259456.0000 - val_rmse: 32206.5117\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 599785088.0000 - rmse: 24490.5098 - val_loss: 1012828736.0000 - val_rmse: 31824.9707\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691104256.0000 - rmse: 26288.8613 - val_loss: 983810112.0000 - val_rmse: 31365.7480\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 620096320.0000 - rmse: 24901.7324 - val_loss: 1001570560.0000 - val_rmse: 31647.5996\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614978368.0000 - rmse: 24798.7559 - val_loss: 988094912.0000 - val_rmse: 31433.9746\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647482048.0000 - rmse: 25445.6680 - val_loss: 1079055488.0000 - val_rmse: 32848.9766\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 607518208.0000 - rmse: 24647.8848 - val_loss: 1056249664.0000 - val_rmse: 32499.9941\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 678288320.0000 - rmse: 26043.9668 - val_loss: 1127531264.0000 - val_rmse: 33578.7305\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 632526080.0000 - rmse: 25150.0703 - val_loss: 1045265472.0000 - val_rmse: 32330.5664\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 642047040.0000 - rmse: 25338.6465 - val_loss: 1043882752.0000 - val_rmse: 32309.1738\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 610009664.0000 - rmse: 24698.3730 - val_loss: 1010774016.0000 - val_rmse: 31792.6719\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 630512512.0000 - rmse: 25110.0078 - val_loss: 1151051264.0000 - val_rmse: 33927.1445\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 572197440.0000 - rmse: 23920.6484 - val_loss: 1091610624.0000 - val_rmse: 33039.5312\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 670478912.0000 - rmse: 25893.6074 - val_loss: 1076336128.0000 - val_rmse: 32807.5625\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574288512.0000 - rmse: 23964.3184 - val_loss: 1100820736.0000 - val_rmse: 33178.6172\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 614414272.0000 - rmse: 24787.3809 - val_loss: 1172850688.0000 - val_rmse: 34246.9062\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 535534976.0000 - rmse: 23141.6289 - val_loss: 1245052544.0000 - val_rmse: 35285.3008\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 576317760.0000 - rmse: 24006.6191 - val_loss: 1191638144.0000 - val_rmse: 34520.1133\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 567107520.0000 - rmse: 23814.0176 - val_loss: 1121787648.0000 - val_rmse: 33493.0977\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 676717632.0000 - rmse: 26013.7969 - val_loss: 1119873280.0000 - val_rmse: 33464.5078\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 602720320.0000 - rmse: 24550.3633 - val_loss: 1302592896.0000 - val_rmse: 36091.4531\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548172928.0000 - rmse: 23413.0938 - val_loss: 1186153472.0000 - val_rmse: 34440.5781\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533521824.0000 - rmse: 23098.0898 - val_loss: 1389540224.0000 - val_rmse: 37276.5391\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 582509568.0000 - rmse: 24135.2344 - val_loss: 1088637824.0000 - val_rmse: 32994.5117\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 520134624.0000 - rmse: 22806.4609 - val_loss: 1147266688.0000 - val_rmse: 33871.3242\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 589436864.0000 - rmse: 24278.3203 - val_loss: 1055741248.0000 - val_rmse: 32492.1719\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 533828992.0000 - rmse: 23104.7402 - val_loss: 1079733760.0000 - val_rmse: 32859.3008\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 578875008.0000 - rmse: 24059.8223 - val_loss: 1053381696.0000 - val_rmse: 32455.8418\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 510775712.0000 - rmse: 22600.3477 - val_loss: 1082760192.0000 - val_rmse: 32905.3203\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 560763200.0000 - rmse: 23680.4395 - val_loss: 1075920000.0000 - val_rmse: 32801.2188\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561027712.0000 - rmse: 23686.0234 - val_loss: 1053131776.0000 - val_rmse: 32451.9922\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525590688.0000 - rmse: 22925.7656 - val_loss: 1038550656.0000 - val_rmse: 32226.5527\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 444227136.0000 - rmse: 21076.6953 - val_loss: 1132294528.0000 - val_rmse: 33649.5859\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 498100992.0000 - rmse: 22318.1758 - val_loss: 1161855104.0000 - val_rmse: 34085.9961\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 534642784.0000 - rmse: 23122.3438 - val_loss: 1098572032.0000 - val_rmse: 33144.7148\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507327520.0000 - rmse: 22523.9316 - val_loss: 1190637056.0000 - val_rmse: 34505.6055\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 495703456.0000 - rmse: 22264.3984 - val_loss: 1083853184.0000 - val_rmse: 32921.9219\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 436347424.0000 - rmse: 20888.9297 - val_loss: 1286837120.0000 - val_rmse: 35872.5117\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461049408.0000 - rmse: 21472.0605 - val_loss: 1192898432.0000 - val_rmse: 34538.3633\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455231104.0000 - rmse: 21336.1445 - val_loss: 1276908800.0000 - val_rmse: 35733.8594\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 496615040.0000 - rmse: 22284.8613 - val_loss: 1191369600.0000 - val_rmse: 34516.2227\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461403776.0000 - rmse: 21480.3105 - val_loss: 1178362880.0000 - val_rmse: 34327.2891\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 542167104.0000 - rmse: 23284.4824 - val_loss: 1263175808.0000 - val_rmse: 35541.1836\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449361248.0000 - rmse: 21198.1426 - val_loss: 1141514880.0000 - val_rmse: 33786.3125\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 485481440.0000 - rmse: 22033.6426 - val_loss: 1240712448.0000 - val_rmse: 35223.7500\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517123712.0000 - rmse: 22740.3516 - val_loss: 1127147904.0000 - val_rmse: 33573.0234\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 529450304.0000 - rmse: 23009.7871 - val_loss: 1196477440.0000 - val_rmse: 34590.1367\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399561792.0000 - rmse: 19989.0410 - val_loss: 1191108736.0000 - val_rmse: 34512.4414\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 408193824.0000 - rmse: 20203.8066 - val_loss: 1272355200.0000 - val_rmse: 35670.0859\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 452440256.0000 - rmse: 21270.6426 - val_loss: 1203664896.0000 - val_rmse: 34693.8750\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468651264.0000 - rmse: 21648.3535 - val_loss: 1217154688.0000 - val_rmse: 34887.7422\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 457803520.0000 - rmse: 21396.3438 - val_loss: 1213192704.0000 - val_rmse: 34830.9141\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 455909280.0000 - rmse: 21352.0312 - val_loss: 1306636800.0000 - val_rmse: 36147.4297\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 421972128.0000 - rmse: 20541.9609 - val_loss: 1263143424.0000 - val_rmse: 35540.7305\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441813024.0000 - rmse: 21019.3496 - val_loss: 1238923648.0000 - val_rmse: 35198.3477\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 426712640.0000 - rmse: 20657.0234 - val_loss: 1110182912.0000 - val_rmse: 33319.4062\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 399985248.0000 - rmse: 19999.6309 - val_loss: 1239345920.0000 - val_rmse: 35204.3438\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413780096.0000 - rmse: 20341.5840 - val_loss: 1191246336.0000 - val_rmse: 34514.4375\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 414491328.0000 - rmse: 20359.0605 - val_loss: 1276054784.0000 - val_rmse: 35721.9062\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 416916064.0000 - rmse: 20418.5234 - val_loss: 1064631232.0000 - val_rmse: 32628.6875\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353660896.0000 - rmse: 18805.8730 - val_loss: 1188861568.0000 - val_rmse: 34479.8711\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398285472.0000 - rmse: 19957.0898 - val_loss: 1323302016.0000 - val_rmse: 36377.2148\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417329792.0000 - rmse: 20428.6484 - val_loss: 1223575296.0000 - val_rmse: 34979.6406\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398170048.0000 - rmse: 19954.1992 - val_loss: 1093420928.0000 - val_rmse: 33066.9141\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 445513088.0000 - rmse: 21107.1816 - val_loss: 1094812800.0000 - val_rmse: 33087.9570\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 396722528.0000 - rmse: 19917.8945 - val_loss: 1030998400.0000 - val_rmse: 32109.1641\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391347776.0000 - rmse: 19782.5117 - val_loss: 1042125056.0000 - val_rmse: 32281.9609\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 391325664.0000 - rmse: 19781.9531 - val_loss: 1105993216.0000 - val_rmse: 33256.4766\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361097728.0000 - rmse: 19002.5703 - val_loss: 1139402752.0000 - val_rmse: 33755.0352\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 424978912.0000 - rmse: 20615.0176 - val_loss: 1116403328.0000 - val_rmse: 33412.6211\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 490601536.0000 - rmse: 22149.5273 - val_loss: 1200547840.0000 - val_rmse: 34648.9219\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373510880.0000 - rmse: 19326.4277 - val_loss: 1031670656.0000 - val_rmse: 32119.6270\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392277536.0000 - rmse: 19805.9961 - val_loss: 1242743040.0000 - val_rmse: 35252.5625\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450100672.0000 - rmse: 21215.5762 - val_loss: 1235440000.0000 - val_rmse: 35148.8281\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 365745280.0000 - rmse: 19124.4688 - val_loss: 1162473088.0000 - val_rmse: 34095.0586\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389834336.0000 - rmse: 19744.2227 - val_loss: 1211806464.0000 - val_rmse: 34811.0117\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 385889856.0000 - rmse: 19644.0801 - val_loss: 1184737664.0000 - val_rmse: 34420.0195\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389323488.0000 - rmse: 19731.2812 - val_loss: 1172148864.0000 - val_rmse: 34236.6602\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344312288.0000 - rmse: 18555.6543 - val_loss: 1227000064.0000 - val_rmse: 35028.5625\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 374201088.0000 - rmse: 19344.2773 - val_loss: 1221574016.0000 - val_rmse: 34951.0234\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 468737120.0000 - rmse: 21650.3359 - val_loss: 1216388864.0000 - val_rmse: 34876.7656\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 352079840.0000 - rmse: 18763.7891 - val_loss: 1116695808.0000 - val_rmse: 33417.0000\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336027488.0000 - rmse: 18331.0527 - val_loss: 1388409984.0000 - val_rmse: 37261.3750\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369087648.0000 - rmse: 19211.6523 - val_loss: 1492264704.0000 - val_rmse: 38629.8438\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 334668064.0000 - rmse: 18293.9336 - val_loss: 1326657920.0000 - val_rmse: 36423.3164\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 405075552.0000 - rmse: 20126.4863 - val_loss: 1322974080.0000 - val_rmse: 36372.7109\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367515232.0000 - rmse: 19170.6875 - val_loss: 1599321856.0000 - val_rmse: 39991.5234\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327362560.0000 - rmse: 18093.1621 - val_loss: 1116529280.0000 - val_rmse: 33414.5078\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351430880.0000 - rmse: 18746.4902 - val_loss: 1488471552.0000 - val_rmse: 38580.7148\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341352128.0000 - rmse: 18475.7168 - val_loss: 1339271424.0000 - val_rmse: 36596.0586\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321405248.0000 - rmse: 17927.7793 - val_loss: 1246328960.0000 - val_rmse: 35303.3828\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321021664.0000 - rmse: 17917.0762 - val_loss: 1459418752.0000 - val_rmse: 38202.3359\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 435396608.0000 - rmse: 20866.1602 - val_loss: 1252421632.0000 - val_rmse: 35389.5703\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360905632.0000 - rmse: 18997.5156 - val_loss: 1171552128.0000 - val_rmse: 34227.9453\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 330560864.0000 - rmse: 18181.3320 - val_loss: 1278668032.0000 - val_rmse: 35758.4688\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 321074400.0000 - rmse: 17918.5488 - val_loss: 1571475968.0000 - val_rmse: 39641.8477\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373158592.0000 - rmse: 19317.3125 - val_loss: 1187201024.0000 - val_rmse: 34455.7852\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361944384.0000 - rmse: 19024.8359 - val_loss: 1214174848.0000 - val_rmse: 34845.0117\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324227168.0000 - rmse: 18006.3086 - val_loss: 1292272896.0000 - val_rmse: 35948.1992\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340374528.0000 - rmse: 18449.2422 - val_loss: 1212265344.0000 - val_rmse: 34817.6016\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 333923552.0000 - rmse: 18273.5742 - val_loss: 1197971584.0000 - val_rmse: 34611.7266\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 392378976.0000 - rmse: 19808.5586 - val_loss: 1257198720.0000 - val_rmse: 35457.0000\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359924384.0000 - rmse: 18971.6719 - val_loss: 1230621952.0000 - val_rmse: 35080.2227\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322261184.0000 - rmse: 17951.6309 - val_loss: 1306932352.0000 - val_rmse: 36151.5195\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325162496.0000 - rmse: 18032.2617 - val_loss: 1396562688.0000 - val_rmse: 37370.6133\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 355336288.0000 - rmse: 18850.3652 - val_loss: 1447488128.0000 - val_rmse: 38045.8672\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383460384.0000 - rmse: 19582.1445 - val_loss: 1247364608.0000 - val_rmse: 35318.0469\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310404064.0000 - rmse: 17618.2871 - val_loss: 1597643904.0000 - val_rmse: 39970.5391\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 354831040.0000 - rmse: 18836.9590 - val_loss: 1375192704.0000 - val_rmse: 37083.5898\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312084288.0000 - rmse: 17665.9082 - val_loss: 1265674624.0000 - val_rmse: 35576.3203\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 300991360.0000 - rmse: 17349.1035 - val_loss: 1082160384.0000 - val_rmse: 32896.2070\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 295485376.0000 - rmse: 17189.6875 - val_loss: 1231796096.0000 - val_rmse: 35096.9531\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296719488.0000 - rmse: 17225.5469 - val_loss: 1297048832.0000 - val_rmse: 36014.5625\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 359415584.0000 - rmse: 18958.2598 - val_loss: 1142029056.0000 - val_rmse: 33793.9219\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285865216.0000 - rmse: 16907.5488 - val_loss: 1307217536.0000 - val_rmse: 36155.4648\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285179264.0000 - rmse: 16887.2520 - val_loss: 1383875840.0000 - val_rmse: 37200.4805\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 361064416.0000 - rmse: 19001.6934 - val_loss: 1139093888.0000 - val_rmse: 33750.4648\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331139424.0000 - rmse: 18197.2363 - val_loss: 1216533248.0000 - val_rmse: 34878.8359\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 322705600.0000 - rmse: 17964.0078 - val_loss: 1160536704.0000 - val_rmse: 34066.6523\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 307735680.0000 - rmse: 17542.3945 - val_loss: 1080130816.0000 - val_rmse: 32865.3438\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305237856.0000 - rmse: 17471.0566 - val_loss: 1406588032.0000 - val_rmse: 37504.5078\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379601760.0000 - rmse: 19483.3711 - val_loss: 1368041216.0000 - val_rmse: 36987.0430\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315011072.0000 - rmse: 17748.5508 - val_loss: 1423577344.0000 - val_rmse: 37730.3242\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324396896.0000 - rmse: 18011.0195 - val_loss: 1151682816.0000 - val_rmse: 33936.4531\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 372447136.0000 - rmse: 19298.8887 - val_loss: 1504859008.0000 - val_rmse: 38792.5117\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310996128.0000 - rmse: 17635.0820 - val_loss: 1430034048.0000 - val_rmse: 37815.7891\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335055072.0000 - rmse: 18304.5098 - val_loss: 1392023936.0000 - val_rmse: 37309.8359\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 293285760.0000 - rmse: 17125.5879 - val_loss: 1364549888.0000 - val_rmse: 36939.8125\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290948416.0000 - rmse: 17057.2090 - val_loss: 1340275840.0000 - val_rmse: 36609.7773\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256923616.0000 - rmse: 16028.8359 - val_loss: 1432241408.0000 - val_rmse: 37844.9648\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297548640.0000 - rmse: 17249.5977 - val_loss: 1375744768.0000 - val_rmse: 37091.0352\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394775424.0000 - rmse: 19868.9551 - val_loss: 1359700992.0000 - val_rmse: 36874.1250\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 353630816.0000 - rmse: 18805.0742 - val_loss: 1277888768.0000 - val_rmse: 35747.5703\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306578016.0000 - rmse: 17509.3691 - val_loss: 1548262784.0000 - val_rmse: 39347.9648\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250023376.0000 - rmse: 15812.1270 - val_loss: 1203128064.0000 - val_rmse: 34686.1328\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288241472.0000 - rmse: 16977.6738 - val_loss: 1359375104.0000 - val_rmse: 36869.7031\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 314651520.0000 - rmse: 17738.4180 - val_loss: 1519556608.0000 - val_rmse: 38981.4922\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290191616.0000 - rmse: 17035.0098 - val_loss: 1295032064.0000 - val_rmse: 35986.5547\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304617408.0000 - rmse: 17453.2910 - val_loss: 1805746944.0000 - val_rmse: 42494.0820\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284429792.0000 - rmse: 16865.0449 - val_loss: 1410188928.0000 - val_rmse: 37552.4805\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 291499520.0000 - rmse: 17073.3555 - val_loss: 1431756288.0000 - val_rmse: 37838.5547\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 319253728.0000 - rmse: 17867.6719 - val_loss: 1304315648.0000 - val_rmse: 36115.3086\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278620896.0000 - rmse: 16691.9395 - val_loss: 1337832448.0000 - val_rmse: 36576.3906\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 415251008.0000 - rmse: 20377.7070 - val_loss: 1505081216.0000 - val_rmse: 38795.3750\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 328166016.0000 - rmse: 18115.3516 - val_loss: 1342451456.0000 - val_rmse: 36639.4805\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 304295840.0000 - rmse: 17444.0762 - val_loss: 1183504384.0000 - val_rmse: 34402.0977\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 258910640.0000 - rmse: 16090.7002 - val_loss: 1125808512.0000 - val_rmse: 33553.0703\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292415328.0000 - rmse: 17100.1562 - val_loss: 1244465792.0000 - val_rmse: 35276.9883\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257830416.0000 - rmse: 16057.0977 - val_loss: 1283745920.0000 - val_rmse: 35829.3984\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260826320.0000 - rmse: 16150.1172 - val_loss: 1595960192.0000 - val_rmse: 39949.4688\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338583936.0000 - rmse: 18400.6504 - val_loss: 1266921856.0000 - val_rmse: 35593.8477\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247346992.0000 - rmse: 15727.2686 - val_loss: 2228975616.0000 - val_rmse: 47212.0273\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 268882368.0000 - rmse: 16397.6328 - val_loss: 1428451712.0000 - val_rmse: 37794.8633\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 253357920.0000 - rmse: 15917.2197 - val_loss: 1688629760.0000 - val_rmse: 41092.9414\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 238848784.0000 - rmse: 15454.7314 - val_loss: 1540331264.0000 - val_rmse: 39247.0547\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222531760.0000 - rmse: 14917.4971 - val_loss: 1376698112.0000 - val_rmse: 37103.8828\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 294596928.0000 - rmse: 17163.8262 - val_loss: 1486847360.0000 - val_rmse: 38559.6602\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245451920.0000 - rmse: 15666.9033 - val_loss: 1921676800.0000 - val_rmse: 43836.9336\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222430112.0000 - rmse: 14914.0898 - val_loss: 1771410944.0000 - val_rmse: 42088.1328\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 348982048.0000 - rmse: 18681.0605 - val_loss: 1422121216.0000 - val_rmse: 37711.0234\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210913136.0000 - rmse: 14522.8477 - val_loss: 1477329536.0000 - val_rmse: 38436.0430\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272189856.0000 - rmse: 16498.1758 - val_loss: 1417814016.0000 - val_rmse: 37653.8672\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301120800.0000 - rmse: 17352.8320 - val_loss: 1558782464.0000 - val_rmse: 39481.4180\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239942592.0000 - rmse: 15490.0801 - val_loss: 1436595968.0000 - val_rmse: 37902.4531\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279962944.0000 - rmse: 16732.0918 - val_loss: 1563830912.0000 - val_rmse: 39545.3008\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277618016.0000 - rmse: 16661.8730 - val_loss: 1611719808.0000 - val_rmse: 40146.2305\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 338914208.0000 - rmse: 18409.6230 - val_loss: 1638738048.0000 - val_rmse: 40481.3281\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 312526848.0000 - rmse: 17678.4297 - val_loss: 1304255104.0000 - val_rmse: 36114.4727\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 209620208.0000 - rmse: 14478.2666 - val_loss: 1447257984.0000 - val_rmse: 38042.8438\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 218527632.0000 - rmse: 14782.6787 - val_loss: 1325218816.0000 - val_rmse: 36403.5547\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255061376.0000 - rmse: 15970.6406 - val_loss: 1704265728.0000 - val_rmse: 41282.7539\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242761568.0000 - rmse: 15580.8066 - val_loss: 1706271488.0000 - val_rmse: 41307.0391\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296305344.0000 - rmse: 17213.5215 - val_loss: 2193030656.0000 - val_rmse: 46829.8047\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 289174400.0000 - rmse: 17005.1270 - val_loss: 1939398912.0000 - val_rmse: 44038.6055\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229850272.0000 - rmse: 15160.8135 - val_loss: 1557356416.0000 - val_rmse: 39463.3555\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246970416.0000 - rmse: 15715.2920 - val_loss: 1834026624.0000 - val_rmse: 42825.5352\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331071168.0000 - rmse: 18195.3613 - val_loss: 1586722432.0000 - val_rmse: 39833.6836\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 259553296.0000 - rmse: 16110.6572 - val_loss: 1694508160.0000 - val_rmse: 41164.4023\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256062688.0000 - rmse: 16001.9590 - val_loss: 1483484160.0000 - val_rmse: 38516.0234\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242130544.0000 - rmse: 15560.5430 - val_loss: 1339604736.0000 - val_rmse: 36600.6094\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254069984.0000 - rmse: 15939.5713 - val_loss: 1630904704.0000 - val_rmse: 40384.4609\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276838112.0000 - rmse: 16638.4531 - val_loss: 1688905472.0000 - val_rmse: 41096.2969\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217114336.0000 - rmse: 14734.7988 - val_loss: 1364924928.0000 - val_rmse: 36944.8906\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249659360.0000 - rmse: 15800.6104 - val_loss: 1808207744.0000 - val_rmse: 42523.0273\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302185824.0000 - rmse: 17383.4922 - val_loss: 1392226048.0000 - val_rmse: 37312.5469\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244244064.0000 - rmse: 15628.3086 - val_loss: 1798678656.0000 - val_rmse: 42410.8320\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 297561376.0000 - rmse: 17249.9668 - val_loss: 1492249344.0000 - val_rmse: 38629.6445\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276341664.0000 - rmse: 16623.5273 - val_loss: 1173472384.0000 - val_rmse: 34255.9844\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276639392.0000 - rmse: 16632.4785 - val_loss: 1244647680.0000 - val_rmse: 35279.5664\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 290373504.0000 - rmse: 17040.3477 - val_loss: 1224495360.0000 - val_rmse: 34992.7891\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222678112.0000 - rmse: 14922.4023 - val_loss: 1520472320.0000 - val_rmse: 38993.2344\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267987248.0000 - rmse: 16370.3145 - val_loss: 1404167424.0000 - val_rmse: 37472.2227\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 267590528.0000 - rmse: 16358.1943 - val_loss: 1386106880.0000 - val_rmse: 37230.4570\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228288640.0000 - rmse: 15109.2236 - val_loss: 1042450304.0000 - val_rmse: 32286.9980\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228228192.0000 - rmse: 15107.2236 - val_loss: 1177995392.0000 - val_rmse: 34321.9375\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 260111344.0000 - rmse: 16127.9658 - val_loss: 1513121664.0000 - val_rmse: 38898.8633\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226299872.0000 - rmse: 15043.2666 - val_loss: 1385660032.0000 - val_rmse: 37224.4531\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 198126736.0000 - rmse: 14075.7490 - val_loss: 1654134656.0000 - val_rmse: 40671.0547\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239728080.0000 - rmse: 15483.1543 - val_loss: 1295934208.0000 - val_rmse: 35999.0859\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 292613696.0000 - rmse: 17105.9531 - val_loss: 1116013312.0000 - val_rmse: 33406.7852\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257094672.0000 - rmse: 16034.1699 - val_loss: 1152924160.0000 - val_rmse: 33954.7383\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 199427792.0000 - rmse: 14121.8896 - val_loss: 1214540160.0000 - val_rmse: 34850.2539\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257746688.0000 - rmse: 16054.4902 - val_loss: 1488658432.0000 - val_rmse: 38583.1367\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220053152.0000 - rmse: 14834.1885 - val_loss: 1646473728.0000 - val_rmse: 40576.7617\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221581776.0000 - rmse: 14885.6211 - val_loss: 1362021632.0000 - val_rmse: 36905.5781\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256380592.0000 - rmse: 16011.8877 - val_loss: 1146516224.0000 - val_rmse: 33860.2461\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 198763440.0000 - rmse: 14098.3467 - val_loss: 1379856512.0000 - val_rmse: 37146.4180\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 233542128.0000 - rmse: 15282.0840 - val_loss: 1295774976.0000 - val_rmse: 35996.8750\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 219038880.0000 - rmse: 14799.9609 - val_loss: 1670972544.0000 - val_rmse: 40877.5312\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237968704.0000 - rmse: 15426.2324 - val_loss: 1264084096.0000 - val_rmse: 35553.9570\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234134784.0000 - rmse: 15301.4619 - val_loss: 1464036992.0000 - val_rmse: 38262.7383\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211433520.0000 - rmse: 14540.7529 - val_loss: 2282111744.0000 - val_rmse: 47771.4531\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280515552.0000 - rmse: 16748.5977 - val_loss: 1114802176.0000 - val_rmse: 33388.6523\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277742016.0000 - rmse: 16665.5918 - val_loss: 1450052352.0000 - val_rmse: 38079.5547\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 270662880.0000 - rmse: 16451.8359 - val_loss: 1242053120.0000 - val_rmse: 35242.7734\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 227315264.0000 - rmse: 15076.9766 - val_loss: 1124362240.0000 - val_rmse: 33531.5117\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 208129984.0000 - rmse: 14426.7100 - val_loss: 1454915456.0000 - val_rmse: 38143.3555\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226367696.0000 - rmse: 15045.5205 - val_loss: 1569998336.0000 - val_rmse: 39623.2031\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236616032.0000 - rmse: 15382.3281 - val_loss: 2155824128.0000 - val_rmse: 46430.8516\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254520608.0000 - rmse: 15953.7012 - val_loss: 1397113088.0000 - val_rmse: 37377.9766\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262756256.0000 - rmse: 16209.7578 - val_loss: 1421820160.0000 - val_rmse: 37707.0312\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 206308496.0000 - rmse: 14363.4404 - val_loss: 1384620544.0000 - val_rmse: 37210.4883\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251968256.0000 - rmse: 15873.5068 - val_loss: 1605678208.0000 - val_rmse: 40070.9141\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324316000.0000 - rmse: 18008.7754 - val_loss: 1482245760.0000 - val_rmse: 38499.9453\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236241824.0000 - rmse: 15370.1602 - val_loss: 1677087872.0000 - val_rmse: 40952.2617\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 273152128.0000 - rmse: 16527.3145 - val_loss: 1544022528.0000 - val_rmse: 39294.0469\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229980144.0000 - rmse: 15165.0947 - val_loss: 2040769280.0000 - val_rmse: 45174.8750\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265871552.0000 - rmse: 16305.5674 - val_loss: 1476461568.0000 - val_rmse: 38424.7539\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248201472.0000 - rmse: 15754.4111 - val_loss: 1339606784.0000 - val_rmse: 36600.6406\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265487376.0000 - rmse: 16293.7832 - val_loss: 1233293440.0000 - val_rmse: 35118.2773\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241362768.0000 - rmse: 15535.8525 - val_loss: 1088923904.0000 - val_rmse: 32998.8477\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 205017984.0000 - rmse: 14318.4473 - val_loss: 1201520512.0000 - val_rmse: 34662.9531\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266241664.0000 - rmse: 16316.9121 - val_loss: 1146895360.0000 - val_rmse: 33865.8438\n",
      "104/104 [==============================] - 0s 713us/step - loss: 466765184.0000 - rmse: 21604.7500\n",
      "[466765184.0, 21604.75]\n",
      "[20003.724609375, 25916.537109375, 29746.79296875, 22110.357421875, 21604.75]\n",
      "23876.432421875\n"
     ]
    }
   ],
   "source": [
    "k_fold(\"baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and get the Baseline Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16589, 123) (16589,)\n",
      "(13271, 123) (13271,) (3318, 123) (3318,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               15872     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 20,873\n",
      "Trainable params: 20,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 24993511424.0000 - rmse: 158093.3594 - val_loss: 23309985792.0000 - val_rmse: 152676.0781\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 14068026368.0000 - rmse: 118608.7109 - val_loss: 3302575104.0000 - val_rmse: 57468.0352\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2992123648.0000 - rmse: 54700.3086 - val_loss: 1896832256.0000 - val_rmse: 43552.6367\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2369993472.0000 - rmse: 48682.5781 - val_loss: 1489420672.0000 - val_rmse: 38593.0117\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 2097799936.0000 - rmse: 45801.7461 - val_loss: 1327195264.0000 - val_rmse: 36430.6914\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1895982080.0000 - rmse: 43542.8750 - val_loss: 1202819712.0000 - val_rmse: 34681.6914\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1880482944.0000 - rmse: 43364.5352 - val_loss: 1146469504.0000 - val_rmse: 33859.5547\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1778710400.0000 - rmse: 42174.7617 - val_loss: 1079971072.0000 - val_rmse: 32862.9141\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1704167936.0000 - rmse: 41281.5703 - val_loss: 1029811456.0000 - val_rmse: 32090.6758\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1649601920.0000 - rmse: 40615.2930 - val_loss: 1086037888.0000 - val_rmse: 32955.0898\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1570510720.0000 - rmse: 39629.6680 - val_loss: 965503488.0000 - val_rmse: 31072.5527\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1538175872.0000 - rmse: 39219.5859 - val_loss: 934476736.0000 - val_rmse: 30569.2129\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1524882688.0000 - rmse: 39049.7461 - val_loss: 911047040.0000 - val_rmse: 30183.5566\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1519703936.0000 - rmse: 38983.3789 - val_loss: 895171520.0000 - val_rmse: 29919.4180\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1467031296.0000 - rmse: 38301.8438 - val_loss: 887403840.0000 - val_rmse: 29789.3242\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1419352064.0000 - rmse: 37674.2891 - val_loss: 858323712.0000 - val_rmse: 29297.1621\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1399294336.0000 - rmse: 37407.1445 - val_loss: 845570688.0000 - val_rmse: 29078.6973\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1382626432.0000 - rmse: 37183.6836 - val_loss: 840635904.0000 - val_rmse: 28993.7227\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1417186560.0000 - rmse: 37645.5391 - val_loss: 835946816.0000 - val_rmse: 28912.7441\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1375970176.0000 - rmse: 37094.0703 - val_loss: 835855104.0000 - val_rmse: 28911.1582\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1333288448.0000 - rmse: 36514.2227 - val_loss: 828531904.0000 - val_rmse: 28784.2305\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1342375168.0000 - rmse: 36638.4375 - val_loss: 800103808.0000 - val_rmse: 28286.1055\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1358521728.0000 - rmse: 36858.1289 - val_loss: 786016000.0000 - val_rmse: 28035.9766\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1349533952.0000 - rmse: 36736.0039 - val_loss: 774402624.0000 - val_rmse: 27828.0898\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1258876672.0000 - rmse: 35480.6523 - val_loss: 842856768.0000 - val_rmse: 29031.9961\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1306650496.0000 - rmse: 36147.6211 - val_loss: 758092736.0000 - val_rmse: 27533.4844\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1270936832.0000 - rmse: 35650.1992 - val_loss: 754746368.0000 - val_rmse: 27472.6484\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1252268672.0000 - rmse: 35387.4102 - val_loss: 750441664.0000 - val_rmse: 27394.1895\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1204969088.0000 - rmse: 34712.6641 - val_loss: 742463808.0000 - val_rmse: 27248.1895\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1187808768.0000 - rmse: 34464.6016 - val_loss: 735550464.0000 - val_rmse: 27121.0332\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1203738240.0000 - rmse: 34694.9297 - val_loss: 760121664.0000 - val_rmse: 27570.3027\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1269308032.0000 - rmse: 35627.3477 - val_loss: 725438976.0000 - val_rmse: 26933.9727\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1221498880.0000 - rmse: 34949.9492 - val_loss: 726966208.0000 - val_rmse: 26962.3105\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1202334464.0000 - rmse: 34674.6953 - val_loss: 729594816.0000 - val_rmse: 27011.0137\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1210492160.0000 - rmse: 34792.1289 - val_loss: 732449984.0000 - val_rmse: 27063.8125\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1171333120.0000 - rmse: 34224.7461 - val_loss: 742534016.0000 - val_rmse: 27249.4766\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1167008640.0000 - rmse: 34161.5078 - val_loss: 706744896.0000 - val_rmse: 26584.6738\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1185105152.0000 - rmse: 34425.3555 - val_loss: 713420800.0000 - val_rmse: 26709.9375\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1156486912.0000 - rmse: 34007.1602 - val_loss: 795681216.0000 - val_rmse: 28207.8203\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1143161344.0000 - rmse: 33810.6680 - val_loss: 703465600.0000 - val_rmse: 26522.9238\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1151582080.0000 - rmse: 33934.9688 - val_loss: 720669312.0000 - val_rmse: 26845.2832\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1208783360.0000 - rmse: 34767.5625 - val_loss: 694337152.0000 - val_rmse: 26350.2773\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1140593024.0000 - rmse: 33772.6680 - val_loss: 697140544.0000 - val_rmse: 26403.4199\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1112495616.0000 - rmse: 33354.0938 - val_loss: 694294848.0000 - val_rmse: 26349.4746\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1148911616.0000 - rmse: 33895.5977 - val_loss: 725374464.0000 - val_rmse: 26932.7773\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1060808960.0000 - rmse: 32570.0625 - val_loss: 740631040.0000 - val_rmse: 27214.5371\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1143458304.0000 - rmse: 33815.0586 - val_loss: 687548928.0000 - val_rmse: 26221.1543\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1120104448.0000 - rmse: 33467.9609 - val_loss: 690610048.0000 - val_rmse: 26279.4590\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1121388672.0000 - rmse: 33487.1406 - val_loss: 698469824.0000 - val_rmse: 26428.5801\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1097696640.0000 - rmse: 33131.5039 - val_loss: 743019840.0000 - val_rmse: 27258.3906\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1073922816.0000 - rmse: 32770.7617 - val_loss: 673403328.0000 - val_rmse: 25950.0156\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1099425536.0000 - rmse: 33157.5859 - val_loss: 674268672.0000 - val_rmse: 25966.6836\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1118587008.0000 - rmse: 33445.2852 - val_loss: 686256768.0000 - val_rmse: 26196.5020\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1134181760.0000 - rmse: 33677.6133 - val_loss: 705290496.0000 - val_rmse: 26557.3066\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1079006464.0000 - rmse: 32848.2344 - val_loss: 687575552.0000 - val_rmse: 26221.6621\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1065466304.0000 - rmse: 32641.4805 - val_loss: 674229696.0000 - val_rmse: 25965.9336\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1078809216.0000 - rmse: 32845.2305 - val_loss: 662629312.0000 - val_rmse: 25741.5879\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1064333504.0000 - rmse: 32624.1230 - val_loss: 668342336.0000 - val_rmse: 25852.3164\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1145725696.0000 - rmse: 33848.5703 - val_loss: 660322176.0000 - val_rmse: 25696.7344\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1092852480.0000 - rmse: 33058.3203 - val_loss: 671381568.0000 - val_rmse: 25911.0312\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1047916992.0000 - rmse: 32371.5469 - val_loss: 659554496.0000 - val_rmse: 25681.7930\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1085623296.0000 - rmse: 32948.7969 - val_loss: 667811712.0000 - val_rmse: 25842.0527\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1014767104.0000 - rmse: 31855.4102 - val_loss: 672047360.0000 - val_rmse: 25923.8750\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1019305664.0000 - rmse: 31926.5645 - val_loss: 657057920.0000 - val_rmse: 25633.1406\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1043759936.0000 - rmse: 32307.2715 - val_loss: 667413056.0000 - val_rmse: 25834.3379\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 982751680.0000 - rmse: 31348.8711 - val_loss: 655012864.0000 - val_rmse: 25593.2188\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1046419072.0000 - rmse: 32348.4023 - val_loss: 683089600.0000 - val_rmse: 26135.9824\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1026727488.0000 - rmse: 32042.5879 - val_loss: 659084224.0000 - val_rmse: 25672.6348\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1082855168.0000 - rmse: 32906.7656 - val_loss: 645104320.0000 - val_rmse: 25398.9043\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1047027648.0000 - rmse: 32357.8066 - val_loss: 710974016.0000 - val_rmse: 26664.0957\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1022042240.0000 - rmse: 31969.3945 - val_loss: 643597120.0000 - val_rmse: 25369.2168\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 997991616.0000 - rmse: 31591.0059 - val_loss: 648083584.0000 - val_rmse: 25457.4863\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1034880192.0000 - rmse: 32169.5547 - val_loss: 650434560.0000 - val_rmse: 25503.6172\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1012332160.0000 - rmse: 31817.1660 - val_loss: 670381760.0000 - val_rmse: 25891.7324\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 992836416.0000 - rmse: 31509.3066 - val_loss: 690330496.0000 - val_rmse: 26274.1406\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 998192320.0000 - rmse: 31594.1816 - val_loss: 652825472.0000 - val_rmse: 25550.4492\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 943697536.0000 - rmse: 30719.6602 - val_loss: 643669824.0000 - val_rmse: 25370.6484\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 944629376.0000 - rmse: 30734.8242 - val_loss: 624068160.0000 - val_rmse: 24981.3555\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 984502848.0000 - rmse: 31376.7891 - val_loss: 614807680.0000 - val_rmse: 24795.3164\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 1004057536.0000 - rmse: 31686.8672 - val_loss: 620242496.0000 - val_rmse: 24904.6660\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 997483904.0000 - rmse: 31582.9688 - val_loss: 646714048.0000 - val_rmse: 25430.5723\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 964432000.0000 - rmse: 31055.3047 - val_loss: 604306432.0000 - val_rmse: 24582.6445\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 916783744.0000 - rmse: 30278.4355 - val_loss: 637009408.0000 - val_rmse: 25239.0449\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 950009280.0000 - rmse: 30822.2207 - val_loss: 599387840.0000 - val_rmse: 24482.3984\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 957537344.0000 - rmse: 30944.0996 - val_loss: 600307264.0000 - val_rmse: 24501.1680\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 883095680.0000 - rmse: 29716.9258 - val_loss: 598196736.0000 - val_rmse: 24458.0605\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 959187392.0000 - rmse: 30970.7500 - val_loss: 597236480.0000 - val_rmse: 24438.4219\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 966502976.0000 - rmse: 31088.6309 - val_loss: 596785472.0000 - val_rmse: 24429.1934\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 924526976.0000 - rmse: 30406.0352 - val_loss: 602069824.0000 - val_rmse: 24537.1113\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 983484352.0000 - rmse: 31360.5547 - val_loss: 582897728.0000 - val_rmse: 24143.2734\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 959234112.0000 - rmse: 30971.5039 - val_loss: 581780928.0000 - val_rmse: 24120.1348\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 878695232.0000 - rmse: 29642.7910 - val_loss: 591367680.0000 - val_rmse: 24318.0508\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 901279744.0000 - rmse: 30021.3223 - val_loss: 621508480.0000 - val_rmse: 24930.0723\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 918841664.0000 - rmse: 30312.4004 - val_loss: 594884096.0000 - val_rmse: 24390.2461\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 931251136.0000 - rmse: 30516.4082 - val_loss: 585411008.0000 - val_rmse: 24195.2676\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 902513792.0000 - rmse: 30041.8672 - val_loss: 634549632.0000 - val_rmse: 25190.2695\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 883182208.0000 - rmse: 29718.3809 - val_loss: 568219712.0000 - val_rmse: 23837.3594\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 963769280.0000 - rmse: 31044.6309 - val_loss: 590643008.0000 - val_rmse: 24303.1465\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 890839808.0000 - rmse: 29846.9395 - val_loss: 632374656.0000 - val_rmse: 25147.0605\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 903558016.0000 - rmse: 30059.2422 - val_loss: 586943552.0000 - val_rmse: 24226.9180\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 905189056.0000 - rmse: 30086.3594 - val_loss: 570966976.0000 - val_rmse: 23894.9160\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 914819648.0000 - rmse: 30245.9863 - val_loss: 578468992.0000 - val_rmse: 24051.3809\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 865658048.0000 - rmse: 29422.0664 - val_loss: 563991680.0000 - val_rmse: 23748.5098\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 888215872.0000 - rmse: 29802.9512 - val_loss: 695667968.0000 - val_rmse: 26375.5176\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 859523072.0000 - rmse: 29317.6211 - val_loss: 566872832.0000 - val_rmse: 23809.0918\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 878467456.0000 - rmse: 29638.9512 - val_loss: 581796224.0000 - val_rmse: 24120.4531\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 874099904.0000 - rmse: 29565.1797 - val_loss: 605940096.0000 - val_rmse: 24615.8496\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 863301248.0000 - rmse: 29381.9883 - val_loss: 623330880.0000 - val_rmse: 24966.5957\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 872492864.0000 - rmse: 29537.9902 - val_loss: 552773248.0000 - val_rmse: 23511.1309\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 860163584.0000 - rmse: 29328.5449 - val_loss: 582679552.0000 - val_rmse: 24138.7559\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 907852608.0000 - rmse: 30130.5918 - val_loss: 554896896.0000 - val_rmse: 23556.2500\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 838329600.0000 - rmse: 28953.9219 - val_loss: 552791232.0000 - val_rmse: 23511.5117\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 897553088.0000 - rmse: 29959.1895 - val_loss: 567485888.0000 - val_rmse: 23821.9629\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 808279872.0000 - rmse: 28430.2617 - val_loss: 572928320.0000 - val_rmse: 23935.9219\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 824675200.0000 - rmse: 28717.1582 - val_loss: 562671808.0000 - val_rmse: 23720.7051\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 875339776.0000 - rmse: 29586.1426 - val_loss: 604388224.0000 - val_rmse: 24584.3086\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 846979584.0000 - rmse: 29102.9141 - val_loss: 604604416.0000 - val_rmse: 24588.7031\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 834389952.0000 - rmse: 28885.8086 - val_loss: 554452160.0000 - val_rmse: 23546.8086\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 817996544.0000 - rmse: 28600.6387 - val_loss: 559351616.0000 - val_rmse: 23650.6152\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 900143040.0000 - rmse: 30002.3848 - val_loss: 549801664.0000 - val_rmse: 23447.8496\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 868680896.0000 - rmse: 29473.3926 - val_loss: 568679680.0000 - val_rmse: 23847.0039\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 893026240.0000 - rmse: 29883.5430 - val_loss: 559672576.0000 - val_rmse: 23657.4004\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 858008448.0000 - rmse: 29291.7812 - val_loss: 555853632.0000 - val_rmse: 23576.5469\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 904346496.0000 - rmse: 30072.3535 - val_loss: 598256576.0000 - val_rmse: 24459.2832\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 801079552.0000 - rmse: 28303.3496 - val_loss: 537203136.0000 - val_rmse: 23177.6406\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 856509696.0000 - rmse: 29266.1875 - val_loss: 531864320.0000 - val_rmse: 23062.1836\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 840521152.0000 - rmse: 28991.7422 - val_loss: 535716576.0000 - val_rmse: 23145.5527\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 830918080.0000 - rmse: 28825.6504 - val_loss: 535490400.0000 - val_rmse: 23140.6641\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 800959936.0000 - rmse: 28301.2363 - val_loss: 550724736.0000 - val_rmse: 23467.5254\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 876376064.0000 - rmse: 29603.6465 - val_loss: 559960128.0000 - val_rmse: 23663.4746\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 788697344.0000 - rmse: 28083.7559 - val_loss: 532234848.0000 - val_rmse: 23070.2148\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 756086976.0000 - rmse: 27497.0352 - val_loss: 529813792.0000 - val_rmse: 23017.6836\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 795854464.0000 - rmse: 28210.8926 - val_loss: 539241344.0000 - val_rmse: 23221.5703\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 789485120.0000 - rmse: 28097.7773 - val_loss: 528657024.0000 - val_rmse: 22992.5430\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 787358208.0000 - rmse: 28059.9043 - val_loss: 525303648.0000 - val_rmse: 22919.5039\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 793301440.0000 - rmse: 28165.6074 - val_loss: 517696000.0000 - val_rmse: 22752.9316\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 798602112.0000 - rmse: 28259.5488 - val_loss: 513997632.0000 - val_rmse: 22671.5156\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 762976512.0000 - rmse: 27622.0273 - val_loss: 543617088.0000 - val_rmse: 23315.5977\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 741110720.0000 - rmse: 27223.3496 - val_loss: 508629504.0000 - val_rmse: 22552.8145\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 777408768.0000 - rmse: 27882.0508 - val_loss: 509924128.0000 - val_rmse: 22581.4980\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 777296000.0000 - rmse: 27880.0293 - val_loss: 519735520.0000 - val_rmse: 22797.7090\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 786802688.0000 - rmse: 28050.0039 - val_loss: 517760160.0000 - val_rmse: 22754.3438\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 776202368.0000 - rmse: 27860.4082 - val_loss: 510683264.0000 - val_rmse: 22598.3027\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 765001536.0000 - rmse: 27658.6621 - val_loss: 503813952.0000 - val_rmse: 22445.8008\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 784428608.0000 - rmse: 28007.6523 - val_loss: 521896352.0000 - val_rmse: 22845.0508\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 751025728.0000 - rmse: 27404.8477 - val_loss: 530814112.0000 - val_rmse: 23039.4043\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 711183168.0000 - rmse: 26668.0176 - val_loss: 581242944.0000 - val_rmse: 24108.9785\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 778055424.0000 - rmse: 27893.6445 - val_loss: 509023680.0000 - val_rmse: 22561.5508\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 743578240.0000 - rmse: 27268.6309 - val_loss: 520729568.0000 - val_rmse: 22819.4980\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 764336320.0000 - rmse: 27646.6328 - val_loss: 505616480.0000 - val_rmse: 22485.9160\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 778275072.0000 - rmse: 27897.5820 - val_loss: 571836096.0000 - val_rmse: 23913.0938\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 750605248.0000 - rmse: 27397.1758 - val_loss: 559255744.0000 - val_rmse: 23648.5879\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 703528448.0000 - rmse: 26524.1113 - val_loss: 505074560.0000 - val_rmse: 22473.8633\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 771361088.0000 - rmse: 27773.3887 - val_loss: 493167360.0000 - val_rmse: 22207.3711\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 701771968.0000 - rmse: 26490.9785 - val_loss: 544285888.0000 - val_rmse: 23329.9355\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 754940416.0000 - rmse: 27476.1777 - val_loss: 483738368.0000 - val_rmse: 21994.0527\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 765736320.0000 - rmse: 27671.9414 - val_loss: 480059168.0000 - val_rmse: 21910.2520\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 730440128.0000 - rmse: 27026.6543 - val_loss: 514858752.0000 - val_rmse: 22690.5000\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 778304256.0000 - rmse: 27898.1055 - val_loss: 474521120.0000 - val_rmse: 21783.5059\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 693667584.0000 - rmse: 26337.5703 - val_loss: 505892448.0000 - val_rmse: 22492.0527\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 711923456.0000 - rmse: 26681.8926 - val_loss: 473751712.0000 - val_rmse: 21765.8379\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 692683648.0000 - rmse: 26318.8828 - val_loss: 502215584.0000 - val_rmse: 22410.1660\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 639346112.0000 - rmse: 25285.2930 - val_loss: 485769856.0000 - val_rmse: 22040.1875\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 683496192.0000 - rmse: 26143.7598 - val_loss: 528257632.0000 - val_rmse: 22983.8555\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 678910272.0000 - rmse: 26055.9062 - val_loss: 461484224.0000 - val_rmse: 21482.1816\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 671655232.0000 - rmse: 25916.3125 - val_loss: 568127552.0000 - val_rmse: 23835.4258\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 691456768.0000 - rmse: 26295.5625 - val_loss: 463615776.0000 - val_rmse: 21531.7383\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 656399232.0000 - rmse: 25620.2891 - val_loss: 500672384.0000 - val_rmse: 22375.7090\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 714191296.0000 - rmse: 26724.3574 - val_loss: 465653952.0000 - val_rmse: 21579.0156\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 661588352.0000 - rmse: 25721.3594 - val_loss: 460339936.0000 - val_rmse: 21455.5332\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 593371712.0000 - rmse: 24359.2227 - val_loss: 451127072.0000 - val_rmse: 21239.7500\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 615655040.0000 - rmse: 24812.3965 - val_loss: 508467680.0000 - val_rmse: 22549.2285\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 585013376.0000 - rmse: 24187.0488 - val_loss: 481433024.0000 - val_rmse: 21941.5820\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 634784064.0000 - rmse: 25194.9199 - val_loss: 493165696.0000 - val_rmse: 22207.3340\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 609368576.0000 - rmse: 24685.3906 - val_loss: 502581728.0000 - val_rmse: 22418.3340\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 644111936.0000 - rmse: 25379.3613 - val_loss: 452747232.0000 - val_rmse: 21277.8574\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626313024.0000 - rmse: 25026.2461 - val_loss: 470672736.0000 - val_rmse: 21694.9922\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 681366080.0000 - rmse: 26102.9883 - val_loss: 453612352.0000 - val_rmse: 21298.1777\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 647932032.0000 - rmse: 25454.5098 - val_loss: 440136128.0000 - val_rmse: 20979.4199\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 626404416.0000 - rmse: 25028.0723 - val_loss: 442312512.0000 - val_rmse: 21031.2246\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 638804800.0000 - rmse: 25274.5859 - val_loss: 463691360.0000 - val_rmse: 21533.4941\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649549824.0000 - rmse: 25486.2676 - val_loss: 432814816.0000 - val_rmse: 20804.2012\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 637419200.0000 - rmse: 25247.1602 - val_loss: 437400992.0000 - val_rmse: 20914.1328\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 590492608.0000 - rmse: 24300.0527 - val_loss: 445858304.0000 - val_rmse: 21115.3555\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 649268928.0000 - rmse: 25480.7539 - val_loss: 450718240.0000 - val_rmse: 21230.1250\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 624733440.0000 - rmse: 24994.6680 - val_loss: 484193888.0000 - val_rmse: 22004.4062\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 574358016.0000 - rmse: 23965.7676 - val_loss: 429536288.0000 - val_rmse: 20725.2559\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 561810816.0000 - rmse: 23702.5488 - val_loss: 426397216.0000 - val_rmse: 20649.3867\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 569740352.0000 - rmse: 23869.2344 - val_loss: 440797056.0000 - val_rmse: 20995.1660\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 548165952.0000 - rmse: 23412.9434 - val_loss: 427366592.0000 - val_rmse: 20672.8457\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 564631232.0000 - rmse: 23761.9707 - val_loss: 418171872.0000 - val_rmse: 20449.2520\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 602660992.0000 - rmse: 24549.1543 - val_loss: 419959616.0000 - val_rmse: 20492.9160\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 544785472.0000 - rmse: 23340.6406 - val_loss: 510728736.0000 - val_rmse: 22599.3086\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 545333888.0000 - rmse: 23352.3848 - val_loss: 441520064.0000 - val_rmse: 21012.3770\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 559519168.0000 - rmse: 23654.1562 - val_loss: 425024544.0000 - val_rmse: 20616.1230\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 506298880.0000 - rmse: 22501.0840 - val_loss: 450575072.0000 - val_rmse: 21226.7539\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517243200.0000 - rmse: 22742.9805 - val_loss: 456902144.0000 - val_rmse: 21375.2676\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 497987584.0000 - rmse: 22315.6348 - val_loss: 448031936.0000 - val_rmse: 21166.7656\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 586869952.0000 - rmse: 24225.3965 - val_loss: 421336288.0000 - val_rmse: 20526.4766\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 502629376.0000 - rmse: 22419.3965 - val_loss: 436834208.0000 - val_rmse: 20900.5781\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 500115040.0000 - rmse: 22363.2520 - val_loss: 431757280.0000 - val_rmse: 20778.7695\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 482843712.0000 - rmse: 21973.7051 - val_loss: 428316672.0000 - val_rmse: 20695.8125\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 537409344.0000 - rmse: 23182.0898 - val_loss: 408048992.0000 - val_rmse: 20200.2227\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472068160.0000 - rmse: 21727.1289 - val_loss: 423545216.0000 - val_rmse: 20580.2129\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 484310304.0000 - rmse: 22007.0488 - val_loss: 411995296.0000 - val_rmse: 20297.6660\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 583910720.0000 - rmse: 24164.2441 - val_loss: 399875200.0000 - val_rmse: 19996.8789\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 443242592.0000 - rmse: 21053.3281 - val_loss: 411735968.0000 - val_rmse: 20291.2773\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 464858848.0000 - rmse: 21560.5859 - val_loss: 434284224.0000 - val_rmse: 20839.4863\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 472040096.0000 - rmse: 21726.4824 - val_loss: 412162144.0000 - val_rmse: 20301.7754\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 517711744.0000 - rmse: 22753.2793 - val_loss: 432967904.0000 - val_rmse: 20807.8789\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525148992.0000 - rmse: 22916.1289 - val_loss: 408497280.0000 - val_rmse: 20211.3164\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 433248832.0000 - rmse: 20814.6309 - val_loss: 439493856.0000 - val_rmse: 20964.1074\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 487354944.0000 - rmse: 22076.1172 - val_loss: 467510528.0000 - val_rmse: 21621.9922\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 450089184.0000 - rmse: 21215.3027 - val_loss: 400363072.0000 - val_rmse: 20009.0742\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461466240.0000 - rmse: 21481.7637 - val_loss: 390698368.0000 - val_rmse: 19766.0918\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 441211072.0000 - rmse: 21005.0234 - val_loss: 407712672.0000 - val_rmse: 20191.8965\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461324000.0000 - rmse: 21478.4551 - val_loss: 501449472.0000 - val_rmse: 22393.0664\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423435904.0000 - rmse: 20577.5566 - val_loss: 398304608.0000 - val_rmse: 19957.5684\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 507216672.0000 - rmse: 22521.4707 - val_loss: 394236896.0000 - val_rmse: 19855.3984\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 461842752.0000 - rmse: 21490.5273 - val_loss: 383802528.0000 - val_rmse: 19590.8789\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 383963712.0000 - rmse: 19594.9922 - val_loss: 442010144.0000 - val_rmse: 21024.0352\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 423295040.0000 - rmse: 20574.1328 - val_loss: 392894624.0000 - val_rmse: 19821.5684\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 394013504.0000 - rmse: 19849.7734 - val_loss: 430119840.0000 - val_rmse: 20739.3301\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440273216.0000 - rmse: 20982.6875 - val_loss: 370415424.0000 - val_rmse: 19246.1797\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 420394272.0000 - rmse: 20503.5176 - val_loss: 434875168.0000 - val_rmse: 20853.6602\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 398266336.0000 - rmse: 19956.6113 - val_loss: 408308256.0000 - val_rmse: 20206.6387\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 440681440.0000 - rmse: 20992.4141 - val_loss: 373131584.0000 - val_rmse: 19316.6133\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 373906688.0000 - rmse: 19336.6660 - val_loss: 458116000.0000 - val_rmse: 21403.6426\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 403831776.0000 - rmse: 20095.5645 - val_loss: 377140096.0000 - val_rmse: 19420.0938\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 525484384.0000 - rmse: 22923.4453 - val_loss: 369146816.0000 - val_rmse: 19213.1934\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 456448416.0000 - rmse: 21364.6543 - val_loss: 370090336.0000 - val_rmse: 19237.7305\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 410437504.0000 - rmse: 20259.2559 - val_loss: 444873312.0000 - val_rmse: 21092.0195\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 350618080.0000 - rmse: 18724.7988 - val_loss: 396259520.0000 - val_rmse: 19906.2656\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 417000032.0000 - rmse: 20420.5781 - val_loss: 395697152.0000 - val_rmse: 19892.1387\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 369229280.0000 - rmse: 19215.3379 - val_loss: 361812320.0000 - val_rmse: 19021.3652\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 389570880.0000 - rmse: 19737.5488 - val_loss: 353695296.0000 - val_rmse: 18806.7871\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 449671584.0000 - rmse: 21205.4590 - val_loss: 392539136.0000 - val_rmse: 19812.5977\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 379105056.0000 - rmse: 19470.6191 - val_loss: 417197920.0000 - val_rmse: 20425.4219\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 437027680.0000 - rmse: 20905.2051 - val_loss: 362853984.0000 - val_rmse: 19048.7246\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 336431264.0000 - rmse: 18342.0625 - val_loss: 354750848.0000 - val_rmse: 18834.8281\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 368368384.0000 - rmse: 19192.9238 - val_loss: 367233856.0000 - val_rmse: 19163.3457\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351755072.0000 - rmse: 18755.1348 - val_loss: 359722656.0000 - val_rmse: 18966.3555\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311210304.0000 - rmse: 17641.1543 - val_loss: 353141440.0000 - val_rmse: 18792.0586\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 360950400.0000 - rmse: 18998.6934 - val_loss: 353706464.0000 - val_rmse: 18807.0840\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341863616.0000 - rmse: 18489.5547 - val_loss: 364626272.0000 - val_rmse: 19095.1895\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 357939072.0000 - rmse: 18919.2773 - val_loss: 345590496.0000 - val_rmse: 18590.0625\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 367795136.0000 - rmse: 19177.9863 - val_loss: 366310048.0000 - val_rmse: 19139.2266\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345318336.0000 - rmse: 18582.7422 - val_loss: 393465824.0000 - val_rmse: 19835.9727\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 345880608.0000 - rmse: 18597.8652 - val_loss: 367156928.0000 - val_rmse: 19161.3398\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285228384.0000 - rmse: 16888.7051 - val_loss: 354807456.0000 - val_rmse: 18836.3320\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 323580896.0000 - rmse: 17988.3516 - val_loss: 345047328.0000 - val_rmse: 18575.4492\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339828000.0000 - rmse: 18434.4219 - val_loss: 348509120.0000 - val_rmse: 18668.3984\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343029312.0000 - rmse: 18521.0488 - val_loss: 340411456.0000 - val_rmse: 18450.2422\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311609664.0000 - rmse: 17652.4668 - val_loss: 412015936.0000 - val_rmse: 20298.1758\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 337507552.0000 - rmse: 18371.3789 - val_loss: 392055296.0000 - val_rmse: 19800.3848\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 438194400.0000 - rmse: 20933.0918 - val_loss: 334118976.0000 - val_rmse: 18278.9199\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 341705600.0000 - rmse: 18485.2812 - val_loss: 340782784.0000 - val_rmse: 18460.3008\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 327566848.0000 - rmse: 18098.8066 - val_loss: 363052288.0000 - val_rmse: 19053.9297\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 339214560.0000 - rmse: 18417.7773 - val_loss: 351233536.0000 - val_rmse: 18741.2246\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 344933184.0000 - rmse: 18572.3770 - val_loss: 351570336.0000 - val_rmse: 18750.2070\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340862464.0000 - rmse: 18462.4590 - val_loss: 375395808.0000 - val_rmse: 19375.1328\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 335262976.0000 - rmse: 18310.1875 - val_loss: 355064960.0000 - val_rmse: 18843.1660\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 351384832.0000 - rmse: 18745.2598 - val_loss: 345985216.0000 - val_rmse: 18600.6777\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 306150784.0000 - rmse: 17497.1641 - val_loss: 372581248.0000 - val_rmse: 19302.3633\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 325100448.0000 - rmse: 18030.5410 - val_loss: 362329312.0000 - val_rmse: 19034.9492\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343178720.0000 - rmse: 18525.0801 - val_loss: 356598496.0000 - val_rmse: 18883.8125\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288225216.0000 - rmse: 16977.1973 - val_loss: 377910880.0000 - val_rmse: 19439.9277\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262800672.0000 - rmse: 16211.1270 - val_loss: 360602016.0000 - val_rmse: 18989.5215\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 296883904.0000 - rmse: 17230.3184 - val_loss: 352196000.0000 - val_rmse: 18766.8848\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286285056.0000 - rmse: 16919.9590 - val_loss: 341470624.0000 - val_rmse: 18478.9238\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 281492864.0000 - rmse: 16777.7480 - val_loss: 400810624.0000 - val_rmse: 20020.2539\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 299322272.0000 - rmse: 17300.9316 - val_loss: 335393312.0000 - val_rmse: 18313.7441\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 388628128.0000 - rmse: 19713.6543 - val_loss: 354913760.0000 - val_rmse: 18839.1543\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 301229568.0000 - rmse: 17355.9668 - val_loss: 392985792.0000 - val_rmse: 19823.8691\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 413758656.0000 - rmse: 20341.0586 - val_loss: 409054080.0000 - val_rmse: 20225.0859\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302084320.0000 - rmse: 17380.5703 - val_loss: 324662336.0000 - val_rmse: 18018.3887\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 280961632.0000 - rmse: 16761.9102 - val_loss: 328335616.0000 - val_rmse: 18120.0332\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 324029088.0000 - rmse: 18000.8066 - val_loss: 398112800.0000 - val_rmse: 19952.7637\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 343871680.0000 - rmse: 18543.7754 - val_loss: 330460416.0000 - val_rmse: 18178.5703\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 340357056.0000 - rmse: 18448.7656 - val_loss: 372488928.0000 - val_rmse: 19299.9707\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287528480.0000 - rmse: 16956.6641 - val_loss: 368963424.0000 - val_rmse: 19208.4199\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265894720.0000 - rmse: 16306.2783 - val_loss: 316275520.0000 - val_rmse: 17784.1367\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302033184.0000 - rmse: 17379.1016 - val_loss: 321875584.0000 - val_rmse: 17940.8906\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 266029472.0000 - rmse: 16310.4092 - val_loss: 323340832.0000 - val_rmse: 17981.6797\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252394080.0000 - rmse: 15886.9141 - val_loss: 336202208.0000 - val_rmse: 18335.8184\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305948192.0000 - rmse: 17491.3730 - val_loss: 320272320.0000 - val_rmse: 17896.1523\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 278580512.0000 - rmse: 16690.7305 - val_loss: 333984480.0000 - val_rmse: 18275.2402\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245609120.0000 - rmse: 15671.9180 - val_loss: 336314432.0000 - val_rmse: 18338.8770\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 302738752.0000 - rmse: 17399.3867 - val_loss: 324475808.0000 - val_rmse: 18013.2129\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276116960.0000 - rmse: 16616.7656 - val_loss: 322947776.0000 - val_rmse: 17970.7461\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286734560.0000 - rmse: 16933.2383 - val_loss: 313215968.0000 - val_rmse: 17697.9082\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 277626208.0000 - rmse: 16662.1172 - val_loss: 303619744.0000 - val_rmse: 17424.6875\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 263131760.0000 - rmse: 16221.3359 - val_loss: 347446336.0000 - val_rmse: 18639.9121\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265273264.0000 - rmse: 16287.2109 - val_loss: 318735296.0000 - val_rmse: 17853.1562\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 286240320.0000 - rmse: 16918.6387 - val_loss: 321226816.0000 - val_rmse: 17922.7988\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 331134208.0000 - rmse: 18197.0918 - val_loss: 314753376.0000 - val_rmse: 17741.2910\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262767312.0000 - rmse: 16210.0977 - val_loss: 336036640.0000 - val_rmse: 18331.3008\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 310601760.0000 - rmse: 17623.8965 - val_loss: 318062368.0000 - val_rmse: 17834.3027\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 262015328.0000 - rmse: 16186.8848 - val_loss: 310200224.0000 - val_rmse: 17612.5020\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250447856.0000 - rmse: 15825.5430 - val_loss: 319809504.0000 - val_rmse: 17883.2168\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 261122512.0000 - rmse: 16159.2852 - val_loss: 413926848.0000 - val_rmse: 20345.1895\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 315821024.0000 - rmse: 17771.3516 - val_loss: 323502208.0000 - val_rmse: 17986.1680\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250966064.0000 - rmse: 15841.9082 - val_loss: 320856832.0000 - val_rmse: 17912.4766\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 248208864.0000 - rmse: 15754.6455 - val_loss: 363760576.0000 - val_rmse: 19072.5059\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287547488.0000 - rmse: 16957.2227 - val_loss: 342917792.0000 - val_rmse: 18518.0391\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254714160.0000 - rmse: 15959.7656 - val_loss: 330638624.0000 - val_rmse: 18183.4707\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256263488.0000 - rmse: 16008.2314 - val_loss: 385978336.0000 - val_rmse: 19646.3320\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255715008.0000 - rmse: 15991.0908 - val_loss: 321429856.0000 - val_rmse: 17928.4648\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 257447824.0000 - rmse: 16045.1797 - val_loss: 407214016.0000 - val_rmse: 20179.5430\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214665760.0000 - rmse: 14651.4746 - val_loss: 324065536.0000 - val_rmse: 18001.8184\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 265324544.0000 - rmse: 16288.7832 - val_loss: 340039424.0000 - val_rmse: 18440.1562\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234728032.0000 - rmse: 15320.8340 - val_loss: 310542720.0000 - val_rmse: 17622.2207\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229299248.0000 - rmse: 15142.6289 - val_loss: 329628960.0000 - val_rmse: 18155.6855\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 245342160.0000 - rmse: 15663.4014 - val_loss: 373915968.0000 - val_rmse: 19336.9043\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228378768.0000 - rmse: 15112.2041 - val_loss: 307933216.0000 - val_rmse: 17548.0254\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 256123584.0000 - rmse: 16003.8604 - val_loss: 323002976.0000 - val_rmse: 17972.2812\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287382752.0000 - rmse: 16952.3672 - val_loss: 336454304.0000 - val_rmse: 18342.6895\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 311472128.0000 - rmse: 17648.5723 - val_loss: 315223680.0000 - val_rmse: 17754.5391\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 279157408.0000 - rmse: 16708.0039 - val_loss: 402301920.0000 - val_rmse: 20057.4648\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 252044832.0000 - rmse: 15875.9180 - val_loss: 319657440.0000 - val_rmse: 17878.9648\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239676864.0000 - rmse: 15481.4990 - val_loss: 295573056.0000 - val_rmse: 17192.2383\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214801616.0000 - rmse: 14656.1104 - val_loss: 298317632.0000 - val_rmse: 17271.8711\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234542752.0000 - rmse: 15314.7852 - val_loss: 309330016.0000 - val_rmse: 17587.7773\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 276006592.0000 - rmse: 16613.4434 - val_loss: 342159840.0000 - val_rmse: 18497.5605\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 236823392.0000 - rmse: 15389.0664 - val_loss: 477307296.0000 - val_rmse: 21847.3633\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228059824.0000 - rmse: 15101.6484 - val_loss: 326320960.0000 - val_rmse: 18064.3555\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 249267904.0000 - rmse: 15788.2188 - val_loss: 293981344.0000 - val_rmse: 17145.8848\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 212680352.0000 - rmse: 14583.5625 - val_loss: 309462560.0000 - val_rmse: 17591.5469\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 232890496.0000 - rmse: 15260.7500 - val_loss: 306219968.0000 - val_rmse: 17499.1426\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264066368.0000 - rmse: 16250.1182 - val_loss: 314608800.0000 - val_rmse: 17737.2148\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 244683376.0000 - rmse: 15642.3574 - val_loss: 301058336.0000 - val_rmse: 17351.0312\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 287469088.0000 - rmse: 16954.9121 - val_loss: 315708256.0000 - val_rmse: 17768.1816\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254337632.0000 - rmse: 15947.9648 - val_loss: 314950208.0000 - val_rmse: 17746.8340\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 212781408.0000 - rmse: 14587.0283 - val_loss: 345513504.0000 - val_rmse: 18587.9941\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 229092224.0000 - rmse: 15135.7910 - val_loss: 279235232.0000 - val_rmse: 16710.3320\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210247632.0000 - rmse: 14499.9160 - val_loss: 340543360.0000 - val_rmse: 18453.8164\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 284006592.0000 - rmse: 16852.4941 - val_loss: 305092000.0000 - val_rmse: 17466.8809\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 305363488.0000 - rmse: 17474.6523 - val_loss: 307582304.0000 - val_rmse: 17538.0234\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 255369344.0000 - rmse: 15980.2793 - val_loss: 317002624.0000 - val_rmse: 17804.5664\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 208334272.0000 - rmse: 14433.7891 - val_loss: 387034336.0000 - val_rmse: 19673.1875\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 238491680.0000 - rmse: 15443.1738 - val_loss: 304475872.0000 - val_rmse: 17449.2344\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 235964672.0000 - rmse: 15361.1406 - val_loss: 310174848.0000 - val_rmse: 17611.7812\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 272641696.0000 - rmse: 16511.8652 - val_loss: 314441792.0000 - val_rmse: 17732.5039\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234963008.0000 - rmse: 15328.5020 - val_loss: 311141120.0000 - val_rmse: 17639.1934\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242383088.0000 - rmse: 15568.6562 - val_loss: 302272032.0000 - val_rmse: 17385.9707\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220026992.0000 - rmse: 14833.3066 - val_loss: 314865088.0000 - val_rmse: 17744.4355\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247740448.0000 - rmse: 15739.7725 - val_loss: 302807680.0000 - val_rmse: 17401.3672\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220236096.0000 - rmse: 14840.3535 - val_loss: 293591136.0000 - val_rmse: 17134.5000\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 240851152.0000 - rmse: 15519.3789 - val_loss: 317279840.0000 - val_rmse: 17812.3496\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239732560.0000 - rmse: 15483.2979 - val_loss: 422882400.0000 - val_rmse: 20564.1035\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237445648.0000 - rmse: 15409.2705 - val_loss: 323900544.0000 - val_rmse: 17997.2363\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222380064.0000 - rmse: 14912.4121 - val_loss: 307885184.0000 - val_rmse: 17546.6582\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 237286944.0000 - rmse: 15404.1211 - val_loss: 317048384.0000 - val_rmse: 17805.8516\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220908768.0000 - rmse: 14862.9990 - val_loss: 293692384.0000 - val_rmse: 17137.4531\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 228075424.0000 - rmse: 15102.1641 - val_loss: 281091968.0000 - val_rmse: 16765.7949\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 225538512.0000 - rmse: 15017.9385 - val_loss: 281544928.0000 - val_rmse: 16779.2988\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 285169440.0000 - rmse: 16886.9590 - val_loss: 318983744.0000 - val_rmse: 17860.1133\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 288784064.0000 - rmse: 16993.6465 - val_loss: 301131392.0000 - val_rmse: 17353.1387\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 222267584.0000 - rmse: 14908.6416 - val_loss: 285355584.0000 - val_rmse: 16892.4707\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 221780512.0000 - rmse: 14892.2959 - val_loss: 349281888.0000 - val_rmse: 18689.0840\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 254800336.0000 - rmse: 15962.4648 - val_loss: 366842848.0000 - val_rmse: 19153.1406\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 234262080.0000 - rmse: 15305.6221 - val_loss: 296205952.0000 - val_rmse: 17210.6348\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 193052784.0000 - rmse: 13894.3418 - val_loss: 313568224.0000 - val_rmse: 17707.8574\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 226894544.0000 - rmse: 15063.0176 - val_loss: 318681952.0000 - val_rmse: 17851.6641\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 246237264.0000 - rmse: 15691.9482 - val_loss: 347329728.0000 - val_rmse: 18636.7832\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217095008.0000 - rmse: 14734.1426 - val_loss: 344192768.0000 - val_rmse: 18552.4316\n",
      "Epoch 367/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239037344.0000 - rmse: 15460.8320 - val_loss: 287276416.0000 - val_rmse: 16949.2305\n",
      "Epoch 368/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 201236128.0000 - rmse: 14185.7705 - val_loss: 290166208.0000 - val_rmse: 17034.2637\n",
      "Epoch 369/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 212967456.0000 - rmse: 14593.4033 - val_loss: 290347680.0000 - val_rmse: 17039.5879\n",
      "Epoch 370/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 242734240.0000 - rmse: 15579.9287 - val_loss: 279740896.0000 - val_rmse: 16725.4551\n",
      "Epoch 371/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 223362128.0000 - rmse: 14945.3037 - val_loss: 287545440.0000 - val_rmse: 16957.1641\n",
      "Epoch 372/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 251887936.0000 - rmse: 15870.9766 - val_loss: 274969984.0000 - val_rmse: 16582.2188\n",
      "Epoch 373/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 231165024.0000 - rmse: 15204.1104 - val_loss: 286042912.0000 - val_rmse: 16912.8027\n",
      "Epoch 374/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210930880.0000 - rmse: 14523.4580 - val_loss: 301943168.0000 - val_rmse: 17376.5098\n",
      "Epoch 375/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 193402544.0000 - rmse: 13906.9229 - val_loss: 309459872.0000 - val_rmse: 17591.4707\n",
      "Epoch 376/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 194630384.0000 - rmse: 13950.9971 - val_loss: 271583104.0000 - val_rmse: 16479.7773\n",
      "Epoch 377/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 216264128.0000 - rmse: 14705.9189 - val_loss: 283209056.0000 - val_rmse: 16828.8145\n",
      "Epoch 378/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 239926080.0000 - rmse: 15489.5459 - val_loss: 300906112.0000 - val_rmse: 17346.6445\n",
      "Epoch 379/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 264831488.0000 - rmse: 16273.6426 - val_loss: 319955424.0000 - val_rmse: 17887.2949\n",
      "Epoch 380/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 188981808.0000 - rmse: 13747.0635 - val_loss: 326079968.0000 - val_rmse: 18057.6836\n",
      "Epoch 381/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 211034064.0000 - rmse: 14527.0107 - val_loss: 316559200.0000 - val_rmse: 17792.1094\n",
      "Epoch 382/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 215841920.0000 - rmse: 14691.5576 - val_loss: 337913504.0000 - val_rmse: 18382.4199\n",
      "Epoch 383/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 220081264.0000 - rmse: 14835.1357 - val_loss: 340712064.0000 - val_rmse: 18458.3848\n",
      "Epoch 384/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 201937968.0000 - rmse: 14210.4883 - val_loss: 335139904.0000 - val_rmse: 18306.8262\n",
      "Epoch 385/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217452592.0000 - rmse: 14746.2715 - val_loss: 289085088.0000 - val_rmse: 17002.5000\n",
      "Epoch 386/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 187883168.0000 - rmse: 13707.0469 - val_loss: 291267616.0000 - val_rmse: 17066.5645\n",
      "Epoch 387/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 241007456.0000 - rmse: 15524.4131 - val_loss: 310527872.0000 - val_rmse: 17621.7988\n",
      "Epoch 388/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 250262336.0000 - rmse: 15819.6797 - val_loss: 298389440.0000 - val_rmse: 17273.9492\n",
      "Epoch 389/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 230560336.0000 - rmse: 15184.2129 - val_loss: 301476544.0000 - val_rmse: 17363.0781\n",
      "Epoch 390/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 210979776.0000 - rmse: 14525.1416 - val_loss: 277734848.0000 - val_rmse: 16665.3770\n",
      "Epoch 391/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 208137872.0000 - rmse: 14426.9834 - val_loss: 310643808.0000 - val_rmse: 17625.0879\n",
      "Epoch 392/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 179334112.0000 - rmse: 13391.5664 - val_loss: 278892896.0000 - val_rmse: 16700.0840\n",
      "Epoch 393/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 202782032.0000 - rmse: 14240.1553 - val_loss: 310511136.0000 - val_rmse: 17621.3262\n",
      "Epoch 394/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 247391536.0000 - rmse: 15728.6836 - val_loss: 351916608.0000 - val_rmse: 18759.4395\n",
      "Epoch 395/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 201609584.0000 - rmse: 14198.9277 - val_loss: 302527424.0000 - val_rmse: 17393.3145\n",
      "Epoch 396/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 217823024.0000 - rmse: 14758.8271 - val_loss: 271668896.0000 - val_rmse: 16482.3789\n",
      "Epoch 397/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 160935248.0000 - rmse: 12686.0234 - val_loss: 284973408.0000 - val_rmse: 16881.1543\n",
      "Epoch 398/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 201972672.0000 - rmse: 14211.7070 - val_loss: 349915136.0000 - val_rmse: 18706.0176\n",
      "Epoch 399/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 185918352.0000 - rmse: 13635.1865 - val_loss: 273807168.0000 - val_rmse: 16547.1172\n",
      "Epoch 400/400\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 214582048.0000 - rmse: 14648.6191 - val_loss: 297742848.0000 - val_rmse: 17255.2266\n",
      "104/104 [==============================] - 0s 881us/step - loss: 481798304.0000 - rmse: 21949.9043\n",
      "[481798304.0, 21949.904296875]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABB1klEQVR4nO3dd3hVVfb/8fcihJIQehEpgn0AEYemYkEZit1RFBQVFAcdna86gqPoKNZRZxhF7JWiDoqIwtgQwfJTKYIgVTpIBIShSYckn98f59zkJjcJAe5NAq7X89wn5+7T9jnqXa6999nHJOGcc87FW5mSroBzzrlDkwcY55xzCeEBxjnnXEJ4gHHOOZcQHmCcc84lhAcY55xzCeEBxrk8zKyRmcnMyhZh215m9nVx1Ks4mNlyM/tDSdfDHRo8wLiDWviDuNvMauYpnxkGiUYlVLXouqSa2VYz+6ik63IgzGyomT1c0vVwBw8PMO5QsAy4IvLFzE4AKpZcdWJ0BXYBncysbklXxrni4gHGHQpeB66J+t4TGB69gZlVMbPhZrbOzFaY2d/NrEy4LsnMBprZ/8xsKXBePvu+amarzexnM3vYzJL2oX49gReAWUCPPMc+zcy+NbNNZrbSzHqF5RXN7N9hXTeb2ddmFhM0zayamX0QXtfGcLl+1PovzOwhM/vGzLaY2afR2Z6ZXR2eY72Z3bMP15S3Hn8ys8VmtsHMxprZ4WG5mdmTZrY2vI5ZZtYsXHeumc0L6/WzmfXb3/O70skDjDsUTAYqm9nvwh/+bsAbebZ5GqgCHAmcSRCQrg3X/Qk4HzgJaEWQcUQbBmQAR4fbdAKuL0rFzKwh0B54M/xck2fdx2HdagEtgJnh6oFAS+BUoDrwNyArn1OUAYYARwANgR3AM3m2uTK81tpAOaBfeP4mwPPA1cDhQA2gPvvIzM4GHgUuB+oCK4C3wtWdgDOAY4GqBP9s1ofrXgVukJQGNAMm7uu5XenmAcYdKiJZTEfgR+DnyIqooNNf0hZJy4F/E/ywQvDDOEjSSkkbCH4sI/vWAc4BbpO0TdJa4EmgexHrdQ0wS9I8YATQ1MxOCtf1AD6TNELSHknrJc0MM6vrgFsl/SwpU9K3knblPXi4z7uStkvaAjxCEECjDZG0UNIOYCRBIIMgkH4g6avw2PeSfxDbmx7Aa5K+D4/THzgl7P/aA6QBxwMmab6k1eF+e4AmZlZZ0kZJ3+/HuV0p5gHGHSpeJ/g/9V7kaR4DahL8n/uKqLIVQL1w+XBgZZ51EUcAycDqsBlrE/AiQTZQFNcQZC5IWgV8SdBkBtAAWJLPPjWBCgWsy8XMUszsxbCZ61fgK6Bqnia8NVHL24FK4XKu65a0jZzsYl8cTtQ9k7Q1PE49SRMJMqpngV/M7CUzqxxueilwLrDCzL40s1P249yuFPMA4w4JklYQdPafC4zOs/p/BP+3fERUWUNyspzVBD/20esiVhJ00NeUVDX8VJbUdG91MrNTgWOA/ma2xszWAG2BK8Ih0CuBo/LZ9X/AzgLW5dUXOA5oK6kyQXMUgBVh31zXbWYpBM1k+2oVUffWzFLD4/wMIGmwpJZAU4KmsjvC8u8kXUQQrN8nyK7cIcQDjDuU9AbODv9PPJukTIIfr0fMLM3MjgBuJ6efZiRwi5nVN7NqwF1R+64GPgX+bWaVzayMmR1lZnmbofLTExgPNCFolmpB0NeQQtDs9ibwBzO73MzKmlkNM2shKQt4DXjCzA4PByGcYmbl8zlHGkG/yyYzqw4MKEK9IkYB54cDDcoBD7L334QkM6sQ9SkH/Ae41sxahHX8BzBF0nIza21mbc0sGdhGEDgzzaycmfUwsyqS9gC/Apn7UHd3EPAA4w4ZkpZImlbA6v8j+IFbCnxN8KP4WrjuZWAc8APwPbEZ0DUETWzzgI0EP8yFDjc2swoEfTtPS1oT9VlG0JzXU9JPBBlXX2ADQQf/ieEh+gGzge/CdY+T/3+vgwiGZP+PYLDDJ4XVK5qkucDNBPdidXht6XvZ7S6CgBb5TJQ0gaD/5t3wOEeR00dVmeD+biRoRltPMIABgj6w5WHT3o3AVUWtuzs4mL9wzDnnXCJ4BuOccy4hPMA455xLCA8wzjnnEsIDjHPOuYTY63TkvxU1a9ZUo0aNSroazjl3UJk+ffr/JNXKb50HmFCjRo2YNq2gEa7OOefyY2YrClrnTWTOOecSwgOMc865hEhYgDGz18J3QMzJU/5/ZrbAzOaa2T+jyvuH75NYYGado8pbmtnscN1gM7OwvLyZvR2WT7GoNxeaWU8zWxR+euKcc67YJbIPZijBLKrZM9ua2VnARUBzSbvMrHZY3oRgaommBDOzfmZmx4ZzSD0P9CGYBuMjoAvBOzR6AxslHW1m3Qmm0ugWNR9TK0DAdDMbK2njvl7Anj17SE9PZ+fOnft1A9zeVahQgfr165OcnFzSVXHOxVnCAoykryz2feh/Bh6LvNcifLcGBEHnrbB8mZktBtqY2XKgsqRJAGY2HLiYIMBcBNwf7j8KeCbMbjoD48P3emBm4wmC0oh9vYb09HTS0tJo1KgRYeLk4kgS69evJz09ncaNG5d0dZxzcVbcfTDHAqeHTVpfmlnrsLweud/HkR6W1SP35HuR8lz7SMoANhNMEV7QsfbZzp07qVGjhgeXBDEzatSo4Rmic4eo4h6mXBaoBpwMtAZGmtmR5P/uChVSzn7uk4uZ9SFofqNhw4b5beLBJcH8/jp36CruDCYdGK3AVILXs9YMy6Nf+FSf4CVG6eR+R3iknOh9wpc3VSGY1rygY8WQ9JKkVpJa1aqV73NCe5WZCT//DNu27X1b55z7LSnuAPM+cDaAmR1L8I6N/wFjge7hyLDGBG8BnBq+7GmLmZ0c9q9cA4wJjzWWnFfPdiV4L4UI3uvRycyqhS+P6hSWJURWFqxe7QHGOefySuQw5RHAJOA4M0s3s94EL3g6Mhy6/BbBS5cUvvhoJMELnT4Bbg5HkEEwMOAVYDHBO8o/DstfBWqEAwJuJ3wLYdi5/xDBi5q+Ax6MdPgn5jqDv4l6rc6mTZt47rnn9nm/c889l02bNsW/Qs45V0T+wrFQq1atlHeqmPnz5/O73/2u0P0yM2HGDKhfHw47LP71Wr58Oeeffz5z5uR6nIjMzEySkpLif8IoGRkZlC2b+G66otxn51zpZGbTJbXKb53PRVZEt90GM2fGlkuwdSuULw/lyu3bMVu0gEGDCt/mrrvuYsmSJbRo0YLk5GQqVapE3bp1mTlzJvPmzePiiy9m5cqV7Ny5k1tvvZU+ffoAOXOrbd26lXPOOYfTTjuNb7/9lnr16jFmzBgqVqyY7/nat2/PqaeeyjfffMOFF15Iw4YNeeCBB0hKSqJKlSp89dVXDB06lPfff5/MzEzmzJlD37592b17N6+//jrly5fno48+onr16ixZsoSbb76ZdevWkZKSwssvv8zxxx+/bzfJOXfQ8gBzgBI9COqxxx5jzpw5zJw5ky+++ILzzjuPOXPmZD838tprr1G9enV27NhB69atufTSS6lRo0auYyxatIgRI0bw8ssvc/nll/Puu+9y1VUFv/5806ZNfPnllwCccMIJjBs3jnr16uVqcpszZw4zZsxg586dHH300Tz++OPMmDGDv/71rwwfPpzbbruNPn368MILL3DMMccwZcoUbrrpJiZOnBj/m+ScK5U8wBRRQZmGBNOnw+GHB59Ea9OmTa6HEgcPHsx7770HwMqVK1m0aFFMgGncuDEtWrQAoGXLlixfvrzQc3Tr1i17uV27dvTq1YvLL7+cSy65JLv8rLPOIi0tjbS0NKpUqcIFF1wABAFp1qxZbN26lW+//ZbLLrsse59du3bt1zU75w5OHmAOUNCHtYusrLIUx+1MTU3NXv7iiy/47LPPmDRpEikpKbRv3z7fhxbLly+fvZyUlMSOHTuKfI4XXniBKVOm8OGHH9KiRQtmhu2E0ccsU6ZM9vcyZcqQkZFBVlYWVatWzd7eOffb47MpH6DMzAxgDtu3J2agWlpaGlu2bMl33ebNm6lWrRopKSn8+OOPTJ48Oe7nX7JkCW3btuXBBx+kZs2arFy5cu87AZUrV6Zx48a88847QBCIf/jhh7jXzzlXenmAKeVq1KhBu3btaNasGXfccUeudV26dCEjI4PmzZtz7733cvLJJ8f9/HfccQcnnHACzZo144wzzuDEE08s8r5vvvkmr776KieeeCJNmzZlzJgxe9/JOXfI8GHKof0dppyRkcHMmTNJS2vAccfVSWQVD1k+TNm5g1dhw5Q9g4kTj9POOZebd/L/Rt1888188803ucpuvfVWrr322hKqkXPuUOMB5gBFZgM+2Joan3322ZKugnPuEOdNZM455xLCA0ycHGQJjHPOJZwHmLjxCOOcc9E8wBygnD6YxBx/f6frBxg0aBDbt2+Pc42cc65oPMCUcsUZYDIzM/e+kXPOFZGPIouTRGUw0dP1d+zYkdq1azNy5Eh27drFH//4Rx544AG2bdvG5ZdfTnp6OpmZmdx777388ssvrFq1irPOOouaNWvy+eef53v8SpUqcfvttzNu3Dj+/e9/88EHHzB27FjKli1Lp06dGDhwIL169aJixYr8+OOPrFixgiFDhjBs2DAmTZpE27ZtGTp0KACffvopAwYMYNeuXRx11FEMGTKESpUqJebGOOdKPQ8wRXTbbbcVOHHjli1bKFOmPKmp+/ZCmBYtWjBoLy+EiZ6u/9NPP2XUqFFMnToVSVx44YV89dVXrFu3jsMPP5wPP/wQCOYoq1KlCk888QSff/45NWvWLPD427Zto1mzZjz44INs2LCB3r178+OPP2Jmuabn37hxIxMnTmTs2LFccMEFfPPNN7zyyiu0bt2amTNnUr9+fR5++GE+++wzUlNTefzxx3niiSe477779umeOOcOHR5gDiKffvopn376KSeddBIAW7duZdGiRZx++un069ePO++8k/PPP5/TTz+9yMdMSkri0ksvBYIJKitUqMD111/Peeedx/nnn5+93QUXXICZccIJJ1CnTh1OOOEEAJo2bcry5ctJT09n3rx5tGvXDoDdu3dzyimnxOvSnXMHIQ8wRVRQpiGJ6dOnU7784ZxwQmJfCCOJ/v37c8MNN8Ssmz59Oh999BH9+/enU6dORc4cKlSokP3q5bJlyzJ16lQmTJjAW2+9xTPPPJP9grDo6fjzTtWfkZFBUlISHTt2ZMSIEQd6mc65Q0TCOvnN7DUzW2tmc/JZ18/MZGY1o8r6m9liM1tgZp2jylua2exw3WALh22ZWXkzezssn2JmjaL26Wlmi8JPz0RdY3iucCkxnTDR0/V37tyZ1157ja1btwLw888/s3btWlatWkVKSgpXXXUV/fr14/vvv4/Ztyi2bt3K5s2bOffccxk0aNA+vcvl5JNP5ptvvmHx4sUAbN++nYULFxZ5f+fcoSeRGcxQ4BlgeHShmTUAOgI/RZU1AboDTYHDgc/M7FhJmcDzQB9gMvAR0AX4GOgNbJR0tJl1Bx4HuplZdWAA0IrgV3+6mY2VtDGB15qwTv7o6frPOeccrrzyyuymp0qVKvHGG2+wePFi7rjjDsqUKUNycjLPP/88AH369OGcc86hbt26BXbyR9uyZQsXXXQRO3fuRBJPPvlkketZq1Ythg4dyhVXXJH95sqHH36YY489dj+u2jl3KEjodP1hVvGBpGZRZaOAh4AxQCtJ/zOz/gCSHg23GQfcDywHPpd0fFh+BdBe0g2RbSRNMrOywBqgFkGgai/phnCfF4EvJBXadrO/0/UDTJs2jXLl6tK8eb29buti+XT9zh28Ss10/WZ2IfCzpLyvNqwHRL8qMT0sqxcu5y3PtY+kDGAzUKOQY+VXnz5mNs3Mpq1bt26/rik80kE32aVzziVasXXym1kKcA/QKb/V+ZSpkPL93Sd3ofQS8BIEGUx+2xwq2rZtm910FfH6669njwZzzrl4K85RZEcBjYEfwo7x+sD3ZtaGIMtoELVtfWBVWF4/n3Ki9kkPm8iqABvC8vZ59vkivpcSq7QnMFOmTCnpKjjnfmOKrYlM0mxJtSU1ktSIIBD8XtIaYCzQPRwZ1hg4BpgqaTWwxcxODkePXUPQd0O4T2SEWFdgooJ2qnFAJzOrZmbVCDKmcQdQ7yJslV/S5IrCmxadO3QlcpjyCGAScJyZpZtZ74K2lTQXGAnMAz4Bbg5HkAH8GXgFWAwsIRhBBvAqUMPMFgO3A3eFx9pAMIjgu/DzYFi2zypUqMD69euL+CPoP5T7ShLr16+nQoUKJV0V51wCJHQU2cEkv1Fke/bsIT09nZ07dxa674oVP1GmTBoNGlRLZBUPSRUqVKB+/fokJyeXdFWcc/uhsFFk/iR/IZKTk2ncuPFet2vevC2pqb3ZtKnoz40459yhzqfrjwOzMmRleSbonHPRPMDEgZkhZZV0NZxzrlTxABMHnsE451wsDzBxYGZkZXkG45xz0TzAxIFZGX+ewznn8vAAEweewTjnXCwPMHHgfTDOORfLA0wc+Cgy55yL5QEmDjyDcc65WB5g4iCYhzOr1M+o7JxzxckDTByYlQFEZuZeN3XOud8MDzBxEMlgMjJKuibOOVd6eICJgzJlPINxzrm8PMDEgWcwzjkXywNMHET6YDzAOOdcDg8wcVCmTJDBeBOZc87l8AATB57BOOdcrIQFGDN7zczWmtmcqLJ/mdmPZjbLzN4zs6pR6/qb2WIzW2BmnaPKW5rZ7HDdYAs6PDCz8mb2dlg+xcwaRe3T08wWhZ+eibrGqPPhfTDOOZdbIjOYoUCXPGXjgWaSmgMLgf4AZtYE6A40Dfd5zsySwn2eB/oAx4SfyDF7AxslHQ08CTweHqs6MABoC7QBBphZtQRcXzYfReacc7ESFmAkfQVsyFP2qaTI/+dPBuqHyxcBb0naJWkZsBhoY2Z1gcqSJimYD384cHHUPsPC5VFAhzC76QyMl7RB0kaCoJY30MWVZzDOORerJPtgrgM+DpfrASuj1qWHZfXC5bzlufYJg9ZmoEYhx4phZn3MbJqZTVu3bt1+X0gkg/EA45xzOUokwJjZPUAG8GakKJ/NVEj5/u6Tu1B6SVIrSa1q1apVeKULERlF5gHGOedyFHuACTvdzwd6KOc1kOlAg6jN6gOrwvL6+ZTn2sfMygJVCJrkCjpWwkQyGH/nmHPO5SjWAGNmXYA7gQslbY9aNRboHo4Ma0zQmT9V0mpgi5mdHPavXAOMidonMkKsKzAxDFjjgE5mVi3s3O8UliXyuoAsDzDOORelbKIObGYjgPZATTNLJxjZ1R8oD4wPRxtPlnSjpLlmNhKYR9B0drOkyJisPxOMSKtI0GcT6bd5FXjdzBYTZC7dASRtMLOHgO/C7R6UlGuwQbwFGYwHGOeci5awACPpinyKXy1k+0eAR/IpnwY0y6d8J3BZAcd6DXityJU9QEGw9GHKzjkXzZ/kjwPPYJxzLpYHmDiIZDAeYJxzLocHmDjwDMY552J5gIkDnyrGOedieYCJAx+m7JxzsTzAxIE/aOmcc7E8wMRBJIPxJjLnnMvhASYOPINxzrlYHmDiIDLZpQcY55zL4QEmDjyDcc65WB5g4sD7YJxzLpYHmDhISvIMxjnn8vIAEwf+HIxzzsXyABMH3gfjnHOxPMDEQWQUmffBOOdcDg8wceAZjHPOxfIAEwf+HIxzzsXyABMHnsE451yshAUYM3vNzNaa2ZyosupmNt7MFoV/q0Wt629mi81sgZl1jipvaWazw3WDLRiyhZmVN7O3w/IpZtYoap+e4TkWmVnPRF1jhPfBOOdcrERmMEOBLnnK7gImSDoGmBB+x8yaAN2BpuE+z5lZUrjP80Af4JjwEzlmb2CjpKOBJ4HHw2NVBwYAbYE2wIDoQJYInsE451yshAUYSV8BG/IUXwQMC5eHARdHlb8laZekZcBioI2Z1QUqS5okScDwPPtEjjUK6BBmN52B8ZI2SNoIjCc20MWV98E451ys4u6DqSNpNUD4t3ZYXg9YGbVdelhWL1zOW55rH0kZwGagRiHHimFmfcxsmplNW7du3X5flGcwzjkXq7R08ls+ZSqkfH/3yV0ovSSplaRWtWrVKlJF8+N9MM45F6u4A8wvYbMX4d+1YXk60CBqu/rAqrC8fj7lufYxs7JAFYImuYKOlTA+F5lzzsUq7gAzFoiM6uoJjIkq7x6ODGtM0Jk/NWxG22JmJ4f9K9fk2SdyrK7AxLCfZhzQycyqhZ37ncKyhPE+GOeci1U2UQc2sxFAe6CmmaUTjOx6DBhpZr2Bn4DLACTNNbORwDwgA7hZUqTB6c8EI9IqAh+HH4BXgdfNbDFB5tI9PNYGM3sI+C7c7kFJeQcbxJX3wTjnXKyEBRhJVxSwqkMB2z8CPJJP+TSgWT7lOwkDVD7rXgNeK3JlD5D3wTjnXKzS0sl/UPM+GOeci+UBJg78fTDOORfLA0wceAbjnHOxPMDEgffBOOdcLA8wceAZjHPOxfIAEwfeB+Occ7E8wMSBZzDOORfLA0wcRDIY74NxzrkcRQowZnarmVW2wKtm9r2ZdUp05Q4W/iS/c87FKmoGc52kXwnm9aoFXEsw7YvD+2Cccy4/RQ0wkSnwzwWGSPqB/KfF/03yDMY552IVNcBMN7NPCQLMODNLA/znNOR9MM45F6uok132BloASyVtD997f23CanWQ8QzGOediFTWDOQVYIGmTmV0F/J3gFcUO74Nxzrn8FDXAPA9sN7MTgb8BK4DhCavVQSaSwXgTmXPO5ShqgMkI3xZ5EfCUpKeAtMRV6+DiGYxzzsUqah/MFjPrD1wNnG5mSUBy4qp1cPE+GOeci1XUDKYbsIvgeZg1QD3gXwmr1UHGMxjnnItVpAATBpU3gSpmdj6wU9J+98GY2V/NbK6ZzTGzEWZWwcyqm9l4M1sU/q0WtX1/M1tsZgvMrHNUeUszmx2uG2zBLz1mVt7M3g7Lp5hZo/2ta1F4H4xzzsUq6lQxlwNTgcuAy4EpZtZ1f05oZvWAW4BWkpoBSUB34C5ggqRjgAnhd8ysSbi+KdAFeC5sooNg8EEf4Jjw0yUs7w1slHQ08CTw+P7UdR+uCc9gnHMut6I2kd0DtJbUU9I1QBvg3gM4b1mgopmVBVKAVQQDCIaF64cBF4fLFwFvSdolaRmwGGhjZnWBypImhQMQhufZJ3KsUUCHSHaTCN4H45xzsYoaYMpIWhv1ff0+7JuLpJ+BgcBPwGpgs6RPgTqSVofbrAZqh7vUA1ZGHSI9LKsXLuctz7WPpAyCZ3Zq5K2LmfUxs2lmNm3dunX7czmR4+ABxjnncitqkPjEzMaZWS8z6wV8CHy0PycM+1YuAhoDhwOp4cObBe6ST5kKKS9sn9wF0kuSWklqVatWrcIrXoggg4GMjJhTOOfcb1aRhilLusPMLgXaEfx4vyTpvf085x+AZZLWAZjZaOBU4Bczqytpddj8FcmY0oEGUfvXJ2hSSw+X85ZH75MeNsNVATbsZ333KtL6lpmZRdCl5JxzrsjNXJLelXS7pL8eQHCBoGnsZDNLCftFOgDzgbFAz3CbnsCYcHks0D0cGdaYoDN/atiMtsXMTg6Pc02efSLH6gpMDPtpEiKSwWRleQbjnHMRhWYwZraFfJqWCLIYSaq8ryeUNMXMRgHfAxnADOAloBIw0sx6EwShy8Lt55rZSGBeuP3NkiIDgv8MDAUqAh+HH4BXgdfNbDFB5tJ9X+u5LyIZTJZ3wjjnXLZCA4ykhEwHI2kAMCBP8S6CbCa/7R8BHsmnfBrQLJ/ynYQBqjh4H4xzzsXar5FgLjfPYJxzLpYHmDjwPhjnnIvlASYOco8ic845Bx5g4iKSwWRmegbjnHMRHmDiwPtgnHMulgeYOPAMxjnnYnmAiQPPYJxzLpYHmDjwDMY552J5gIkDz2Cccy6WB5g48OdgnHMulgeYOPDnYJxzLpYHmDjwPhjnnIvlASYOvA/GOedieYCJA++Dcc65WB5g4sAzGOeci+UBJg68D8Y552J5gIkDz2Cccy6WB5g48D4Y55yLVSIBxsyqmtkoM/vRzOab2SlmVt3MxpvZovBvtajt+5vZYjNbYGado8pbmtnscN1gC1MJMytvZm+H5VPMrFGCrwfwDMY556KVVAbzFPCJpOOBE4H5wF3ABEnHABPC75hZE6A70BToAjxnZknhcZ4H+gDHhJ8uYXlvYKOko4EngccTeTGewTjnXKxiDzBmVhk4A3gVQNJuSZuAi4Bh4WbDgIvD5YuAtyTtkrQMWAy0MbO6QGVJkyQJGJ5nn8ixRgEdItlNgq4J8AzGOeeilUQGcySwDhhiZjPM7BUzSwXqSFoNEP6tHW5fD1gZtX96WFYvXM5bnmsfSRnAZqBGYi7HMxjnnMtPSQSYssDvgeclnQRsI2wOK0B+mYcKKS9sn9wHNutjZtPMbNq6desKr3VhFfQMxjnnYpREgEkH0iVNCb+PIgg4v4TNXoR/10Zt3yBq//rAqrC8fj7lufYxs7JAFWBD3opIeklSK0mtatWqtd8X5M/BOOdcrGIPMJLWACvN7LiwqAMwDxgL9AzLegJjwuWxQPdwZFhjgs78qWEz2hYzOznsX7kmzz6RY3UFJob9NAnhGYxzzsUqW0Ln/T/gTTMrBywFriUIdiPNrDfwE3AZgKS5ZjaSIAhlADdLygyP82dgKFAR+Dj8QDCA4HUzW0yQuXRP5MWUK1cOgKys3Yk8jXPOHVRKJMBImgm0ymdVhwK2fwR4JJ/yaUCzfMp3Egao4pCSkgJAZub24jqlc86Vev4kfxykpqYCkJXlAcY55yI8wMRBTgazrYRr4pxzpYcHmDiIBJidO7ezZ08JV8Y550oJDzBxEGkig+2sXVvops4595vhASYOIhkMbGPNmhKtinPOlRoeYOIgJ8Bs9wDjnHMhDzBxUK5cOcqWLQts55dfSro2zjlXOniAiZMgi/EmMueci/AAEycpKSkkJ3sTmXPORXiAiZPU1FQqVNjO6tUlXRPnnCsdPMDESUpKCikp21iwoKRr4pxzpYMHmDhJSUkhNXU78+bBjh0lXRvnnCt5HmDiJDU1lfLlt5OZCbNnl3RtnHOu5HmAiZOUlBTKlAnmIps+vYQr45xzpYAHmDhJSUkhI2M7hx0G33xT0rVxzrmS5wEmTlJTU9m+fTtnnglffAGJe3+mc84dHDzAxElKSgrbtm3jzDPh55/hH/8o6Ro551zJ8gATJzVq1GDjxo388Y97aNEC/v53mDq1pGvlnHMlxwNMnDRo0ABJ7Nq1iq++gmrV4IEHSrpWzjlXckoswJhZkpnNMLMPwu/VzWy8mS0K/1aL2ra/mS02swVm1jmqvKWZzQ7XDTYzC8vLm9nbYfkUM2uU6Otp0KABACtXriQtDfr3h48+gpEjE31m55wrnUoyg7kVmB/1/S5ggqRjgAnhd8ysCdAdaAp0AZ4zs6Rwn+eBPsAx4adLWN4b2CjpaOBJ4PHEXkruAANw223Qpg1cdx3+dL9z7jepRAKMmdUHzgNeiSq+CBgWLg8DLo4qf0vSLknLgMVAGzOrC1SWNEmSgOF59okcaxTQIZLdJEreAJOcDO+9B2XLwq23QmZmIs/unHOlT0llMIOAvwFZUWV1JK0GCP/WDsvrASujtksPy+qFy3nLc+0jKQPYDNTIWwkz62Nm08xs2rp16w7ogtLS0qhSpUp2gAE4/HB48EEYNw5OPBH+8hdYteqATuOccweNYg8wZnY+sFZSUZ93zy/zUCHlhe2Tu0B6SVIrSa1q1apVxOoUrGHDhixbtixX2S23wH/+AzVqwLPPwnnnwdatB3wq55wr9Uoig2kHXGhmy4G3gLPN7A3gl7DZi/Dv2nD7dKBB1P71gVVhef18ynPtY2ZlgSrAhkRcTLTWrVvz7bffkpmnPeyKK+DLL+GTT4J5yjp1gueeg8svh5UrCziYc84d5Io9wEjqL6m+pEYEnfcTJV0FjAV6hpv1BMaEy2OB7uHIsMYEnflTw2a0LWZ2cti/ck2efSLH6hqeI+HP1nfo0IGNGzcyc+bMfNd37gzDhsGSJXDzzfDOO3D66fDoo3DVVTB8eKJr6Jxzxac0PQfzGNDRzBYBHcPvSJoLjATmAZ8AN0uKpAh/JhgosBhYAnwclr8K1DCzxcDthCPSEu2ss84CYPz48QVu06MH/PQTfPghfPYZbNgAd98Nb74JPXvCWWfBf/+bex+fdsY5dzCyYvgf+4NCq1atNG3atAM+TuvWrUlKSmLy5MlF2n7tWli/HmrVCj4RV18Nr7wSBJ7774ePP4YmTQ64es45F1dmNl1Sq/zWlS3uyhzqLrnkEu6++26WLVtG48aN97p97drBB+D114OXlS1cCAMHwpo1MHFiMMS5bVs46ii4806oXx8aNIAjjoC//jX43q9fgi/MOef2kWcwoXhlMD/99BO/+93vOP300/n444/Z38dvnnwSBgwIgsijjwaZzPjxQbYDkJIC7doFZQBPPAE7d0KdOtCwYRCQ0tKCdbNmBc1sJ54YfH/nnSBbat/+wK7VOecKy2A8wITiFWAABg8ezK233sqoUaOoWLEinTt3Jikpae875kOCSIzasgXeeisY5jxpEnz9NbRuDd9+C//7X+79ypcPpqv54Yfggc+UFJg8OciKzj0XypUL+oHS0qBly9znmzYNTjopeEjUOecK4wGmCOIZYDIyMjjxxBOZN28eAEOGDKFXr15xOXZ+Vq6E558PMh2Ap58OmtYigeXqq+HllyEr6rHWGjVysqHJk2HGjODB0EcfDb5fcEGQNc2eHQSbihUTVn3n3EHMA0wRxDPAAHz33Xe0adMGgG7duvHWW2/F7dgFeecdKFMGLr00+P7119CoUdBHs2gRjB4dZCxVq0K9esGItp9/zv9YZsF2GzcGzXTdugXHPf74oDluw4ZgvrUjj4zd96efgnOWKU1jFJ1zCeEBpgjiHWAAli5dyn333ccHH3zArFmzaNiwYVyPHw9vvx00iY0fD3PnBpnQpZfCvHlw443w+98H2c3s2UFwqlMHFi8OAlCdOvDww8H+bdrA0KGwdCmkpwcPk374YdDMJgX9Q9dcA3/7W9Cs55w7NHiAKYJEBBiAmTNn0r59e5KTk3n00Ue55JJLqF69etzPc6B27w4CQ34ZiRT0+XToABkZQaZ07LHQrFnhz+jceWfwrM/ChdC8OXzzTVD+j38EAezmm6FFC6hQIWefXbvgtdegY8fg+9FH56zLzISvvoITTgia+z76KMjCkpMP+PKdc/vJA0wRJCrAAMyfP59evXoxdepUKlSowLPPPku3bt1ITU3NtZ2k/R51VhzWrw8CSs2awfePPoKxY4PmsHvvDYZWp6XBKacEfTgrVgRNdMnJQRNdQRo3DoZqH3UUjBkD27blrJs0CR55BB5/HD7/PJgwNNqAAXD++dCqFQweHDT5PZ7wlzM45yI8wBRBIgMMBMFj0qRJ3HnnnXz99dcA1KpVi9q1a3PDDTfwl7/8hT/84Q/UqlWrWPpr4mnnziDruPbanMEAEycGw6NvvDEIMJGsZ+PGoInsxx+DJrPly+Gww4I+oSVLgueAfv019hzJyVCpUrB/48aQZ07RXGU//xwMWHDOJZ4HmCJIdICJyMzMZPz48UyfPp2ffvqJefPm8fXXX9OiRYvsOcxWr17NYYcdlr193iHOmzZt4qyzzmLgwIF06NAh4XVOpKysoD8nkrhJsHp1MAP1v/4VzHQwfDhMnRo8iPrCC9C9O+zZA127BhnU5ZfD++8HzXwQDETYsSPItDZvDjKikSODvqNZs2DdOjj7bJg+PWjGmzEjCH7z5wfnP/98+O47OPXUoE9p9+6gf6pr1xK7Tc6VWh5giqC4AkxeWVlZPPfcczz99NMsXLgQgMqVK3Pcccdx/PHHM2rUKB544AGOP/54Vq5cyfLlyznssMPo27cvxx57LLNnz6ZcuXLs2rWLoUOHUr9+fc477zw+/PBDtm7dSrdu3Yr9muLl55+DjKigLqu1a+H776FLl2B5xgx49dUgW/rd74IHTtPSgvIlS3L2q149GAUHQWDL+59AzZrBc0WVKuV+tcKqVVC3bnyv0bmDXWEBBkn+kWjZsqVKUlZWlvbs2aPhw4frpptu0hlnnKGKFSuK4D02BX6qVq2qe+65R7///e+zyx566KHs5TVr1ujpp59W//79tWrVqkLrsHv3bu3evVsTJkzQ+eefrx07dkiS1qxZo0WLFsX9mtesWaPPPvssrsfcsUN65hlp7dqcspUrpXPPlf7v/6TMTGnPHmnyZOn996UgvEhNmkgvvRRsFykDqXHj3N8feEDavl369Vdp40bp22+lCy+UXngh//pkZMT18pwrdYBpKuB31TOYUEllMIWRxJ49e/j2229JTU2ldu3ajBs3jg8++IALL7yQF154gXnz5rFjxw6Sk5O54447GDhwILsjbUVRzIwKFSrQoUMHUlNTWb9+ffYL0tLS0ihXrhyff/45VapUYenSpQAMHDiQqlWr0rdvXzIyMpgwYQJt27YFYPPmzVSuXPmABiXceuutPPPMM2zYsIEqVars93EOxOjRQR/SlVcG39esgeuvDwYurF0Lo0YFfUJ//GOQLUX3D5Url9MsB7n7fqTgeaCTT4Y//Sl4s+nixUEGlJkZrC+hS3YurjyDOQgymP2VkZGhX3/9Vdu2bZMkzZ8/XzNnztSePXv0z3/+U3369NHHH3+sRYsW6dprr1Xjxo1Vv359HXvssapUqZJatmyp1NRUVa5cOVdmVLZs2ezlU089VbVr1xagyy+/XI8++qiSk5OVlpamTp06acuWLfnW7YMPPtDcuXP14osv6oYbbtDcuXN11VVXaW2YXrRu3VqAxo8frxkzZmjYsGHKysqKOU5mZma+5cVp9+4g+4lkNH36SFddJdWoIR1+uGQm9e4dZEbnnislJ+fOfI4+OvhbrZqUkiKVKydNm1ail+RcXFBIBlPiP+yl5XOwBph4yMzMVGZmprZs2aItW7bov//9r+bMmaOhQ4fqyy+/VEZGhn755Rfdd999Kl++fEwzXZ06dXT88cerefPmatiwoa6++mr169evwGa9Sy+9VJ999ln298MOOyx7edCgQUpPT5ckbdy4UW+99ZYqVKigE044Qbt27cqu848//qgLL7xQK1asOKBrX7ZsmRYsWCApCNZ79uwpdPvly6XXX8/5npUVNIOdd17ugBL53H+/dPfdUrt20r/+lXtdixZB0JKkUaOkk06S1qzJfb5hw6S8LZvz50szZhzQZTsXNx5gPMDEzebNm7Vw4UL9+uuvGjp0qIYMGaIePXqoa9euuuiii9SpUyeVKVNGgNq1a6d27drlyobatGmTb9A59dRTs5dTU1NzfY8ORG3atMkVkPr06aNdu3Zp5syZ6tOnj+644w7t2bMnO0uSpC1btmjZsmUx17J79+7s42RmZuq6665TlSpV9O6772r9+vWF9jstXbpUDRs21KxZsyRJ338vnXaaVLGidNNN0ldfBcEnr9GjpYsukoYMCf7rK1cuyIIiQaddO+mKK6QffpDmzcspk6SdO4NjRrZ1rjTwAOMBplht2rRJ69evz1U2fvx4TZkyRZL05Zdf6uWXX9bdd9+tTz75RCNGjFBGRobuvvtuNW7cWI0bN9YRRxyhv//977rxxhu1cOFCXXnllTrqqKNUpUoVtW/fXs2bN1dKSooAmVmuQJSUlCRAzZs31zXXXKNjjjlGgJ588kn17dtXjz/+uD755BOdc8452fs0bdo0e/nwww/XEUccIUDr1q3Tf//7X23dulW7du1SZmamBgwYoDp16mQPqIg0323btk1r166TFGSFhcnKCoLMZZdJ1atLLVtKrVv/T7BEEDSxtW2bE0w2bAiC0R135JRt3y5995105ZXSmDGx59i4MRiMUJB586Rrrw0GRji3vzzAeIA5JG3atEmPPfaYBgwYoMGDB2vt2rV65ZVXdOONN+ree+9Vly5dsgNBfn1LVatWVb9+/VSlSpXssieffHKvI/fyfjp27Kjdu3frggsuEKAzzjhDqampmjNnToGB5v3339fKlSslSbt2BQGnWbMTBGjAgF35Nrfl/Xz1ldSgQbB8zDHSwIFBlvPFF18IUL16S1S7dv6ZlCRdfHGw75tvJuqf0MFv2bJl+rWwKO1KV4ABGgCfA/OBucCtYXl1YDywKPxbLWqf/sBiYAHQOaq8JTA7XDeYnOd6ygNvh+VTgEZ7q5cHmENTVlaWdu/erXXr1unbb7/Vjh07tGbNGr333nvZw7bT09P1008/aerUqZKk4cOHq3fv3mrXrl2+ASVvxgRkZ1OAGjVqlL3cpEkTjRkzRr169dIdd9yhDRs2aNCgQQJ04oknKjMzU3v27FFGRkb2Pq+//qZ6935InTv31bBhC/X001Lz5gUHmpNPluA/gmaCNWrU6MLwWE8LpCFDgqxq587gnuzeLf3jH/9Uy5ZfCKSOHdO1efPm7Hs2bpy0aVPx/nMqjbKysrKbb13BSluAqQv8PlxOAxYCTYB/AneF5XcBj4fLTYAfwqDRGFgCJIXrpgKnAAZ8DJwTlt8EvBAudwfe3lu9PMC4vLKysrR27Vpt27ZNixcv1syZM7Vt2zZlZWWpf//+GjJkiO677z4NHjxYJ598smrWrKmffvpJmZmZeuSRR9SsWTM1bNiw0OwnLS1N1apVy85+oj/lypUToOTkZDVt2lR//vNN6t37n5o8ea7gW0FznXpqVx1++FFR+w0StAyXrxV8JEBHH322UlL+T1WqtNRRR70ZtX2mADVseLS++kqaNUuC3YJJatIkq8DsJ6+NGzfqrruyVLVqwRlTYZYty4zrM0OLFy8+4GOsWLEi+z7FwwcffKDvv/8+LscqTUpVgImpAIwBOobZSV3lBKEFysle+kdtPy4MKnWBH6PKrwBejN4mXC4L/C+S3RT08QDjEmHLli36z3/+oylTpui+++7TAw88oG+++Ubr1q3TJZdcojPPPDO7LygtLU2jR49WmzZt9Pbbb2v+/Plq0KCBLrroIp111lnZP3ZlypRR2bJls/uaAJUvX1lHH328UlNT96l575prJkR9v05HHrlNcH74fazCAXa5fP3113r33Xezv6enp4dNjx0Ee1TYb3ukv2rLli26+OKLNWHCD+rXb4ggWddf/7ZefPFFvfHGGwd0zz/6KAiq0XVct26dvvvuO9WtW1cTJkzId78PP/xQXbt2za5j5DhFDTBbtmzRzJkz810XyYbiFaxKk1IbYIBGwE9AZWBTnnUbw7/PAFdFlb8KdAVaAZ9FlZ8OfBAuzwHqR61bAtTM5/x9gGnAtIYNG8b1pju3LzZs2KDt27cXuD4rK0ujR4/W1KlTdc8996hv375asWKFpk2bps8//1xLly7VRx99pHr16ql9+/YaMmSIjjoqktnUL0KwaRz+rR1V1kSVKv1NffoM0pgxP6hfv0H6059eVXJyEMSeeuop7d69W88991zUPu+qT59hmjRpkrp166bLLrtMw4e/oQ4dOuiWW25RzZo1NWDAAD399NMClJJSX5AsQPXrX5j9PNYpp5yivn377tfzT3feeacA3XTTTRo9erTmzp2runXrZtfxxhtvlCTNmDFDp556avaIw8j62bNnS5IGDhyYXVbYP5uI8847T4A2bNgQs27RokXZxyqoX27SpEn697//nfBnvqKH+8dDqQwwQCVgOnBJ+L2gAPNsPgHmUqB1PgHmv+Hy3HwCTI3C6uMZjDvUZGVl6YcfZmn69J3KysrSrbfeqokTJ+q9995Tjx6vqXfvB3XKKafoqqvuFkjlyt0sQMcd10Q9ekR+XGP7m6C+kpLODpvxgumMKlVqIDgsn22L8qkg6Jjvuq+++kqzZ8/Wc889p1GjRumRRx7R3LlzlZmZqUmTJql169ZasGCBvvxyowYNek5btmxRly5dBMEgjoLO+cQTT2SPHBw4cKCeffbZ7HVPPfWUJOnqq6/OLosMR4/YsGGDOnbsqH/84x+6//77c2Uo77//fsw/ixEjRmSvnz9/fsz6+fPn5wpwb7/9tho2bKjbbrutSP+sN23apDlz5uj666/Xn//8Zy1dujTf7T744AMBmjFjhmbOnKnk5GT98MMPRTpHQUpdgAGSCZqxbo8q8yYy50pAVpY0cqS0eXOWJk+enD26rWvXeerU6SfBj4I3ZDZOHTr003PPLRVkCcYKrhOcI3hZcL+SkioIblRKygtq2vRjlSv3taCvYJVgoVq02KkvvvhC3bp1U48eNwjqCT4WDM0TBLorOTklT1nOp169emrcuHFM+eGHN4lpJqxVq1ahAS7voI0zzjhDkydPVnJycvYcfz169NBjjz2mPn36qH///tkzW0Q+7drlDHk/7bTT1K1bN/Xt21c33XSTBg4cqGuvvTZ7/ZFHHqmePXtq165dGjlypDp37qzrr78+1/GSk5Oz65Z3DsHt27dr+PDhmjRpknbs2KHu3btn7xd5EPrss8/W//t//08vvviilixZorlz52rkyJE6+uijBahnz57q06dP9vKBjJQrVQGGoEN+ODAoT/m/yN3J/89wuSm5O/mXktPJ/x1wMjmd/OeG5TeTu5N/5N7q5QHGufyNGBE8kxP9vMwzzwS/HikpUnq6dOqpwWwEmZmZmjo19yi0J54IHkC99NJgnzPPDKbOOf304Hvw2SV4QzBHDz20Wm3bZigl5SUlJfVS27b/Uo0aLwteUO/eKwRPqWrVMwXoj3+8VMcff4Ogk+BGQXM1b95GFSp8IXhA//jHGPXtm6HZs9dq9OjR+u677/T222+rY8eO6tWrlx5++GHVrVtXrVoNFlytNm0eyP6xrl27jhYtWq0zzjgjJijVrn1sPmUn65RTThGghg0bKikpKfuhY0DXXXed+vfvn/397LPPVvnyFbK/n3766dnB8OKLL9b48eMFqFWrVrrwwgt1yy23qH///mrevLkg6IuLjHS88cYbdcUVV2jevHl67LHHCg2o5cqVU5kyZXJNptuyZcu9PrtVkNIWYE4LL2oWMDP8nAvUACYQDFOeAFSP2ucegmauBYQjxcLyVgT9LUsI+moiw5QrAO8QDFOeChy5t3p5gHGu6NaulY4/XiqgvzzGjh3BPjkBJZi/7bbbpGXLpNWrg+dy6teXtm6V3nor97b5f5bosMMyY8qvuCJnuWzZ4O9pp0n/+Y80dGhw7L//PahXVlaW3nsvSyA1aiRBlv7+96F66qmn1KrVetWpIy1dulz33/+AFixYoVtu+VXw/wRZuv/+twWobt1PBOmqVClLGzZs1eefr1HVqtI776xSenq6Jk6cqHvuuSd7dvI9e/Zo8ODBKleunJKSKgpeVnJyS7355tt6773pGjduXPZ969Gjh0444YRcs1dUr15dt99+e/b3nj2H6ZVXcu71hg0b1LlzZ/Xo0UOvvz5C7du3F6AaNWqoXbt2uUbHRUYqvh49/9E+KlUBprR+PMA4l3gffSR17hzMMhB5LiciKyt3ljR4sPTgg8EEoRBMJgrBzAWzZgVzwp10knT99dLcuUEWFXl4NCVFqlKl8AB1+eXBA6ognXiitH59EOBq15ZeeSVnu+bNgwlNb7459/5nnSV9+eUqgXT22UHZDTfkPu/ll0vHHSfdeGNQ7379gtdEDB8uLV26SSkpG7O3jWR0kfWnnCI9/ngwW8OIETt1+ukblJT0ia69dp2ysqQ333xTL700S2bBA7d5jRkjVa0qdemSpYULl2nz5s369tudOvJI6T//+ULjx4/XunXr9MknnxzQP1MPMB5gnDtobdkSBJAdO6QBA6Rffil426ws6cMPpRUrpAULgua5+fODueLmz5cOOyz/YBOZPHTOHOnII4OypCSpbt3c2zVrFtRhwIDc5d98I5Upk/P9lFNiz1G9ejDdT97yvJOgFuXzxBNBRli3bs55Bw2SevYMZvq++26pTp2c7a+8MphItVev4Hu9etLYscE1v/lm8NnfwWseYDzAOOcUzFJwxBHBrNjffBMEounTc28zZUoQZMaODYLa2LHBD/Nxx0nrgkkR9MsvUq1awS9ohQrB7AiRDGvu3GCb3buDjOVf/wqyrA0bgiDXt690yy3S7bcHWdru3UHAuuaa4KV3kaBw9925g0r//tLnn+f0ZdWqFQSsF1/M2SYtLTZ43XNPwYGqadPg75lneoDxAOOcKxGR1zJEy8gIgkPkFQvbtgUTiB6IPXuCLOnhh4Pvu3YFb1GNngBg2zapQ4egGfDll4NtIJgwNSNDevZZ6Xe/C8oWLgz2GT06J6i8+6501FG5A8133+1/nQsLMP5Gy1BpfKOlc84VRILIC2V//hlq14bk5OB7Zib8+CM0bZqz/TPPQJ06cNllsHAhHHdcUD55MoQvqt0vhb3R0gNMyAOMc+63QoKHHoILL4QWLQ7sWIUFmLIHdmjnnHMHGzO4777En6dM4k/hnHPut8gDjHPOuYTwAOOccy4hPMA455xLCA8wzjnnEsIDjHPOuYTwAOOccy4hPMA455xLCH+SP2Rm64AVB3CImgRvzixtvF77xuu1b0prvaD01u1Qq9cRkmrlt8IDTJyY2bSCpksoSV6vfeP12jeltV5Qeuv2W6qXN5E555xLCA8wzjnnEsIDTPy8VNIVKIDXa994vfZNaa0XlN66/Wbq5X0wzjnnEsIzGOeccwnhAcY551xCeIA5QGbWxcwWmNliM7urhOuy3Mxmm9lMM5sWllU3s/Fmtij8W62Y6vKama01szlRZQXWxcz6h/dwgZl1LuZ63W9mP4f3baaZnVuc9TKzBmb2uZnNN7O5ZnZrWF4a7ldBdSvpe1bBzKaa2Q9hvR4Iy0v0nhVSrxK9X1HnSjKzGWb2Qfg9sfdLkn/28wMkAUuAI4FywA9AkxKsz3KgZp6yfwJ3hct3AY8XU13OAH4PzNlbXYAm4b0rDzQO72lSMdbrfqBfPtsWS72AusDvw+U0YGF47tJwvwqqW0nfMwMqhcvJwBTg5JK+Z4XUq0TvV9T5bgf+A3wQfk/o/fIM5sC0ARZLWippN/AWcFEJ1ymvi4Bh4fIw4OLiOKmkr4ANRazLRcBbknZJWgYsJri3xVWvghRLvSStlvR9uLwFmA/Uo3Tcr4LqVpDiumeStDX8mhx+RAnfs0LqVZBi+2dpZvWB84BX8pw/YffLA8yBqQesjPqeTuH/8SWagE/NbLqZ9QnL6khaDcGPBVC7xGpXcF1Kw338i5nNCpvQIs0ExV4vM2sEnETwf76l6n7lqRuU8D0Lm3tmAmuB8ZJKxT0roF5Q8v+ODQL+BmRFlSX0fnmAOTCWT1lJjvtuJ+n3wDnAzWZ2RgnWZV+U9H18HjgKaAGsBv4dlhdrvcysEvAucJukXwvbNJ+yhN6vfOpW4vdMUqakFkB9oI2ZNStk85KuV4neLzM7H1graXpRd8mnbJ/r5QHmwKQDDaK+1wdWlVBdkLQq/LsWeI8gpf3FzOoChH/XllT9CqlLid5HSb+EPwpZwMvkNAUUW73MLJngB/xNSaPD4lJxv/KrW2m4ZxGSNgFfAF0oJfcsb71Kwf1qB1xoZssJmvLPNrM3SPD98gBzYL4DjjGzxmZWDugOjC2JiphZqpmlRZaBTsCcsD49w816AmNKon6hguoyFuhuZuXNrDFwDDC1uCoV+Q8s9EeC+1Zs9TIzA14F5kt6ImpVid+vgupWCu5ZLTOrGi5XBP4A/EgJ37OC6lXS90tSf0n1JTUi+J2aKOkqEn2/EjVa4bfyAc4lGFmzBLinBOtxJMGojx+AuZG6ADWACcCi8G/1YqrPCIKmgD0E/zfUu7C6APeE93ABcE4x1+t1YDYwK/wPq25x1gs4jaD5YRYwM/ycW0ruV0F1K+l71hyYEZ5/DnDf3v59L+F6lej9ylPH9uSMIkvo/fKpYpxzziWEN5E555xLCA8wzjnnEsIDjHPOuYTwAOOccy4hPMA455xLCA8wzh0CzKx9ZIZc50oLDzDOOecSwgOMc8XIzK4K3xcy08xeDCdG3Gpm/zaz781sgpnVCrdtYWaTwwkS34tMkGhmR5vZZ+E7R743s6PCw1cys1Fm9qOZvRk+he9cifEA41wxMbPfAd0IJiVtAWQCPYBU4HsFE5V+CQwIdxkO3CmpOcFT4JHyN4FnJZ0InEowMwEEMx3fRvAujyMJ5p9yrsSULekKOPcb0gFoCXwXJhcVCSYXzALeDrd5AxhtZlWAqpK+DMuHAe+E883Vk/QegKSdAOHxpkpKD7/PBBoBXyf8qpwrgAcY54qPAcMk9c9VaHZvnu0Km7+psGavXVHLmfh/366EeROZc8VnAtDVzGpD9vvQjyD477BruM2VwNeSNgMbzez0sPxq4EsF72JJN7OLw2OUN7OU4rwI54rK/w/HuWIiaZ6Z/Z3graNlCGZ0vhnYBjQ1s+nAZoJ+GgimT38hDCBLgWvD8quBF83swfAYlxXjZThXZD6bsnMlzMy2SqpU0vVwLt68icw551xCeAbjnHMuITyDcc45lxAeYJxzziWEBxjnnHMJ4QHGOedcQniAcc45lxD/HxJlGxx2+iB8AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "baseline_model = train_and_get_deep_learning_model(\"baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Result of Baseline Model and Generate the CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[171288.61  296763.6   225639.78  164268.11   67766.63  113638.914\n",
      "  44488.5    44426.99   75725.62  112283.48 ] (5000,)\n"
     ]
    }
   ],
   "source": [
    "predict(baseline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run k-fold for embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<src.model.emb_model object at 0x7f8798433cd0>\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 21486362624.0000 - rmse: 146582.2656 - val_loss: 3449835264.0000 - val_rmse: 58735.2969\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2821952768.0000 - rmse: 53122.0547 - val_loss: 1395036544.0000 - val_rmse: 37350.1875\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2125130240.0000 - rmse: 46099.1367 - val_loss: 1193041408.0000 - val_rmse: 34540.4297\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1947135744.0000 - rmse: 44126.3594 - val_loss: 1061434688.0000 - val_rmse: 32579.6660\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1750267136.0000 - rmse: 41836.1953 - val_loss: 964750272.0000 - val_rmse: 31060.4297\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1706206592.0000 - rmse: 41306.2539 - val_loss: 880832384.0000 - val_rmse: 29678.8203\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1713645824.0000 - rmse: 41396.2070 - val_loss: 837002688.0000 - val_rmse: 28930.9980\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1639913728.0000 - rmse: 40495.8477 - val_loss: 797237056.0000 - val_rmse: 28235.3867\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1576545536.0000 - rmse: 39705.7383 - val_loss: 801862592.0000 - val_rmse: 28317.1777\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1603340544.0000 - rmse: 40041.7344 - val_loss: 734169344.0000 - val_rmse: 27095.5586\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1457061376.0000 - rmse: 38171.4727 - val_loss: 724141376.0000 - val_rmse: 26909.8750\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1477238656.0000 - rmse: 38434.8633 - val_loss: 695461120.0000 - val_rmse: 26371.5977\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1413431936.0000 - rmse: 37595.6367 - val_loss: 687916096.0000 - val_rmse: 26228.1543\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1366361088.0000 - rmse: 36964.3203 - val_loss: 659662144.0000 - val_rmse: 25683.8887\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1412894848.0000 - rmse: 37588.4922 - val_loss: 654143360.0000 - val_rmse: 25576.2266\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1312564352.0000 - rmse: 36229.3281 - val_loss: 638581376.0000 - val_rmse: 25270.1680\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1333220608.0000 - rmse: 36513.2930 - val_loss: 633364864.0000 - val_rmse: 25166.7422\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1307420928.0000 - rmse: 36158.2773 - val_loss: 609038784.0000 - val_rmse: 24678.7109\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1258179328.0000 - rmse: 35470.8242 - val_loss: 594223104.0000 - val_rmse: 24376.6914\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1211943296.0000 - rmse: 34812.9766 - val_loss: 601344384.0000 - val_rmse: 24522.3242\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1156898432.0000 - rmse: 34013.2109 - val_loss: 632925696.0000 - val_rmse: 25158.0137\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1192766080.0000 - rmse: 34536.4453 - val_loss: 590249408.0000 - val_rmse: 24295.0488\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1200443264.0000 - rmse: 34647.4141 - val_loss: 638500160.0000 - val_rmse: 25268.5605\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1114886016.0000 - rmse: 33389.9102 - val_loss: 576587072.0000 - val_rmse: 24012.2285\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1095096832.0000 - rmse: 33092.2461 - val_loss: 643619648.0000 - val_rmse: 25369.6602\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1156303872.0000 - rmse: 34004.4688 - val_loss: 556937344.0000 - val_rmse: 23599.5195\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1085197696.0000 - rmse: 32942.3398 - val_loss: 550083392.0000 - val_rmse: 23453.8574\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1096846720.0000 - rmse: 33118.6758 - val_loss: 573642240.0000 - val_rmse: 23950.8301\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1066970496.0000 - rmse: 32664.5137 - val_loss: 590817536.0000 - val_rmse: 24306.7383\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1115543808.0000 - rmse: 33399.7578 - val_loss: 635997312.0000 - val_rmse: 25218.9863\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1086539520.0000 - rmse: 32962.6992 - val_loss: 563576960.0000 - val_rmse: 23739.7754\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1075680000.0000 - rmse: 32797.5625 - val_loss: 565825728.0000 - val_rmse: 23787.0918\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1076035200.0000 - rmse: 32802.9766 - val_loss: 556424960.0000 - val_rmse: 23588.6621\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1093498240.0000 - rmse: 33068.0859 - val_loss: 556028288.0000 - val_rmse: 23580.2520\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 998237568.0000 - rmse: 31594.8984 - val_loss: 668040192.0000 - val_rmse: 25846.4727\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1045841152.0000 - rmse: 32339.4668 - val_loss: 543516224.0000 - val_rmse: 23313.4336\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 979879552.0000 - rmse: 31303.0273 - val_loss: 548229632.0000 - val_rmse: 23414.3047\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1054916160.0000 - rmse: 32479.4727 - val_loss: 536944960.0000 - val_rmse: 23172.0723\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 907770176.0000 - rmse: 30129.2246 - val_loss: 594286912.0000 - val_rmse: 24378.0000\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1051648448.0000 - rmse: 32429.1289 - val_loss: 549975744.0000 - val_rmse: 23451.5625\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1093702656.0000 - rmse: 33071.1758 - val_loss: 516273664.0000 - val_rmse: 22721.6562\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 937869760.0000 - rmse: 30624.6602 - val_loss: 528790304.0000 - val_rmse: 22995.4414\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 966668928.0000 - rmse: 31091.3008 - val_loss: 519664000.0000 - val_rmse: 22796.1406\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 948511552.0000 - rmse: 30797.9141 - val_loss: 517019808.0000 - val_rmse: 22738.0703\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 935071424.0000 - rmse: 30578.9375 - val_loss: 544617408.0000 - val_rmse: 23337.0391\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 905382848.0000 - rmse: 30089.5801 - val_loss: 516660544.0000 - val_rmse: 22730.1680\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 942065856.0000 - rmse: 30693.0918 - val_loss: 494786912.0000 - val_rmse: 22243.8066\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 862673024.0000 - rmse: 29371.2969 - val_loss: 497701280.0000 - val_rmse: 22309.2188\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 906355968.0000 - rmse: 30105.7461 - val_loss: 541815360.0000 - val_rmse: 23276.9277\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 949995072.0000 - rmse: 30821.9902 - val_loss: 532229728.0000 - val_rmse: 23070.1055\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 968690240.0000 - rmse: 31123.7891 - val_loss: 482964000.0000 - val_rmse: 21976.4414\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 827735488.0000 - rmse: 28770.3926 - val_loss: 520044320.0000 - val_rmse: 22804.4805\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 844835968.0000 - rmse: 29066.0625 - val_loss: 467058560.0000 - val_rmse: 21611.5371\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 868114048.0000 - rmse: 29463.7754 - val_loss: 487741216.0000 - val_rmse: 22084.8633\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 779401152.0000 - rmse: 27917.7578 - val_loss: 474845024.0000 - val_rmse: 21790.9395\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 839433600.0000 - rmse: 28972.9805 - val_loss: 448851936.0000 - val_rmse: 21186.1270\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 896706112.0000 - rmse: 29945.0508 - val_loss: 447286816.0000 - val_rmse: 21149.1562\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 814080640.0000 - rmse: 28532.0977 - val_loss: 471035488.0000 - val_rmse: 21703.3516\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 828809536.0000 - rmse: 28789.0527 - val_loss: 448647520.0000 - val_rmse: 21181.3008\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 849081664.0000 - rmse: 29139.0059 - val_loss: 435868160.0000 - val_rmse: 20877.4551\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 796253696.0000 - rmse: 28217.9668 - val_loss: 425209408.0000 - val_rmse: 20620.6055\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 889379200.0000 - rmse: 29822.4609 - val_loss: 437075008.0000 - val_rmse: 20906.3398\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 874645632.0000 - rmse: 29574.4082 - val_loss: 458990848.0000 - val_rmse: 21424.0723\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 814624640.0000 - rmse: 28541.6309 - val_loss: 428238496.0000 - val_rmse: 20693.9238\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 882910976.0000 - rmse: 29713.8184 - val_loss: 416710528.0000 - val_rmse: 20413.4883\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 800429248.0000 - rmse: 28291.8574 - val_loss: 417631488.0000 - val_rmse: 20436.0332\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 849548160.0000 - rmse: 29147.0098 - val_loss: 415541536.0000 - val_rmse: 20384.8359\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 730418240.0000 - rmse: 27026.2500 - val_loss: 435969280.0000 - val_rmse: 20879.8770\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 789726208.0000 - rmse: 28102.0684 - val_loss: 398383776.0000 - val_rmse: 19959.5527\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 834110976.0000 - rmse: 28880.9785 - val_loss: 500542336.0000 - val_rmse: 22372.8027\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 827629888.0000 - rmse: 28768.5566 - val_loss: 417061760.0000 - val_rmse: 20422.0898\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 784872000.0000 - rmse: 28015.5664 - val_loss: 395802272.0000 - val_rmse: 19894.7793\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 785244672.0000 - rmse: 28022.2168 - val_loss: 401559904.0000 - val_rmse: 20038.9590\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 784482688.0000 - rmse: 28008.6172 - val_loss: 428447456.0000 - val_rmse: 20698.9727\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 784979904.0000 - rmse: 28017.4922 - val_loss: 438169536.0000 - val_rmse: 20932.5000\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 750395072.0000 - rmse: 27393.3398 - val_loss: 452455168.0000 - val_rmse: 21270.9941\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 785200576.0000 - rmse: 28021.4297 - val_loss: 392684160.0000 - val_rmse: 19816.2598\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 729702592.0000 - rmse: 27013.0078 - val_loss: 389276544.0000 - val_rmse: 19730.0918\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 720089984.0000 - rmse: 26834.4922 - val_loss: 375570336.0000 - val_rmse: 19379.6367\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 799112128.0000 - rmse: 28268.5723 - val_loss: 383874144.0000 - val_rmse: 19592.7070\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 671407808.0000 - rmse: 25911.5391 - val_loss: 376579296.0000 - val_rmse: 19405.6504\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 716927232.0000 - rmse: 26775.4961 - val_loss: 375456608.0000 - val_rmse: 19376.7031\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 723285888.0000 - rmse: 26893.9746 - val_loss: 367828992.0000 - val_rmse: 19178.8691\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 730706368.0000 - rmse: 27031.5801 - val_loss: 361102688.0000 - val_rmse: 19002.7012\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 722364480.0000 - rmse: 26876.8398 - val_loss: 354055200.0000 - val_rmse: 18816.3555\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 756056768.0000 - rmse: 27496.4863 - val_loss: 359584672.0000 - val_rmse: 18962.7188\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 669862464.0000 - rmse: 25881.7012 - val_loss: 369048544.0000 - val_rmse: 19210.6367\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 730406464.0000 - rmse: 27026.0332 - val_loss: 379268160.0000 - val_rmse: 19474.8086\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 746580224.0000 - rmse: 27323.6211 - val_loss: 363560320.0000 - val_rmse: 19067.2578\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 682583744.0000 - rmse: 26126.3027 - val_loss: 361529760.0000 - val_rmse: 19013.9355\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 672289920.0000 - rmse: 25928.5547 - val_loss: 348144096.0000 - val_rmse: 18658.6191\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 688459008.0000 - rmse: 26238.5020 - val_loss: 359685248.0000 - val_rmse: 18965.3691\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 671181760.0000 - rmse: 25907.1758 - val_loss: 462074304.0000 - val_rmse: 21495.9141\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 663216704.0000 - rmse: 25752.9941 - val_loss: 356580896.0000 - val_rmse: 18883.3496\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 696409664.0000 - rmse: 26389.5742 - val_loss: 348907840.0000 - val_rmse: 18679.0742\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 688492672.0000 - rmse: 26239.1445 - val_loss: 424151072.0000 - val_rmse: 20594.9277\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 650961664.0000 - rmse: 25513.9512 - val_loss: 396537440.0000 - val_rmse: 19913.2480\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 695798464.0000 - rmse: 26377.9922 - val_loss: 338475552.0000 - val_rmse: 18397.7051\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 625105472.0000 - rmse: 25002.1094 - val_loss: 334570848.0000 - val_rmse: 18291.2773\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 636523136.0000 - rmse: 25229.4102 - val_loss: 366644864.0000 - val_rmse: 19147.9727\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 681863232.0000 - rmse: 26112.5117 - val_loss: 355895520.0000 - val_rmse: 18865.1934\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 654110080.0000 - rmse: 25575.5762 - val_loss: 378849792.0000 - val_rmse: 19464.0645\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 644049088.0000 - rmse: 25378.1230 - val_loss: 467467584.0000 - val_rmse: 21620.9980\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 670669248.0000 - rmse: 25897.2832 - val_loss: 348104736.0000 - val_rmse: 18657.5645\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 593111296.0000 - rmse: 24353.8770 - val_loss: 352568032.0000 - val_rmse: 18776.7949\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 677614400.0000 - rmse: 26031.0273 - val_loss: 361964128.0000 - val_rmse: 19025.3555\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 616332736.0000 - rmse: 24826.0488 - val_loss: 340254880.0000 - val_rmse: 18446.0000\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 692701120.0000 - rmse: 26319.2148 - val_loss: 399486272.0000 - val_rmse: 19987.1523\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 654790336.0000 - rmse: 25588.8711 - val_loss: 338096224.0000 - val_rmse: 18387.3926\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 623819328.0000 - rmse: 24976.3750 - val_loss: 332657856.0000 - val_rmse: 18238.9102\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 625165440.0000 - rmse: 25003.3086 - val_loss: 331316160.0000 - val_rmse: 18202.0918\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 577964864.0000 - rmse: 24040.9004 - val_loss: 388231680.0000 - val_rmse: 19703.5957\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 642488768.0000 - rmse: 25347.3613 - val_loss: 347221504.0000 - val_rmse: 18633.8809\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 687778816.0000 - rmse: 26225.5371 - val_loss: 341096960.0000 - val_rmse: 18468.8105\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 611356416.0000 - rmse: 24725.6230 - val_loss: 343402848.0000 - val_rmse: 18531.1328\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 571784512.0000 - rmse: 23912.0156 - val_loss: 345778048.0000 - val_rmse: 18595.1074\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 649422656.0000 - rmse: 25483.7715 - val_loss: 344423840.0000 - val_rmse: 18558.6602\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 561027520.0000 - rmse: 23686.0195 - val_loss: 454115008.0000 - val_rmse: 21309.9746\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 669388480.0000 - rmse: 25872.5430 - val_loss: 434798272.0000 - val_rmse: 20851.8164\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 605631232.0000 - rmse: 24609.5762 - val_loss: 367049024.0000 - val_rmse: 19158.5234\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 552781888.0000 - rmse: 23511.3145 - val_loss: 377343040.0000 - val_rmse: 19425.3203\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 612002816.0000 - rmse: 24738.6914 - val_loss: 373270144.0000 - val_rmse: 19320.2012\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 591979712.0000 - rmse: 24330.6328 - val_loss: 377561760.0000 - val_rmse: 19430.9492\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 568087616.0000 - rmse: 23834.5879 - val_loss: 351109664.0000 - val_rmse: 18737.9199\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 547817920.0000 - rmse: 23405.5098 - val_loss: 387952000.0000 - val_rmse: 19696.4980\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 608892928.0000 - rmse: 24675.7559 - val_loss: 404656256.0000 - val_rmse: 20116.0703\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 559282176.0000 - rmse: 23649.1484 - val_loss: 476640128.0000 - val_rmse: 21832.0898\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 575466496.0000 - rmse: 23988.8828 - val_loss: 352486560.0000 - val_rmse: 18774.6250\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 617605440.0000 - rmse: 24851.6680 - val_loss: 409396160.0000 - val_rmse: 20233.5410\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 613453952.0000 - rmse: 24768.0020 - val_loss: 350921920.0000 - val_rmse: 18732.9102\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 598688704.0000 - rmse: 24468.1152 - val_loss: 337842560.0000 - val_rmse: 18380.4941\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 576229184.0000 - rmse: 24004.7734 - val_loss: 352139648.0000 - val_rmse: 18765.3848\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 536855264.0000 - rmse: 23170.1367 - val_loss: 365268768.0000 - val_rmse: 19112.0059\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 567574080.0000 - rmse: 23823.8125 - val_loss: 360771456.0000 - val_rmse: 18993.9844\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 629253888.0000 - rmse: 25084.9336 - val_loss: 354583072.0000 - val_rmse: 18830.3770\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 576939008.0000 - rmse: 24019.5547 - val_loss: 364773088.0000 - val_rmse: 19099.0332\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 605489920.0000 - rmse: 24606.7051 - val_loss: 388717792.0000 - val_rmse: 19715.9277\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 517189568.0000 - rmse: 22741.8027 - val_loss: 407847712.0000 - val_rmse: 20195.2402\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 565138880.0000 - rmse: 23772.6504 - val_loss: 373514560.0000 - val_rmse: 19326.5254\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 553645888.0000 - rmse: 23529.6816 - val_loss: 384466016.0000 - val_rmse: 19607.8047\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 540582784.0000 - rmse: 23250.4355 - val_loss: 377681760.0000 - val_rmse: 19434.0352\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 547390976.0000 - rmse: 23396.3887 - val_loss: 357388544.0000 - val_rmse: 18904.7227\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 516532576.0000 - rmse: 22727.3535 - val_loss: 339298240.0000 - val_rmse: 18420.0508\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 533807424.0000 - rmse: 23104.2734 - val_loss: 452884960.0000 - val_rmse: 21281.0938\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 555388480.0000 - rmse: 23566.6816 - val_loss: 342872096.0000 - val_rmse: 18516.8066\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 512387008.0000 - rmse: 22635.9668 - val_loss: 364461248.0000 - val_rmse: 19090.8691\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 516524224.0000 - rmse: 22727.1699 - val_loss: 359002112.0000 - val_rmse: 18947.3516\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 574673152.0000 - rmse: 23972.3418 - val_loss: 586151040.0000 - val_rmse: 24210.5566\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 526955552.0000 - rmse: 22955.5117 - val_loss: 388420064.0000 - val_rmse: 19708.3750\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 538200576.0000 - rmse: 23199.1504 - val_loss: 380058016.0000 - val_rmse: 19495.0762\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 527869760.0000 - rmse: 22975.4160 - val_loss: 331097952.0000 - val_rmse: 18196.0977\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 512444736.0000 - rmse: 22637.2422 - val_loss: 386694880.0000 - val_rmse: 19664.5586\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 475225440.0000 - rmse: 21799.6660 - val_loss: 356946144.0000 - val_rmse: 18893.0176\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 561608192.0000 - rmse: 23698.2734 - val_loss: 350616672.0000 - val_rmse: 18724.7617\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 496140320.0000 - rmse: 22274.2070 - val_loss: 340423616.0000 - val_rmse: 18450.5723\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 520260832.0000 - rmse: 22809.2266 - val_loss: 350240416.0000 - val_rmse: 18714.7109\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 476945280.0000 - rmse: 21839.0762 - val_loss: 346369504.0000 - val_rmse: 18611.0059\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 479192032.0000 - rmse: 21890.4551 - val_loss: 376427616.0000 - val_rmse: 19401.7422\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 496847520.0000 - rmse: 22290.0762 - val_loss: 348526240.0000 - val_rmse: 18668.8574\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 526134208.0000 - rmse: 22937.6152 - val_loss: 345978432.0000 - val_rmse: 18600.4961\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 490053216.0000 - rmse: 22137.1465 - val_loss: 347536320.0000 - val_rmse: 18642.3262\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 496627552.0000 - rmse: 22285.1426 - val_loss: 405211840.0000 - val_rmse: 20129.8750\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 493092864.0000 - rmse: 22205.6953 - val_loss: 341372960.0000 - val_rmse: 18476.2812\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 471679744.0000 - rmse: 21718.1895 - val_loss: 354487072.0000 - val_rmse: 18827.8262\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 466991520.0000 - rmse: 21609.9863 - val_loss: 345298944.0000 - val_rmse: 18582.2207\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 478729472.0000 - rmse: 21879.8867 - val_loss: 344547520.0000 - val_rmse: 18561.9922\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 487685216.0000 - rmse: 22083.5957 - val_loss: 346462688.0000 - val_rmse: 18613.5078\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 470245504.0000 - rmse: 21685.1445 - val_loss: 331043872.0000 - val_rmse: 18194.6113\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 436437120.0000 - rmse: 20891.0781 - val_loss: 366479360.0000 - val_rmse: 19143.6504\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 483514624.0000 - rmse: 21988.9668 - val_loss: 342429664.0000 - val_rmse: 18504.8555\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 454680288.0000 - rmse: 21323.2344 - val_loss: 342373344.0000 - val_rmse: 18503.3340\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 574423488.0000 - rmse: 23967.1328 - val_loss: 351292544.0000 - val_rmse: 18742.7988\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 507153856.0000 - rmse: 22520.0762 - val_loss: 391239712.0000 - val_rmse: 19779.7812\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 498750880.0000 - rmse: 22332.7305 - val_loss: 369091872.0000 - val_rmse: 19211.7637\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 519456128.0000 - rmse: 22791.5801 - val_loss: 345286560.0000 - val_rmse: 18581.8887\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 479073408.0000 - rmse: 21887.7461 - val_loss: 378881280.0000 - val_rmse: 19464.8730\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 466971296.0000 - rmse: 21609.5195 - val_loss: 335144512.0000 - val_rmse: 18306.9531\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 493157408.0000 - rmse: 22207.1484 - val_loss: 371868288.0000 - val_rmse: 19283.8867\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 457525472.0000 - rmse: 21389.8457 - val_loss: 354837568.0000 - val_rmse: 18837.1328\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 571246080.0000 - rmse: 23900.7539 - val_loss: 337193024.0000 - val_rmse: 18362.8164\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 480039008.0000 - rmse: 21909.7930 - val_loss: 336322688.0000 - val_rmse: 18339.1035\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 473536416.0000 - rmse: 21760.8926 - val_loss: 354417472.0000 - val_rmse: 18825.9785\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 482169152.0000 - rmse: 21958.3496 - val_loss: 370718752.0000 - val_rmse: 19254.0586\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 447499072.0000 - rmse: 21154.1738 - val_loss: 401375072.0000 - val_rmse: 20034.3477\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 592562496.0000 - rmse: 24342.6074 - val_loss: 384300192.0000 - val_rmse: 19603.5762\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 426860896.0000 - rmse: 20660.6113 - val_loss: 332681312.0000 - val_rmse: 18239.5527\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 439543520.0000 - rmse: 20965.2930 - val_loss: 328994112.0000 - val_rmse: 18138.1953\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 487497152.0000 - rmse: 22079.3379 - val_loss: 352931616.0000 - val_rmse: 18786.4746\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 421573888.0000 - rmse: 20532.2637 - val_loss: 418339136.0000 - val_rmse: 20453.3398\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 454923296.0000 - rmse: 21328.9316 - val_loss: 355391040.0000 - val_rmse: 18851.8184\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 466497408.0000 - rmse: 21598.5508 - val_loss: 342118912.0000 - val_rmse: 18496.4570\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 455130880.0000 - rmse: 21333.7969 - val_loss: 391735840.0000 - val_rmse: 19792.3184\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 514658688.0000 - rmse: 22686.0898 - val_loss: 429305728.0000 - val_rmse: 20719.6934\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 418948544.0000 - rmse: 20468.2324 - val_loss: 349858336.0000 - val_rmse: 18704.5000\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 462023904.0000 - rmse: 21494.7422 - val_loss: 360273856.0000 - val_rmse: 18980.8809\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 465844224.0000 - rmse: 21583.4238 - val_loss: 347743232.0000 - val_rmse: 18647.8750\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 398933792.0000 - rmse: 19973.3262 - val_loss: 547337984.0000 - val_rmse: 23395.2559\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 482981600.0000 - rmse: 21976.8418 - val_loss: 371385216.0000 - val_rmse: 19271.3574\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 455133792.0000 - rmse: 21333.8652 - val_loss: 348331424.0000 - val_rmse: 18663.6387\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 400129856.0000 - rmse: 20003.2461 - val_loss: 371071424.0000 - val_rmse: 19263.2148\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 446483840.0000 - rmse: 21130.1641 - val_loss: 393617216.0000 - val_rmse: 19839.7891\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 438870944.0000 - rmse: 20949.2461 - val_loss: 360412864.0000 - val_rmse: 18984.5430\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 428129600.0000 - rmse: 20691.2930 - val_loss: 348232736.0000 - val_rmse: 18660.9941\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 426939936.0000 - rmse: 20662.5254 - val_loss: 406273472.0000 - val_rmse: 20156.2266\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 403833376.0000 - rmse: 20095.6055 - val_loss: 367502400.0000 - val_rmse: 19170.3516\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 420378464.0000 - rmse: 20503.1328 - val_loss: 367889600.0000 - val_rmse: 19180.4492\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 479265024.0000 - rmse: 21892.1230 - val_loss: 424852192.0000 - val_rmse: 20611.9434\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 414847680.0000 - rmse: 20367.8105 - val_loss: 363239872.0000 - val_rmse: 19058.8535\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 449104864.0000 - rmse: 21192.0938 - val_loss: 362965632.0000 - val_rmse: 19051.6562\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 417919360.0000 - rmse: 20443.0762 - val_loss: 364166816.0000 - val_rmse: 19083.1562\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 376226944.0000 - rmse: 19396.5703 - val_loss: 360969504.0000 - val_rmse: 18999.1973\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 470552736.0000 - rmse: 21692.2285 - val_loss: 416069536.0000 - val_rmse: 20397.7832\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 426124000.0000 - rmse: 20642.7715 - val_loss: 392783456.0000 - val_rmse: 19818.7656\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 401907328.0000 - rmse: 20047.6270 - val_loss: 348735968.0000 - val_rmse: 18674.4746\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 409806400.0000 - rmse: 20243.6758 - val_loss: 402905696.0000 - val_rmse: 20072.5117\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 427004288.0000 - rmse: 20664.0820 - val_loss: 373821376.0000 - val_rmse: 19334.4609\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 409161472.0000 - rmse: 20227.7402 - val_loss: 359732480.0000 - val_rmse: 18966.6152\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 439929184.0000 - rmse: 20974.4883 - val_loss: 363878592.0000 - val_rmse: 19075.6016\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 379440800.0000 - rmse: 19479.2402 - val_loss: 346575904.0000 - val_rmse: 18616.5488\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 377802656.0000 - rmse: 19437.1465 - val_loss: 359243136.0000 - val_rmse: 18953.7109\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 428292736.0000 - rmse: 20695.2344 - val_loss: 350457664.0000 - val_rmse: 18720.5137\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 390273440.0000 - rmse: 19755.3398 - val_loss: 377680384.0000 - val_rmse: 19434.0000\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 467488192.0000 - rmse: 21621.4746 - val_loss: 715354176.0000 - val_rmse: 26746.1055\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 407621184.0000 - rmse: 20189.6309 - val_loss: 355328416.0000 - val_rmse: 18850.1562\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 400166944.0000 - rmse: 20004.1738 - val_loss: 377776352.0000 - val_rmse: 19436.4688\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 358955936.0000 - rmse: 18946.1328 - val_loss: 374537056.0000 - val_rmse: 19352.9590\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 428549920.0000 - rmse: 20701.4473 - val_loss: 343050816.0000 - val_rmse: 18521.6309\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 378628256.0000 - rmse: 19458.3730 - val_loss: 382435040.0000 - val_rmse: 19555.9473\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 390942176.0000 - rmse: 19772.2578 - val_loss: 341291776.0000 - val_rmse: 18474.0840\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 421419552.0000 - rmse: 20528.5059 - val_loss: 376701088.0000 - val_rmse: 19408.7891\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 478146368.0000 - rmse: 21866.5586 - val_loss: 463828800.0000 - val_rmse: 21536.6855\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 419091296.0000 - rmse: 20471.7188 - val_loss: 353078336.0000 - val_rmse: 18790.3789\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 380843840.0000 - rmse: 19515.2207 - val_loss: 359333216.0000 - val_rmse: 18956.0859\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 383922272.0000 - rmse: 19593.9355 - val_loss: 388120320.0000 - val_rmse: 19700.7695\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 407521056.0000 - rmse: 20187.1504 - val_loss: 391710816.0000 - val_rmse: 19791.6855\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 370598176.0000 - rmse: 19250.9258 - val_loss: 335322816.0000 - val_rmse: 18311.8223\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 404339680.0000 - rmse: 20108.1992 - val_loss: 478801088.0000 - val_rmse: 21881.5234\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 413420256.0000 - rmse: 20332.7383 - val_loss: 351306368.0000 - val_rmse: 18743.1680\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 355857728.0000 - rmse: 18864.1914 - val_loss: 330611808.0000 - val_rmse: 18182.7344\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 351684320.0000 - rmse: 18753.2480 - val_loss: 341269120.0000 - val_rmse: 18473.4707\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 389758400.0000 - rmse: 19742.2988 - val_loss: 339424224.0000 - val_rmse: 18423.4688\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 349883520.0000 - rmse: 18705.1738 - val_loss: 401134464.0000 - val_rmse: 20028.3418\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 384893504.0000 - rmse: 19618.7031 - val_loss: 381102688.0000 - val_rmse: 19521.8516\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 392682368.0000 - rmse: 19816.2148 - val_loss: 363293376.0000 - val_rmse: 19060.2559\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 393204928.0000 - rmse: 19829.3965 - val_loss: 370470848.0000 - val_rmse: 19247.6191\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 352452096.0000 - rmse: 18773.7070 - val_loss: 356989120.0000 - val_rmse: 18894.1562\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 351026080.0000 - rmse: 18735.6895 - val_loss: 347107872.0000 - val_rmse: 18630.8320\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 387206528.0000 - rmse: 19677.5645 - val_loss: 436942400.0000 - val_rmse: 20903.1680\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 393666016.0000 - rmse: 19841.0176 - val_loss: 405579680.0000 - val_rmse: 20139.0098\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 395670880.0000 - rmse: 19891.4785 - val_loss: 359731680.0000 - val_rmse: 18966.5938\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 340605888.0000 - rmse: 18455.5117 - val_loss: 327202688.0000 - val_rmse: 18088.7441\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 328838400.0000 - rmse: 18133.9023 - val_loss: 346389344.0000 - val_rmse: 18611.5371\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 356424832.0000 - rmse: 18879.2168 - val_loss: 355365376.0000 - val_rmse: 18851.1367\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 385316832.0000 - rmse: 19629.4883 - val_loss: 380121376.0000 - val_rmse: 19496.7012\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 342049728.0000 - rmse: 18494.5859 - val_loss: 424425696.0000 - val_rmse: 20601.5938\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 368911392.0000 - rmse: 19207.0664 - val_loss: 344342592.0000 - val_rmse: 18556.4707\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 397231264.0000 - rmse: 19930.6621 - val_loss: 346706976.0000 - val_rmse: 18620.0684\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 385444832.0000 - rmse: 19632.7480 - val_loss: 330020256.0000 - val_rmse: 18166.4590\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 315582848.0000 - rmse: 17764.6523 - val_loss: 330252416.0000 - val_rmse: 18172.8477\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 357240512.0000 - rmse: 18900.8066 - val_loss: 368493888.0000 - val_rmse: 19196.1953\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 392799296.0000 - rmse: 19819.1641 - val_loss: 339382912.0000 - val_rmse: 18422.3477\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 371481568.0000 - rmse: 19273.8574 - val_loss: 352732160.0000 - val_rmse: 18781.1641\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 382737952.0000 - rmse: 19563.6895 - val_loss: 388228544.0000 - val_rmse: 19703.5156\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 366489472.0000 - rmse: 19143.9141 - val_loss: 337919456.0000 - val_rmse: 18382.5859\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 328532416.0000 - rmse: 18125.4629 - val_loss: 385021568.0000 - val_rmse: 19621.9668\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 315675712.0000 - rmse: 17767.2656 - val_loss: 371135616.0000 - val_rmse: 19264.8809\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 349380320.0000 - rmse: 18691.7188 - val_loss: 451138624.0000 - val_rmse: 21240.0234\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 347883840.0000 - rmse: 18651.6445 - val_loss: 335219392.0000 - val_rmse: 18308.9980\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 369836256.0000 - rmse: 19231.1270 - val_loss: 352703840.0000 - val_rmse: 18780.4102\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 329033824.0000 - rmse: 18139.2891 - val_loss: 335983168.0000 - val_rmse: 18329.8438\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 308941952.0000 - rmse: 17576.7441 - val_loss: 372908736.0000 - val_rmse: 19310.8457\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 337929312.0000 - rmse: 18382.8535 - val_loss: 339852896.0000 - val_rmse: 18435.0996\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 334672224.0000 - rmse: 18294.0488 - val_loss: 358698368.0000 - val_rmse: 18939.3340\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 383428352.0000 - rmse: 19581.3262 - val_loss: 316829952.0000 - val_rmse: 17799.7168\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 349171040.0000 - rmse: 18686.1191 - val_loss: 343480416.0000 - val_rmse: 18533.2246\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 335255584.0000 - rmse: 18309.9863 - val_loss: 417927552.0000 - val_rmse: 20443.2773\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 403825856.0000 - rmse: 20095.4180 - val_loss: 356517088.0000 - val_rmse: 18881.6602\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 364449408.0000 - rmse: 19090.5586 - val_loss: 347267904.0000 - val_rmse: 18635.1250\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 381013312.0000 - rmse: 19519.5625 - val_loss: 368151168.0000 - val_rmse: 19187.2656\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 288454144.0000 - rmse: 16983.9375 - val_loss: 340178720.0000 - val_rmse: 18443.9336\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 303527840.0000 - rmse: 17422.0508 - val_loss: 420980320.0000 - val_rmse: 20517.8047\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 345687936.0000 - rmse: 18592.6855 - val_loss: 396246400.0000 - val_rmse: 19905.9395\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 334111168.0000 - rmse: 18278.7090 - val_loss: 356727648.0000 - val_rmse: 18887.2344\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 330289696.0000 - rmse: 18173.8730 - val_loss: 339128832.0000 - val_rmse: 18415.4512\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 328832128.0000 - rmse: 18133.7285 - val_loss: 350762880.0000 - val_rmse: 18728.6641\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 344671776.0000 - rmse: 18565.3379 - val_loss: 427864928.0000 - val_rmse: 20684.8965\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 293194784.0000 - rmse: 17122.9316 - val_loss: 381813856.0000 - val_rmse: 19540.0586\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 327242976.0000 - rmse: 18089.8574 - val_loss: 341948928.0000 - val_rmse: 18491.8613\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 322876640.0000 - rmse: 17968.7676 - val_loss: 343801120.0000 - val_rmse: 18541.8750\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 321883040.0000 - rmse: 17941.0996 - val_loss: 344669472.0000 - val_rmse: 18565.2754\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 316672960.0000 - rmse: 17795.3066 - val_loss: 382778240.0000 - val_rmse: 19564.7188\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309112448.0000 - rmse: 17581.5938 - val_loss: 355738656.0000 - val_rmse: 18861.0352\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 373698560.0000 - rmse: 19331.2852 - val_loss: 363256960.0000 - val_rmse: 19059.3008\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 361893024.0000 - rmse: 19023.4863 - val_loss: 354998432.0000 - val_rmse: 18841.4023\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 372546048.0000 - rmse: 19301.4512 - val_loss: 379811520.0000 - val_rmse: 19488.7539\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299698016.0000 - rmse: 17311.7891 - val_loss: 363837248.0000 - val_rmse: 19074.5176\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 294252032.0000 - rmse: 17153.7754 - val_loss: 340706272.0000 - val_rmse: 18458.2305\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 304667968.0000 - rmse: 17454.7402 - val_loss: 369226112.0000 - val_rmse: 19215.2578\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286521632.0000 - rmse: 16926.9492 - val_loss: 363756064.0000 - val_rmse: 19072.3906\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 276479808.0000 - rmse: 16627.6816 - val_loss: 353757536.0000 - val_rmse: 18808.4434\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 285309312.0000 - rmse: 16891.1016 - val_loss: 339262976.0000 - val_rmse: 18419.0918\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 294782848.0000 - rmse: 17169.2422 - val_loss: 348849376.0000 - val_rmse: 18677.5098\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 324780128.0000 - rmse: 18021.6562 - val_loss: 366970656.0000 - val_rmse: 19156.4785\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 298820096.0000 - rmse: 17286.4141 - val_loss: 382534592.0000 - val_rmse: 19558.4922\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 380660160.0000 - rmse: 19510.5137 - val_loss: 389532544.0000 - val_rmse: 19736.5781\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 304955712.0000 - rmse: 17462.9805 - val_loss: 383871168.0000 - val_rmse: 19592.6309\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 278017408.0000 - rmse: 16673.8535 - val_loss: 372588416.0000 - val_rmse: 19302.5488\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 288191296.0000 - rmse: 16976.1973 - val_loss: 387653280.0000 - val_rmse: 19688.9121\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 337951072.0000 - rmse: 18383.4453 - val_loss: 397655008.0000 - val_rmse: 19941.2891\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 296345376.0000 - rmse: 17214.6855 - val_loss: 385640512.0000 - val_rmse: 19637.7324\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 308532416.0000 - rmse: 17565.0918 - val_loss: 376614720.0000 - val_rmse: 19406.5645\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 323140448.0000 - rmse: 17976.1074 - val_loss: 380223200.0000 - val_rmse: 19499.3125\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 291619040.0000 - rmse: 17076.8574 - val_loss: 375684000.0000 - val_rmse: 19382.5703\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 270434048.0000 - rmse: 16444.8789 - val_loss: 340750304.0000 - val_rmse: 18459.4238\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 261715936.0000 - rmse: 16177.6367 - val_loss: 350341632.0000 - val_rmse: 18717.4160\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309529920.0000 - rmse: 17593.4629 - val_loss: 326397088.0000 - val_rmse: 18066.4629\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 318435168.0000 - rmse: 17844.7520 - val_loss: 362706784.0000 - val_rmse: 19044.8633\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309694112.0000 - rmse: 17598.1289 - val_loss: 361924704.0000 - val_rmse: 19024.3184\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 281464608.0000 - rmse: 16776.9062 - val_loss: 370549792.0000 - val_rmse: 19249.6699\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 304470592.0000 - rmse: 17449.0859 - val_loss: 391971392.0000 - val_rmse: 19798.2676\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 296949152.0000 - rmse: 17232.2129 - val_loss: 394077344.0000 - val_rmse: 19851.3809\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 275783328.0000 - rmse: 16606.7246 - val_loss: 340992832.0000 - val_rmse: 18465.9922\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 275725984.0000 - rmse: 16604.9980 - val_loss: 336021312.0000 - val_rmse: 18330.8848\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286770272.0000 - rmse: 16934.2930 - val_loss: 370015552.0000 - val_rmse: 19235.7891\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 258696864.0000 - rmse: 16084.0566 - val_loss: 409274912.0000 - val_rmse: 20230.5449\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 276756000.0000 - rmse: 16635.9844 - val_loss: 340159232.0000 - val_rmse: 18443.4062\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 310199136.0000 - rmse: 17612.4707 - val_loss: 396786016.0000 - val_rmse: 19919.4883\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 273664192.0000 - rmse: 16542.7988 - val_loss: 364006528.0000 - val_rmse: 19078.9551\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 275403424.0000 - rmse: 16595.2832 - val_loss: 408063584.0000 - val_rmse: 20200.5840\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 275609024.0000 - rmse: 16601.4766 - val_loss: 400946496.0000 - val_rmse: 20023.6484\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 265155104.0000 - rmse: 16283.5840 - val_loss: 373374400.0000 - val_rmse: 19322.8984\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 301064288.0000 - rmse: 17351.2051 - val_loss: 332364544.0000 - val_rmse: 18230.8672\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 277043776.0000 - rmse: 16644.6328 - val_loss: 369002528.0000 - val_rmse: 19209.4395\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 276584416.0000 - rmse: 16630.8281 - val_loss: 360891232.0000 - val_rmse: 18997.1367\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280129728.0000 - rmse: 16737.0762 - val_loss: 373128096.0000 - val_rmse: 19316.5234\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 254254944.0000 - rmse: 15945.3740 - val_loss: 337660160.0000 - val_rmse: 18375.5312\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 269736704.0000 - rmse: 16423.6621 - val_loss: 357442784.0000 - val_rmse: 18906.1582\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 291544320.0000 - rmse: 17074.6699 - val_loss: 336284032.0000 - val_rmse: 18338.0488\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 279238272.0000 - rmse: 16710.4238 - val_loss: 391589216.0000 - val_rmse: 19788.6133\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 259513472.0000 - rmse: 16109.4219 - val_loss: 336648160.0000 - val_rmse: 18347.9746\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 256136640.0000 - rmse: 16004.2695 - val_loss: 367932992.0000 - val_rmse: 19181.5801\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 272849824.0000 - rmse: 16518.1660 - val_loss: 391960896.0000 - val_rmse: 19798.0020\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309469280.0000 - rmse: 17591.7383 - val_loss: 419658272.0000 - val_rmse: 20485.5625\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 257252576.0000 - rmse: 16039.0957 - val_loss: 368294400.0000 - val_rmse: 19190.9980\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 313324768.0000 - rmse: 17700.9824 - val_loss: 400038048.0000 - val_rmse: 20000.9512\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 257194272.0000 - rmse: 16037.2773 - val_loss: 370918592.0000 - val_rmse: 19259.2461\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 254773520.0000 - rmse: 15961.6270 - val_loss: 351127840.0000 - val_rmse: 18738.4062\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260338256.0000 - rmse: 16135.0010 - val_loss: 362138272.0000 - val_rmse: 19029.9316\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 270348672.0000 - rmse: 16442.2832 - val_loss: 366992896.0000 - val_rmse: 19157.0586\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 239982704.0000 - rmse: 15491.3750 - val_loss: 376171392.0000 - val_rmse: 19395.1387\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 244979024.0000 - rmse: 15651.8057 - val_loss: 363266464.0000 - val_rmse: 19059.5508\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 244468000.0000 - rmse: 15635.4727 - val_loss: 366159200.0000 - val_rmse: 19135.2871\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 282298208.0000 - rmse: 16801.7324 - val_loss: 369875744.0000 - val_rmse: 19232.1543\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 272950048.0000 - rmse: 16521.1992 - val_loss: 373189248.0000 - val_rmse: 19318.1074\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264036368.0000 - rmse: 16249.1963 - val_loss: 398931168.0000 - val_rmse: 19973.2617\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 238673520.0000 - rmse: 15449.0625 - val_loss: 351085792.0000 - val_rmse: 18737.2832\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 249822592.0000 - rmse: 15805.7773 - val_loss: 359804256.0000 - val_rmse: 18968.5078\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 250852720.0000 - rmse: 15838.3311 - val_loss: 355827232.0000 - val_rmse: 18863.3828\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 251221040.0000 - rmse: 15849.9541 - val_loss: 357389664.0000 - val_rmse: 18904.7520\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 254497728.0000 - rmse: 15952.9854 - val_loss: 414376640.0000 - val_rmse: 20356.2441\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280914176.0000 - rmse: 16760.4941 - val_loss: 370970144.0000 - val_rmse: 19260.5859\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 275197472.0000 - rmse: 16589.0762 - val_loss: 385105984.0000 - val_rmse: 19624.1172\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 256298208.0000 - rmse: 16009.3164 - val_loss: 427224608.0000 - val_rmse: 20669.4121\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 240235664.0000 - rmse: 15499.5371 - val_loss: 367567168.0000 - val_rmse: 19172.0410\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 240801456.0000 - rmse: 15517.7783 - val_loss: 368422080.0000 - val_rmse: 19194.3242\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260302048.0000 - rmse: 16133.8789 - val_loss: 400451552.0000 - val_rmse: 20011.2852\n",
      "Epoch 367/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 293724928.0000 - rmse: 17138.4043 - val_loss: 331589248.0000 - val_rmse: 18209.5918\n",
      "Epoch 368/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 252493936.0000 - rmse: 15890.0576 - val_loss: 371509952.0000 - val_rmse: 19274.5938\n",
      "Epoch 369/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 255500784.0000 - rmse: 15984.3916 - val_loss: 387069312.0000 - val_rmse: 19674.0781\n",
      "Epoch 370/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 251752592.0000 - rmse: 15866.7129 - val_loss: 359336000.0000 - val_rmse: 18956.1602\n",
      "Epoch 371/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 241184304.0000 - rmse: 15530.1094 - val_loss: 366989024.0000 - val_rmse: 19156.9570\n",
      "Epoch 372/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 251160528.0000 - rmse: 15848.0449 - val_loss: 347618400.0000 - val_rmse: 18644.5273\n",
      "Epoch 373/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 236007680.0000 - rmse: 15362.5410 - val_loss: 368032768.0000 - val_rmse: 19184.1797\n",
      "Epoch 374/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 224052400.0000 - rmse: 14968.3799 - val_loss: 342904160.0000 - val_rmse: 18517.6719\n",
      "Epoch 375/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 228678464.0000 - rmse: 15122.1182 - val_loss: 367829056.0000 - val_rmse: 19178.8691\n",
      "Epoch 376/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280370720.0000 - rmse: 16744.2734 - val_loss: 371799968.0000 - val_rmse: 19282.1152\n",
      "Epoch 377/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 270861920.0000 - rmse: 16457.8828 - val_loss: 387645312.0000 - val_rmse: 19688.7109\n",
      "Epoch 378/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 243612096.0000 - rmse: 15608.0781 - val_loss: 352494016.0000 - val_rmse: 18774.8242\n",
      "Epoch 379/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 245585424.0000 - rmse: 15671.1650 - val_loss: 416395840.0000 - val_rmse: 20405.7793\n",
      "Epoch 380/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 232557184.0000 - rmse: 15249.8262 - val_loss: 349816640.0000 - val_rmse: 18703.3867\n",
      "Epoch 381/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 257605392.0000 - rmse: 16050.0898 - val_loss: 389938464.0000 - val_rmse: 19746.8594\n",
      "Epoch 382/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260375856.0000 - rmse: 16136.1660 - val_loss: 367979776.0000 - val_rmse: 19182.7988\n",
      "Epoch 383/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260009504.0000 - rmse: 16124.8105 - val_loss: 333677088.0000 - val_rmse: 18266.8301\n",
      "Epoch 384/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 242515568.0000 - rmse: 15572.9111 - val_loss: 343918048.0000 - val_rmse: 18545.0273\n",
      "Epoch 385/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268980864.0000 - rmse: 16400.6367 - val_loss: 387762336.0000 - val_rmse: 19691.6816\n",
      "Epoch 386/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 277717728.0000 - rmse: 16664.8652 - val_loss: 356812160.0000 - val_rmse: 18889.4727\n",
      "Epoch 387/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 283343104.0000 - rmse: 16832.7988 - val_loss: 404533664.0000 - val_rmse: 20113.0215\n",
      "Epoch 388/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 270084064.0000 - rmse: 16434.2344 - val_loss: 361348160.0000 - val_rmse: 19009.1602\n",
      "Epoch 389/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 234909968.0000 - rmse: 15326.7725 - val_loss: 341228512.0000 - val_rmse: 18472.3711\n",
      "Epoch 390/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 233371200.0000 - rmse: 15276.4922 - val_loss: 333007232.0000 - val_rmse: 18248.4863\n",
      "Epoch 391/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 232402336.0000 - rmse: 15244.7480 - val_loss: 336349472.0000 - val_rmse: 18339.8320\n",
      "Epoch 392/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 251177168.0000 - rmse: 15848.5703 - val_loss: 363743136.0000 - val_rmse: 19072.0508\n",
      "Epoch 393/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268138080.0000 - rmse: 16374.9219 - val_loss: 355683008.0000 - val_rmse: 18859.5605\n",
      "Epoch 394/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 255505456.0000 - rmse: 15984.5381 - val_loss: 332805312.0000 - val_rmse: 18242.9531\n",
      "Epoch 395/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 228548480.0000 - rmse: 15117.8203 - val_loss: 383130624.0000 - val_rmse: 19573.7227\n",
      "Epoch 396/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 239285392.0000 - rmse: 15468.8525 - val_loss: 344514720.0000 - val_rmse: 18561.1074\n",
      "Epoch 397/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 247530160.0000 - rmse: 15733.0908 - val_loss: 303380128.0000 - val_rmse: 17417.8105\n",
      "Epoch 398/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 237695424.0000 - rmse: 15417.3740 - val_loss: 333244896.0000 - val_rmse: 18254.9961\n",
      "Epoch 399/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 245535136.0000 - rmse: 15669.5605 - val_loss: 373708352.0000 - val_rmse: 19331.5371\n",
      "Epoch 400/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 248831104.0000 - rmse: 15774.3809 - val_loss: 352353536.0000 - val_rmse: 18771.0820\n",
      "104/104 [==============================] - 0s 851us/step - loss: 378631232.0000 - rmse: 19458.4492\n",
      "[378631232.0, 19458.44921875]\n",
      "<src.model.emb_model object at 0x7f846770d650>\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 20018059264.0000 - rmse: 141485.1875 - val_loss: 2780025344.0000 - val_rmse: 52725.9453\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2532239616.0000 - rmse: 50321.3633 - val_loss: 1446901376.0000 - val_rmse: 38038.1562\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1789557888.0000 - rmse: 42303.1680 - val_loss: 1322458880.0000 - val_rmse: 36365.6289\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1778725248.0000 - rmse: 42174.9375 - val_loss: 1280145280.0000 - val_rmse: 35779.1172\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1630091776.0000 - rmse: 40374.3945 - val_loss: 1266680448.0000 - val_rmse: 35590.4531\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1643917312.0000 - rmse: 40545.2500 - val_loss: 1231878784.0000 - val_rmse: 35098.1289\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1551894656.0000 - rmse: 39394.0938 - val_loss: 1201922816.0000 - val_rmse: 34668.7578\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1594030464.0000 - rmse: 39925.3125 - val_loss: 1210429184.0000 - val_rmse: 34791.2227\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1562743808.0000 - rmse: 39531.5547 - val_loss: 1139835648.0000 - val_rmse: 33761.4531\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1499862272.0000 - rmse: 38728.0547 - val_loss: 1144389632.0000 - val_rmse: 33828.8281\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1490424704.0000 - rmse: 38606.0195 - val_loss: 1073654848.0000 - val_rmse: 32766.6719\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1385897600.0000 - rmse: 37227.6445 - val_loss: 1063664256.0000 - val_rmse: 32613.8652\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1381999488.0000 - rmse: 37175.2539 - val_loss: 1140645376.0000 - val_rmse: 33773.4414\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1499097472.0000 - rmse: 38718.1797 - val_loss: 978890304.0000 - val_rmse: 31287.2227\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1373088384.0000 - rmse: 37055.2070 - val_loss: 1032050880.0000 - val_rmse: 32125.5488\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1303763456.0000 - rmse: 36107.6641 - val_loss: 939447488.0000 - val_rmse: 30650.4082\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1269163776.0000 - rmse: 35625.3242 - val_loss: 923589376.0000 - val_rmse: 30390.6133\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1299700608.0000 - rmse: 36051.3594 - val_loss: 948270848.0000 - val_rmse: 30794.0059\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1225731456.0000 - rmse: 35010.4492 - val_loss: 961456384.0000 - val_rmse: 31007.3594\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1282370560.0000 - rmse: 35810.2031 - val_loss: 871258624.0000 - val_rmse: 29517.0898\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1160923136.0000 - rmse: 34072.3203 - val_loss: 831835072.0000 - val_rmse: 28841.5508\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1135102464.0000 - rmse: 33691.2812 - val_loss: 813295040.0000 - val_rmse: 28518.3281\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1087337856.0000 - rmse: 32974.8047 - val_loss: 808189824.0000 - val_rmse: 28428.6797\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1091007872.0000 - rmse: 33030.4102 - val_loss: 823638016.0000 - val_rmse: 28699.0938\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1145693568.0000 - rmse: 33848.0938 - val_loss: 819299648.0000 - val_rmse: 28623.4102\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1112528128.0000 - rmse: 33354.5820 - val_loss: 778911360.0000 - val_rmse: 27908.9844\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1090486272.0000 - rmse: 33022.5117 - val_loss: 765385408.0000 - val_rmse: 27665.5996\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1066082880.0000 - rmse: 32650.9238 - val_loss: 720592896.0000 - val_rmse: 26843.8613\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1050237248.0000 - rmse: 32407.3633 - val_loss: 758110848.0000 - val_rmse: 27533.8125\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1085426304.0000 - rmse: 32945.8086 - val_loss: 720904384.0000 - val_rmse: 26849.6621\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1033675584.0000 - rmse: 32150.8262 - val_loss: 739565824.0000 - val_rmse: 27194.9590\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 993237824.0000 - rmse: 31515.6758 - val_loss: 711792896.0000 - val_rmse: 26679.4473\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 973378112.0000 - rmse: 31199.0078 - val_loss: 713498112.0000 - val_rmse: 26711.3848\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 983880832.0000 - rmse: 31366.8750 - val_loss: 758177920.0000 - val_rmse: 27535.0312\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 973451904.0000 - rmse: 31200.1914 - val_loss: 727638208.0000 - val_rmse: 26974.7695\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 957343296.0000 - rmse: 30940.9648 - val_loss: 677622784.0000 - val_rmse: 26031.1895\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 936023488.0000 - rmse: 30594.5000 - val_loss: 673872192.0000 - val_rmse: 25959.0488\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 967687808.0000 - rmse: 31107.6816 - val_loss: 708196672.0000 - val_rmse: 26611.9648\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 921151104.0000 - rmse: 30350.4707 - val_loss: 713236544.0000 - val_rmse: 26706.4883\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 903766848.0000 - rmse: 30062.7148 - val_loss: 672452224.0000 - val_rmse: 25931.6836\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 855050240.0000 - rmse: 29241.2422 - val_loss: 652710336.0000 - val_rmse: 25548.1973\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1042774336.0000 - rmse: 32292.0156 - val_loss: 651197312.0000 - val_rmse: 25518.5684\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 913626560.0000 - rmse: 30226.2559 - val_loss: 659552512.0000 - val_rmse: 25681.7539\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 881173952.0000 - rmse: 29684.5742 - val_loss: 646875840.0000 - val_rmse: 25433.7539\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 931325440.0000 - rmse: 30517.6250 - val_loss: 669200000.0000 - val_rmse: 25868.9004\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 904677440.0000 - rmse: 30077.8555 - val_loss: 647682432.0000 - val_rmse: 25449.6055\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 886284864.0000 - rmse: 29770.5371 - val_loss: 660621248.0000 - val_rmse: 25702.5527\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 902612480.0000 - rmse: 30043.5098 - val_loss: 671688576.0000 - val_rmse: 25916.9551\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 874045568.0000 - rmse: 29564.2617 - val_loss: 699246528.0000 - val_rmse: 26443.2695\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 858149696.0000 - rmse: 29294.1914 - val_loss: 614660800.0000 - val_rmse: 24792.3535\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 774047104.0000 - rmse: 27821.7012 - val_loss: 609113280.0000 - val_rmse: 24680.2207\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 866885440.0000 - rmse: 29442.9180 - val_loss: 740847168.0000 - val_rmse: 27218.5078\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 865269888.0000 - rmse: 29415.4707 - val_loss: 628750080.0000 - val_rmse: 25074.8887\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 831723520.0000 - rmse: 28839.6172 - val_loss: 603580864.0000 - val_rmse: 24567.8828\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 852641344.0000 - rmse: 29200.0234 - val_loss: 609154368.0000 - val_rmse: 24681.0527\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 893433344.0000 - rmse: 29890.3555 - val_loss: 610649536.0000 - val_rmse: 24711.3242\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 876632128.0000 - rmse: 29607.9746 - val_loss: 613887936.0000 - val_rmse: 24776.7617\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 896494272.0000 - rmse: 29941.5137 - val_loss: 615484480.0000 - val_rmse: 24808.9590\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 837664256.0000 - rmse: 28942.4297 - val_loss: 608717760.0000 - val_rmse: 24672.2070\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 849749376.0000 - rmse: 29150.4609 - val_loss: 598294784.0000 - val_rmse: 24460.0645\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 872992384.0000 - rmse: 29546.4453 - val_loss: 591790336.0000 - val_rmse: 24326.7402\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 771164096.0000 - rmse: 27769.8418 - val_loss: 595570816.0000 - val_rmse: 24404.3203\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 804909120.0000 - rmse: 28370.9199 - val_loss: 591850752.0000 - val_rmse: 24327.9824\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 885282304.0000 - rmse: 29753.6934 - val_loss: 614130304.0000 - val_rmse: 24781.6523\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 793672704.0000 - rmse: 28172.1973 - val_loss: 582256384.0000 - val_rmse: 24129.9902\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 808778816.0000 - rmse: 28439.0371 - val_loss: 590210688.0000 - val_rmse: 24294.2520\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 826418752.0000 - rmse: 28747.5000 - val_loss: 570355392.0000 - val_rmse: 23882.1133\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 848938624.0000 - rmse: 29136.5508 - val_loss: 557023168.0000 - val_rmse: 23601.3379\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 774697920.0000 - rmse: 27833.3965 - val_loss: 576210944.0000 - val_rmse: 24004.3945\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 752211328.0000 - rmse: 27426.4707 - val_loss: 563062464.0000 - val_rmse: 23728.9375\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 767558848.0000 - rmse: 27704.8516 - val_loss: 605008192.0000 - val_rmse: 24596.9141\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 797531648.0000 - rmse: 28240.6035 - val_loss: 564851648.0000 - val_rmse: 23766.6074\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 796035200.0000 - rmse: 28214.0957 - val_loss: 616007936.0000 - val_rmse: 24819.5078\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 763345472.0000 - rmse: 27628.7070 - val_loss: 543145472.0000 - val_rmse: 23305.4824\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 849143296.0000 - rmse: 29140.0625 - val_loss: 525493952.0000 - val_rmse: 22923.6543\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 690511168.0000 - rmse: 26277.5801 - val_loss: 534253376.0000 - val_rmse: 23113.9219\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 717750272.0000 - rmse: 26790.8613 - val_loss: 524383936.0000 - val_rmse: 22899.4316\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 780142976.0000 - rmse: 27931.0391 - val_loss: 526366528.0000 - val_rmse: 22942.6797\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 743574528.0000 - rmse: 27268.5625 - val_loss: 533034144.0000 - val_rmse: 23087.5312\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 778759872.0000 - rmse: 27906.2695 - val_loss: 588384000.0000 - val_rmse: 24256.6289\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 756220160.0000 - rmse: 27499.4570 - val_loss: 564639808.0000 - val_rmse: 23762.1504\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 790840960.0000 - rmse: 28121.8945 - val_loss: 566468416.0000 - val_rmse: 23800.5977\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 713958272.0000 - rmse: 26719.9980 - val_loss: 607574784.0000 - val_rmse: 24649.0312\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 724234560.0000 - rmse: 26911.6055 - val_loss: 524845376.0000 - val_rmse: 22909.5039\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 665553088.0000 - rmse: 25798.3164 - val_loss: 501372224.0000 - val_rmse: 22391.3418\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 706948096.0000 - rmse: 26588.4961 - val_loss: 529965280.0000 - val_rmse: 23020.9746\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 737234240.0000 - rmse: 27152.0586 - val_loss: 523380672.0000 - val_rmse: 22877.5137\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 711691456.0000 - rmse: 26677.5469 - val_loss: 499141024.0000 - val_rmse: 22341.4648\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 744207040.0000 - rmse: 27280.1582 - val_loss: 497593728.0000 - val_rmse: 22306.8086\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 724280448.0000 - rmse: 26912.4590 - val_loss: 524451168.0000 - val_rmse: 22900.8984\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 648037504.0000 - rmse: 25456.5801 - val_loss: 506584352.0000 - val_rmse: 22507.4297\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 742259136.0000 - rmse: 27244.4336 - val_loss: 506520064.0000 - val_rmse: 22506.0000\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 696727296.0000 - rmse: 26395.5918 - val_loss: 509251040.0000 - val_rmse: 22566.5918\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 761502784.0000 - rmse: 27595.3398 - val_loss: 482068608.0000 - val_rmse: 21956.0605\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 669990208.0000 - rmse: 25884.1699 - val_loss: 476257088.0000 - val_rmse: 21823.3145\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 643582848.0000 - rmse: 25368.9355 - val_loss: 504998016.0000 - val_rmse: 22472.1602\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 671243328.0000 - rmse: 25908.3633 - val_loss: 486769216.0000 - val_rmse: 22062.8477\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 787356032.0000 - rmse: 28059.8652 - val_loss: 483230528.0000 - val_rmse: 21982.5059\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 688504576.0000 - rmse: 26239.3711 - val_loss: 685278976.0000 - val_rmse: 26177.8340\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 678373696.0000 - rmse: 26045.6074 - val_loss: 495275680.0000 - val_rmse: 22254.7910\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 667120768.0000 - rmse: 25828.6816 - val_loss: 479113440.0000 - val_rmse: 21888.6602\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 687576576.0000 - rmse: 26221.6816 - val_loss: 542748288.0000 - val_rmse: 23296.9590\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 668722048.0000 - rmse: 25859.6602 - val_loss: 475537152.0000 - val_rmse: 21806.8145\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 621703808.0000 - rmse: 24933.9883 - val_loss: 474159648.0000 - val_rmse: 21775.2070\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 588799808.0000 - rmse: 24265.1973 - val_loss: 484067168.0000 - val_rmse: 22001.5273\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 635192704.0000 - rmse: 25203.0293 - val_loss: 466597312.0000 - val_rmse: 21600.8633\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 655334080.0000 - rmse: 25599.4941 - val_loss: 513429120.0000 - val_rmse: 22658.9746\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 726392256.0000 - rmse: 26951.6660 - val_loss: 468588672.0000 - val_rmse: 21646.9082\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 596408768.0000 - rmse: 24421.4824 - val_loss: 537011264.0000 - val_rmse: 23173.5039\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 657502912.0000 - rmse: 25641.8203 - val_loss: 479118752.0000 - val_rmse: 21888.7812\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 617869824.0000 - rmse: 24856.9883 - val_loss: 452500992.0000 - val_rmse: 21272.0703\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 681156480.0000 - rmse: 26098.9746 - val_loss: 456615360.0000 - val_rmse: 21368.5605\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 582606912.0000 - rmse: 24137.2520 - val_loss: 443760224.0000 - val_rmse: 21065.6172\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 641969920.0000 - rmse: 25337.1250 - val_loss: 467647264.0000 - val_rmse: 21625.1543\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 609005568.0000 - rmse: 24678.0391 - val_loss: 480424928.0000 - val_rmse: 21918.5977\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 632544832.0000 - rmse: 25150.4434 - val_loss: 447092256.0000 - val_rmse: 21144.5566\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 637644736.0000 - rmse: 25251.6289 - val_loss: 472359040.0000 - val_rmse: 21733.8223\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 581948096.0000 - rmse: 24123.5996 - val_loss: 460260384.0000 - val_rmse: 21453.6797\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 628004416.0000 - rmse: 25060.0156 - val_loss: 510344640.0000 - val_rmse: 22590.8086\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 653559552.0000 - rmse: 25564.8105 - val_loss: 432030240.0000 - val_rmse: 20785.3379\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 630750080.0000 - rmse: 25114.7383 - val_loss: 444111488.0000 - val_rmse: 21073.9531\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 584810432.0000 - rmse: 24182.8535 - val_loss: 470119904.0000 - val_rmse: 21682.2480\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 626099968.0000 - rmse: 25021.9902 - val_loss: 434291456.0000 - val_rmse: 20839.6602\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 569298816.0000 - rmse: 23859.9844 - val_loss: 445398368.0000 - val_rmse: 21104.4629\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 585889792.0000 - rmse: 24205.1602 - val_loss: 447133088.0000 - val_rmse: 21145.5215\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 622717184.0000 - rmse: 24954.3027 - val_loss: 460573472.0000 - val_rmse: 21460.9746\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 539141568.0000 - rmse: 23219.4219 - val_loss: 446664992.0000 - val_rmse: 21134.4512\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 605511680.0000 - rmse: 24607.1465 - val_loss: 427455072.0000 - val_rmse: 20674.9863\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 592634816.0000 - rmse: 24344.0918 - val_loss: 437244928.0000 - val_rmse: 20910.4023\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 630551936.0000 - rmse: 25110.7930 - val_loss: 443778688.0000 - val_rmse: 21066.0547\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 574655808.0000 - rmse: 23971.9805 - val_loss: 451634336.0000 - val_rmse: 21251.6895\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 567983104.0000 - rmse: 23832.3965 - val_loss: 448342400.0000 - val_rmse: 21174.0977\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 603881024.0000 - rmse: 24573.9902 - val_loss: 446265056.0000 - val_rmse: 21124.9863\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 597266560.0000 - rmse: 24439.0371 - val_loss: 442724000.0000 - val_rmse: 21041.0078\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 575208640.0000 - rmse: 23983.5078 - val_loss: 456241184.0000 - val_rmse: 21359.8027\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 602072320.0000 - rmse: 24537.1621 - val_loss: 456137280.0000 - val_rmse: 21357.3711\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 560328128.0000 - rmse: 23671.2520 - val_loss: 431040768.0000 - val_rmse: 20761.5215\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 555855232.0000 - rmse: 23576.5820 - val_loss: 415739008.0000 - val_rmse: 20389.6797\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 553810048.0000 - rmse: 23533.1699 - val_loss: 409095712.0000 - val_rmse: 20226.1152\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 537260672.0000 - rmse: 23178.8848 - val_loss: 402512608.0000 - val_rmse: 20062.7168\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 551077568.0000 - rmse: 23475.0410 - val_loss: 448677312.0000 - val_rmse: 21182.0039\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 539093824.0000 - rmse: 23218.3945 - val_loss: 401518112.0000 - val_rmse: 20037.9160\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 601839680.0000 - rmse: 24532.4219 - val_loss: 424906496.0000 - val_rmse: 20613.2598\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 526816800.0000 - rmse: 22952.4902 - val_loss: 416987424.0000 - val_rmse: 20420.2695\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 577493632.0000 - rmse: 24031.0977 - val_loss: 476524576.0000 - val_rmse: 21829.4434\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 566923904.0000 - rmse: 23810.1641 - val_loss: 410286720.0000 - val_rmse: 20255.5352\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 556478720.0000 - rmse: 23589.8008 - val_loss: 465300704.0000 - val_rmse: 21570.8301\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 561452672.0000 - rmse: 23694.9922 - val_loss: 412647840.0000 - val_rmse: 20313.7344\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 521005728.0000 - rmse: 22825.5508 - val_loss: 423172928.0000 - val_rmse: 20571.1680\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 584094976.0000 - rmse: 24168.0566 - val_loss: 429008736.0000 - val_rmse: 20712.5254\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 545637888.0000 - rmse: 23358.8926 - val_loss: 406488192.0000 - val_rmse: 20161.5527\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 556405504.0000 - rmse: 23588.2500 - val_loss: 496749344.0000 - val_rmse: 22287.8750\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 533453856.0000 - rmse: 23096.6191 - val_loss: 440261856.0000 - val_rmse: 20982.4180\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 567831616.0000 - rmse: 23829.2168 - val_loss: 441096192.0000 - val_rmse: 21002.2910\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 591322048.0000 - rmse: 24317.1152 - val_loss: 408584288.0000 - val_rmse: 20213.4688\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 500608864.0000 - rmse: 22374.2910 - val_loss: 418628224.0000 - val_rmse: 20460.4062\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 495073952.0000 - rmse: 22250.2578 - val_loss: 418327424.0000 - val_rmse: 20453.0547\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 521925504.0000 - rmse: 22845.6895 - val_loss: 404943904.0000 - val_rmse: 20123.2188\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 552274816.0000 - rmse: 23500.5273 - val_loss: 474453536.0000 - val_rmse: 21781.9551\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 548085248.0000 - rmse: 23411.2207 - val_loss: 475605920.0000 - val_rmse: 21808.3906\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 516550208.0000 - rmse: 22727.7402 - val_loss: 424346400.0000 - val_rmse: 20599.6699\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 537055424.0000 - rmse: 23174.4570 - val_loss: 397031360.0000 - val_rmse: 19925.6465\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 501466016.0000 - rmse: 22393.4375 - val_loss: 398977728.0000 - val_rmse: 19974.4277\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 511701568.0000 - rmse: 22620.8223 - val_loss: 427998624.0000 - val_rmse: 20688.1270\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 505326464.0000 - rmse: 22479.4668 - val_loss: 400955520.0000 - val_rmse: 20023.8730\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 493426816.0000 - rmse: 22213.2129 - val_loss: 412045344.0000 - val_rmse: 20298.9004\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 456654240.0000 - rmse: 21369.4707 - val_loss: 432879776.0000 - val_rmse: 20805.7637\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 501076608.0000 - rmse: 22384.7402 - val_loss: 405541312.0000 - val_rmse: 20138.0566\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 480258912.0000 - rmse: 21914.8105 - val_loss: 425896288.0000 - val_rmse: 20637.2539\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 473149280.0000 - rmse: 21751.9941 - val_loss: 418277920.0000 - val_rmse: 20451.8438\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 488593152.0000 - rmse: 22104.1426 - val_loss: 419610944.0000 - val_rmse: 20484.4082\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 523804960.0000 - rmse: 22886.7852 - val_loss: 412503168.0000 - val_rmse: 20310.1738\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 432826944.0000 - rmse: 20804.4941 - val_loss: 388297568.0000 - val_rmse: 19705.2676\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 531214464.0000 - rmse: 23048.0898 - val_loss: 450107072.0000 - val_rmse: 21215.7266\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 467706912.0000 - rmse: 21626.5332 - val_loss: 395831552.0000 - val_rmse: 19895.5156\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 460054016.0000 - rmse: 21448.8691 - val_loss: 381845728.0000 - val_rmse: 19540.8730\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 453984416.0000 - rmse: 21306.9102 - val_loss: 408545312.0000 - val_rmse: 20212.5039\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 472998880.0000 - rmse: 21748.5371 - val_loss: 387067200.0000 - val_rmse: 19674.0234\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 490689408.0000 - rmse: 22151.5098 - val_loss: 408094208.0000 - val_rmse: 20201.3418\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 507961440.0000 - rmse: 22538.0000 - val_loss: 408514112.0000 - val_rmse: 20211.7324\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 437496608.0000 - rmse: 20916.4199 - val_loss: 449439968.0000 - val_rmse: 21200.0000\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 465792000.0000 - rmse: 21582.2148 - val_loss: 387647968.0000 - val_rmse: 19688.7773\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 500441792.0000 - rmse: 22370.5566 - val_loss: 412374816.0000 - val_rmse: 20307.0137\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 457394368.0000 - rmse: 21386.7793 - val_loss: 407010784.0000 - val_rmse: 20174.5078\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 462978752.0000 - rmse: 21516.9414 - val_loss: 444711552.0000 - val_rmse: 21088.1855\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 456589120.0000 - rmse: 21367.9453 - val_loss: 375962144.0000 - val_rmse: 19389.7441\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 462385216.0000 - rmse: 21503.1445 - val_loss: 381966528.0000 - val_rmse: 19543.9648\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 474486624.0000 - rmse: 21782.7148 - val_loss: 375554208.0000 - val_rmse: 19379.2207\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 485661184.0000 - rmse: 22037.7227 - val_loss: 419125152.0000 - val_rmse: 20472.5469\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 484015008.0000 - rmse: 22000.3418 - val_loss: 419430656.0000 - val_rmse: 20480.0059\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 476500800.0000 - rmse: 21828.8984 - val_loss: 389857536.0000 - val_rmse: 19744.8105\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 440323904.0000 - rmse: 20983.8965 - val_loss: 357437120.0000 - val_rmse: 18906.0078\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 477595488.0000 - rmse: 21853.9590 - val_loss: 361774080.0000 - val_rmse: 19020.3594\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 433986592.0000 - rmse: 20832.3457 - val_loss: 377945888.0000 - val_rmse: 19440.8301\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 443883616.0000 - rmse: 21068.5449 - val_loss: 362777280.0000 - val_rmse: 19046.7129\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 432246848.0000 - rmse: 20790.5469 - val_loss: 368882784.0000 - val_rmse: 19206.3223\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 416140992.0000 - rmse: 20399.5332 - val_loss: 375042272.0000 - val_rmse: 19366.0078\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 437905888.0000 - rmse: 20926.2012 - val_loss: 363337696.0000 - val_rmse: 19061.4199\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 452956576.0000 - rmse: 21282.7773 - val_loss: 355004896.0000 - val_rmse: 18841.5742\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 456424768.0000 - rmse: 21364.0996 - val_loss: 499751360.0000 - val_rmse: 22355.1191\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 443644288.0000 - rmse: 21062.8652 - val_loss: 352592384.0000 - val_rmse: 18777.4434\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 418524960.0000 - rmse: 20457.8828 - val_loss: 394489088.0000 - val_rmse: 19861.7500\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 400010048.0000 - rmse: 20000.2520 - val_loss: 365195520.0000 - val_rmse: 19110.0898\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 413360288.0000 - rmse: 20331.2637 - val_loss: 361925792.0000 - val_rmse: 19024.3477\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 420850720.0000 - rmse: 20514.6465 - val_loss: 371899648.0000 - val_rmse: 19284.6992\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 432157760.0000 - rmse: 20788.4043 - val_loss: 374273952.0000 - val_rmse: 19346.1621\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 439450720.0000 - rmse: 20963.0801 - val_loss: 360135168.0000 - val_rmse: 18977.2285\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 421717824.0000 - rmse: 20535.7695 - val_loss: 364353760.0000 - val_rmse: 19088.0527\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 405239392.0000 - rmse: 20130.5586 - val_loss: 395779616.0000 - val_rmse: 19894.2109\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 427738912.0000 - rmse: 20681.8496 - val_loss: 388922496.0000 - val_rmse: 19721.1172\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 387696736.0000 - rmse: 19690.0156 - val_loss: 400219456.0000 - val_rmse: 20005.4863\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 434835072.0000 - rmse: 20852.6992 - val_loss: 373585024.0000 - val_rmse: 19328.3477\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 391201760.0000 - rmse: 19778.8203 - val_loss: 348549824.0000 - val_rmse: 18669.4883\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 436034176.0000 - rmse: 20881.4316 - val_loss: 396581664.0000 - val_rmse: 19914.3574\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 372924640.0000 - rmse: 19311.2559 - val_loss: 368912480.0000 - val_rmse: 19207.0938\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 423723872.0000 - rmse: 20584.5547 - val_loss: 369265376.0000 - val_rmse: 19216.2793\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 389707776.0000 - rmse: 19741.0176 - val_loss: 348445600.0000 - val_rmse: 18666.6973\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 386747328.0000 - rmse: 19665.8926 - val_loss: 402336032.0000 - val_rmse: 20058.3164\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 415308736.0000 - rmse: 20379.1250 - val_loss: 407211904.0000 - val_rmse: 20179.4922\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 389102048.0000 - rmse: 19725.6699 - val_loss: 358333856.0000 - val_rmse: 18929.7090\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 436838496.0000 - rmse: 20900.6816 - val_loss: 348285408.0000 - val_rmse: 18662.4062\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 393370816.0000 - rmse: 19833.5781 - val_loss: 343365504.0000 - val_rmse: 18530.1250\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 351400832.0000 - rmse: 18745.6875 - val_loss: 371106944.0000 - val_rmse: 19264.1367\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 380990016.0000 - rmse: 19518.9648 - val_loss: 338427456.0000 - val_rmse: 18396.3984\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 382853024.0000 - rmse: 19566.6309 - val_loss: 335866336.0000 - val_rmse: 18326.6562\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 356196960.0000 - rmse: 18873.1816 - val_loss: 361436832.0000 - val_rmse: 19011.4922\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 342466784.0000 - rmse: 18505.8574 - val_loss: 332216224.0000 - val_rmse: 18226.7988\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 392820448.0000 - rmse: 19819.6992 - val_loss: 379115456.0000 - val_rmse: 19470.8867\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 398434496.0000 - rmse: 19960.8242 - val_loss: 444493280.0000 - val_rmse: 21083.0098\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 353892672.0000 - rmse: 18812.0352 - val_loss: 344008352.0000 - val_rmse: 18547.4629\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 348203872.0000 - rmse: 18660.2207 - val_loss: 349481376.0000 - val_rmse: 18694.4219\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 366447808.0000 - rmse: 19142.8262 - val_loss: 338508256.0000 - val_rmse: 18398.5938\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 332328064.0000 - rmse: 18229.8672 - val_loss: 333609216.0000 - val_rmse: 18264.9727\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 364736736.0000 - rmse: 19098.0820 - val_loss: 343421312.0000 - val_rmse: 18531.6309\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 349005632.0000 - rmse: 18681.6934 - val_loss: 334031648.0000 - val_rmse: 18276.5332\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 346991104.0000 - rmse: 18627.6973 - val_loss: 334537344.0000 - val_rmse: 18290.3613\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 386060288.0000 - rmse: 19648.4160 - val_loss: 330456960.0000 - val_rmse: 18178.4746\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 366904064.0000 - rmse: 19154.7402 - val_loss: 354316896.0000 - val_rmse: 18823.3066\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 368434784.0000 - rmse: 19194.6543 - val_loss: 346066752.0000 - val_rmse: 18602.8691\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 362856768.0000 - rmse: 19048.7988 - val_loss: 352532768.0000 - val_rmse: 18775.8555\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 357105248.0000 - rmse: 18897.2285 - val_loss: 330963328.0000 - val_rmse: 18192.3984\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 349311040.0000 - rmse: 18689.8652 - val_loss: 332274048.0000 - val_rmse: 18228.3867\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 378351648.0000 - rmse: 19451.2637 - val_loss: 328407648.0000 - val_rmse: 18122.0215\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 340239168.0000 - rmse: 18445.5723 - val_loss: 334966560.0000 - val_rmse: 18302.0918\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 301253344.0000 - rmse: 17356.6504 - val_loss: 327039168.0000 - val_rmse: 18084.2246\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 368471520.0000 - rmse: 19195.6113 - val_loss: 339426432.0000 - val_rmse: 18423.5293\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 327820736.0000 - rmse: 18105.8203 - val_loss: 330530400.0000 - val_rmse: 18180.4941\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 341798336.0000 - rmse: 18487.7891 - val_loss: 335390624.0000 - val_rmse: 18313.6738\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 337538976.0000 - rmse: 18372.2344 - val_loss: 342052512.0000 - val_rmse: 18494.6621\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 323629920.0000 - rmse: 17989.7168 - val_loss: 360945248.0000 - val_rmse: 18998.5586\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 344536000.0000 - rmse: 18561.6816 - val_loss: 385929728.0000 - val_rmse: 19645.0938\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 365478592.0000 - rmse: 19117.4941 - val_loss: 380150432.0000 - val_rmse: 19497.4473\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 348570240.0000 - rmse: 18670.0352 - val_loss: 312991328.0000 - val_rmse: 17691.5605\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 295790848.0000 - rmse: 17198.5703 - val_loss: 324978272.0000 - val_rmse: 18027.1543\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 329732864.0000 - rmse: 18158.5488 - val_loss: 308876480.0000 - val_rmse: 17574.8828\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 298821856.0000 - rmse: 17286.4648 - val_loss: 303203232.0000 - val_rmse: 17412.7324\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 327629216.0000 - rmse: 18100.5312 - val_loss: 302893440.0000 - val_rmse: 17403.8340\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 330117536.0000 - rmse: 18169.1367 - val_loss: 305420256.0000 - val_rmse: 17476.2773\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 330497216.0000 - rmse: 18179.5820 - val_loss: 344856864.0000 - val_rmse: 18570.3223\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 300349824.0000 - rmse: 17330.6035 - val_loss: 317057088.0000 - val_rmse: 17806.0977\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 304918208.0000 - rmse: 17461.9082 - val_loss: 310444480.0000 - val_rmse: 17619.4355\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 294161280.0000 - rmse: 17151.1309 - val_loss: 305206240.0000 - val_rmse: 17470.1523\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 326539360.0000 - rmse: 18070.4004 - val_loss: 312473408.0000 - val_rmse: 17676.9180\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 316286656.0000 - rmse: 17784.4492 - val_loss: 314814240.0000 - val_rmse: 17743.0059\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 357020448.0000 - rmse: 18894.9844 - val_loss: 332565376.0000 - val_rmse: 18236.3750\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 336995680.0000 - rmse: 18357.4414 - val_loss: 330874784.0000 - val_rmse: 18189.9629\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 284585952.0000 - rmse: 16869.6758 - val_loss: 321515040.0000 - val_rmse: 17930.8398\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 282158048.0000 - rmse: 16797.5605 - val_loss: 305703904.0000 - val_rmse: 17484.3906\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286137984.0000 - rmse: 16915.6133 - val_loss: 293727872.0000 - val_rmse: 17138.4902\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299913568.0000 - rmse: 17318.0137 - val_loss: 297593376.0000 - val_rmse: 17250.8945\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 336138304.0000 - rmse: 18334.0742 - val_loss: 301502016.0000 - val_rmse: 17363.8125\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 311984896.0000 - rmse: 17663.0938 - val_loss: 298728320.0000 - val_rmse: 17283.7598\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280747648.0000 - rmse: 16755.5254 - val_loss: 301451712.0000 - val_rmse: 17362.3652\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 335513344.0000 - rmse: 18317.0234 - val_loss: 322304480.0000 - val_rmse: 17952.8398\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 305808224.0000 - rmse: 17487.3730 - val_loss: 289107296.0000 - val_rmse: 17003.1562\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 327796288.0000 - rmse: 18105.1445 - val_loss: 305249152.0000 - val_rmse: 17471.3809\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 323553984.0000 - rmse: 17987.6055 - val_loss: 320299936.0000 - val_rmse: 17896.9258\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299527616.0000 - rmse: 17306.8652 - val_loss: 331258528.0000 - val_rmse: 18200.5098\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 288848800.0000 - rmse: 16995.5527 - val_loss: 321127296.0000 - val_rmse: 17920.0254\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 281816672.0000 - rmse: 16787.3965 - val_loss: 314166720.0000 - val_rmse: 17724.7480\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 312965728.0000 - rmse: 17690.8379 - val_loss: 308546432.0000 - val_rmse: 17565.4902\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286461600.0000 - rmse: 16925.1758 - val_loss: 311561664.0000 - val_rmse: 17651.1094\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 285435072.0000 - rmse: 16894.8242 - val_loss: 297898368.0000 - val_rmse: 17259.7324\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 315552928.0000 - rmse: 17763.8086 - val_loss: 312658048.0000 - val_rmse: 17682.1387\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 294620288.0000 - rmse: 17164.5059 - val_loss: 351889280.0000 - val_rmse: 18758.7129\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 312956128.0000 - rmse: 17690.5664 - val_loss: 320861664.0000 - val_rmse: 17912.6113\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260243360.0000 - rmse: 16132.0596 - val_loss: 316299552.0000 - val_rmse: 17784.8125\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 279744736.0000 - rmse: 16725.5723 - val_loss: 296144672.0000 - val_rmse: 17208.8535\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 274325280.0000 - rmse: 16562.7676 - val_loss: 318946336.0000 - val_rmse: 17859.0684\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 283268224.0000 - rmse: 16830.5742 - val_loss: 291556992.0000 - val_rmse: 17075.0391\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286190880.0000 - rmse: 16917.1777 - val_loss: 322767936.0000 - val_rmse: 17965.7441\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 295340288.0000 - rmse: 17185.4668 - val_loss: 313652352.0000 - val_rmse: 17710.2324\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 296596480.0000 - rmse: 17221.9766 - val_loss: 327033600.0000 - val_rmse: 18084.0703\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 272466816.0000 - rmse: 16506.5684 - val_loss: 302087200.0000 - val_rmse: 17380.6562\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 263674544.0000 - rmse: 16238.0586 - val_loss: 287142912.0000 - val_rmse: 16945.2910\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299744512.0000 - rmse: 17313.1309 - val_loss: 367939008.0000 - val_rmse: 19181.7363\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260380432.0000 - rmse: 16136.3076 - val_loss: 289101376.0000 - val_rmse: 17002.9805\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 250387776.0000 - rmse: 15823.6465 - val_loss: 301938432.0000 - val_rmse: 17376.3750\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 253821824.0000 - rmse: 15931.7861 - val_loss: 298018688.0000 - val_rmse: 17263.2188\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 254265904.0000 - rmse: 15945.7178 - val_loss: 304723200.0000 - val_rmse: 17456.3223\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 254935408.0000 - rmse: 15966.6973 - val_loss: 292552960.0000 - val_rmse: 17104.1797\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 290811264.0000 - rmse: 17053.1895 - val_loss: 303658496.0000 - val_rmse: 17425.7988\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 292987296.0000 - rmse: 17116.8711 - val_loss: 287958560.0000 - val_rmse: 16969.3418\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 245146912.0000 - rmse: 15657.1680 - val_loss: 303633472.0000 - val_rmse: 17425.0820\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 292927360.0000 - rmse: 17115.1211 - val_loss: 274511104.0000 - val_rmse: 16568.3770\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264454944.0000 - rmse: 16262.0703 - val_loss: 332794752.0000 - val_rmse: 18242.6621\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 295624416.0000 - rmse: 17193.7324 - val_loss: 289439904.0000 - val_rmse: 17012.9336\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 288794912.0000 - rmse: 16993.9668 - val_loss: 296149952.0000 - val_rmse: 17209.0078\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 251953232.0000 - rmse: 15873.0352 - val_loss: 297171360.0000 - val_rmse: 17238.6582\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280561408.0000 - rmse: 16749.9668 - val_loss: 311271712.0000 - val_rmse: 17642.8945\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 279506336.0000 - rmse: 16718.4434 - val_loss: 298036672.0000 - val_rmse: 17263.7383\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 252354128.0000 - rmse: 15885.6582 - val_loss: 293496288.0000 - val_rmse: 17131.7324\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268608096.0000 - rmse: 16389.2676 - val_loss: 306465824.0000 - val_rmse: 17506.1660\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 281144064.0000 - rmse: 16767.3516 - val_loss: 283254752.0000 - val_rmse: 16830.1738\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 274949152.0000 - rmse: 16581.5898 - val_loss: 269712544.0000 - val_rmse: 16422.9277\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309511104.0000 - rmse: 17592.9277 - val_loss: 281209376.0000 - val_rmse: 16769.2988\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260184656.0000 - rmse: 16130.2402 - val_loss: 278162272.0000 - val_rmse: 16678.1973\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 246771536.0000 - rmse: 15708.9639 - val_loss: 297098688.0000 - val_rmse: 17236.5508\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 230513568.0000 - rmse: 15182.6729 - val_loss: 282942272.0000 - val_rmse: 16820.8887\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 327559712.0000 - rmse: 18098.6113 - val_loss: 289085952.0000 - val_rmse: 17002.5273\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 252022880.0000 - rmse: 15875.2285 - val_loss: 285228480.0000 - val_rmse: 16888.7090\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280867456.0000 - rmse: 16759.1016 - val_loss: 358966784.0000 - val_rmse: 18946.4180\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 297803200.0000 - rmse: 17256.9746 - val_loss: 300371648.0000 - val_rmse: 17331.2324\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 258871392.0000 - rmse: 16089.4805 - val_loss: 294004800.0000 - val_rmse: 17146.5684\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264184464.0000 - rmse: 16253.7520 - val_loss: 294340096.0000 - val_rmse: 17156.3418\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 271559776.0000 - rmse: 16479.0703 - val_loss: 306635968.0000 - val_rmse: 17511.0234\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264489488.0000 - rmse: 16263.1328 - val_loss: 366025024.0000 - val_rmse: 19131.7812\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 263077456.0000 - rmse: 16219.6631 - val_loss: 286301856.0000 - val_rmse: 16920.4570\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 265188192.0000 - rmse: 16284.5996 - val_loss: 349908096.0000 - val_rmse: 18705.8301\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 281340416.0000 - rmse: 16773.2051 - val_loss: 279015552.0000 - val_rmse: 16703.7578\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 246207376.0000 - rmse: 15690.9971 - val_loss: 285536160.0000 - val_rmse: 16897.8145\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 244460656.0000 - rmse: 15635.2373 - val_loss: 288307904.0000 - val_rmse: 16979.6328\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 244594576.0000 - rmse: 15639.5195 - val_loss: 335608224.0000 - val_rmse: 18319.6133\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 243207200.0000 - rmse: 15595.1016 - val_loss: 289680256.0000 - val_rmse: 17019.9961\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 237462192.0000 - rmse: 15409.8086 - val_loss: 307080928.0000 - val_rmse: 17523.7246\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 245291808.0000 - rmse: 15661.7949 - val_loss: 282848832.0000 - val_rmse: 16818.1094\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 296135680.0000 - rmse: 17208.5938 - val_loss: 305768096.0000 - val_rmse: 17486.2266\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 289306400.0000 - rmse: 17009.0098 - val_loss: 284701280.0000 - val_rmse: 16873.0938\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 253576016.0000 - rmse: 15924.0703 - val_loss: 304811776.0000 - val_rmse: 17458.8594\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 246930800.0000 - rmse: 15714.0322 - val_loss: 296959424.0000 - val_rmse: 17232.5098\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 232484928.0000 - rmse: 15247.4561 - val_loss: 290432000.0000 - val_rmse: 17042.0664\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 243096896.0000 - rmse: 15591.5645 - val_loss: 311795680.0000 - val_rmse: 17657.7363\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 247171696.0000 - rmse: 15721.6953 - val_loss: 294022048.0000 - val_rmse: 17147.0703\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 296851008.0000 - rmse: 17229.3652 - val_loss: 330059200.0000 - val_rmse: 18167.5312\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 230913520.0000 - rmse: 15195.8389 - val_loss: 297446720.0000 - val_rmse: 17246.6445\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 279488768.0000 - rmse: 16717.9180 - val_loss: 281476960.0000 - val_rmse: 16777.2754\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 261648208.0000 - rmse: 16175.5439 - val_loss: 287141984.0000 - val_rmse: 16945.2637\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 271165184.0000 - rmse: 16467.0938 - val_loss: 288799072.0000 - val_rmse: 16994.0898\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 225435152.0000 - rmse: 15014.4980 - val_loss: 285451008.0000 - val_rmse: 16895.2949\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 277231296.0000 - rmse: 16650.2637 - val_loss: 285769312.0000 - val_rmse: 16904.7129\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260329312.0000 - rmse: 16134.7236 - val_loss: 303851104.0000 - val_rmse: 17431.3262\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 234919824.0000 - rmse: 15327.0947 - val_loss: 291121984.0000 - val_rmse: 17062.2969\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264673104.0000 - rmse: 16268.7773 - val_loss: 294741728.0000 - val_rmse: 17168.0430\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 227107280.0000 - rmse: 15070.0791 - val_loss: 311839360.0000 - val_rmse: 17658.9746\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260247328.0000 - rmse: 16132.1826 - val_loss: 291545856.0000 - val_rmse: 17074.7148\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 246817792.0000 - rmse: 15710.4355 - val_loss: 295655104.0000 - val_rmse: 17194.6250\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 285128544.0000 - rmse: 16885.7500 - val_loss: 315686816.0000 - val_rmse: 17767.5781\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264250752.0000 - rmse: 16255.7910 - val_loss: 292485760.0000 - val_rmse: 17102.2148\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 226736368.0000 - rmse: 15057.7676 - val_loss: 282421952.0000 - val_rmse: 16805.4141\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 261260048.0000 - rmse: 16163.5410 - val_loss: 287286112.0000 - val_rmse: 16949.5156\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 227343568.0000 - rmse: 15077.9170 - val_loss: 281894368.0000 - val_rmse: 16789.7109\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 216753040.0000 - rmse: 14722.5352 - val_loss: 280140928.0000 - val_rmse: 16737.4102\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 215177360.0000 - rmse: 14668.9248 - val_loss: 302492384.0000 - val_rmse: 17392.3086\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 258958016.0000 - rmse: 16092.1729 - val_loss: 282405920.0000 - val_rmse: 16804.9375\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 244511264.0000 - rmse: 15636.8555 - val_loss: 290222816.0000 - val_rmse: 17035.9277\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 249963568.0000 - rmse: 15810.2363 - val_loss: 281404128.0000 - val_rmse: 16775.1035\n",
      "Epoch 367/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 225536768.0000 - rmse: 15017.8818 - val_loss: 284518912.0000 - val_rmse: 16867.6875\n",
      "Epoch 368/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 228560128.0000 - rmse: 15118.2051 - val_loss: 320386880.0000 - val_rmse: 17899.3535\n",
      "Epoch 369/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 238669248.0000 - rmse: 15448.9238 - val_loss: 293039616.0000 - val_rmse: 17118.4004\n",
      "Epoch 370/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 257486944.0000 - rmse: 16046.3994 - val_loss: 327801600.0000 - val_rmse: 18105.2930\n",
      "Epoch 371/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260200928.0000 - rmse: 16130.7451 - val_loss: 296458400.0000 - val_rmse: 17217.9668\n",
      "Epoch 372/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 228554560.0000 - rmse: 15118.0215 - val_loss: 288431584.0000 - val_rmse: 16983.2734\n",
      "Epoch 373/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 258778752.0000 - rmse: 16086.6016 - val_loss: 293352320.0000 - val_rmse: 17127.5312\n",
      "Epoch 374/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 240730720.0000 - rmse: 15515.4990 - val_loss: 294880832.0000 - val_rmse: 17172.0938\n",
      "Epoch 375/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 236148896.0000 - rmse: 15367.1367 - val_loss: 294931264.0000 - val_rmse: 17173.5625\n",
      "Epoch 376/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 235160720.0000 - rmse: 15334.9512 - val_loss: 297464224.0000 - val_rmse: 17247.1504\n",
      "Epoch 377/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 222521632.0000 - rmse: 14917.1592 - val_loss: 283530816.0000 - val_rmse: 16838.3730\n",
      "Epoch 378/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 228319600.0000 - rmse: 15110.2480 - val_loss: 298328128.0000 - val_rmse: 17272.1777\n",
      "Epoch 379/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 218636992.0000 - rmse: 14786.3789 - val_loss: 281250624.0000 - val_rmse: 16770.5293\n",
      "Epoch 380/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 231096624.0000 - rmse: 15201.8623 - val_loss: 317589536.0000 - val_rmse: 17821.0410\n",
      "Epoch 381/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 265361488.0000 - rmse: 16289.9199 - val_loss: 306749504.0000 - val_rmse: 17514.2656\n",
      "Epoch 382/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 237888464.0000 - rmse: 15423.6328 - val_loss: 296356576.0000 - val_rmse: 17215.0098\n",
      "Epoch 383/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 234304160.0000 - rmse: 15306.9971 - val_loss: 295759616.0000 - val_rmse: 17197.6641\n",
      "Epoch 384/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 214804336.0000 - rmse: 14656.2051 - val_loss: 335768064.0000 - val_rmse: 18323.9746\n",
      "Epoch 385/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 247622496.0000 - rmse: 15736.0254 - val_loss: 318606496.0000 - val_rmse: 17849.5508\n",
      "Epoch 386/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 244628608.0000 - rmse: 15640.6074 - val_loss: 298881888.0000 - val_rmse: 17288.2012\n",
      "Epoch 387/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 222775088.0000 - rmse: 14925.6523 - val_loss: 292364384.0000 - val_rmse: 17098.6660\n",
      "Epoch 388/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 266197632.0000 - rmse: 16315.5645 - val_loss: 289572480.0000 - val_rmse: 17016.8301\n",
      "Epoch 389/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 237577456.0000 - rmse: 15413.5479 - val_loss: 293500832.0000 - val_rmse: 17131.8652\n",
      "Epoch 390/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 206978096.0000 - rmse: 14386.7334 - val_loss: 287080992.0000 - val_rmse: 16943.4648\n",
      "Epoch 391/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 238805152.0000 - rmse: 15453.3213 - val_loss: 294633632.0000 - val_rmse: 17164.8945\n",
      "Epoch 392/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 236686160.0000 - rmse: 15384.6074 - val_loss: 294846880.0000 - val_rmse: 17171.1055\n",
      "Epoch 393/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 236817232.0000 - rmse: 15388.8672 - val_loss: 295857312.0000 - val_rmse: 17200.5039\n",
      "Epoch 394/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 187721008.0000 - rmse: 13701.1318 - val_loss: 287593184.0000 - val_rmse: 16958.5723\n",
      "Epoch 395/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 220821136.0000 - rmse: 14860.0518 - val_loss: 286607136.0000 - val_rmse: 16929.4746\n",
      "Epoch 396/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268720576.0000 - rmse: 16392.6992 - val_loss: 279087488.0000 - val_rmse: 16705.9121\n",
      "Epoch 397/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 205888832.0000 - rmse: 14348.8271 - val_loss: 299760640.0000 - val_rmse: 17313.5977\n",
      "Epoch 398/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 235486656.0000 - rmse: 15345.5742 - val_loss: 308793312.0000 - val_rmse: 17572.5156\n",
      "Epoch 399/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 228560064.0000 - rmse: 15118.2031 - val_loss: 291694976.0000 - val_rmse: 17079.0801\n",
      "Epoch 400/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 266450752.0000 - rmse: 16323.3193 - val_loss: 343250016.0000 - val_rmse: 18527.0078\n",
      "104/104 [==============================] - 0s 883us/step - loss: 576125824.0000 - rmse: 24002.6211\n",
      "[576125824.0, 24002.62109375]\n",
      "<src.model.emb_model object at 0x7f8467747450>\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 19702681600.0000 - rmse: 140366.2344 - val_loss: 3524039936.0000 - val_rmse: 59363.6250\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2724598784.0000 - rmse: 52197.6875 - val_loss: 1752733056.0000 - val_rmse: 41865.6562\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1841881088.0000 - rmse: 42917.1406 - val_loss: 1438911744.0000 - val_rmse: 37932.9883\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1731552768.0000 - rmse: 41611.9297 - val_loss: 1276355328.0000 - val_rmse: 35726.1172\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1518239232.0000 - rmse: 38964.5898 - val_loss: 1310935552.0000 - val_rmse: 36206.8438\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1512014592.0000 - rmse: 38884.6328 - val_loss: 1117136128.0000 - val_rmse: 33423.5859\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1410434688.0000 - rmse: 37555.7539 - val_loss: 1153401088.0000 - val_rmse: 33961.7578\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1360626688.0000 - rmse: 36886.6719 - val_loss: 1045568576.0000 - val_rmse: 32335.2520\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1291666944.0000 - rmse: 35939.7695 - val_loss: 1016595008.0000 - val_rmse: 31884.0879\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1279122688.0000 - rmse: 35764.8242 - val_loss: 974447232.0000 - val_rmse: 31216.1367\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1197921792.0000 - rmse: 34611.0078 - val_loss: 935613248.0000 - val_rmse: 30587.7949\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1167585536.0000 - rmse: 34169.9492 - val_loss: 899649152.0000 - val_rmse: 29994.1523\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1176693632.0000 - rmse: 34302.9688 - val_loss: 914435584.0000 - val_rmse: 30239.6367\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1199561984.0000 - rmse: 34634.6914 - val_loss: 842338112.0000 - val_rmse: 29023.0625\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1141520512.0000 - rmse: 33786.3945 - val_loss: 810302400.0000 - val_rmse: 28465.8105\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1073508608.0000 - rmse: 32764.4414 - val_loss: 823287552.0000 - val_rmse: 28692.9883\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1079857024.0000 - rmse: 32861.1797 - val_loss: 1074582400.0000 - val_rmse: 32780.8242\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1062192384.0000 - rmse: 32591.2930 - val_loss: 747248960.0000 - val_rmse: 27335.8555\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 964842624.0000 - rmse: 31061.9160 - val_loss: 756856064.0000 - val_rmse: 27511.0176\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1017541568.0000 - rmse: 31898.9277 - val_loss: 732382016.0000 - val_rmse: 27062.5566\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 944662592.0000 - rmse: 30735.3633 - val_loss: 719750592.0000 - val_rmse: 26828.1680\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1049326976.0000 - rmse: 32393.3164 - val_loss: 737102336.0000 - val_rmse: 27149.6289\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1001191360.0000 - rmse: 31641.6074 - val_loss: 703752896.0000 - val_rmse: 26528.3418\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 975875072.0000 - rmse: 31239.0000 - val_loss: 683572608.0000 - val_rmse: 26145.2207\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 904550080.0000 - rmse: 30075.7383 - val_loss: 971618816.0000 - val_rmse: 31170.8008\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 884890560.0000 - rmse: 29747.1094 - val_loss: 731615296.0000 - val_rmse: 27048.3887\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 907475392.0000 - rmse: 30124.3320 - val_loss: 679742336.0000 - val_rmse: 26071.8691\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 948197440.0000 - rmse: 30792.8145 - val_loss: 750915904.0000 - val_rmse: 27402.8457\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 925576320.0000 - rmse: 30423.2852 - val_loss: 742840192.0000 - val_rmse: 27255.0957\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 928290816.0000 - rmse: 30467.8652 - val_loss: 675312192.0000 - val_rmse: 25986.7695\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 878799936.0000 - rmse: 29644.5605 - val_loss: 656150592.0000 - val_rmse: 25615.4375\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 880287168.0000 - rmse: 29669.6328 - val_loss: 633715264.0000 - val_rmse: 25173.7012\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 825897088.0000 - rmse: 28738.4258 - val_loss: 696340416.0000 - val_rmse: 26388.2637\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 881534144.0000 - rmse: 29690.6406 - val_loss: 627847488.0000 - val_rmse: 25056.8848\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 823540160.0000 - rmse: 28697.3887 - val_loss: 718922560.0000 - val_rmse: 26812.7305\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 761776384.0000 - rmse: 27600.2969 - val_loss: 656085440.0000 - val_rmse: 25614.1641\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 813948352.0000 - rmse: 28529.7793 - val_loss: 771783616.0000 - val_rmse: 27780.9941\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 885164288.0000 - rmse: 29751.7109 - val_loss: 594010816.0000 - val_rmse: 24372.3379\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 803928704.0000 - rmse: 28353.6367 - val_loss: 654370752.0000 - val_rmse: 25580.6719\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 832174336.0000 - rmse: 28847.4316 - val_loss: 797914432.0000 - val_rmse: 28247.3789\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 827499392.0000 - rmse: 28766.2891 - val_loss: 603204864.0000 - val_rmse: 24560.2285\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 885626112.0000 - rmse: 29759.4707 - val_loss: 644301952.0000 - val_rmse: 25383.1035\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 790898560.0000 - rmse: 28122.9180 - val_loss: 655779456.0000 - val_rmse: 25608.1914\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 805729728.0000 - rmse: 28385.3789 - val_loss: 565967808.0000 - val_rmse: 23790.0781\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 743843712.0000 - rmse: 27273.4980 - val_loss: 567762816.0000 - val_rmse: 23827.7734\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 718059456.0000 - rmse: 26796.6309 - val_loss: 609614272.0000 - val_rmse: 24690.3672\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 786970688.0000 - rmse: 28052.9980 - val_loss: 574164224.0000 - val_rmse: 23961.7246\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 789885056.0000 - rmse: 28104.8945 - val_loss: 564569344.0000 - val_rmse: 23760.6680\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 728500032.0000 - rmse: 26990.7402 - val_loss: 558903296.0000 - val_rmse: 23641.1348\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 770153856.0000 - rmse: 27751.6465 - val_loss: 539787264.0000 - val_rmse: 23233.3223\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 713188096.0000 - rmse: 26705.5820 - val_loss: 547479424.0000 - val_rmse: 23398.2773\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 718874880.0000 - rmse: 26811.8418 - val_loss: 707252480.0000 - val_rmse: 26594.2188\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 742205888.0000 - rmse: 27243.4551 - val_loss: 535258144.0000 - val_rmse: 23135.6465\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 695510016.0000 - rmse: 26372.5234 - val_loss: 626197824.0000 - val_rmse: 25023.9453\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 752651072.0000 - rmse: 27434.4863 - val_loss: 612495232.0000 - val_rmse: 24748.6406\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 734208320.0000 - rmse: 27096.2793 - val_loss: 516875680.0000 - val_rmse: 22734.9004\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 630192832.0000 - rmse: 25103.6426 - val_loss: 493217408.0000 - val_rmse: 22208.4980\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 701142528.0000 - rmse: 26479.0957 - val_loss: 513154720.0000 - val_rmse: 22652.9180\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 721331904.0000 - rmse: 26857.6230 - val_loss: 505205984.0000 - val_rmse: 22476.7871\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 767136384.0000 - rmse: 27697.2266 - val_loss: 501249408.0000 - val_rmse: 22388.5996\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 654385920.0000 - rmse: 25580.9688 - val_loss: 500282624.0000 - val_rmse: 22366.9980\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 699405888.0000 - rmse: 26446.2832 - val_loss: 531116768.0000 - val_rmse: 23045.9707\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 686478400.0000 - rmse: 26200.7324 - val_loss: 480571328.0000 - val_rmse: 21921.9375\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 672985664.0000 - rmse: 25941.9668 - val_loss: 472664384.0000 - val_rmse: 21740.8457\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 660844672.0000 - rmse: 25706.8984 - val_loss: 481829568.0000 - val_rmse: 21950.6172\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 645516736.0000 - rmse: 25407.0215 - val_loss: 508076704.0000 - val_rmse: 22540.5566\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 602715840.0000 - rmse: 24550.2715 - val_loss: 478113632.0000 - val_rmse: 21865.8105\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 675999488.0000 - rmse: 25999.9902 - val_loss: 488224736.0000 - val_rmse: 22095.8086\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 628236736.0000 - rmse: 25064.6504 - val_loss: 878606464.0000 - val_rmse: 29641.2969\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 648953344.0000 - rmse: 25474.5625 - val_loss: 532793120.0000 - val_rmse: 23082.3125\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 703037632.0000 - rmse: 26514.8574 - val_loss: 494678848.0000 - val_rmse: 22241.3770\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 650547968.0000 - rmse: 25505.8418 - val_loss: 481308864.0000 - val_rmse: 21938.7520\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 596176384.0000 - rmse: 24416.7227 - val_loss: 434536032.0000 - val_rmse: 20845.5273\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 617712768.0000 - rmse: 24853.8281 - val_loss: 519115040.0000 - val_rmse: 22784.0957\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 674766336.0000 - rmse: 25976.2656 - val_loss: 472911392.0000 - val_rmse: 21746.5254\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 620302656.0000 - rmse: 24905.8750 - val_loss: 451616832.0000 - val_rmse: 21251.2793\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 703887872.0000 - rmse: 26530.8848 - val_loss: 431480544.0000 - val_rmse: 20772.1094\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 628871744.0000 - rmse: 25077.3145 - val_loss: 664650432.0000 - val_rmse: 25780.8145\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 650966912.0000 - rmse: 25514.0527 - val_loss: 426623104.0000 - val_rmse: 20654.8574\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 670251072.0000 - rmse: 25889.2070 - val_loss: 436620672.0000 - val_rmse: 20895.4707\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 677603200.0000 - rmse: 26030.8125 - val_loss: 445478688.0000 - val_rmse: 21106.3652\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 620013312.0000 - rmse: 24900.0664 - val_loss: 445929952.0000 - val_rmse: 21117.0527\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 659711680.0000 - rmse: 25684.8535 - val_loss: 433754336.0000 - val_rmse: 20826.7695\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 640928832.0000 - rmse: 25316.5723 - val_loss: 489787936.0000 - val_rmse: 22131.1523\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 633435200.0000 - rmse: 25168.1387 - val_loss: 407109824.0000 - val_rmse: 20176.9629\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 697162112.0000 - rmse: 26403.8281 - val_loss: 468886912.0000 - val_rmse: 21653.7969\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 593214592.0000 - rmse: 24355.9961 - val_loss: 456715904.0000 - val_rmse: 21370.9121\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 601585920.0000 - rmse: 24527.2480 - val_loss: 437158112.0000 - val_rmse: 20908.3262\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 582460288.0000 - rmse: 24134.2148 - val_loss: 453840928.0000 - val_rmse: 21303.5430\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 568633088.0000 - rmse: 23846.0293 - val_loss: 459067104.0000 - val_rmse: 21425.8516\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 656229568.0000 - rmse: 25616.9785 - val_loss: 546039296.0000 - val_rmse: 23367.4844\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 559136192.0000 - rmse: 23646.0605 - val_loss: 420814176.0000 - val_rmse: 20513.7559\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 587360320.0000 - rmse: 24235.5176 - val_loss: 478903328.0000 - val_rmse: 21883.8594\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 620622336.0000 - rmse: 24912.2930 - val_loss: 418762208.0000 - val_rmse: 20463.6797\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 613336832.0000 - rmse: 24765.6387 - val_loss: 620916864.0000 - val_rmse: 24918.2031\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 607891968.0000 - rmse: 24655.4648 - val_loss: 457703168.0000 - val_rmse: 21393.9980\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 599582656.0000 - rmse: 24486.3770 - val_loss: 411479072.0000 - val_rmse: 20284.9473\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 569189120.0000 - rmse: 23857.6855 - val_loss: 446928576.0000 - val_rmse: 21140.6855\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 580302528.0000 - rmse: 24089.4688 - val_loss: 408067136.0000 - val_rmse: 20200.6719\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 577086016.0000 - rmse: 24022.6152 - val_loss: 411085088.0000 - val_rmse: 20275.2324\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 645011200.0000 - rmse: 25397.0703 - val_loss: 414077536.0000 - val_rmse: 20348.8945\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 535730336.0000 - rmse: 23145.8496 - val_loss: 445120224.0000 - val_rmse: 21097.8730\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 607102784.0000 - rmse: 24639.4551 - val_loss: 391314400.0000 - val_rmse: 19781.6680\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 643289600.0000 - rmse: 25363.1543 - val_loss: 527826784.0000 - val_rmse: 22974.4805\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 570727232.0000 - rmse: 23889.8984 - val_loss: 393408800.0000 - val_rmse: 19834.5352\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 680767808.0000 - rmse: 26091.5273 - val_loss: 566655104.0000 - val_rmse: 23804.5195\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 504931424.0000 - rmse: 22470.6797 - val_loss: 406609248.0000 - val_rmse: 20164.5547\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 552788032.0000 - rmse: 23511.4453 - val_loss: 406704384.0000 - val_rmse: 20166.9141\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 599059264.0000 - rmse: 24475.6875 - val_loss: 413829344.0000 - val_rmse: 20342.7949\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 577067520.0000 - rmse: 24022.2305 - val_loss: 418895840.0000 - val_rmse: 20466.9453\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 562739712.0000 - rmse: 23722.1348 - val_loss: 494294272.0000 - val_rmse: 22232.7305\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 541910656.0000 - rmse: 23278.9746 - val_loss: 401388512.0000 - val_rmse: 20034.6836\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 558481792.0000 - rmse: 23632.2188 - val_loss: 456283488.0000 - val_rmse: 21360.7930\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 533204352.0000 - rmse: 23091.2188 - val_loss: 436548640.0000 - val_rmse: 20893.7461\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 568296128.0000 - rmse: 23838.9629 - val_loss: 396371776.0000 - val_rmse: 19909.0879\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 608803584.0000 - rmse: 24673.9453 - val_loss: 405290816.0000 - val_rmse: 20131.8359\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 564376576.0000 - rmse: 23756.6113 - val_loss: 419054752.0000 - val_rmse: 20470.8262\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 605252352.0000 - rmse: 24601.8770 - val_loss: 395137440.0000 - val_rmse: 19878.0645\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 586263616.0000 - rmse: 24212.8809 - val_loss: 405232064.0000 - val_rmse: 20130.3770\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 525902688.0000 - rmse: 22932.5684 - val_loss: 392402720.0000 - val_rmse: 19809.1582\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 542956160.0000 - rmse: 23301.4199 - val_loss: 387212352.0000 - val_rmse: 19677.7129\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 550143616.0000 - rmse: 23455.1406 - val_loss: 386654496.0000 - val_rmse: 19663.5312\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 509628832.0000 - rmse: 22574.9609 - val_loss: 404871680.0000 - val_rmse: 20121.4238\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 604367488.0000 - rmse: 24583.8867 - val_loss: 391626496.0000 - val_rmse: 19789.5547\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 502168832.0000 - rmse: 22409.1230 - val_loss: 420236416.0000 - val_rmse: 20499.6680\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 622254016.0000 - rmse: 24945.0195 - val_loss: 392757088.0000 - val_rmse: 19818.0996\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 568171392.0000 - rmse: 23836.3457 - val_loss: 444330368.0000 - val_rmse: 21079.1445\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 590339904.0000 - rmse: 24296.9121 - val_loss: 388824288.0000 - val_rmse: 19718.6289\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 609994048.0000 - rmse: 24698.0566 - val_loss: 384048896.0000 - val_rmse: 19597.1660\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 583754560.0000 - rmse: 24161.0137 - val_loss: 389877280.0000 - val_rmse: 19745.3105\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 547825280.0000 - rmse: 23405.6680 - val_loss: 417021920.0000 - val_rmse: 20421.1152\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 565975680.0000 - rmse: 23790.2441 - val_loss: 410331840.0000 - val_rmse: 20256.6484\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 517153184.0000 - rmse: 22741.0020 - val_loss: 377031648.0000 - val_rmse: 19417.3027\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 539612736.0000 - rmse: 23229.5664 - val_loss: 393936512.0000 - val_rmse: 19847.8340\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 574013504.0000 - rmse: 23958.5781 - val_loss: 417332992.0000 - val_rmse: 20428.7305\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 509843904.0000 - rmse: 22579.7227 - val_loss: 382100704.0000 - val_rmse: 19547.3965\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 510173280.0000 - rmse: 22587.0156 - val_loss: 378859456.0000 - val_rmse: 19464.3125\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 534877408.0000 - rmse: 23127.4160 - val_loss: 389824064.0000 - val_rmse: 19743.9629\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 468804000.0000 - rmse: 21651.8828 - val_loss: 431027776.0000 - val_rmse: 20761.2090\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 490791552.0000 - rmse: 22153.8164 - val_loss: 394649440.0000 - val_rmse: 19865.7852\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 481799136.0000 - rmse: 21949.9238 - val_loss: 388145504.0000 - val_rmse: 19701.4082\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 495107008.0000 - rmse: 22251.0000 - val_loss: 393723776.0000 - val_rmse: 19842.4746\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 501694816.0000 - rmse: 22398.5449 - val_loss: 392395456.0000 - val_rmse: 19808.9746\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 561446144.0000 - rmse: 23694.8555 - val_loss: 379043744.0000 - val_rmse: 19469.0449\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 617449152.0000 - rmse: 24848.5234 - val_loss: 362450816.0000 - val_rmse: 19038.1406\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 473367904.0000 - rmse: 21757.0195 - val_loss: 372533184.0000 - val_rmse: 19301.1191\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 512499808.0000 - rmse: 22638.4590 - val_loss: 386418112.0000 - val_rmse: 19657.5195\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 509597728.0000 - rmse: 22574.2715 - val_loss: 380783456.0000 - val_rmse: 19513.6738\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 444298528.0000 - rmse: 21078.3906 - val_loss: 407619392.0000 - val_rmse: 20189.5859\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 515911936.0000 - rmse: 22713.6953 - val_loss: 365516224.0000 - val_rmse: 19118.4785\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 523094080.0000 - rmse: 22871.2500 - val_loss: 425632608.0000 - val_rmse: 20630.8652\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 482778336.0000 - rmse: 21972.2168 - val_loss: 390651520.0000 - val_rmse: 19764.9062\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 492443872.0000 - rmse: 22191.0762 - val_loss: 395589280.0000 - val_rmse: 19889.4258\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 490498720.0000 - rmse: 22147.2051 - val_loss: 388520672.0000 - val_rmse: 19710.9277\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 515181952.0000 - rmse: 22697.6191 - val_loss: 412101568.0000 - val_rmse: 20300.2852\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 548750912.0000 - rmse: 23425.4336 - val_loss: 398419488.0000 - val_rmse: 19960.4473\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 529140544.0000 - rmse: 23003.0547 - val_loss: 416348608.0000 - val_rmse: 20404.6230\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 574548160.0000 - rmse: 23969.7344 - val_loss: 418327360.0000 - val_rmse: 20453.0527\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 479862656.0000 - rmse: 21905.7676 - val_loss: 399238528.0000 - val_rmse: 19980.9551\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 561688768.0000 - rmse: 23699.9746 - val_loss: 482594016.0000 - val_rmse: 21968.0234\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 520232832.0000 - rmse: 22808.6133 - val_loss: 411709728.0000 - val_rmse: 20290.6309\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 522875744.0000 - rmse: 22866.4766 - val_loss: 477678400.0000 - val_rmse: 21855.8555\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 500174496.0000 - rmse: 22364.5820 - val_loss: 416371072.0000 - val_rmse: 20405.1719\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 500621312.0000 - rmse: 22374.5684 - val_loss: 481792128.0000 - val_rmse: 21949.7637\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 468205440.0000 - rmse: 21638.0547 - val_loss: 398531040.0000 - val_rmse: 19963.2422\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 529686848.0000 - rmse: 23014.9258 - val_loss: 375037120.0000 - val_rmse: 19365.8750\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 512320480.0000 - rmse: 22634.4980 - val_loss: 355404160.0000 - val_rmse: 18852.1660\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 490191296.0000 - rmse: 22140.2637 - val_loss: 361333600.0000 - val_rmse: 19008.7773\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 482008352.0000 - rmse: 21954.6895 - val_loss: 358922784.0000 - val_rmse: 18945.2578\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 492824192.0000 - rmse: 22199.6445 - val_loss: 349379872.0000 - val_rmse: 18691.7051\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 510531168.0000 - rmse: 22594.9375 - val_loss: 372749984.0000 - val_rmse: 19306.7344\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 524245024.0000 - rmse: 22896.3984 - val_loss: 424495264.0000 - val_rmse: 20603.2832\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 510301632.0000 - rmse: 22589.8574 - val_loss: 411829952.0000 - val_rmse: 20293.5938\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 457446720.0000 - rmse: 21388.0039 - val_loss: 367825120.0000 - val_rmse: 19178.7676\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 455453376.0000 - rmse: 21341.3535 - val_loss: 389524864.0000 - val_rmse: 19736.3848\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 541674304.0000 - rmse: 23273.8984 - val_loss: 364885792.0000 - val_rmse: 19101.9844\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 536671136.0000 - rmse: 23166.1641 - val_loss: 351571328.0000 - val_rmse: 18750.2363\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 461574656.0000 - rmse: 21484.2891 - val_loss: 360422656.0000 - val_rmse: 18984.8008\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 481363200.0000 - rmse: 21939.9902 - val_loss: 361630592.0000 - val_rmse: 19016.5879\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 539187264.0000 - rmse: 23220.4062 - val_loss: 404981888.0000 - val_rmse: 20124.1621\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 440265184.0000 - rmse: 20982.4980 - val_loss: 353228032.0000 - val_rmse: 18794.3613\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 500324928.0000 - rmse: 22367.9434 - val_loss: 346529056.0000 - val_rmse: 18615.2910\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 496933920.0000 - rmse: 22292.0156 - val_loss: 354864960.0000 - val_rmse: 18837.8594\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 470149088.0000 - rmse: 21682.9219 - val_loss: 355024832.0000 - val_rmse: 18842.1035\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 446503680.0000 - rmse: 21130.6328 - val_loss: 368504160.0000 - val_rmse: 19196.4629\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 471974112.0000 - rmse: 21724.9648 - val_loss: 361599520.0000 - val_rmse: 19015.7695\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 521523392.0000 - rmse: 22836.8867 - val_loss: 429340736.0000 - val_rmse: 20720.5391\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 469841696.0000 - rmse: 21675.8320 - val_loss: 357935328.0000 - val_rmse: 18919.1797\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 447154944.0000 - rmse: 21146.0391 - val_loss: 338838464.0000 - val_rmse: 18407.5645\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 491343008.0000 - rmse: 22166.2578 - val_loss: 359144864.0000 - val_rmse: 18951.1172\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 476111872.0000 - rmse: 21819.9883 - val_loss: 343927456.0000 - val_rmse: 18545.2812\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 490613376.0000 - rmse: 22149.7949 - val_loss: 357640928.0000 - val_rmse: 18911.3965\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 437502400.0000 - rmse: 20916.5586 - val_loss: 368382368.0000 - val_rmse: 19193.2891\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 507762208.0000 - rmse: 22533.5801 - val_loss: 379911488.0000 - val_rmse: 19491.3184\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 557815808.0000 - rmse: 23618.1250 - val_loss: 388072928.0000 - val_rmse: 19699.5664\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 456021376.0000 - rmse: 21354.6562 - val_loss: 358944800.0000 - val_rmse: 18945.8379\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 463329696.0000 - rmse: 21525.0938 - val_loss: 337673408.0000 - val_rmse: 18375.8926\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 470262368.0000 - rmse: 21685.5332 - val_loss: 339617824.0000 - val_rmse: 18428.7227\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 500230080.0000 - rmse: 22365.8242 - val_loss: 351896032.0000 - val_rmse: 18758.8926\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 480146208.0000 - rmse: 21912.2383 - val_loss: 455357760.0000 - val_rmse: 21339.1133\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 501613152.0000 - rmse: 22396.7227 - val_loss: 338241056.0000 - val_rmse: 18391.3301\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 486278304.0000 - rmse: 22051.7188 - val_loss: 365102624.0000 - val_rmse: 19107.6582\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 473713504.0000 - rmse: 21764.9609 - val_loss: 354946816.0000 - val_rmse: 18840.0332\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 444851488.0000 - rmse: 21091.5020 - val_loss: 395143936.0000 - val_rmse: 19878.2285\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 413945760.0000 - rmse: 20345.6562 - val_loss: 351245312.0000 - val_rmse: 18741.5391\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 440754400.0000 - rmse: 20994.1523 - val_loss: 428881792.0000 - val_rmse: 20709.4609\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 468196832.0000 - rmse: 21637.8574 - val_loss: 367860736.0000 - val_rmse: 19179.6953\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 485090752.0000 - rmse: 22024.7754 - val_loss: 438496224.0000 - val_rmse: 20940.3008\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 450084224.0000 - rmse: 21215.1895 - val_loss: 398741664.0000 - val_rmse: 19968.5176\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 444284704.0000 - rmse: 21078.0625 - val_loss: 392736256.0000 - val_rmse: 19817.5742\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 456286112.0000 - rmse: 21360.8555 - val_loss: 403244384.0000 - val_rmse: 20080.9453\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 500035872.0000 - rmse: 22361.4824 - val_loss: 385330880.0000 - val_rmse: 19629.8457\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 493401024.0000 - rmse: 22212.6328 - val_loss: 369043424.0000 - val_rmse: 19210.5039\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 469101408.0000 - rmse: 21658.7480 - val_loss: 423550016.0000 - val_rmse: 20580.3301\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 491749344.0000 - rmse: 22175.4219 - val_loss: 340030304.0000 - val_rmse: 18439.9102\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 499948032.0000 - rmse: 22359.5176 - val_loss: 389293920.0000 - val_rmse: 19730.5332\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 466067232.0000 - rmse: 21588.5898 - val_loss: 407027872.0000 - val_rmse: 20174.9316\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 428966848.0000 - rmse: 20711.5156 - val_loss: 338171520.0000 - val_rmse: 18389.4414\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 461940544.0000 - rmse: 21492.8027 - val_loss: 333162816.0000 - val_rmse: 18252.7480\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 450070496.0000 - rmse: 21214.8652 - val_loss: 335876576.0000 - val_rmse: 18326.9355\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 448285216.0000 - rmse: 21172.7461 - val_loss: 407851040.0000 - val_rmse: 20195.3223\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 443189440.0000 - rmse: 21052.0645 - val_loss: 362646048.0000 - val_rmse: 19043.2676\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 505457920.0000 - rmse: 22482.3906 - val_loss: 354510464.0000 - val_rmse: 18828.4492\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 457761664.0000 - rmse: 21395.3652 - val_loss: 339126816.0000 - val_rmse: 18415.3965\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 416354592.0000 - rmse: 20404.7695 - val_loss: 351130880.0000 - val_rmse: 18738.4863\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 455737984.0000 - rmse: 21348.0215 - val_loss: 377707040.0000 - val_rmse: 19434.6855\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 436253792.0000 - rmse: 20886.6895 - val_loss: 341647936.0000 - val_rmse: 18483.7207\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 416645440.0000 - rmse: 20411.8945 - val_loss: 335227552.0000 - val_rmse: 18309.2207\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 426393760.0000 - rmse: 20649.3047 - val_loss: 344579008.0000 - val_rmse: 18562.8398\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 420886848.0000 - rmse: 20515.5273 - val_loss: 327811552.0000 - val_rmse: 18105.5664\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 412512704.0000 - rmse: 20310.4082 - val_loss: 354190528.0000 - val_rmse: 18819.9512\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 458050304.0000 - rmse: 21402.1094 - val_loss: 327451040.0000 - val_rmse: 18095.6074\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 433093056.0000 - rmse: 20810.8887 - val_loss: 336079264.0000 - val_rmse: 18332.4648\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 446866464.0000 - rmse: 21139.2168 - val_loss: 341931328.0000 - val_rmse: 18491.3848\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 423045792.0000 - rmse: 20568.0762 - val_loss: 348622592.0000 - val_rmse: 18671.4375\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 394140992.0000 - rmse: 19852.9844 - val_loss: 353221184.0000 - val_rmse: 18794.1797\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 452552608.0000 - rmse: 21273.2832 - val_loss: 347235168.0000 - val_rmse: 18634.2480\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 428346752.0000 - rmse: 20696.5391 - val_loss: 343811392.0000 - val_rmse: 18542.1523\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 420818464.0000 - rmse: 20513.8594 - val_loss: 337788736.0000 - val_rmse: 18379.0293\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 438977056.0000 - rmse: 20951.7793 - val_loss: 315469024.0000 - val_rmse: 17761.4473\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 464534912.0000 - rmse: 21553.0723 - val_loss: 365022720.0000 - val_rmse: 19105.5684\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 443012160.0000 - rmse: 21047.8535 - val_loss: 334116096.0000 - val_rmse: 18278.8438\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 391051904.0000 - rmse: 19775.0332 - val_loss: 326258944.0000 - val_rmse: 18062.6387\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 426457024.0000 - rmse: 20650.8359 - val_loss: 382606208.0000 - val_rmse: 19560.3223\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 384223136.0000 - rmse: 19601.6113 - val_loss: 338970912.0000 - val_rmse: 18411.1621\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 411272416.0000 - rmse: 20279.8516 - val_loss: 331975648.0000 - val_rmse: 18220.1992\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 434648576.0000 - rmse: 20848.2266 - val_loss: 338838208.0000 - val_rmse: 18407.5586\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 410460320.0000 - rmse: 20259.8203 - val_loss: 323058880.0000 - val_rmse: 17973.8379\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 416630720.0000 - rmse: 20411.5332 - val_loss: 326921152.0000 - val_rmse: 18080.9609\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 423883840.0000 - rmse: 20588.4395 - val_loss: 330611456.0000 - val_rmse: 18182.7246\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 424964800.0000 - rmse: 20614.6738 - val_loss: 321765728.0000 - val_rmse: 17937.8301\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 427533600.0000 - rmse: 20676.8848 - val_loss: 334605280.0000 - val_rmse: 18292.2188\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 374272256.0000 - rmse: 19346.1172 - val_loss: 316588832.0000 - val_rmse: 17792.9434\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 373847808.0000 - rmse: 19335.1445 - val_loss: 445833248.0000 - val_rmse: 21114.7637\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 419470688.0000 - rmse: 20480.9844 - val_loss: 346068448.0000 - val_rmse: 18602.9160\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 367322976.0000 - rmse: 19165.6719 - val_loss: 303390496.0000 - val_rmse: 17418.1074\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 447752864.0000 - rmse: 21160.1719 - val_loss: 334861248.0000 - val_rmse: 18299.2148\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 404766208.0000 - rmse: 20118.8027 - val_loss: 314769248.0000 - val_rmse: 17741.7383\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 476486080.0000 - rmse: 21828.5605 - val_loss: 308978720.0000 - val_rmse: 17577.7910\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 417952960.0000 - rmse: 20443.8984 - val_loss: 296979392.0000 - val_rmse: 17233.0898\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 399090944.0000 - rmse: 19977.2598 - val_loss: 310774432.0000 - val_rmse: 17628.7949\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 387001056.0000 - rmse: 19672.3418 - val_loss: 304325536.0000 - val_rmse: 17444.9277\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 423432352.0000 - rmse: 20577.4727 - val_loss: 294379008.0000 - val_rmse: 17157.4766\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 466702304.0000 - rmse: 21603.2930 - val_loss: 307178208.0000 - val_rmse: 17526.5000\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 364103232.0000 - rmse: 19081.4883 - val_loss: 296139520.0000 - val_rmse: 17208.7051\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 379566624.0000 - rmse: 19482.4688 - val_loss: 318325024.0000 - val_rmse: 17841.6660\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 423034624.0000 - rmse: 20567.8047 - val_loss: 292635264.0000 - val_rmse: 17106.5859\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 446247072.0000 - rmse: 21124.5605 - val_loss: 339535040.0000 - val_rmse: 18426.4766\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 377973536.0000 - rmse: 19441.5410 - val_loss: 289438976.0000 - val_rmse: 17012.9062\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 380764544.0000 - rmse: 19513.1895 - val_loss: 290619712.0000 - val_rmse: 17047.5723\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 349750592.0000 - rmse: 18701.6191 - val_loss: 302962784.0000 - val_rmse: 17405.8262\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 390436832.0000 - rmse: 19759.4746 - val_loss: 281402592.0000 - val_rmse: 16775.0586\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 391864448.0000 - rmse: 19795.5664 - val_loss: 352484160.0000 - val_rmse: 18774.5605\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 471604128.0000 - rmse: 21716.4492 - val_loss: 343734336.0000 - val_rmse: 18540.0742\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 377865312.0000 - rmse: 19438.7578 - val_loss: 314406496.0000 - val_rmse: 17731.5117\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 391185632.0000 - rmse: 19778.4141 - val_loss: 440321760.0000 - val_rmse: 20983.8457\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 401073344.0000 - rmse: 20026.8164 - val_loss: 320566752.0000 - val_rmse: 17904.3789\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 384401728.0000 - rmse: 19606.1660 - val_loss: 382476736.0000 - val_rmse: 19557.0117\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 386854176.0000 - rmse: 19668.6094 - val_loss: 323583360.0000 - val_rmse: 17988.4238\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 411180192.0000 - rmse: 20277.5781 - val_loss: 338840384.0000 - val_rmse: 18407.6172\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 369401088.0000 - rmse: 19219.8105 - val_loss: 290264960.0000 - val_rmse: 17037.1641\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 408949120.0000 - rmse: 20222.4902 - val_loss: 317641824.0000 - val_rmse: 17822.5098\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 407034016.0000 - rmse: 20175.0840 - val_loss: 342085984.0000 - val_rmse: 18495.5664\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 381994304.0000 - rmse: 19544.6738 - val_loss: 353849440.0000 - val_rmse: 18810.8867\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 377744224.0000 - rmse: 19435.6426 - val_loss: 295422176.0000 - val_rmse: 17187.8496\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 364053408.0000 - rmse: 19080.1836 - val_loss: 285320448.0000 - val_rmse: 16891.4316\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 367046048.0000 - rmse: 19158.4453 - val_loss: 330016736.0000 - val_rmse: 18166.3633\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 413937760.0000 - rmse: 20345.4609 - val_loss: 383984704.0000 - val_rmse: 19595.5273\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 441348800.0000 - rmse: 21008.3027 - val_loss: 439156352.0000 - val_rmse: 20956.0586\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 374431680.0000 - rmse: 19350.2363 - val_loss: 308850944.0000 - val_rmse: 17574.1562\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 385912608.0000 - rmse: 19644.6582 - val_loss: 324294848.0000 - val_rmse: 18008.1875\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 369003744.0000 - rmse: 19209.4707 - val_loss: 396459808.0000 - val_rmse: 19911.2988\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 444086592.0000 - rmse: 21073.3613 - val_loss: 389808608.0000 - val_rmse: 19743.5723\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 363879232.0000 - rmse: 19075.6191 - val_loss: 386881504.0000 - val_rmse: 19669.3027\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 360812192.0000 - rmse: 18995.0566 - val_loss: 314275680.0000 - val_rmse: 17727.8223\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 357707872.0000 - rmse: 18913.1660 - val_loss: 305547744.0000 - val_rmse: 17479.9238\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 366918688.0000 - rmse: 19155.1211 - val_loss: 431809952.0000 - val_rmse: 20780.0371\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 416953280.0000 - rmse: 20419.4336 - val_loss: 406740160.0000 - val_rmse: 20167.8008\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 362817632.0000 - rmse: 19047.7715 - val_loss: 341377248.0000 - val_rmse: 18476.3965\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 378211776.0000 - rmse: 19447.6680 - val_loss: 316939808.0000 - val_rmse: 17802.8027\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 362194144.0000 - rmse: 19031.3984 - val_loss: 349143648.0000 - val_rmse: 18685.3867\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 359402400.0000 - rmse: 18957.9121 - val_loss: 483751200.0000 - val_rmse: 21994.3457\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 364088448.0000 - rmse: 19081.1016 - val_loss: 429848576.0000 - val_rmse: 20732.7891\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 356256224.0000 - rmse: 18874.7500 - val_loss: 374227072.0000 - val_rmse: 19344.9492\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 367651040.0000 - rmse: 19174.2285 - val_loss: 333069888.0000 - val_rmse: 18250.2031\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 380022080.0000 - rmse: 19494.1543 - val_loss: 342634368.0000 - val_rmse: 18510.3848\n",
      "104/104 [==============================] - 0s 872us/step - loss: 1060295744.0000 - rmse: 32562.1836\n",
      "[1060295744.0, 32562.18359375]\n",
      "<src.model.emb_model object at 0x7f8469604b50>\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 19793625088.0000 - rmse: 140689.8125 - val_loss: 2321650432.0000 - val_rmse: 48183.5078\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2351005696.0000 - rmse: 48487.1719 - val_loss: 1336475520.0000 - val_rmse: 36557.8398\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1914397184.0000 - rmse: 43753.8242 - val_loss: 1290578816.0000 - val_rmse: 35924.6250\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1871391488.0000 - rmse: 43259.5820 - val_loss: 1115189504.0000 - val_rmse: 33394.4531\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1779947904.0000 - rmse: 42189.4297 - val_loss: 1063829248.0000 - val_rmse: 32616.3945\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1650932736.0000 - rmse: 40631.6719 - val_loss: 1075968384.0000 - val_rmse: 32801.9570\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1650567040.0000 - rmse: 40627.1719 - val_loss: 976356160.0000 - val_rmse: 31246.6992\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1624233984.0000 - rmse: 40301.7852 - val_loss: 942697600.0000 - val_rmse: 30703.3809\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1593886976.0000 - rmse: 39923.5156 - val_loss: 924922624.0000 - val_rmse: 30412.5410\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1510804608.0000 - rmse: 38869.0703 - val_loss: 899693312.0000 - val_rmse: 29994.8887\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1535778176.0000 - rmse: 39189.0039 - val_loss: 885529920.0000 - val_rmse: 29757.8555\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1439748096.0000 - rmse: 37944.0117 - val_loss: 859400768.0000 - val_rmse: 29315.5371\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1445089536.0000 - rmse: 38014.3320 - val_loss: 982792384.0000 - val_rmse: 31349.5195\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1422622336.0000 - rmse: 37717.6680 - val_loss: 830977920.0000 - val_rmse: 28826.6875\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1466271104.0000 - rmse: 38291.9180 - val_loss: 827657280.0000 - val_rmse: 28769.0332\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1343443328.0000 - rmse: 36653.0117 - val_loss: 899085184.0000 - val_rmse: 29984.7500\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1444050176.0000 - rmse: 38000.6602 - val_loss: 791144896.0000 - val_rmse: 28127.2988\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1284896000.0000 - rmse: 35845.4453 - val_loss: 800544832.0000 - val_rmse: 28293.9004\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1284115584.0000 - rmse: 35834.5586 - val_loss: 768072064.0000 - val_rmse: 27714.1133\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1249441280.0000 - rmse: 35347.4375 - val_loss: 764910720.0000 - val_rmse: 27657.0195\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1196288384.0000 - rmse: 34587.4023 - val_loss: 755735424.0000 - val_rmse: 27490.6426\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1222547200.0000 - rmse: 34964.9414 - val_loss: 724336384.0000 - val_rmse: 26913.4980\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1273992960.0000 - rmse: 35693.0391 - val_loss: 759579648.0000 - val_rmse: 27560.4727\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1233924992.0000 - rmse: 35127.2695 - val_loss: 722257152.0000 - val_rmse: 26874.8418\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1102367872.0000 - rmse: 33201.9258 - val_loss: 784095936.0000 - val_rmse: 28001.7129\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1164108160.0000 - rmse: 34119.0273 - val_loss: 710615104.0000 - val_rmse: 26657.3652\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1111363456.0000 - rmse: 33337.1172 - val_loss: 729945024.0000 - val_rmse: 27017.4941\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1132359040.0000 - rmse: 33650.5430 - val_loss: 702881408.0000 - val_rmse: 26511.9102\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1061699776.0000 - rmse: 32583.7344 - val_loss: 727440256.0000 - val_rmse: 26971.0996\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1099759104.0000 - rmse: 33162.6172 - val_loss: 696214016.0000 - val_rmse: 26385.8672\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1105783296.0000 - rmse: 33253.3203 - val_loss: 780753280.0000 - val_rmse: 27941.9629\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1095190912.0000 - rmse: 33093.6680 - val_loss: 670875200.0000 - val_rmse: 25901.2578\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1032408128.0000 - rmse: 32131.1094 - val_loss: 654334016.0000 - val_rmse: 25579.9531\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1021735296.0000 - rmse: 31964.5938 - val_loss: 662826176.0000 - val_rmse: 25745.4102\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1042601216.0000 - rmse: 32289.3359 - val_loss: 654329216.0000 - val_rmse: 25579.8594\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1015691712.0000 - rmse: 31869.9180 - val_loss: 690160768.0000 - val_rmse: 26270.9102\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1016068672.0000 - rmse: 31875.8320 - val_loss: 684763840.0000 - val_rmse: 26167.9922\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 990901248.0000 - rmse: 31478.5840 - val_loss: 689866176.0000 - val_rmse: 26265.3027\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 994575936.0000 - rmse: 31536.8984 - val_loss: 623914688.0000 - val_rmse: 24978.2852\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1006785984.0000 - rmse: 31729.8906 - val_loss: 643660864.0000 - val_rmse: 25370.4727\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 996843776.0000 - rmse: 31572.8320 - val_loss: 621881344.0000 - val_rmse: 24937.5488\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 994100352.0000 - rmse: 31529.3574 - val_loss: 622869696.0000 - val_rmse: 24957.3574\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1001472640.0000 - rmse: 31646.0527 - val_loss: 632635456.0000 - val_rmse: 25152.2461\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 934134464.0000 - rmse: 30563.6133 - val_loss: 633908032.0000 - val_rmse: 25177.5312\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 899611648.0000 - rmse: 29993.5273 - val_loss: 643351360.0000 - val_rmse: 25364.3711\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 945953408.0000 - rmse: 30756.3555 - val_loss: 624388672.0000 - val_rmse: 24987.7695\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 959563584.0000 - rmse: 30976.8242 - val_loss: 597957632.0000 - val_rmse: 24453.1719\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 881161984.0000 - rmse: 29684.3730 - val_loss: 589343488.0000 - val_rmse: 24276.3984\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 943654208.0000 - rmse: 30718.9551 - val_loss: 588711040.0000 - val_rmse: 24263.3691\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 959051328.0000 - rmse: 30968.5547 - val_loss: 648084416.0000 - val_rmse: 25457.5020\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 857764672.0000 - rmse: 29287.6191 - val_loss: 582686656.0000 - val_rmse: 24138.9043\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 888760128.0000 - rmse: 29812.0801 - val_loss: 624410688.0000 - val_rmse: 24988.2109\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 946876672.0000 - rmse: 30771.3613 - val_loss: 555362176.0000 - val_rmse: 23566.1230\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 865453376.0000 - rmse: 29418.5898 - val_loss: 580485760.0000 - val_rmse: 24093.2715\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 877809024.0000 - rmse: 29627.8418 - val_loss: 553913600.0000 - val_rmse: 23535.3691\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 878737280.0000 - rmse: 29643.5039 - val_loss: 548395200.0000 - val_rmse: 23417.8398\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 868066752.0000 - rmse: 29462.9727 - val_loss: 522717152.0000 - val_rmse: 22863.0078\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 821685248.0000 - rmse: 28665.0527 - val_loss: 517397664.0000 - val_rmse: 22746.3770\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 905960960.0000 - rmse: 30099.1855 - val_loss: 516689824.0000 - val_rmse: 22730.8125\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 869419456.0000 - rmse: 29485.9199 - val_loss: 613557504.0000 - val_rmse: 24770.0938\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 819202496.0000 - rmse: 28621.7129 - val_loss: 608381952.0000 - val_rmse: 24665.4004\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 872616640.0000 - rmse: 29540.0859 - val_loss: 500856512.0000 - val_rmse: 22379.8242\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 879038784.0000 - rmse: 29648.5879 - val_loss: 584572160.0000 - val_rmse: 24177.9277\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 814259136.0000 - rmse: 28535.2266 - val_loss: 516791904.0000 - val_rmse: 22733.0566\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 915318464.0000 - rmse: 30254.2305 - val_loss: 497863136.0000 - val_rmse: 22312.8477\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 822748288.0000 - rmse: 28683.5898 - val_loss: 524875328.0000 - val_rmse: 22910.1582\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 781082560.0000 - rmse: 27947.8535 - val_loss: 530403200.0000 - val_rmse: 23030.4844\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 738787072.0000 - rmse: 27180.6387 - val_loss: 584243776.0000 - val_rmse: 24171.1348\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 783172224.0000 - rmse: 27985.2148 - val_loss: 543418624.0000 - val_rmse: 23311.3418\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 746625024.0000 - rmse: 27324.4395 - val_loss: 510157248.0000 - val_rmse: 22586.6602\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 754303232.0000 - rmse: 27464.5820 - val_loss: 483927456.0000 - val_rmse: 21998.3516\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 742188672.0000 - rmse: 27243.1406 - val_loss: 457264064.0000 - val_rmse: 21383.7344\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 743289280.0000 - rmse: 27263.3320 - val_loss: 465362880.0000 - val_rmse: 21572.2715\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 774598784.0000 - rmse: 27831.6152 - val_loss: 463658528.0000 - val_rmse: 21532.7324\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 801259264.0000 - rmse: 28306.5234 - val_loss: 475642528.0000 - val_rmse: 21809.2305\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 762382720.0000 - rmse: 27611.2793 - val_loss: 462176608.0000 - val_rmse: 21498.2930\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 776792256.0000 - rmse: 27870.9922 - val_loss: 463050176.0000 - val_rmse: 21518.6016\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 695776192.0000 - rmse: 26377.5703 - val_loss: 481167840.0000 - val_rmse: 21935.5391\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 697239104.0000 - rmse: 26405.2852 - val_loss: 512608512.0000 - val_rmse: 22640.8594\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 704898560.0000 - rmse: 26549.9258 - val_loss: 498220032.0000 - val_rmse: 22320.8438\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 725841920.0000 - rmse: 26941.4531 - val_loss: 473962816.0000 - val_rmse: 21770.6875\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 755841472.0000 - rmse: 27492.5703 - val_loss: 515332928.0000 - val_rmse: 22700.9453\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 683825792.0000 - rmse: 26150.0625 - val_loss: 455650176.0000 - val_rmse: 21345.9648\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 646751680.0000 - rmse: 25431.3125 - val_loss: 474120096.0000 - val_rmse: 21774.2988\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 680693184.0000 - rmse: 26090.0977 - val_loss: 426207136.0000 - val_rmse: 20644.7852\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 651679232.0000 - rmse: 25528.0078 - val_loss: 473346176.0000 - val_rmse: 21756.5195\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 677350592.0000 - rmse: 26025.9609 - val_loss: 481826144.0000 - val_rmse: 21950.5391\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 709383488.0000 - rmse: 26634.2539 - val_loss: 474510848.0000 - val_rmse: 21783.2695\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 663388992.0000 - rmse: 25756.3398 - val_loss: 476933728.0000 - val_rmse: 21838.8125\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 708003328.0000 - rmse: 26608.3320 - val_loss: 444420128.0000 - val_rmse: 21081.2734\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 676243904.0000 - rmse: 26004.6895 - val_loss: 438530272.0000 - val_rmse: 20941.1152\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 654781824.0000 - rmse: 25588.7051 - val_loss: 446772160.0000 - val_rmse: 21136.9863\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 693929280.0000 - rmse: 26342.5371 - val_loss: 476653120.0000 - val_rmse: 21832.3867\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 692197632.0000 - rmse: 26309.6484 - val_loss: 444571776.0000 - val_rmse: 21084.8711\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 690941120.0000 - rmse: 26285.7598 - val_loss: 469643808.0000 - val_rmse: 21671.2676\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 616023872.0000 - rmse: 24819.8281 - val_loss: 471595488.0000 - val_rmse: 21716.2500\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 659111680.0000 - rmse: 25673.1699 - val_loss: 437808928.0000 - val_rmse: 20923.8848\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 624745728.0000 - rmse: 24994.9141 - val_loss: 429577312.0000 - val_rmse: 20726.2461\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 709962624.0000 - rmse: 26645.1230 - val_loss: 485359904.0000 - val_rmse: 22030.8848\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 640724160.0000 - rmse: 25312.5293 - val_loss: 439190912.0000 - val_rmse: 20956.8828\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 628078848.0000 - rmse: 25061.5020 - val_loss: 481654016.0000 - val_rmse: 21946.6172\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 620359488.0000 - rmse: 24907.0176 - val_loss: 432696256.0000 - val_rmse: 20801.3516\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 666245120.0000 - rmse: 25811.7246 - val_loss: 442591296.0000 - val_rmse: 21037.8535\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 612616512.0000 - rmse: 24751.0918 - val_loss: 445211840.0000 - val_rmse: 21100.0430\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 608428352.0000 - rmse: 24666.3398 - val_loss: 420251680.0000 - val_rmse: 20500.0410\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 629353152.0000 - rmse: 25086.9121 - val_loss: 406743168.0000 - val_rmse: 20167.8750\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 622381376.0000 - rmse: 24947.5723 - val_loss: 411668448.0000 - val_rmse: 20289.6152\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 611736256.0000 - rmse: 24733.3027 - val_loss: 476732256.0000 - val_rmse: 21834.1992\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 590601600.0000 - rmse: 24302.2969 - val_loss: 467433856.0000 - val_rmse: 21620.2188\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 597937024.0000 - rmse: 24452.7500 - val_loss: 412308960.0000 - val_rmse: 20305.3926\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 614478976.0000 - rmse: 24788.6855 - val_loss: 602498432.0000 - val_rmse: 24545.8438\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 615402944.0000 - rmse: 24807.3164 - val_loss: 403529824.0000 - val_rmse: 20088.0527\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 592496256.0000 - rmse: 24341.2461 - val_loss: 491684800.0000 - val_rmse: 22173.9668\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 583265216.0000 - rmse: 24150.8848 - val_loss: 506435936.0000 - val_rmse: 22504.1309\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 564279552.0000 - rmse: 23754.5684 - val_loss: 405379392.0000 - val_rmse: 20134.0352\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 624977792.0000 - rmse: 24999.5566 - val_loss: 459871072.0000 - val_rmse: 21444.6055\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 636600448.0000 - rmse: 25230.9414 - val_loss: 395846144.0000 - val_rmse: 19895.8828\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 633072256.0000 - rmse: 25160.9277 - val_loss: 465482720.0000 - val_rmse: 21575.0488\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 578369536.0000 - rmse: 24049.3145 - val_loss: 428830560.0000 - val_rmse: 20708.2246\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 618472064.0000 - rmse: 24869.0977 - val_loss: 410916896.0000 - val_rmse: 20271.0859\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 605617344.0000 - rmse: 24609.2930 - val_loss: 427270368.0000 - val_rmse: 20670.5195\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 578715264.0000 - rmse: 24056.5020 - val_loss: 395261312.0000 - val_rmse: 19881.1797\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 520705248.0000 - rmse: 22818.9668 - val_loss: 435074240.0000 - val_rmse: 20858.4336\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 555268480.0000 - rmse: 23564.1348 - val_loss: 443400960.0000 - val_rmse: 21057.0879\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 578812864.0000 - rmse: 24058.5293 - val_loss: 401097632.0000 - val_rmse: 20027.4219\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 554042624.0000 - rmse: 23538.1094 - val_loss: 412626080.0000 - val_rmse: 20313.1992\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 532767040.0000 - rmse: 23081.7461 - val_loss: 390261952.0000 - val_rmse: 19755.0488\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 579556160.0000 - rmse: 24073.9727 - val_loss: 406352384.0000 - val_rmse: 20158.1836\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 587427840.0000 - rmse: 24236.9102 - val_loss: 382455040.0000 - val_rmse: 19556.4570\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 527593536.0000 - rmse: 22969.4043 - val_loss: 410969568.0000 - val_rmse: 20272.3848\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 540240000.0000 - rmse: 23243.0645 - val_loss: 405438176.0000 - val_rmse: 20135.4961\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 609731456.0000 - rmse: 24692.7402 - val_loss: 432364064.0000 - val_rmse: 20793.3652\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 535356416.0000 - rmse: 23137.7695 - val_loss: 402947424.0000 - val_rmse: 20073.5508\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 615435264.0000 - rmse: 24807.9668 - val_loss: 407452672.0000 - val_rmse: 20185.4570\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 579250816.0000 - rmse: 24067.6309 - val_loss: 429479712.0000 - val_rmse: 20723.8926\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 548424896.0000 - rmse: 23418.4727 - val_loss: 390314080.0000 - val_rmse: 19756.3672\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 541175680.0000 - rmse: 23263.1836 - val_loss: 513110560.0000 - val_rmse: 22651.9434\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 544357248.0000 - rmse: 23331.4648 - val_loss: 444917664.0000 - val_rmse: 21093.0723\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 570709888.0000 - rmse: 23889.5352 - val_loss: 429486816.0000 - val_rmse: 20724.0645\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 572506112.0000 - rmse: 23927.0996 - val_loss: 388009568.0000 - val_rmse: 19697.9590\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 524770464.0000 - rmse: 22907.8691 - val_loss: 510648608.0000 - val_rmse: 22597.5352\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 539854336.0000 - rmse: 23234.7656 - val_loss: 450850656.0000 - val_rmse: 21233.2441\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 512945184.0000 - rmse: 22648.2930 - val_loss: 465286336.0000 - val_rmse: 21570.4961\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 522613088.0000 - rmse: 22860.7324 - val_loss: 388713696.0000 - val_rmse: 19715.8242\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 538755584.0000 - rmse: 23211.1094 - val_loss: 391193504.0000 - val_rmse: 19778.6113\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 518215232.0000 - rmse: 22764.3418 - val_loss: 415580032.0000 - val_rmse: 20385.7793\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 542376832.0000 - rmse: 23288.9844 - val_loss: 611320768.0000 - val_rmse: 24724.9023\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 520941376.0000 - rmse: 22824.1406 - val_loss: 413537152.0000 - val_rmse: 20335.6133\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 532096352.0000 - rmse: 23067.2129 - val_loss: 394503840.0000 - val_rmse: 19862.1211\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 492807744.0000 - rmse: 22199.2734 - val_loss: 396079456.0000 - val_rmse: 19901.7441\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 535258400.0000 - rmse: 23135.6523 - val_loss: 367801600.0000 - val_rmse: 19178.1543\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 529988288.0000 - rmse: 23021.4746 - val_loss: 444095232.0000 - val_rmse: 21073.5664\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 544724416.0000 - rmse: 23339.3320 - val_loss: 388311712.0000 - val_rmse: 19705.6270\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 519470240.0000 - rmse: 22791.8906 - val_loss: 469030624.0000 - val_rmse: 21657.1152\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 512120320.0000 - rmse: 22630.0762 - val_loss: 395500096.0000 - val_rmse: 19887.1836\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 463648032.0000 - rmse: 21532.4883 - val_loss: 437497216.0000 - val_rmse: 20916.4336\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 488746656.0000 - rmse: 22107.6152 - val_loss: 376649536.0000 - val_rmse: 19407.4609\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 461181216.0000 - rmse: 21475.1309 - val_loss: 397189888.0000 - val_rmse: 19929.6230\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 514414624.0000 - rmse: 22680.7109 - val_loss: 368696448.0000 - val_rmse: 19201.4707\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 484519488.0000 - rmse: 22011.8027 - val_loss: 469769344.0000 - val_rmse: 21674.1621\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 522094048.0000 - rmse: 22849.3770 - val_loss: 379285184.0000 - val_rmse: 19475.2461\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 455350592.0000 - rmse: 21338.9453 - val_loss: 422484608.0000 - val_rmse: 20554.4297\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 491714944.0000 - rmse: 22174.6465 - val_loss: 402175872.0000 - val_rmse: 20054.3223\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 485416000.0000 - rmse: 22032.1582 - val_loss: 456064992.0000 - val_rmse: 21355.6777\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 485582208.0000 - rmse: 22035.9297 - val_loss: 409457888.0000 - val_rmse: 20235.0664\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 443973120.0000 - rmse: 21070.6699 - val_loss: 382042624.0000 - val_rmse: 19545.9102\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 487321056.0000 - rmse: 22075.3496 - val_loss: 379101376.0000 - val_rmse: 19470.5254\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 497025760.0000 - rmse: 22294.0742 - val_loss: 408077344.0000 - val_rmse: 20200.9238\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 476827296.0000 - rmse: 21836.3750 - val_loss: 416379360.0000 - val_rmse: 20405.3750\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 428149216.0000 - rmse: 20691.7676 - val_loss: 383503360.0000 - val_rmse: 19583.2422\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 455701888.0000 - rmse: 21347.1758 - val_loss: 410986944.0000 - val_rmse: 20272.8125\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 443517120.0000 - rmse: 21059.8457 - val_loss: 416804384.0000 - val_rmse: 20415.7871\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 498585248.0000 - rmse: 22329.0234 - val_loss: 377586048.0000 - val_rmse: 19431.5742\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 487936992.0000 - rmse: 22089.2949 - val_loss: 367560384.0000 - val_rmse: 19171.8652\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 493023392.0000 - rmse: 22204.1309 - val_loss: 388258656.0000 - val_rmse: 19704.2793\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 473749920.0000 - rmse: 21765.7969 - val_loss: 393686144.0000 - val_rmse: 19841.5254\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 440660960.0000 - rmse: 20991.9258 - val_loss: 396006240.0000 - val_rmse: 19899.9062\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 484719520.0000 - rmse: 22016.3457 - val_loss: 357914592.0000 - val_rmse: 18918.6309\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 477138688.0000 - rmse: 21843.5039 - val_loss: 363408352.0000 - val_rmse: 19063.2715\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 453786272.0000 - rmse: 21302.2598 - val_loss: 359938720.0000 - val_rmse: 18972.0508\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 471455360.0000 - rmse: 21713.0234 - val_loss: 373646112.0000 - val_rmse: 19329.9277\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 481434112.0000 - rmse: 21941.6074 - val_loss: 355649344.0000 - val_rmse: 18858.6680\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 430833856.0000 - rmse: 20756.5371 - val_loss: 409526976.0000 - val_rmse: 20236.7734\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 455585440.0000 - rmse: 21344.4473 - val_loss: 359492384.0000 - val_rmse: 18960.2852\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 439760192.0000 - rmse: 20970.4609 - val_loss: 372948256.0000 - val_rmse: 19311.8691\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 420222368.0000 - rmse: 20499.3262 - val_loss: 352539968.0000 - val_rmse: 18776.0469\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 458574592.0000 - rmse: 21414.3555 - val_loss: 383873472.0000 - val_rmse: 19592.6895\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 446474752.0000 - rmse: 21129.9492 - val_loss: 394584000.0000 - val_rmse: 19864.1387\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 418557696.0000 - rmse: 20458.6836 - val_loss: 391177856.0000 - val_rmse: 19778.2168\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 471572800.0000 - rmse: 21715.7266 - val_loss: 388396480.0000 - val_rmse: 19707.7773\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 432067872.0000 - rmse: 20786.2422 - val_loss: 367175808.0000 - val_rmse: 19161.8320\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 433445632.0000 - rmse: 20819.3574 - val_loss: 367124800.0000 - val_rmse: 19160.5020\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 415661216.0000 - rmse: 20387.7715 - val_loss: 375204736.0000 - val_rmse: 19370.2031\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 408448800.0000 - rmse: 20210.1172 - val_loss: 364691040.0000 - val_rmse: 19096.8848\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 507451680.0000 - rmse: 22526.6875 - val_loss: 390320032.0000 - val_rmse: 19756.5195\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 427063936.0000 - rmse: 20665.5254 - val_loss: 388559744.0000 - val_rmse: 19711.9180\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 464399168.0000 - rmse: 21549.9219 - val_loss: 347500448.0000 - val_rmse: 18641.3633\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 431086080.0000 - rmse: 20762.6133 - val_loss: 417139328.0000 - val_rmse: 20423.9883\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 463725632.0000 - rmse: 21534.2891 - val_loss: 529516864.0000 - val_rmse: 23011.2344\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 415152672.0000 - rmse: 20375.2949 - val_loss: 404246400.0000 - val_rmse: 20105.8789\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 416088896.0000 - rmse: 20398.2578 - val_loss: 401250720.0000 - val_rmse: 20031.2441\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 453184160.0000 - rmse: 21288.1230 - val_loss: 395456352.0000 - val_rmse: 19886.0840\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 399587552.0000 - rmse: 19989.6855 - val_loss: 385819936.0000 - val_rmse: 19642.2988\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 403727488.0000 - rmse: 20092.9707 - val_loss: 367910624.0000 - val_rmse: 19180.9961\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 421259424.0000 - rmse: 20524.6055 - val_loss: 489734112.0000 - val_rmse: 22129.9375\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 443534080.0000 - rmse: 21060.2480 - val_loss: 365924192.0000 - val_rmse: 19129.1445\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 447245824.0000 - rmse: 21148.1875 - val_loss: 366838176.0000 - val_rmse: 19153.0195\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 416348192.0000 - rmse: 20404.6113 - val_loss: 349561824.0000 - val_rmse: 18696.5723\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 384217568.0000 - rmse: 19601.4688 - val_loss: 383329504.0000 - val_rmse: 19578.8027\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 432614496.0000 - rmse: 20799.3867 - val_loss: 364011584.0000 - val_rmse: 19079.0879\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 397720576.0000 - rmse: 19942.9336 - val_loss: 367402560.0000 - val_rmse: 19167.7480\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 410855488.0000 - rmse: 20269.5703 - val_loss: 376241088.0000 - val_rmse: 19396.9355\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 449486976.0000 - rmse: 21201.1074 - val_loss: 357441120.0000 - val_rmse: 18906.1133\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 412855168.0000 - rmse: 20318.8379 - val_loss: 380654592.0000 - val_rmse: 19510.3711\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 383154848.0000 - rmse: 19574.3418 - val_loss: 371438560.0000 - val_rmse: 19272.7422\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 423987264.0000 - rmse: 20590.9512 - val_loss: 423262208.0000 - val_rmse: 20573.3379\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 403998624.0000 - rmse: 20099.7168 - val_loss: 441459584.0000 - val_rmse: 21010.9395\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 375631232.0000 - rmse: 19381.2090 - val_loss: 396377568.0000 - val_rmse: 19909.2324\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 395860992.0000 - rmse: 19896.2559 - val_loss: 379089728.0000 - val_rmse: 19470.2266\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 417514048.0000 - rmse: 20433.1602 - val_loss: 374414048.0000 - val_rmse: 19349.7812\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 389035168.0000 - rmse: 19723.9746 - val_loss: 399892992.0000 - val_rmse: 19997.3242\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 376434624.0000 - rmse: 19401.9238 - val_loss: 403344384.0000 - val_rmse: 20083.4355\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 364421856.0000 - rmse: 19089.8359 - val_loss: 375587520.0000 - val_rmse: 19380.0801\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 367041984.0000 - rmse: 19158.3398 - val_loss: 351050656.0000 - val_rmse: 18736.3457\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 439154080.0000 - rmse: 20956.0039 - val_loss: 369398656.0000 - val_rmse: 19219.7461\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 404352672.0000 - rmse: 20108.5215 - val_loss: 365669792.0000 - val_rmse: 19122.4941\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 378765632.0000 - rmse: 19461.9023 - val_loss: 394200896.0000 - val_rmse: 19854.4922\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 374029312.0000 - rmse: 19339.8379 - val_loss: 507937120.0000 - val_rmse: 22537.4609\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 379644640.0000 - rmse: 19484.4727 - val_loss: 433860800.0000 - val_rmse: 20829.3262\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 384984320.0000 - rmse: 19621.0176 - val_loss: 364383072.0000 - val_rmse: 19088.8203\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 385932896.0000 - rmse: 19645.1758 - val_loss: 379167712.0000 - val_rmse: 19472.2285\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 357223072.0000 - rmse: 18900.3457 - val_loss: 359247648.0000 - val_rmse: 18953.8301\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 350522912.0000 - rmse: 18722.2578 - val_loss: 416582752.0000 - val_rmse: 20410.3594\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 375785952.0000 - rmse: 19385.1992 - val_loss: 368980864.0000 - val_rmse: 19208.8750\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 403046080.0000 - rmse: 20076.0078 - val_loss: 349993472.0000 - val_rmse: 18708.1133\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 406231904.0000 - rmse: 20155.1953 - val_loss: 478821824.0000 - val_rmse: 21881.9980\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 369875040.0000 - rmse: 19232.1348 - val_loss: 379284480.0000 - val_rmse: 19475.2266\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 367438784.0000 - rmse: 19168.6934 - val_loss: 394292704.0000 - val_rmse: 19856.8047\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 387928416.0000 - rmse: 19695.8984 - val_loss: 401261824.0000 - val_rmse: 20031.5215\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 343341952.0000 - rmse: 18529.4883 - val_loss: 350029280.0000 - val_rmse: 18709.0703\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 379611680.0000 - rmse: 19483.6250 - val_loss: 381772192.0000 - val_rmse: 19538.9922\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 370048960.0000 - rmse: 19236.6562 - val_loss: 394576768.0000 - val_rmse: 19863.9570\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 426387488.0000 - rmse: 20649.1523 - val_loss: 377051648.0000 - val_rmse: 19417.8184\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 395583776.0000 - rmse: 19889.2871 - val_loss: 400278912.0000 - val_rmse: 20006.9707\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 366699616.0000 - rmse: 19149.4023 - val_loss: 358850784.0000 - val_rmse: 18943.3574\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 422382880.0000 - rmse: 20551.9551 - val_loss: 384861632.0000 - val_rmse: 19617.8906\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 409797536.0000 - rmse: 20243.4570 - val_loss: 429111264.0000 - val_rmse: 20715.0000\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 405636480.0000 - rmse: 20140.4199 - val_loss: 407289344.0000 - val_rmse: 20181.4102\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 438103136.0000 - rmse: 20930.9141 - val_loss: 347650880.0000 - val_rmse: 18645.3984\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 354744160.0000 - rmse: 18834.6523 - val_loss: 365464480.0000 - val_rmse: 19117.1250\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 353896608.0000 - rmse: 18812.1406 - val_loss: 421912416.0000 - val_rmse: 20540.5059\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 353804448.0000 - rmse: 18809.6895 - val_loss: 361715424.0000 - val_rmse: 19018.8184\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 383622944.0000 - rmse: 19586.2949 - val_loss: 382683520.0000 - val_rmse: 19562.2988\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 317558752.0000 - rmse: 17820.1777 - val_loss: 365658208.0000 - val_rmse: 19122.1914\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 334386112.0000 - rmse: 18286.2266 - val_loss: 373590208.0000 - val_rmse: 19328.4824\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 336377152.0000 - rmse: 18340.5879 - val_loss: 359345664.0000 - val_rmse: 18956.4141\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 381490336.0000 - rmse: 19531.7773 - val_loss: 364217088.0000 - val_rmse: 19084.4727\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 340061056.0000 - rmse: 18440.7441 - val_loss: 361978304.0000 - val_rmse: 19025.7266\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 321671872.0000 - rmse: 17935.2129 - val_loss: 382283712.0000 - val_rmse: 19552.0762\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 352005824.0000 - rmse: 18761.8184 - val_loss: 357882528.0000 - val_rmse: 18917.7832\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 351982784.0000 - rmse: 18761.2051 - val_loss: 372718080.0000 - val_rmse: 19305.9082\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 344037888.0000 - rmse: 18548.2578 - val_loss: 345471072.0000 - val_rmse: 18586.8516\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 369914720.0000 - rmse: 19233.1680 - val_loss: 357948512.0000 - val_rmse: 18919.5273\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 359901600.0000 - rmse: 18971.0723 - val_loss: 354215424.0000 - val_rmse: 18820.6113\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 320730528.0000 - rmse: 17908.9512 - val_loss: 345398432.0000 - val_rmse: 18584.8984\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 375561408.0000 - rmse: 19379.4062 - val_loss: 474695104.0000 - val_rmse: 21787.4980\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 333083072.0000 - rmse: 18250.5645 - val_loss: 364105408.0000 - val_rmse: 19081.5469\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 345217088.0000 - rmse: 18580.0176 - val_loss: 380003584.0000 - val_rmse: 19493.6797\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 427426816.0000 - rmse: 20674.3027 - val_loss: 362971968.0000 - val_rmse: 19051.8223\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 301109216.0000 - rmse: 17352.4980 - val_loss: 370299392.0000 - val_rmse: 19243.1641\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 341562560.0000 - rmse: 18481.4121 - val_loss: 378771360.0000 - val_rmse: 19462.0488\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 335742592.0000 - rmse: 18323.2793 - val_loss: 361395168.0000 - val_rmse: 19010.3965\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 332223200.0000 - rmse: 18226.9902 - val_loss: 367826432.0000 - val_rmse: 19178.8008\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 332042944.0000 - rmse: 18222.0449 - val_loss: 352488832.0000 - val_rmse: 18774.6855\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 278394112.0000 - rmse: 16685.1465 - val_loss: 384028672.0000 - val_rmse: 19596.6504\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 332192832.0000 - rmse: 18226.1582 - val_loss: 358558336.0000 - val_rmse: 18935.6367\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 336381088.0000 - rmse: 18340.6953 - val_loss: 380638336.0000 - val_rmse: 19509.9551\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 332596288.0000 - rmse: 18237.2227 - val_loss: 373362848.0000 - val_rmse: 19322.5996\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 330781024.0000 - rmse: 18187.3867 - val_loss: 377505792.0000 - val_rmse: 19429.5078\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 369137312.0000 - rmse: 19212.9473 - val_loss: 340262656.0000 - val_rmse: 18446.2090\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 321173920.0000 - rmse: 17921.3262 - val_loss: 381308960.0000 - val_rmse: 19527.1348\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 393541344.0000 - rmse: 19837.8770 - val_loss: 394801344.0000 - val_rmse: 19869.6094\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 341503776.0000 - rmse: 18479.8203 - val_loss: 360971424.0000 - val_rmse: 18999.2480\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 316637760.0000 - rmse: 17794.3184 - val_loss: 356772864.0000 - val_rmse: 18888.4316\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 338276896.0000 - rmse: 18392.3047 - val_loss: 388853248.0000 - val_rmse: 19719.3613\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 311493568.0000 - rmse: 17649.1797 - val_loss: 404778848.0000 - val_rmse: 20119.1172\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 331999200.0000 - rmse: 18220.8457 - val_loss: 353631264.0000 - val_rmse: 18805.0859\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 327816928.0000 - rmse: 18105.7148 - val_loss: 383251200.0000 - val_rmse: 19576.8027\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 321710880.0000 - rmse: 17936.3008 - val_loss: 366587456.0000 - val_rmse: 19146.4746\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 332045024.0000 - rmse: 18222.1035 - val_loss: 375701120.0000 - val_rmse: 19383.0117\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 345647104.0000 - rmse: 18591.5879 - val_loss: 426045152.0000 - val_rmse: 20640.8613\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 345084832.0000 - rmse: 18576.4590 - val_loss: 350743328.0000 - val_rmse: 18728.1426\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 319119264.0000 - rmse: 17863.9102 - val_loss: 357618048.0000 - val_rmse: 18910.7910\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 317608800.0000 - rmse: 17821.5820 - val_loss: 374150144.0000 - val_rmse: 19342.9609\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309974880.0000 - rmse: 17606.1035 - val_loss: 363361312.0000 - val_rmse: 19062.0391\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 292532960.0000 - rmse: 17103.5957 - val_loss: 381903360.0000 - val_rmse: 19542.3477\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 322047200.0000 - rmse: 17945.6738 - val_loss: 354427424.0000 - val_rmse: 18826.2422\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 326074848.0000 - rmse: 18057.5430 - val_loss: 352918624.0000 - val_rmse: 18786.1289\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 319882496.0000 - rmse: 17885.2598 - val_loss: 415654880.0000 - val_rmse: 20387.6152\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 319220480.0000 - rmse: 17866.7422 - val_loss: 364472128.0000 - val_rmse: 19091.1523\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 359524800.0000 - rmse: 18961.1387 - val_loss: 421428768.0000 - val_rmse: 20528.7305\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 318300288.0000 - rmse: 17840.9727 - val_loss: 362938112.0000 - val_rmse: 19050.9355\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 307588960.0000 - rmse: 17538.2148 - val_loss: 370338880.0000 - val_rmse: 19244.1914\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 325699040.0000 - rmse: 18047.1348 - val_loss: 410895968.0000 - val_rmse: 20270.5684\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 328436512.0000 - rmse: 18122.8184 - val_loss: 436901472.0000 - val_rmse: 20902.1875\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 313462208.0000 - rmse: 17704.8633 - val_loss: 431430464.0000 - val_rmse: 20770.9043\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 335079360.0000 - rmse: 18305.1738 - val_loss: 421539296.0000 - val_rmse: 20531.4219\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 285156480.0000 - rmse: 16886.5762 - val_loss: 361233184.0000 - val_rmse: 19006.1348\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 338976160.0000 - rmse: 18411.3047 - val_loss: 400411712.0000 - val_rmse: 20010.2910\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268663712.0000 - rmse: 16390.9648 - val_loss: 343176320.0000 - val_rmse: 18525.0195\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 308221152.0000 - rmse: 17556.2285 - val_loss: 357888160.0000 - val_rmse: 18917.9316\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 308172416.0000 - rmse: 17554.8398 - val_loss: 365428416.0000 - val_rmse: 19116.1816\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 291536288.0000 - rmse: 17074.4336 - val_loss: 362452256.0000 - val_rmse: 19038.1797\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 317826496.0000 - rmse: 17827.6895 - val_loss: 345615008.0000 - val_rmse: 18590.7246\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 314399872.0000 - rmse: 17731.3242 - val_loss: 365811264.0000 - val_rmse: 19126.1934\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286377600.0000 - rmse: 16922.6953 - val_loss: 358152128.0000 - val_rmse: 18924.9082\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 320230400.0000 - rmse: 17894.9824 - val_loss: 384167328.0000 - val_rmse: 19600.1875\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 291439392.0000 - rmse: 17071.5957 - val_loss: 400072832.0000 - val_rmse: 20001.8203\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 319334912.0000 - rmse: 17869.9453 - val_loss: 392283552.0000 - val_rmse: 19806.1484\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 315730304.0000 - rmse: 17768.8008 - val_loss: 391855520.0000 - val_rmse: 19795.3418\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 295021600.0000 - rmse: 17176.1934 - val_loss: 355655232.0000 - val_rmse: 18858.8242\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 267257120.0000 - rmse: 16348.0010 - val_loss: 359202496.0000 - val_rmse: 18952.6387\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 284078816.0000 - rmse: 16854.6387 - val_loss: 367839200.0000 - val_rmse: 19179.1348\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 267621088.0000 - rmse: 16359.1289 - val_loss: 420253120.0000 - val_rmse: 20500.0762\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 302857408.0000 - rmse: 17402.7988 - val_loss: 369979392.0000 - val_rmse: 19234.8477\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280584512.0000 - rmse: 16750.6562 - val_loss: 358955520.0000 - val_rmse: 18946.1211\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 284907872.0000 - rmse: 16879.2148 - val_loss: 357703776.0000 - val_rmse: 18913.0586\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 285461472.0000 - rmse: 16895.6055 - val_loss: 364321568.0000 - val_rmse: 19087.2090\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 319656384.0000 - rmse: 17878.9375 - val_loss: 359703616.0000 - val_rmse: 18965.8535\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309115104.0000 - rmse: 17581.6699 - val_loss: 360144672.0000 - val_rmse: 18977.4785\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264164112.0000 - rmse: 16253.1260 - val_loss: 345602592.0000 - val_rmse: 18590.3906\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 288626496.0000 - rmse: 16989.0117 - val_loss: 358936832.0000 - val_rmse: 18945.6289\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 271454240.0000 - rmse: 16475.8691 - val_loss: 358800224.0000 - val_rmse: 18942.0234\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309560512.0000 - rmse: 17594.3320 - val_loss: 367816224.0000 - val_rmse: 19178.5352\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299110304.0000 - rmse: 17294.8066 - val_loss: 355544032.0000 - val_rmse: 18855.8750\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 272268800.0000 - rmse: 16500.5703 - val_loss: 360949440.0000 - val_rmse: 18998.6699\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 274710304.0000 - rmse: 16574.3867 - val_loss: 371561824.0000 - val_rmse: 19275.9395\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 262289376.0000 - rmse: 16195.3506 - val_loss: 453648448.0000 - val_rmse: 21299.0254\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 296341600.0000 - rmse: 17214.5762 - val_loss: 356777920.0000 - val_rmse: 18888.5664\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 291050944.0000 - rmse: 17060.2148 - val_loss: 361494304.0000 - val_rmse: 19013.0039\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 277302848.0000 - rmse: 16652.4121 - val_loss: 382793664.0000 - val_rmse: 19565.1133\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 274197568.0000 - rmse: 16558.9121 - val_loss: 366607360.0000 - val_rmse: 19146.9941\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 307801120.0000 - rmse: 17544.2617 - val_loss: 368902944.0000 - val_rmse: 19206.8457\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 292530912.0000 - rmse: 17103.5352 - val_loss: 358152800.0000 - val_rmse: 18924.9258\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 315474464.0000 - rmse: 17761.6016 - val_loss: 348501824.0000 - val_rmse: 18668.2031\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 273881888.0000 - rmse: 16549.3770 - val_loss: 382200256.0000 - val_rmse: 19549.9434\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268645696.0000 - rmse: 16390.4141 - val_loss: 351332224.0000 - val_rmse: 18743.8574\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 296646048.0000 - rmse: 17223.4160 - val_loss: 353459424.0000 - val_rmse: 18800.5156\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 326526560.0000 - rmse: 18070.0469 - val_loss: 346580704.0000 - val_rmse: 18616.6777\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 274690912.0000 - rmse: 16573.8027 - val_loss: 365672768.0000 - val_rmse: 19122.5723\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 307800928.0000 - rmse: 17544.2559 - val_loss: 338940384.0000 - val_rmse: 18410.3340\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 265914768.0000 - rmse: 16306.8936 - val_loss: 346953088.0000 - val_rmse: 18626.6777\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 294258752.0000 - rmse: 17153.9727 - val_loss: 338462432.0000 - val_rmse: 18397.3477\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 277649600.0000 - rmse: 16662.8203 - val_loss: 455406816.0000 - val_rmse: 21340.2637\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 238730912.0000 - rmse: 15450.9199 - val_loss: 341034752.0000 - val_rmse: 18467.1270\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264505152.0000 - rmse: 16263.6143 - val_loss: 333520960.0000 - val_rmse: 18262.5566\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 294406112.0000 - rmse: 17158.2656 - val_loss: 411711136.0000 - val_rmse: 20290.6660\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 275043936.0000 - rmse: 16584.4492 - val_loss: 339191488.0000 - val_rmse: 18417.1523\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 263957008.0000 - rmse: 16246.7539 - val_loss: 384684832.0000 - val_rmse: 19613.3848\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 283780000.0000 - rmse: 16845.7715 - val_loss: 339867264.0000 - val_rmse: 18435.4883\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 273014336.0000 - rmse: 16523.1445 - val_loss: 409428640.0000 - val_rmse: 20234.3438\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 310098240.0000 - rmse: 17609.6074 - val_loss: 346602496.0000 - val_rmse: 18617.2637\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 246793952.0000 - rmse: 15709.6768 - val_loss: 444678624.0000 - val_rmse: 21087.4043\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 266727440.0000 - rmse: 16331.7920 - val_loss: 353517312.0000 - val_rmse: 18802.0566\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 265898128.0000 - rmse: 16306.3828 - val_loss: 338243552.0000 - val_rmse: 18391.3984\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 274973536.0000 - rmse: 16582.3262 - val_loss: 365723552.0000 - val_rmse: 19123.9004\n",
      "Epoch 367/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 301152672.0000 - rmse: 17353.7500 - val_loss: 334773920.0000 - val_rmse: 18296.8281\n",
      "Epoch 368/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 260792480.0000 - rmse: 16149.0703 - val_loss: 336802080.0000 - val_rmse: 18352.1680\n",
      "Epoch 369/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 215626976.0000 - rmse: 14684.2422 - val_loss: 339286496.0000 - val_rmse: 18419.7305\n",
      "Epoch 370/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 300548448.0000 - rmse: 17336.3340 - val_loss: 356773408.0000 - val_rmse: 18888.4473\n",
      "Epoch 371/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264435888.0000 - rmse: 16261.4844 - val_loss: 335928736.0000 - val_rmse: 18328.3594\n",
      "Epoch 372/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 266798688.0000 - rmse: 16333.9736 - val_loss: 354728256.0000 - val_rmse: 18834.2305\n",
      "Epoch 373/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 281192544.0000 - rmse: 16768.7969 - val_loss: 340333472.0000 - val_rmse: 18448.1289\n",
      "Epoch 374/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280513440.0000 - rmse: 16748.5352 - val_loss: 329749664.0000 - val_rmse: 18159.0098\n",
      "Epoch 375/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 255798064.0000 - rmse: 15993.6885 - val_loss: 342986784.0000 - val_rmse: 18519.9023\n",
      "Epoch 376/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 246596848.0000 - rmse: 15703.4023 - val_loss: 322257088.0000 - val_rmse: 17951.5195\n",
      "Epoch 377/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 263541200.0000 - rmse: 16233.9521 - val_loss: 342912096.0000 - val_rmse: 18517.8867\n",
      "Epoch 378/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 243184608.0000 - rmse: 15594.3779 - val_loss: 324853728.0000 - val_rmse: 18023.6992\n",
      "Epoch 379/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 238822000.0000 - rmse: 15453.8672 - val_loss: 335429824.0000 - val_rmse: 18314.7441\n",
      "Epoch 380/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 243408752.0000 - rmse: 15601.5625 - val_loss: 327930752.0000 - val_rmse: 18108.8574\n",
      "Epoch 381/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268203632.0000 - rmse: 16376.9238 - val_loss: 337829600.0000 - val_rmse: 18380.1406\n",
      "Epoch 382/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 259661632.0000 - rmse: 16114.0195 - val_loss: 318294400.0000 - val_rmse: 17840.8066\n",
      "Epoch 383/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 238118560.0000 - rmse: 15431.0908 - val_loss: 326278624.0000 - val_rmse: 18063.1836\n",
      "Epoch 384/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 233883344.0000 - rmse: 15293.2451 - val_loss: 347538752.0000 - val_rmse: 18642.3906\n",
      "Epoch 385/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 280859968.0000 - rmse: 16758.8770 - val_loss: 325037184.0000 - val_rmse: 18028.7871\n",
      "Epoch 386/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 251965072.0000 - rmse: 15873.4072 - val_loss: 348299488.0000 - val_rmse: 18662.7832\n",
      "Epoch 387/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264353376.0000 - rmse: 16258.9473 - val_loss: 338995712.0000 - val_rmse: 18411.8359\n",
      "Epoch 388/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 274643904.0000 - rmse: 16572.3828 - val_loss: 328451616.0000 - val_rmse: 18123.2344\n",
      "Epoch 389/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 243252688.0000 - rmse: 15596.5605 - val_loss: 330579936.0000 - val_rmse: 18181.8574\n",
      "Epoch 390/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 258964768.0000 - rmse: 16092.3818 - val_loss: 333182496.0000 - val_rmse: 18253.2871\n",
      "Epoch 391/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 271753408.0000 - rmse: 16484.9453 - val_loss: 344020448.0000 - val_rmse: 18547.7891\n",
      "Epoch 392/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 276475200.0000 - rmse: 16627.5430 - val_loss: 330796544.0000 - val_rmse: 18187.8125\n",
      "Epoch 393/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 292589856.0000 - rmse: 17105.2578 - val_loss: 341699520.0000 - val_rmse: 18485.1152\n",
      "Epoch 394/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 241437616.0000 - rmse: 15538.2627 - val_loss: 326957440.0000 - val_rmse: 18081.9648\n",
      "Epoch 395/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 282008288.0000 - rmse: 16793.1016 - val_loss: 328008512.0000 - val_rmse: 18111.0059\n",
      "Epoch 396/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 250112144.0000 - rmse: 15814.9346 - val_loss: 319712992.0000 - val_rmse: 17880.5195\n",
      "Epoch 397/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 287224096.0000 - rmse: 16947.6875 - val_loss: 335700128.0000 - val_rmse: 18322.1211\n",
      "Epoch 398/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 225813936.0000 - rmse: 15027.1064 - val_loss: 323963488.0000 - val_rmse: 17998.9863\n",
      "Epoch 399/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 265597648.0000 - rmse: 16297.1670 - val_loss: 319253600.0000 - val_rmse: 17867.6699\n",
      "Epoch 400/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 234746768.0000 - rmse: 15321.4482 - val_loss: 336835424.0000 - val_rmse: 18353.0762\n",
      "104/104 [==============================] - 0s 810us/step - loss: 599811200.0000 - rmse: 24491.0430\n",
      "[599811200.0, 24491.04296875]\n",
      "<src.model.emb_model object at 0x7f846442f390>\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 20526061568.0000 - rmse: 143269.1875 - val_loss: 2668288768.0000 - val_rmse: 51655.4805\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2843836160.0000 - rmse: 53327.6289 - val_loss: 1490602624.0000 - val_rmse: 38608.3242\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 2108875136.0000 - rmse: 45922.4922 - val_loss: 1402694272.0000 - val_rmse: 37452.5586\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1961682304.0000 - rmse: 44290.8828 - val_loss: 1307876608.0000 - val_rmse: 36164.5781\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1970409344.0000 - rmse: 44389.2930 - val_loss: 1229192448.0000 - val_rmse: 35059.8398\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1952961920.0000 - rmse: 44192.3281 - val_loss: 1205680000.0000 - val_rmse: 34722.9023\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1860113152.0000 - rmse: 43129.0273 - val_loss: 1225210112.0000 - val_rmse: 35003.0000\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1738839424.0000 - rmse: 41699.3945 - val_loss: 1101552256.0000 - val_rmse: 33189.6406\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1727559040.0000 - rmse: 41563.9141 - val_loss: 1044053376.0000 - val_rmse: 32311.8145\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1684716032.0000 - rmse: 41045.2930 - val_loss: 1274547968.0000 - val_rmse: 35700.8125\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1578761472.0000 - rmse: 39733.6328 - val_loss: 1010322048.0000 - val_rmse: 31785.5645\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1562897920.0000 - rmse: 39533.5039 - val_loss: 1030029760.0000 - val_rmse: 32094.0762\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1554840064.0000 - rmse: 39431.4609 - val_loss: 918797760.0000 - val_rmse: 30311.6777\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1494144128.0000 - rmse: 38654.1602 - val_loss: 936469120.0000 - val_rmse: 30601.7832\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1464290816.0000 - rmse: 38266.0547 - val_loss: 929496000.0000 - val_rmse: 30487.6367\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1401536896.0000 - rmse: 37437.1055 - val_loss: 899054720.0000 - val_rmse: 29984.2402\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1342855552.0000 - rmse: 36644.9922 - val_loss: 824741952.0000 - val_rmse: 28718.3203\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1482835328.0000 - rmse: 38507.6016 - val_loss: 928893696.0000 - val_rmse: 30477.7578\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1301018368.0000 - rmse: 36069.6328 - val_loss: 865999680.0000 - val_rmse: 29427.8730\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1323252352.0000 - rmse: 36376.5352 - val_loss: 800803008.0000 - val_rmse: 28298.4629\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1264738944.0000 - rmse: 35563.1680 - val_loss: 781938944.0000 - val_rmse: 27963.1719\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1306047360.0000 - rmse: 36139.2773 - val_loss: 1022755776.0000 - val_rmse: 31980.5527\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1283744512.0000 - rmse: 35829.3789 - val_loss: 1263451776.0000 - val_rmse: 35545.0664\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1234758656.0000 - rmse: 35139.1328 - val_loss: 826005504.0000 - val_rmse: 28740.3105\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1141442048.0000 - rmse: 33785.2344 - val_loss: 914717184.0000 - val_rmse: 30244.2910\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1207120768.0000 - rmse: 34743.6445 - val_loss: 767173184.0000 - val_rmse: 27697.8906\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1174222720.0000 - rmse: 34266.9336 - val_loss: 825251200.0000 - val_rmse: 28727.1855\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1215010816.0000 - rmse: 34857.0039 - val_loss: 718246528.0000 - val_rmse: 26800.1211\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1087150976.0000 - rmse: 32971.9727 - val_loss: 800057472.0000 - val_rmse: 28285.2871\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1185476352.0000 - rmse: 34430.7461 - val_loss: 726382080.0000 - val_rmse: 26951.4766\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1028589184.0000 - rmse: 32071.6250 - val_loss: 835196288.0000 - val_rmse: 28899.7637\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1107655296.0000 - rmse: 33281.4570 - val_loss: 735272768.0000 - val_rmse: 27115.9141\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1081249664.0000 - rmse: 32882.3594 - val_loss: 728621376.0000 - val_rmse: 26992.9883\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1039495872.0000 - rmse: 32241.2148 - val_loss: 911852736.0000 - val_rmse: 30196.9004\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1095944832.0000 - rmse: 33105.0586 - val_loss: 689962240.0000 - val_rmse: 26267.1328\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1085550208.0000 - rmse: 32947.6875 - val_loss: 809477440.0000 - val_rmse: 28451.3164\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1030945472.0000 - rmse: 32108.3398 - val_loss: 798343680.0000 - val_rmse: 28254.9766\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1085605248.0000 - rmse: 32948.5234 - val_loss: 695362880.0000 - val_rmse: 26369.7344\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 999694912.0000 - rmse: 31617.9531 - val_loss: 701141568.0000 - val_rmse: 26479.0781\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1035787968.0000 - rmse: 32183.6602 - val_loss: 809146752.0000 - val_rmse: 28445.5059\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1051794304.0000 - rmse: 32431.3789 - val_loss: 655769536.0000 - val_rmse: 25607.9980\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1033371200.0000 - rmse: 32146.0918 - val_loss: 670878592.0000 - val_rmse: 25901.3242\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 995323328.0000 - rmse: 31548.7461 - val_loss: 633174848.0000 - val_rmse: 25162.9648\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1010418496.0000 - rmse: 31787.0801 - val_loss: 641574208.0000 - val_rmse: 25329.3145\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 973027072.0000 - rmse: 31193.3828 - val_loss: 609888448.0000 - val_rmse: 24695.9199\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 957802112.0000 - rmse: 30948.3789 - val_loss: 645028672.0000 - val_rmse: 25397.4141\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 926283712.0000 - rmse: 30434.9102 - val_loss: 648416000.0000 - val_rmse: 25464.0137\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 893345984.0000 - rmse: 29888.8945 - val_loss: 642852416.0000 - val_rmse: 25354.5352\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1013504448.0000 - rmse: 31835.5840 - val_loss: 727533824.0000 - val_rmse: 26972.8340\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 979875456.0000 - rmse: 31302.9629 - val_loss: 617543552.0000 - val_rmse: 24850.4238\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 933420416.0000 - rmse: 30551.9297 - val_loss: 659627456.0000 - val_rmse: 25683.2129\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 992716544.0000 - rmse: 31507.4043 - val_loss: 678473536.0000 - val_rmse: 26047.5254\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 1011804416.0000 - rmse: 31808.8730 - val_loss: 748273920.0000 - val_rmse: 27354.5957\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 974022976.0000 - rmse: 31209.3418 - val_loss: 598526336.0000 - val_rmse: 24464.7988\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 937797888.0000 - rmse: 30623.4863 - val_loss: 621312320.0000 - val_rmse: 24926.1367\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 893387328.0000 - rmse: 29889.5859 - val_loss: 607380544.0000 - val_rmse: 24645.0918\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 931267264.0000 - rmse: 30516.6719 - val_loss: 588011968.0000 - val_rmse: 24248.9590\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 862630208.0000 - rmse: 29370.5664 - val_loss: 656623424.0000 - val_rmse: 25624.6641\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 945571328.0000 - rmse: 30750.1426 - val_loss: 573464192.0000 - val_rmse: 23947.1133\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 881724800.0000 - rmse: 29693.8516 - val_loss: 599310656.0000 - val_rmse: 24480.8223\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 923517760.0000 - rmse: 30389.4355 - val_loss: 591595904.0000 - val_rmse: 24322.7441\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 907532608.0000 - rmse: 30125.2812 - val_loss: 601360448.0000 - val_rmse: 24522.6523\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 994077696.0000 - rmse: 31528.9980 - val_loss: 606744320.0000 - val_rmse: 24632.1797\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 898198336.0000 - rmse: 29969.9570 - val_loss: 627811456.0000 - val_rmse: 25056.1660\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 829827968.0000 - rmse: 28806.7344 - val_loss: 630928064.0000 - val_rmse: 25118.2812\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 903489920.0000 - rmse: 30058.1094 - val_loss: 581598528.0000 - val_rmse: 24116.3535\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 807458752.0000 - rmse: 28415.8184 - val_loss: 1033203136.0000 - val_rmse: 32143.4766\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 914270464.0000 - rmse: 30236.9062 - val_loss: 568723904.0000 - val_rmse: 23847.9336\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 837590784.0000 - rmse: 28941.1602 - val_loss: 551242112.0000 - val_rmse: 23478.5449\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 823320128.0000 - rmse: 28693.5547 - val_loss: 574321408.0000 - val_rmse: 23965.0039\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 861480128.0000 - rmse: 29350.9824 - val_loss: 622175360.0000 - val_rmse: 24943.4434\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 803850368.0000 - rmse: 28352.2559 - val_loss: 553093184.0000 - val_rmse: 23517.9336\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 898227968.0000 - rmse: 29970.4512 - val_loss: 587750592.0000 - val_rmse: 24243.5684\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 822988544.0000 - rmse: 28687.7773 - val_loss: 594429632.0000 - val_rmse: 24380.9277\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 833370112.0000 - rmse: 28868.1504 - val_loss: 615549632.0000 - val_rmse: 24810.2734\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 779742656.0000 - rmse: 27923.8730 - val_loss: 546833600.0000 - val_rmse: 23384.4727\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 830752576.0000 - rmse: 28822.7793 - val_loss: 552650752.0000 - val_rmse: 23508.5254\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 784413056.0000 - rmse: 28007.3750 - val_loss: 545335680.0000 - val_rmse: 23352.4238\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 783203584.0000 - rmse: 27985.7754 - val_loss: 543255552.0000 - val_rmse: 23307.8438\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 753154560.0000 - rmse: 27443.6621 - val_loss: 635390784.0000 - val_rmse: 25206.9590\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 804534656.0000 - rmse: 28364.3203 - val_loss: 568950208.0000 - val_rmse: 23852.6777\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 828779392.0000 - rmse: 28788.5293 - val_loss: 560423424.0000 - val_rmse: 23673.2637\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 765250944.0000 - rmse: 27663.1699 - val_loss: 534954432.0000 - val_rmse: 23129.0820\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 787839680.0000 - rmse: 28068.4824 - val_loss: 555695616.0000 - val_rmse: 23573.1973\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 730576384.0000 - rmse: 27029.1758 - val_loss: 568847872.0000 - val_rmse: 23850.5312\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 778863616.0000 - rmse: 27908.1289 - val_loss: 539489216.0000 - val_rmse: 23226.9062\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 768548992.0000 - rmse: 27722.7168 - val_loss: 551599680.0000 - val_rmse: 23486.1602\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 825541696.0000 - rmse: 28732.2422 - val_loss: 613516096.0000 - val_rmse: 24769.2578\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 736898432.0000 - rmse: 27145.8730 - val_loss: 538327104.0000 - val_rmse: 23201.8770\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 746912896.0000 - rmse: 27329.7070 - val_loss: 547264192.0000 - val_rmse: 23393.6777\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 759975488.0000 - rmse: 27567.6523 - val_loss: 588155072.0000 - val_rmse: 24251.9082\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 788025344.0000 - rmse: 28071.7891 - val_loss: 523971744.0000 - val_rmse: 22890.4297\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 812657664.0000 - rmse: 28507.1504 - val_loss: 597847104.0000 - val_rmse: 24450.9121\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 731928512.0000 - rmse: 27054.1777 - val_loss: 722617152.0000 - val_rmse: 26881.5391\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 757267328.0000 - rmse: 27518.4902 - val_loss: 601569152.0000 - val_rmse: 24526.9062\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 757718016.0000 - rmse: 27526.6777 - val_loss: 525725440.0000 - val_rmse: 22928.7031\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 741514624.0000 - rmse: 27230.7656 - val_loss: 583438144.0000 - val_rmse: 24154.4648\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 721896000.0000 - rmse: 26868.1230 - val_loss: 551136832.0000 - val_rmse: 23476.3027\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 761116864.0000 - rmse: 27588.3457 - val_loss: 528854240.0000 - val_rmse: 22996.8320\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 719312000.0000 - rmse: 26819.9922 - val_loss: 519143456.0000 - val_rmse: 22784.7207\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 719675136.0000 - rmse: 26826.7617 - val_loss: 531056288.0000 - val_rmse: 23044.6582\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 701592512.0000 - rmse: 26487.5918 - val_loss: 511736992.0000 - val_rmse: 22621.6055\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 695072128.0000 - rmse: 26364.2207 - val_loss: 558692224.0000 - val_rmse: 23636.6719\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 728170304.0000 - rmse: 26984.6309 - val_loss: 524590592.0000 - val_rmse: 22903.9434\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 754146112.0000 - rmse: 27461.7207 - val_loss: 514899104.0000 - val_rmse: 22691.3887\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 677012288.0000 - rmse: 26019.4590 - val_loss: 527506432.0000 - val_rmse: 22967.5078\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 659638848.0000 - rmse: 25683.4355 - val_loss: 507907008.0000 - val_rmse: 22536.7930\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 716477632.0000 - rmse: 26767.0996 - val_loss: 532869088.0000 - val_rmse: 23083.9570\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 734045440.0000 - rmse: 27093.2734 - val_loss: 521756256.0000 - val_rmse: 22841.9844\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 687635264.0000 - rmse: 26222.8008 - val_loss: 514655072.0000 - val_rmse: 22686.0098\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 640242880.0000 - rmse: 25303.0215 - val_loss: 519888032.0000 - val_rmse: 22801.0527\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 696310656.0000 - rmse: 26387.6992 - val_loss: 530604192.0000 - val_rmse: 23034.8477\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 639478720.0000 - rmse: 25287.9160 - val_loss: 495264032.0000 - val_rmse: 22254.5293\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 694656640.0000 - rmse: 26356.3398 - val_loss: 515049120.0000 - val_rmse: 22694.6934\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 702586304.0000 - rmse: 26506.3438 - val_loss: 512515872.0000 - val_rmse: 22638.8125\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 633561280.0000 - rmse: 25170.6426 - val_loss: 505354048.0000 - val_rmse: 22480.0820\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 602997440.0000 - rmse: 24556.0059 - val_loss: 524587520.0000 - val_rmse: 22903.8750\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 653533184.0000 - rmse: 25564.2949 - val_loss: 516704768.0000 - val_rmse: 22731.1406\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 652055616.0000 - rmse: 25535.3789 - val_loss: 530504672.0000 - val_rmse: 23032.6875\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 633189760.0000 - rmse: 25163.2617 - val_loss: 505357664.0000 - val_rmse: 22480.1621\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 637506752.0000 - rmse: 25248.8965 - val_loss: 516665920.0000 - val_rmse: 22730.2871\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 666722496.0000 - rmse: 25820.9707 - val_loss: 497075424.0000 - val_rmse: 22295.1875\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 594913792.0000 - rmse: 24390.8555 - val_loss: 501088512.0000 - val_rmse: 22385.0059\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 650068736.0000 - rmse: 25496.4453 - val_loss: 522595008.0000 - val_rmse: 22860.3379\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 664480000.0000 - rmse: 25777.5098 - val_loss: 773506752.0000 - val_rmse: 27811.9902\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 593105600.0000 - rmse: 24353.7598 - val_loss: 501403968.0000 - val_rmse: 22392.0508\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 617653184.0000 - rmse: 24852.6289 - val_loss: 578659008.0000 - val_rmse: 24055.3320\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 621500992.0000 - rmse: 24929.9219 - val_loss: 523756960.0000 - val_rmse: 22885.7363\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 600394752.0000 - rmse: 24502.9531 - val_loss: 495076128.0000 - val_rmse: 22250.3066\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 629296384.0000 - rmse: 25085.7812 - val_loss: 587482752.0000 - val_rmse: 24238.0430\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 596876736.0000 - rmse: 24431.0605 - val_loss: 517451712.0000 - val_rmse: 22747.5645\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 610658112.0000 - rmse: 24711.4980 - val_loss: 521275520.0000 - val_rmse: 22831.4590\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 632531840.0000 - rmse: 25150.1855 - val_loss: 520422176.0000 - val_rmse: 22812.7637\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 618517120.0000 - rmse: 24870.0039 - val_loss: 546824320.0000 - val_rmse: 23384.2754\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 677987200.0000 - rmse: 26038.1875 - val_loss: 508763456.0000 - val_rmse: 22555.7852\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 586620224.0000 - rmse: 24220.2441 - val_loss: 509505440.0000 - val_rmse: 22572.2266\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 615726080.0000 - rmse: 24813.8281 - val_loss: 502370976.0000 - val_rmse: 22413.6328\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 584297344.0000 - rmse: 24172.2441 - val_loss: 487396352.0000 - val_rmse: 22077.0547\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 574972672.0000 - rmse: 23978.5879 - val_loss: 509600640.0000 - val_rmse: 22574.3359\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 584094336.0000 - rmse: 24168.0430 - val_loss: 525169728.0000 - val_rmse: 22916.5820\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 542423744.0000 - rmse: 23289.9922 - val_loss: 516908960.0000 - val_rmse: 22735.6328\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 597597440.0000 - rmse: 24445.8066 - val_loss: 507830624.0000 - val_rmse: 22535.0977\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 544115968.0000 - rmse: 23326.2930 - val_loss: 517874016.0000 - val_rmse: 22756.8457\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 579259712.0000 - rmse: 24067.8145 - val_loss: 582418816.0000 - val_rmse: 24133.3555\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 619401728.0000 - rmse: 24887.7832 - val_loss: 478472128.0000 - val_rmse: 21874.0059\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 597299584.0000 - rmse: 24439.7129 - val_loss: 485282272.0000 - val_rmse: 22029.1230\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 583839680.0000 - rmse: 24162.7754 - val_loss: 507766848.0000 - val_rmse: 22533.6816\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 630532032.0000 - rmse: 25110.3965 - val_loss: 490391680.0000 - val_rmse: 22144.7891\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 567374464.0000 - rmse: 23819.6230 - val_loss: 481003040.0000 - val_rmse: 21931.7812\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 613690304.0000 - rmse: 24772.7734 - val_loss: 492209408.0000 - val_rmse: 22185.7930\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 593505792.0000 - rmse: 24361.9746 - val_loss: 483490400.0000 - val_rmse: 21988.4160\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 595410176.0000 - rmse: 24401.0273 - val_loss: 536181248.0000 - val_rmse: 23155.5879\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 563274496.0000 - rmse: 23733.4043 - val_loss: 539358016.0000 - val_rmse: 23224.0820\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 604192704.0000 - rmse: 24580.3320 - val_loss: 494391488.0000 - val_rmse: 22234.9160\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 592811776.0000 - rmse: 24347.7266 - val_loss: 518412960.0000 - val_rmse: 22768.6836\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 529425152.0000 - rmse: 23009.2402 - val_loss: 465443296.0000 - val_rmse: 21574.1348\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 552725632.0000 - rmse: 23510.1172 - val_loss: 477289920.0000 - val_rmse: 21846.9668\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 541788224.0000 - rmse: 23276.3457 - val_loss: 502516704.0000 - val_rmse: 22416.8848\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 606649792.0000 - rmse: 24630.2617 - val_loss: 480059168.0000 - val_rmse: 21910.2520\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 543737664.0000 - rmse: 23318.1836 - val_loss: 486456320.0000 - val_rmse: 22055.7539\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 531502048.0000 - rmse: 23054.3281 - val_loss: 529203872.0000 - val_rmse: 23004.4316\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 584319360.0000 - rmse: 24172.6992 - val_loss: 498089088.0000 - val_rmse: 22317.9102\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 561174208.0000 - rmse: 23689.1152 - val_loss: 494697024.0000 - val_rmse: 22241.7852\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 528881312.0000 - rmse: 22997.4199 - val_loss: 480002880.0000 - val_rmse: 21908.9688\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 593247488.0000 - rmse: 24356.6719 - val_loss: 528169600.0000 - val_rmse: 22981.9414\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 484292480.0000 - rmse: 22006.6465 - val_loss: 494378272.0000 - val_rmse: 22234.6191\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 520277696.0000 - rmse: 22809.5957 - val_loss: 492327328.0000 - val_rmse: 22188.4512\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 532722592.0000 - rmse: 23080.7832 - val_loss: 472861632.0000 - val_rmse: 21745.3828\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 544801728.0000 - rmse: 23340.9883 - val_loss: 475333184.0000 - val_rmse: 21802.1367\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 543662464.0000 - rmse: 23316.5703 - val_loss: 509605408.0000 - val_rmse: 22574.4414\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 506282816.0000 - rmse: 22500.7285 - val_loss: 502352576.0000 - val_rmse: 22413.2227\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 559182848.0000 - rmse: 23647.0469 - val_loss: 499376704.0000 - val_rmse: 22346.7383\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 533678752.0000 - rmse: 23101.4883 - val_loss: 479515968.0000 - val_rmse: 21897.8535\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 559317888.0000 - rmse: 23649.9023 - val_loss: 484455840.0000 - val_rmse: 22010.3574\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 514302368.0000 - rmse: 22678.2363 - val_loss: 492853280.0000 - val_rmse: 22200.2988\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 475959520.0000 - rmse: 21816.4961 - val_loss: 501158560.0000 - val_rmse: 22386.5703\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 533109248.0000 - rmse: 23089.1582 - val_loss: 486488704.0000 - val_rmse: 22056.4883\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 518374720.0000 - rmse: 22767.8438 - val_loss: 530861888.0000 - val_rmse: 23040.4395\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 514037728.0000 - rmse: 22672.4004 - val_loss: 494912768.0000 - val_rmse: 22246.6348\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 515661408.0000 - rmse: 22708.1797 - val_loss: 495878304.0000 - val_rmse: 22268.3242\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 542271936.0000 - rmse: 23286.7324 - val_loss: 497833664.0000 - val_rmse: 22312.1855\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 532126656.0000 - rmse: 23067.8711 - val_loss: 507758400.0000 - val_rmse: 22533.4941\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 489722464.0000 - rmse: 22129.6738 - val_loss: 565437504.0000 - val_rmse: 23778.9297\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 486960736.0000 - rmse: 22067.1875 - val_loss: 489515456.0000 - val_rmse: 22124.9961\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 474917952.0000 - rmse: 21792.6133 - val_loss: 545040640.0000 - val_rmse: 23346.1055\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 503788960.0000 - rmse: 22445.2441 - val_loss: 548321472.0000 - val_rmse: 23416.2656\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 483487328.0000 - rmse: 21988.3457 - val_loss: 460043456.0000 - val_rmse: 21448.6230\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 535371424.0000 - rmse: 23138.0938 - val_loss: 490416640.0000 - val_rmse: 22145.3535\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 463148416.0000 - rmse: 21520.8828 - val_loss: 522562848.0000 - val_rmse: 22859.6328\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 448143776.0000 - rmse: 21169.4062 - val_loss: 486174720.0000 - val_rmse: 22049.3691\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 496335520.0000 - rmse: 22278.5879 - val_loss: 478870432.0000 - val_rmse: 21883.1074\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 535523616.0000 - rmse: 23141.3828 - val_loss: 495384096.0000 - val_rmse: 22257.2266\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 522064032.0000 - rmse: 22848.7207 - val_loss: 470087136.0000 - val_rmse: 21681.4922\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 468549984.0000 - rmse: 21646.0156 - val_loss: 447699008.0000 - val_rmse: 21158.8984\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 467541696.0000 - rmse: 21622.7129 - val_loss: 501812192.0000 - val_rmse: 22401.1641\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 436745024.0000 - rmse: 20898.4453 - val_loss: 472441248.0000 - val_rmse: 21735.7129\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 486622976.0000 - rmse: 22059.5332 - val_loss: 471018912.0000 - val_rmse: 21702.9707\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 446046784.0000 - rmse: 21119.8203 - val_loss: 462597024.0000 - val_rmse: 21508.0684\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 551281984.0000 - rmse: 23479.3945 - val_loss: 473714304.0000 - val_rmse: 21764.9785\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 446799840.0000 - rmse: 21137.6406 - val_loss: 483349696.0000 - val_rmse: 21985.2148\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 485345984.0000 - rmse: 22030.5684 - val_loss: 502683936.0000 - val_rmse: 22420.6133\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 487938048.0000 - rmse: 22089.3203 - val_loss: 455380576.0000 - val_rmse: 21339.6484\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 456025920.0000 - rmse: 21354.7637 - val_loss: 479347872.0000 - val_rmse: 21894.0137\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 435977952.0000 - rmse: 20880.0859 - val_loss: 515767968.0000 - val_rmse: 22710.5254\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 482194400.0000 - rmse: 21958.9258 - val_loss: 485179168.0000 - val_rmse: 22026.7832\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 431037440.0000 - rmse: 20761.4414 - val_loss: 488998432.0000 - val_rmse: 22113.3086\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 451056896.0000 - rmse: 21238.0996 - val_loss: 452883616.0000 - val_rmse: 21281.0625\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 472259584.0000 - rmse: 21731.5352 - val_loss: 594213888.0000 - val_rmse: 24376.5020\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 448493056.0000 - rmse: 21177.6543 - val_loss: 516269152.0000 - val_rmse: 22721.5566\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 465236480.0000 - rmse: 21569.3418 - val_loss: 578288064.0000 - val_rmse: 24047.6211\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 484129632.0000 - rmse: 22002.9453 - val_loss: 468724096.0000 - val_rmse: 21650.0371\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 481887520.0000 - rmse: 21951.9375 - val_loss: 468259328.0000 - val_rmse: 21639.3008\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 465224928.0000 - rmse: 21569.0742 - val_loss: 508457760.0000 - val_rmse: 22549.0078\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 471223200.0000 - rmse: 21707.6758 - val_loss: 501476768.0000 - val_rmse: 22393.6777\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 437582048.0000 - rmse: 20918.4609 - val_loss: 461163520.0000 - val_rmse: 21474.7188\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 461029088.0000 - rmse: 21471.5879 - val_loss: 491850656.0000 - val_rmse: 22177.7070\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 447176640.0000 - rmse: 21146.5508 - val_loss: 441744224.0000 - val_rmse: 21017.7129\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 473658816.0000 - rmse: 21763.7031 - val_loss: 451552032.0000 - val_rmse: 21249.7539\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 455067360.0000 - rmse: 21332.3086 - val_loss: 433292832.0000 - val_rmse: 20815.6875\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 474967488.0000 - rmse: 21793.7480 - val_loss: 512991328.0000 - val_rmse: 22649.3125\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 441389792.0000 - rmse: 21009.2793 - val_loss: 491406144.0000 - val_rmse: 22167.6816\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 471116640.0000 - rmse: 21705.2207 - val_loss: 450145280.0000 - val_rmse: 21216.6270\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 422193760.0000 - rmse: 20547.3535 - val_loss: 445638624.0000 - val_rmse: 21110.1543\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 437261824.0000 - rmse: 20910.8066 - val_loss: 433945376.0000 - val_rmse: 20831.3555\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 399559232.0000 - rmse: 19988.9785 - val_loss: 458456480.0000 - val_rmse: 21411.5977\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 404094176.0000 - rmse: 20102.0938 - val_loss: 438908800.0000 - val_rmse: 20950.1504\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 436723904.0000 - rmse: 20897.9395 - val_loss: 535167456.0000 - val_rmse: 23133.6875\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 421157632.0000 - rmse: 20522.1250 - val_loss: 427073248.0000 - val_rmse: 20665.7500\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 418303648.0000 - rmse: 20452.4727 - val_loss: 435639584.0000 - val_rmse: 20871.9805\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 417370240.0000 - rmse: 20429.6406 - val_loss: 521789888.0000 - val_rmse: 22842.7207\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 409539136.0000 - rmse: 20237.0742 - val_loss: 465869888.0000 - val_rmse: 21584.0195\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 442859200.0000 - rmse: 21044.2207 - val_loss: 490119296.0000 - val_rmse: 22138.6387\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 447021376.0000 - rmse: 21142.8809 - val_loss: 426901696.0000 - val_rmse: 20661.5996\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 414877248.0000 - rmse: 20368.5352 - val_loss: 408730432.0000 - val_rmse: 20217.0820\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 470307872.0000 - rmse: 21686.5820 - val_loss: 556328256.0000 - val_rmse: 23586.6113\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 428536096.0000 - rmse: 20701.1133 - val_loss: 461757472.0000 - val_rmse: 21488.5430\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 434092128.0000 - rmse: 20834.8770 - val_loss: 446059296.0000 - val_rmse: 21120.1152\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 428990688.0000 - rmse: 20712.0898 - val_loss: 455035424.0000 - val_rmse: 21331.5586\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 363619712.0000 - rmse: 19068.8145 - val_loss: 469368736.0000 - val_rmse: 21664.9199\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 390365632.0000 - rmse: 19757.6719 - val_loss: 491070016.0000 - val_rmse: 22160.0996\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 427861760.0000 - rmse: 20684.8203 - val_loss: 469035008.0000 - val_rmse: 21657.2168\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 411380736.0000 - rmse: 20282.5234 - val_loss: 454107296.0000 - val_rmse: 21309.7930\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 386340192.0000 - rmse: 19655.5391 - val_loss: 461714144.0000 - val_rmse: 21487.5352\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 384888032.0000 - rmse: 19618.5625 - val_loss: 430015008.0000 - val_rmse: 20736.8027\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 403408064.0000 - rmse: 20085.0215 - val_loss: 414905184.0000 - val_rmse: 20369.2207\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 363788032.0000 - rmse: 19073.2285 - val_loss: 424814176.0000 - val_rmse: 20611.0215\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 383664736.0000 - rmse: 19587.3613 - val_loss: 452948448.0000 - val_rmse: 21282.5859\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 404663680.0000 - rmse: 20116.2539 - val_loss: 431062720.0000 - val_rmse: 20762.0508\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 402853312.0000 - rmse: 20071.2051 - val_loss: 434730976.0000 - val_rmse: 20850.2031\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 403214496.0000 - rmse: 20080.2012 - val_loss: 478795456.0000 - val_rmse: 21881.3945\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 397350688.0000 - rmse: 19933.6562 - val_loss: 475488000.0000 - val_rmse: 21805.6875\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 390853792.0000 - rmse: 19770.0234 - val_loss: 527889888.0000 - val_rmse: 22975.8535\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 406355136.0000 - rmse: 20158.2520 - val_loss: 411401920.0000 - val_rmse: 20283.0449\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 387204192.0000 - rmse: 19677.5039 - val_loss: 403786176.0000 - val_rmse: 20094.4316\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 414667616.0000 - rmse: 20363.3887 - val_loss: 433707680.0000 - val_rmse: 20825.6504\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 403703264.0000 - rmse: 20092.3691 - val_loss: 419090048.0000 - val_rmse: 20471.6895\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 374724384.0000 - rmse: 19357.7988 - val_loss: 399246560.0000 - val_rmse: 19981.1543\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 398562592.0000 - rmse: 19964.0332 - val_loss: 408581920.0000 - val_rmse: 20213.4102\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 391008416.0000 - rmse: 19773.9336 - val_loss: 416868928.0000 - val_rmse: 20417.3691\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 388882880.0000 - rmse: 19720.1133 - val_loss: 422183104.0000 - val_rmse: 20547.0957\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 398811136.0000 - rmse: 19970.2559 - val_loss: 420986528.0000 - val_rmse: 20517.9570\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 387583680.0000 - rmse: 19687.1445 - val_loss: 426049120.0000 - val_rmse: 20640.9570\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 374857120.0000 - rmse: 19361.2266 - val_loss: 411248832.0000 - val_rmse: 20279.2715\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 366895616.0000 - rmse: 19154.5195 - val_loss: 399674688.0000 - val_rmse: 19991.8652\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 396566272.0000 - rmse: 19913.9727 - val_loss: 410763456.0000 - val_rmse: 20267.3008\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 383306592.0000 - rmse: 19578.2168 - val_loss: 423770304.0000 - val_rmse: 20585.6816\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 415994368.0000 - rmse: 20395.9395 - val_loss: 457324032.0000 - val_rmse: 21385.1367\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 375925248.0000 - rmse: 19388.7910 - val_loss: 393393952.0000 - val_rmse: 19834.1621\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 343440576.0000 - rmse: 18532.1504 - val_loss: 381182528.0000 - val_rmse: 19523.8965\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 375898976.0000 - rmse: 19388.1152 - val_loss: 428296192.0000 - val_rmse: 20695.3184\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 367825504.0000 - rmse: 19178.7773 - val_loss: 396083232.0000 - val_rmse: 19901.8398\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 369223808.0000 - rmse: 19215.1973 - val_loss: 397409568.0000 - val_rmse: 19935.1348\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 357917312.0000 - rmse: 18918.7031 - val_loss: 446647520.0000 - val_rmse: 21134.0371\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 400257408.0000 - rmse: 20006.4336 - val_loss: 420557024.0000 - val_rmse: 20507.4863\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 364495424.0000 - rmse: 19091.7637 - val_loss: 406227808.0000 - val_rmse: 20155.0938\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 348282080.0000 - rmse: 18662.3164 - val_loss: 475184416.0000 - val_rmse: 21798.7246\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 354685312.0000 - rmse: 18833.0918 - val_loss: 480801024.0000 - val_rmse: 21927.1758\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 379225632.0000 - rmse: 19473.7168 - val_loss: 450230976.0000 - val_rmse: 21218.6465\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 416019680.0000 - rmse: 20396.5605 - val_loss: 415980256.0000 - val_rmse: 20395.5938\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 376326176.0000 - rmse: 19399.1289 - val_loss: 408664288.0000 - val_rmse: 20215.4473\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 341608928.0000 - rmse: 18482.6660 - val_loss: 422327904.0000 - val_rmse: 20550.6172\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 381307040.0000 - rmse: 19527.0840 - val_loss: 426076352.0000 - val_rmse: 20641.6172\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 305699680.0000 - rmse: 17484.2695 - val_loss: 429857600.0000 - val_rmse: 20733.0078\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 354212352.0000 - rmse: 18820.5293 - val_loss: 443570112.0000 - val_rmse: 21061.1035\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 363603936.0000 - rmse: 19068.4023 - val_loss: 389557280.0000 - val_rmse: 19737.2051\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 331336416.0000 - rmse: 18202.6484 - val_loss: 389781024.0000 - val_rmse: 19742.8730\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 321561600.0000 - rmse: 17932.1387 - val_loss: 430196864.0000 - val_rmse: 20741.1875\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 387971488.0000 - rmse: 19696.9922 - val_loss: 397607968.0000 - val_rmse: 19940.1094\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 373989184.0000 - rmse: 19338.8008 - val_loss: 446250688.0000 - val_rmse: 21124.6465\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 357379968.0000 - rmse: 18904.4961 - val_loss: 393288064.0000 - val_rmse: 19831.4922\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 350997504.0000 - rmse: 18734.9277 - val_loss: 399488320.0000 - val_rmse: 19987.2031\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 360819744.0000 - rmse: 18995.2559 - val_loss: 394508448.0000 - val_rmse: 19862.2363\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 348232096.0000 - rmse: 18660.9785 - val_loss: 380707360.0000 - val_rmse: 19511.7246\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 348720256.0000 - rmse: 18674.0527 - val_loss: 374637248.0000 - val_rmse: 19355.5488\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 365317056.0000 - rmse: 19113.2695 - val_loss: 370533152.0000 - val_rmse: 19249.2383\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 346408832.0000 - rmse: 18612.0605 - val_loss: 398483360.0000 - val_rmse: 19962.0488\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 322743968.0000 - rmse: 17965.0762 - val_loss: 396086208.0000 - val_rmse: 19901.9141\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 327872352.0000 - rmse: 18107.2461 - val_loss: 423630784.0000 - val_rmse: 20582.2930\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 336115264.0000 - rmse: 18333.4473 - val_loss: 404862464.0000 - val_rmse: 20121.1953\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 346734944.0000 - rmse: 18620.8203 - val_loss: 398090496.0000 - val_rmse: 19952.2051\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 366449888.0000 - rmse: 19142.8809 - val_loss: 401084384.0000 - val_rmse: 20027.0918\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 346390304.0000 - rmse: 18611.5645 - val_loss: 398402656.0000 - val_rmse: 19960.0273\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 328661056.0000 - rmse: 18129.0117 - val_loss: 398567392.0000 - val_rmse: 19964.1523\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 327074080.0000 - rmse: 18085.1895 - val_loss: 396357760.0000 - val_rmse: 19908.7363\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 329872480.0000 - rmse: 18162.3926 - val_loss: 414161888.0000 - val_rmse: 20350.9668\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 346071392.0000 - rmse: 18602.9941 - val_loss: 388957088.0000 - val_rmse: 19721.9941\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 314296160.0000 - rmse: 17728.4004 - val_loss: 406191872.0000 - val_rmse: 20154.2031\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 315132224.0000 - rmse: 17751.9648 - val_loss: 414317664.0000 - val_rmse: 20354.7949\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299425216.0000 - rmse: 17303.9082 - val_loss: 422539840.0000 - val_rmse: 20555.7734\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 334527264.0000 - rmse: 18290.0859 - val_loss: 423636352.0000 - val_rmse: 20582.4277\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 391626176.0000 - rmse: 19789.5469 - val_loss: 415066848.0000 - val_rmse: 20373.1895\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 334898048.0000 - rmse: 18300.2207 - val_loss: 424861568.0000 - val_rmse: 20612.1699\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 321557728.0000 - rmse: 17932.0312 - val_loss: 464371200.0000 - val_rmse: 21549.2734\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 317070080.0000 - rmse: 17806.4609 - val_loss: 410948960.0000 - val_rmse: 20271.8770\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 316828512.0000 - rmse: 17799.6777 - val_loss: 444950784.0000 - val_rmse: 21093.8574\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 313562496.0000 - rmse: 17707.6953 - val_loss: 402897760.0000 - val_rmse: 20072.3125\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 300839712.0000 - rmse: 17344.7324 - val_loss: 383606432.0000 - val_rmse: 19585.8730\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 322046176.0000 - rmse: 17945.6445 - val_loss: 377386944.0000 - val_rmse: 19426.4492\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299021120.0000 - rmse: 17292.2266 - val_loss: 391494560.0000 - val_rmse: 19786.2207\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 326545856.0000 - rmse: 18070.5801 - val_loss: 412556960.0000 - val_rmse: 20311.4980\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309274304.0000 - rmse: 17586.1973 - val_loss: 434146592.0000 - val_rmse: 20836.1855\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 324146464.0000 - rmse: 18004.0684 - val_loss: 430343040.0000 - val_rmse: 20744.7109\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 308659744.0000 - rmse: 17568.7148 - val_loss: 437153888.0000 - val_rmse: 20908.2246\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286563968.0000 - rmse: 16928.2012 - val_loss: 508511616.0000 - val_rmse: 22550.2012\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 304859808.0000 - rmse: 17460.2344 - val_loss: 429152032.0000 - val_rmse: 20715.9844\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 331625088.0000 - rmse: 18210.5762 - val_loss: 404072832.0000 - val_rmse: 20101.5625\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 328771360.0000 - rmse: 18132.0527 - val_loss: 406293056.0000 - val_rmse: 20156.7129\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 320416800.0000 - rmse: 17900.1895 - val_loss: 396120576.0000 - val_rmse: 19902.7773\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 322704736.0000 - rmse: 17963.9844 - val_loss: 428742528.0000 - val_rmse: 20706.0996\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 300803552.0000 - rmse: 17343.6895 - val_loss: 447680096.0000 - val_rmse: 21158.4512\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 289223168.0000 - rmse: 17006.5625 - val_loss: 407432352.0000 - val_rmse: 20184.9531\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 316178784.0000 - rmse: 17781.4160 - val_loss: 398932416.0000 - val_rmse: 19973.2930\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 349643808.0000 - rmse: 18698.7656 - val_loss: 437830976.0000 - val_rmse: 20924.4102\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 293175520.0000 - rmse: 17122.3691 - val_loss: 378717728.0000 - val_rmse: 19460.6719\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 348741504.0000 - rmse: 18674.6211 - val_loss: 378058112.0000 - val_rmse: 19443.7168\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 311837888.0000 - rmse: 17658.9316 - val_loss: 386492800.0000 - val_rmse: 19659.4199\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 307191616.0000 - rmse: 17526.8828 - val_loss: 375977792.0000 - val_rmse: 19390.1465\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 319662432.0000 - rmse: 17879.1055 - val_loss: 370875616.0000 - val_rmse: 19258.1309\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 287199424.0000 - rmse: 16946.9590 - val_loss: 437294944.0000 - val_rmse: 20911.5977\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 278052896.0000 - rmse: 16674.9180 - val_loss: 388377088.0000 - val_rmse: 19707.2852\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 300262464.0000 - rmse: 17328.0840 - val_loss: 397078016.0000 - val_rmse: 19926.8164\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 279872544.0000 - rmse: 16729.3926 - val_loss: 387946976.0000 - val_rmse: 19696.3691\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 279970592.0000 - rmse: 16732.3223 - val_loss: 444570784.0000 - val_rmse: 21084.8477\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 296377088.0000 - rmse: 17215.6055 - val_loss: 424454432.0000 - val_rmse: 20602.2910\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 358710016.0000 - rmse: 18939.6406 - val_loss: 338496192.0000 - val_rmse: 18398.2656\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 313026112.0000 - rmse: 17692.5449 - val_loss: 382484576.0000 - val_rmse: 19557.2129\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 300399488.0000 - rmse: 17332.0371 - val_loss: 366118496.0000 - val_rmse: 19134.2227\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 301469184.0000 - rmse: 17362.8672 - val_loss: 366202880.0000 - val_rmse: 19136.4277\n",
      "Epoch 349/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 295374624.0000 - rmse: 17186.4668 - val_loss: 394372064.0000 - val_rmse: 19858.8027\n",
      "Epoch 350/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 289872608.0000 - rmse: 17025.6465 - val_loss: 506225248.0000 - val_rmse: 22499.4492\n",
      "Epoch 351/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 288730944.0000 - rmse: 16992.0840 - val_loss: 428226432.0000 - val_rmse: 20693.6328\n",
      "Epoch 352/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 267948224.0000 - rmse: 16369.1240 - val_loss: 394572992.0000 - val_rmse: 19863.8613\n",
      "Epoch 353/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268253136.0000 - rmse: 16378.4355 - val_loss: 352094688.0000 - val_rmse: 18764.1855\n",
      "Epoch 354/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299499072.0000 - rmse: 17306.0410 - val_loss: 412975392.0000 - val_rmse: 20321.7969\n",
      "Epoch 355/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 273704288.0000 - rmse: 16544.0098 - val_loss: 340791360.0000 - val_rmse: 18460.5352\n",
      "Epoch 356/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 279109568.0000 - rmse: 16706.5723 - val_loss: 360863584.0000 - val_rmse: 18996.4102\n",
      "Epoch 357/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 278784960.0000 - rmse: 16696.8555 - val_loss: 366947104.0000 - val_rmse: 19155.8633\n",
      "Epoch 358/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 262827216.0000 - rmse: 16211.9463 - val_loss: 405566400.0000 - val_rmse: 20138.6797\n",
      "Epoch 359/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 298862720.0000 - rmse: 17287.6465 - val_loss: 389212736.0000 - val_rmse: 19728.4746\n",
      "Epoch 360/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 282203520.0000 - rmse: 16798.9141 - val_loss: 392724992.0000 - val_rmse: 19817.2910\n",
      "Epoch 361/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 295602464.0000 - rmse: 17193.0938 - val_loss: 439056352.0000 - val_rmse: 20953.6719\n",
      "Epoch 362/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 303071072.0000 - rmse: 17408.9375 - val_loss: 415396064.0000 - val_rmse: 20381.2676\n",
      "Epoch 363/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268247120.0000 - rmse: 16378.2510 - val_loss: 408866080.0000 - val_rmse: 20220.4375\n",
      "Epoch 364/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 288386240.0000 - rmse: 16981.9395 - val_loss: 361550720.0000 - val_rmse: 19014.4863\n",
      "Epoch 365/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 274598976.0000 - rmse: 16571.0273 - val_loss: 399526848.0000 - val_rmse: 19988.1680\n",
      "Epoch 366/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 300856640.0000 - rmse: 17345.2188 - val_loss: 391700896.0000 - val_rmse: 19791.4355\n",
      "Epoch 367/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 291890496.0000 - rmse: 17084.8027 - val_loss: 353328800.0000 - val_rmse: 18797.0430\n",
      "Epoch 368/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 273383872.0000 - rmse: 16534.3242 - val_loss: 372737088.0000 - val_rmse: 19306.4004\n",
      "Epoch 369/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 283450688.0000 - rmse: 16835.9941 - val_loss: 539723968.0000 - val_rmse: 23231.9609\n",
      "Epoch 370/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268014752.0000 - rmse: 16371.1562 - val_loss: 399969056.0000 - val_rmse: 19999.2266\n",
      "Epoch 371/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 287854112.0000 - rmse: 16966.2637 - val_loss: 398612224.0000 - val_rmse: 19965.2754\n",
      "Epoch 372/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 262864160.0000 - rmse: 16213.0859 - val_loss: 386212096.0000 - val_rmse: 19652.2793\n",
      "Epoch 373/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 272537440.0000 - rmse: 16508.7070 - val_loss: 373044992.0000 - val_rmse: 19314.3730\n",
      "Epoch 374/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 279109088.0000 - rmse: 16706.5586 - val_loss: 406466080.0000 - val_rmse: 20161.0039\n",
      "Epoch 375/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 261708592.0000 - rmse: 16177.4102 - val_loss: 501391776.0000 - val_rmse: 22391.7793\n",
      "Epoch 376/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 283154240.0000 - rmse: 16827.1875 - val_loss: 399469088.0000 - val_rmse: 19986.7227\n",
      "Epoch 377/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 283064864.0000 - rmse: 16824.5312 - val_loss: 400854976.0000 - val_rmse: 20021.3633\n",
      "Epoch 378/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 267646880.0000 - rmse: 16359.9170 - val_loss: 371984992.0000 - val_rmse: 19286.9121\n",
      "Epoch 379/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 273734848.0000 - rmse: 16544.9336 - val_loss: 468807072.0000 - val_rmse: 21651.9531\n",
      "Epoch 380/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 267951248.0000 - rmse: 16369.2168 - val_loss: 455406784.0000 - val_rmse: 21340.2617\n",
      "Epoch 381/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 272514784.0000 - rmse: 16508.0215 - val_loss: 349800064.0000 - val_rmse: 18702.9434\n",
      "Epoch 382/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309711840.0000 - rmse: 17598.6309 - val_loss: 397613536.0000 - val_rmse: 19940.2500\n",
      "Epoch 383/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 246948144.0000 - rmse: 15714.5840 - val_loss: 404906304.0000 - val_rmse: 20122.2832\n",
      "Epoch 384/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 314616096.0000 - rmse: 17737.4199 - val_loss: 372667808.0000 - val_rmse: 19304.6055\n",
      "Epoch 385/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 250710208.0000 - rmse: 15833.8311 - val_loss: 382232704.0000 - val_rmse: 19550.7734\n",
      "Epoch 386/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 237522432.0000 - rmse: 15411.7627 - val_loss: 366304416.0000 - val_rmse: 19139.0801\n",
      "Epoch 387/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264238784.0000 - rmse: 16255.4229 - val_loss: 374370080.0000 - val_rmse: 19348.6445\n",
      "Epoch 388/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 263229520.0000 - rmse: 16224.3496 - val_loss: 366347200.0000 - val_rmse: 19140.1992\n",
      "Epoch 389/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 287966624.0000 - rmse: 16969.5801 - val_loss: 491073408.0000 - val_rmse: 22160.1758\n",
      "Epoch 390/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264952320.0000 - rmse: 16277.3564 - val_loss: 543785728.0000 - val_rmse: 23319.2129\n",
      "Epoch 391/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 271211872.0000 - rmse: 16468.5117 - val_loss: 389433280.0000 - val_rmse: 19734.0645\n",
      "Epoch 392/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 299654272.0000 - rmse: 17310.5254 - val_loss: 368231712.0000 - val_rmse: 19189.3652\n",
      "Epoch 393/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 259495568.0000 - rmse: 16108.8662 - val_loss: 365946048.0000 - val_rmse: 19129.7168\n",
      "Epoch 394/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 268151888.0000 - rmse: 16375.3438 - val_loss: 365306976.0000 - val_rmse: 19113.0059\n",
      "Epoch 395/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 264923680.0000 - rmse: 16276.4766 - val_loss: 454375712.0000 - val_rmse: 21316.0898\n",
      "Epoch 396/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 286550368.0000 - rmse: 16927.7988 - val_loss: 382416800.0000 - val_rmse: 19555.4805\n",
      "Epoch 397/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 309683584.0000 - rmse: 17597.8281 - val_loss: 368464288.0000 - val_rmse: 19195.4238\n",
      "Epoch 398/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 256407696.0000 - rmse: 16012.7354 - val_loss: 346103872.0000 - val_rmse: 18603.8672\n",
      "Epoch 399/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 294019104.0000 - rmse: 17146.9844 - val_loss: 405866880.0000 - val_rmse: 20146.1387\n",
      "Epoch 400/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 247307616.0000 - rmse: 15726.0176 - val_loss: 344357696.0000 - val_rmse: 18556.8770\n",
      "104/104 [==============================] - 0s 825us/step - loss: 331810080.0000 - rmse: 18215.6543\n",
      "[331810080.0, 18215.654296875]\n",
      "[19458.44921875, 24002.62109375, 32562.18359375, 24491.04296875, 18215.654296875]\n",
      "23745.990234375\n"
     ]
    }
   ],
   "source": [
    "k_fold(\"emb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and get the Embedding Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16589, 123) (16589,)\n",
      "(13271, 123) (13271,) (3318, 123) (3318,)\n",
      "<src.model.emb_model object at 0x7f99401d1b90>\n",
      "Epoch 1/400\n",
      "166/166 [==============================] - 2s 5ms/step - loss: 20009486336.0000 - rmse: 141454.8906 - val_loss: 2769871360.0000 - val_rmse: 52629.5664\n",
      "Epoch 2/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 2472165376.0000 - rmse: 49720.8750 - val_loss: 1370116736.0000 - val_rmse: 37015.0898\n",
      "Epoch 3/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 2031512704.0000 - rmse: 45072.3047 - val_loss: 1181259776.0000 - val_rmse: 34369.4609\n",
      "Epoch 4/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1821971968.0000 - rmse: 42684.5625 - val_loss: 1113358208.0000 - val_rmse: 33367.0234\n",
      "Epoch 5/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1766556800.0000 - rmse: 42030.4258 - val_loss: 1008083264.0000 - val_rmse: 31750.3262\n",
      "Epoch 6/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1673404416.0000 - rmse: 40907.2656 - val_loss: 952990784.0000 - val_rmse: 30870.5488\n",
      "Epoch 7/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1576553216.0000 - rmse: 39705.8320 - val_loss: 938690944.0000 - val_rmse: 30638.0645\n",
      "Epoch 8/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1538083840.0000 - rmse: 39218.4102 - val_loss: 907211392.0000 - val_rmse: 30119.9492\n",
      "Epoch 9/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1491479168.0000 - rmse: 38619.6719 - val_loss: 858223488.0000 - val_rmse: 29295.4512\n",
      "Epoch 10/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1429367680.0000 - rmse: 37806.9805 - val_loss: 850857152.0000 - val_rmse: 29169.4551\n",
      "Epoch 11/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1433058048.0000 - rmse: 37855.7539 - val_loss: 820253824.0000 - val_rmse: 28640.0742\n",
      "Epoch 12/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1366124288.0000 - rmse: 36961.1172 - val_loss: 803567296.0000 - val_rmse: 28347.2617\n",
      "Epoch 13/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1359805312.0000 - rmse: 36875.5391 - val_loss: 786093824.0000 - val_rmse: 28037.3652\n",
      "Epoch 14/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1335891072.0000 - rmse: 36549.8438 - val_loss: 750535040.0000 - val_rmse: 27395.8945\n",
      "Epoch 15/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1262760576.0000 - rmse: 35535.3438 - val_loss: 749679936.0000 - val_rmse: 27380.2832\n",
      "Epoch 16/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1181032448.0000 - rmse: 34366.1523 - val_loss: 839155840.0000 - val_rmse: 28968.1875\n",
      "Epoch 17/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1234872576.0000 - rmse: 35140.7539 - val_loss: 744357568.0000 - val_rmse: 27282.9180\n",
      "Epoch 18/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1197310080.0000 - rmse: 34602.1680 - val_loss: 684218112.0000 - val_rmse: 26157.5625\n",
      "Epoch 19/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1163392640.0000 - rmse: 34108.5430 - val_loss: 712443392.0000 - val_rmse: 26691.6348\n",
      "Epoch 20/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1170092928.0000 - rmse: 34206.6211 - val_loss: 683951488.0000 - val_rmse: 26152.4668\n",
      "Epoch 21/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1050369408.0000 - rmse: 32409.4023 - val_loss: 661753216.0000 - val_rmse: 25724.5645\n",
      "Epoch 22/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1127587072.0000 - rmse: 33579.5625 - val_loss: 647458816.0000 - val_rmse: 25445.2129\n",
      "Epoch 23/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 955993152.0000 - rmse: 30919.1387 - val_loss: 633243072.0000 - val_rmse: 25164.3223\n",
      "Epoch 24/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1019092480.0000 - rmse: 31923.2285 - val_loss: 653917440.0000 - val_rmse: 25571.8086\n",
      "Epoch 25/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1072625216.0000 - rmse: 32750.9570 - val_loss: 699012416.0000 - val_rmse: 26438.8438\n",
      "Epoch 26/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1049969600.0000 - rmse: 32403.2344 - val_loss: 627910784.0000 - val_rmse: 25058.1484\n",
      "Epoch 27/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1027136768.0000 - rmse: 32048.9746 - val_loss: 708706176.0000 - val_rmse: 26621.5352\n",
      "Epoch 28/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 853674176.0000 - rmse: 29217.7031 - val_loss: 605322496.0000 - val_rmse: 24603.3027\n",
      "Epoch 29/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1019972352.0000 - rmse: 31937.0059 - val_loss: 616869760.0000 - val_rmse: 24836.8633\n",
      "Epoch 30/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 936707264.0000 - rmse: 30605.6738 - val_loss: 615831168.0000 - val_rmse: 24815.9453\n",
      "Epoch 31/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 949311616.0000 - rmse: 30810.9004 - val_loss: 616247104.0000 - val_rmse: 24824.3242\n",
      "Epoch 32/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 957315328.0000 - rmse: 30940.5137 - val_loss: 619776768.0000 - val_rmse: 24895.3164\n",
      "Epoch 33/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 970302976.0000 - rmse: 31149.6875 - val_loss: 576540928.0000 - val_rmse: 24011.2676\n",
      "Epoch 34/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 939803008.0000 - rmse: 30656.2070 - val_loss: 593233216.0000 - val_rmse: 24356.3789\n",
      "Epoch 35/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1000440192.0000 - rmse: 31629.7363 - val_loss: 588530176.0000 - val_rmse: 24259.6406\n",
      "Epoch 36/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 923118272.0000 - rmse: 30382.8613 - val_loss: 561145920.0000 - val_rmse: 23688.5195\n",
      "Epoch 37/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 1014763584.0000 - rmse: 31855.3535 - val_loss: 625103552.0000 - val_rmse: 25002.0703\n",
      "Epoch 38/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 878492032.0000 - rmse: 29639.3672 - val_loss: 546484480.0000 - val_rmse: 23377.0078\n",
      "Epoch 39/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 874365120.0000 - rmse: 29569.6660 - val_loss: 561094016.0000 - val_rmse: 23687.4238\n",
      "Epoch 40/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 913489280.0000 - rmse: 30223.9844 - val_loss: 549434304.0000 - val_rmse: 23440.0156\n",
      "Epoch 41/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 892704448.0000 - rmse: 29878.1602 - val_loss: 549572864.0000 - val_rmse: 23442.9707\n",
      "Epoch 42/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 917101312.0000 - rmse: 30283.6797 - val_loss: 602247488.0000 - val_rmse: 24540.7305\n",
      "Epoch 43/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 898342912.0000 - rmse: 29972.3691 - val_loss: 567417664.0000 - val_rmse: 23820.5312\n",
      "Epoch 44/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 827412928.0000 - rmse: 28764.7871 - val_loss: 562695808.0000 - val_rmse: 23721.2109\n",
      "Epoch 45/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 827232960.0000 - rmse: 28761.6582 - val_loss: 525665312.0000 - val_rmse: 22927.3926\n",
      "Epoch 46/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 798728000.0000 - rmse: 28261.7754 - val_loss: 524924256.0000 - val_rmse: 22911.2246\n",
      "Epoch 47/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 876458624.0000 - rmse: 29605.0430 - val_loss: 511017696.0000 - val_rmse: 22605.7012\n",
      "Epoch 48/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 880713536.0000 - rmse: 29676.8184 - val_loss: 542660928.0000 - val_rmse: 23295.0840\n",
      "Epoch 49/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 911189056.0000 - rmse: 30185.9082 - val_loss: 512297056.0000 - val_rmse: 22633.9805\n",
      "Epoch 50/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 884131776.0000 - rmse: 29734.3535 - val_loss: 520757888.0000 - val_rmse: 22820.1211\n",
      "Epoch 51/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 867110720.0000 - rmse: 29446.7441 - val_loss: 541587840.0000 - val_rmse: 23272.0391\n",
      "Epoch 52/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 861967680.0000 - rmse: 29359.2852 - val_loss: 562420544.0000 - val_rmse: 23715.4082\n",
      "Epoch 53/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 790497280.0000 - rmse: 28115.7832 - val_loss: 498286528.0000 - val_rmse: 22322.3320\n",
      "Epoch 54/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 817542656.0000 - rmse: 28592.7031 - val_loss: 479530496.0000 - val_rmse: 21898.1855\n",
      "Epoch 55/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 777783232.0000 - rmse: 27888.7656 - val_loss: 486690720.0000 - val_rmse: 22061.0684\n",
      "Epoch 56/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 842508480.0000 - rmse: 29025.9961 - val_loss: 485834976.0000 - val_rmse: 22041.6641\n",
      "Epoch 57/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 791442240.0000 - rmse: 28132.5840 - val_loss: 481433440.0000 - val_rmse: 21941.5918\n",
      "Epoch 58/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 797374656.0000 - rmse: 28237.8223 - val_loss: 477943104.0000 - val_rmse: 21861.9102\n",
      "Epoch 59/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 798602304.0000 - rmse: 28259.5527 - val_loss: 476148896.0000 - val_rmse: 21820.8359\n",
      "Epoch 60/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 747824000.0000 - rmse: 27346.3711 - val_loss: 456974752.0000 - val_rmse: 21376.9688\n",
      "Epoch 61/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 781239424.0000 - rmse: 27950.6602 - val_loss: 459822304.0000 - val_rmse: 21443.4668\n",
      "Epoch 62/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 762357952.0000 - rmse: 27610.8301 - val_loss: 506687872.0000 - val_rmse: 22509.7285\n",
      "Epoch 63/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 685004160.0000 - rmse: 26172.5840 - val_loss: 447346848.0000 - val_rmse: 21150.5762\n",
      "Epoch 64/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 695868032.0000 - rmse: 26379.3105 - val_loss: 452157472.0000 - val_rmse: 21263.9941\n",
      "Epoch 65/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 765483840.0000 - rmse: 27667.3789 - val_loss: 440679808.0000 - val_rmse: 20992.3750\n",
      "Epoch 66/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 713458304.0000 - rmse: 26710.6406 - val_loss: 450891744.0000 - val_rmse: 21234.2109\n",
      "Epoch 67/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 778829696.0000 - rmse: 27907.5195 - val_loss: 511297248.0000 - val_rmse: 22611.8828\n",
      "Epoch 68/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 706793024.0000 - rmse: 26585.5801 - val_loss: 497272448.0000 - val_rmse: 22299.6074\n",
      "Epoch 69/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 751132032.0000 - rmse: 27406.7871 - val_loss: 435323584.0000 - val_rmse: 20864.4102\n",
      "Epoch 70/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 745429312.0000 - rmse: 27302.5508 - val_loss: 527629568.0000 - val_rmse: 22970.1895\n",
      "Epoch 71/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 722266816.0000 - rmse: 26875.0215 - val_loss: 414435936.0000 - val_rmse: 20357.6992\n",
      "Epoch 72/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 682653952.0000 - rmse: 26127.6465 - val_loss: 459474208.0000 - val_rmse: 21435.3496\n",
      "Epoch 73/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 698474816.0000 - rmse: 26428.6738 - val_loss: 447275040.0000 - val_rmse: 21148.8789\n",
      "Epoch 74/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 705754048.0000 - rmse: 26566.0312 - val_loss: 451692192.0000 - val_rmse: 21253.0508\n",
      "Epoch 75/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 742733888.0000 - rmse: 27253.1445 - val_loss: 426106784.0000 - val_rmse: 20642.3535\n",
      "Epoch 76/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 731897728.0000 - rmse: 27053.6094 - val_loss: 406684384.0000 - val_rmse: 20166.4180\n",
      "Epoch 77/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 655330304.0000 - rmse: 25599.4199 - val_loss: 664050496.0000 - val_rmse: 25769.1777\n",
      "Epoch 78/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 638288512.0000 - rmse: 25264.3730 - val_loss: 469957184.0000 - val_rmse: 21678.4961\n",
      "Epoch 79/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 641978240.0000 - rmse: 25337.2891 - val_loss: 420859904.0000 - val_rmse: 20514.8711\n",
      "Epoch 80/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 616771328.0000 - rmse: 24834.8809 - val_loss: 425878592.0000 - val_rmse: 20636.8262\n",
      "Epoch 81/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 667209024.0000 - rmse: 25830.3887 - val_loss: 416729280.0000 - val_rmse: 20413.9473\n",
      "Epoch 82/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 632857536.0000 - rmse: 25156.6602 - val_loss: 484968832.0000 - val_rmse: 22022.0078\n",
      "Epoch 83/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 686743168.0000 - rmse: 26205.7852 - val_loss: 455079808.0000 - val_rmse: 21332.5996\n",
      "Epoch 84/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 677875776.0000 - rmse: 26036.0469 - val_loss: 403780160.0000 - val_rmse: 20094.2812\n",
      "Epoch 85/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 630035840.0000 - rmse: 25100.5156 - val_loss: 444157440.0000 - val_rmse: 21075.0430\n",
      "Epoch 86/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 699622848.0000 - rmse: 26450.3848 - val_loss: 421220032.0000 - val_rmse: 20523.6465\n",
      "Epoch 87/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 670562624.0000 - rmse: 25895.2246 - val_loss: 411898624.0000 - val_rmse: 20295.2852\n",
      "Epoch 88/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 715196224.0000 - rmse: 26743.1523 - val_loss: 442986784.0000 - val_rmse: 21047.2520\n",
      "Epoch 89/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 595151104.0000 - rmse: 24395.7188 - val_loss: 425961792.0000 - val_rmse: 20638.8418\n",
      "Epoch 90/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 633253760.0000 - rmse: 25164.5332 - val_loss: 428410144.0000 - val_rmse: 20698.0703\n",
      "Epoch 91/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 560476096.0000 - rmse: 23674.3770 - val_loss: 407846688.0000 - val_rmse: 20195.2148\n",
      "Epoch 92/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 628368768.0000 - rmse: 25067.2852 - val_loss: 565314048.0000 - val_rmse: 23776.3340\n",
      "Epoch 93/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 568922880.0000 - rmse: 23852.1035 - val_loss: 416195712.0000 - val_rmse: 20400.8750\n",
      "Epoch 94/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 628977472.0000 - rmse: 25079.4238 - val_loss: 393492704.0000 - val_rmse: 19836.6504\n",
      "Epoch 95/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 627296576.0000 - rmse: 25045.8887 - val_loss: 414326944.0000 - val_rmse: 20355.0234\n",
      "Epoch 96/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 617608640.0000 - rmse: 24851.7324 - val_loss: 385199072.0000 - val_rmse: 19626.4883\n",
      "Epoch 97/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 613031104.0000 - rmse: 24759.4648 - val_loss: 406794560.0000 - val_rmse: 20169.1484\n",
      "Epoch 98/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 546457472.0000 - rmse: 23376.4297 - val_loss: 401770528.0000 - val_rmse: 20044.2148\n",
      "Epoch 99/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 557252736.0000 - rmse: 23606.2012 - val_loss: 398824192.0000 - val_rmse: 19970.5840\n",
      "Epoch 100/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 617692288.0000 - rmse: 24853.4160 - val_loss: 568877568.0000 - val_rmse: 23851.1543\n",
      "Epoch 101/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 609034432.0000 - rmse: 24678.6230 - val_loss: 384861568.0000 - val_rmse: 19617.8887\n",
      "Epoch 102/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 581773184.0000 - rmse: 24119.9746 - val_loss: 431853152.0000 - val_rmse: 20781.0762\n",
      "Epoch 103/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 532128032.0000 - rmse: 23067.9004 - val_loss: 444389088.0000 - val_rmse: 21080.5391\n",
      "Epoch 104/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 577935232.0000 - rmse: 24040.2832 - val_loss: 377588480.0000 - val_rmse: 19431.6367\n",
      "Epoch 105/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 575197888.0000 - rmse: 23983.2832 - val_loss: 417745184.0000 - val_rmse: 20438.8164\n",
      "Epoch 106/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 535442688.0000 - rmse: 23139.6348 - val_loss: 399154016.0000 - val_rmse: 19978.8398\n",
      "Epoch 107/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 511778144.0000 - rmse: 22622.5137 - val_loss: 433621632.0000 - val_rmse: 20823.5840\n",
      "Epoch 108/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 578106240.0000 - rmse: 24043.8398 - val_loss: 396040896.0000 - val_rmse: 19900.7754\n",
      "Epoch 109/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 520449344.0000 - rmse: 22813.3594 - val_loss: 370687264.0000 - val_rmse: 19253.2402\n",
      "Epoch 110/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 571029248.0000 - rmse: 23896.2188 - val_loss: 431824384.0000 - val_rmse: 20780.3848\n",
      "Epoch 111/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 665188352.0000 - rmse: 25791.2461 - val_loss: 393764096.0000 - val_rmse: 19843.4902\n",
      "Epoch 112/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 532145152.0000 - rmse: 23068.2715 - val_loss: 435970208.0000 - val_rmse: 20879.9004\n",
      "Epoch 113/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 554187776.0000 - rmse: 23541.1934 - val_loss: 409700352.0000 - val_rmse: 20241.0566\n",
      "Epoch 114/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 549134272.0000 - rmse: 23433.6133 - val_loss: 422616576.0000 - val_rmse: 20557.6406\n",
      "Epoch 115/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 558856384.0000 - rmse: 23640.1426 - val_loss: 419586048.0000 - val_rmse: 20483.7988\n",
      "Epoch 116/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 564257920.0000 - rmse: 23754.1133 - val_loss: 476795808.0000 - val_rmse: 21835.6543\n",
      "Epoch 117/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 549355840.0000 - rmse: 23438.3418 - val_loss: 383061248.0000 - val_rmse: 19571.9512\n",
      "Epoch 118/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 500800640.0000 - rmse: 22378.5762 - val_loss: 411934304.0000 - val_rmse: 20296.1641\n",
      "Epoch 119/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 540996608.0000 - rmse: 23259.3340 - val_loss: 397445664.0000 - val_rmse: 19936.0391\n",
      "Epoch 120/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 510205152.0000 - rmse: 22587.7207 - val_loss: 417432832.0000 - val_rmse: 20431.1738\n",
      "Epoch 121/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 518749536.0000 - rmse: 22776.0742 - val_loss: 419730496.0000 - val_rmse: 20487.3262\n",
      "Epoch 122/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 554973696.0000 - rmse: 23557.8789 - val_loss: 451861920.0000 - val_rmse: 21257.0449\n",
      "Epoch 123/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 541849600.0000 - rmse: 23277.6641 - val_loss: 501449984.0000 - val_rmse: 22393.0781\n",
      "Epoch 124/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 528521408.0000 - rmse: 22989.5938 - val_loss: 600371392.0000 - val_rmse: 24502.4766\n",
      "Epoch 125/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 507435776.0000 - rmse: 22526.3359 - val_loss: 361658112.0000 - val_rmse: 19017.3105\n",
      "Epoch 126/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 545731264.0000 - rmse: 23360.8926 - val_loss: 405492384.0000 - val_rmse: 20136.8418\n",
      "Epoch 127/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 508156768.0000 - rmse: 22542.3320 - val_loss: 395142976.0000 - val_rmse: 19878.2031\n",
      "Epoch 128/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 441589664.0000 - rmse: 21014.0352 - val_loss: 387101664.0000 - val_rmse: 19674.8984\n",
      "Epoch 129/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 508681888.0000 - rmse: 22553.9766 - val_loss: 377999264.0000 - val_rmse: 19442.2031\n",
      "Epoch 130/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 480888352.0000 - rmse: 21929.1660 - val_loss: 399630016.0000 - val_rmse: 19990.7480\n",
      "Epoch 131/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 534296928.0000 - rmse: 23114.8633 - val_loss: 391526144.0000 - val_rmse: 19787.0195\n",
      "Epoch 132/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 454950624.0000 - rmse: 21329.5723 - val_loss: 579448896.0000 - val_rmse: 24071.7441\n",
      "Epoch 133/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 535056704.0000 - rmse: 23131.2930 - val_loss: 371154592.0000 - val_rmse: 19265.3730\n",
      "Epoch 134/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 463048832.0000 - rmse: 21518.5703 - val_loss: 461360064.0000 - val_rmse: 21479.2930\n",
      "Epoch 135/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 449336096.0000 - rmse: 21197.5488 - val_loss: 386955040.0000 - val_rmse: 19671.1719\n",
      "Epoch 136/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 505974656.0000 - rmse: 22493.8809 - val_loss: 433827648.0000 - val_rmse: 20828.5293\n",
      "Epoch 137/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 500790464.0000 - rmse: 22378.3477 - val_loss: 355025952.0000 - val_rmse: 18842.1328\n",
      "Epoch 138/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 463830912.0000 - rmse: 21536.7344 - val_loss: 369269440.0000 - val_rmse: 19216.3848\n",
      "Epoch 139/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 496104160.0000 - rmse: 22273.3965 - val_loss: 401653312.0000 - val_rmse: 20041.2910\n",
      "Epoch 140/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 459986176.0000 - rmse: 21447.2891 - val_loss: 401098400.0000 - val_rmse: 20027.4414\n",
      "Epoch 141/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 482574848.0000 - rmse: 21967.5859 - val_loss: 480952672.0000 - val_rmse: 21930.6328\n",
      "Epoch 142/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 493844320.0000 - rmse: 22222.6074 - val_loss: 381262784.0000 - val_rmse: 19525.9512\n",
      "Epoch 143/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 488679424.0000 - rmse: 22106.0938 - val_loss: 392883776.0000 - val_rmse: 19821.2969\n",
      "Epoch 144/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 511484288.0000 - rmse: 22616.0176 - val_loss: 398780384.0000 - val_rmse: 19969.4863\n",
      "Epoch 145/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 447403360.0000 - rmse: 21151.9121 - val_loss: 394721664.0000 - val_rmse: 19867.6035\n",
      "Epoch 146/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 513602720.0000 - rmse: 22662.8047 - val_loss: 500747264.0000 - val_rmse: 22377.3828\n",
      "Epoch 147/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 445859296.0000 - rmse: 21115.3809 - val_loss: 384687808.0000 - val_rmse: 19613.4590\n",
      "Epoch 148/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 451240160.0000 - rmse: 21242.4141 - val_loss: 434486400.0000 - val_rmse: 20844.3379\n",
      "Epoch 149/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 463310624.0000 - rmse: 21524.6523 - val_loss: 404255264.0000 - val_rmse: 20106.0996\n",
      "Epoch 150/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 458301792.0000 - rmse: 21407.9844 - val_loss: 375723840.0000 - val_rmse: 19383.5977\n",
      "Epoch 151/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 529073376.0000 - rmse: 23001.5957 - val_loss: 387866560.0000 - val_rmse: 19694.3281\n",
      "Epoch 152/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 511437952.0000 - rmse: 22614.9941 - val_loss: 408884544.0000 - val_rmse: 20220.8945\n",
      "Epoch 153/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 402434208.0000 - rmse: 20060.7637 - val_loss: 385762656.0000 - val_rmse: 19640.8418\n",
      "Epoch 154/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 434510880.0000 - rmse: 20844.9238 - val_loss: 396291808.0000 - val_rmse: 19907.0801\n",
      "Epoch 155/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 425058592.0000 - rmse: 20616.9492 - val_loss: 369948992.0000 - val_rmse: 19234.0586\n",
      "Epoch 156/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 484062336.0000 - rmse: 22001.4160 - val_loss: 393817632.0000 - val_rmse: 19844.8398\n",
      "Epoch 157/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 440945408.0000 - rmse: 20998.6992 - val_loss: 499815584.0000 - val_rmse: 22356.5566\n",
      "Epoch 158/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 482648672.0000 - rmse: 21969.2656 - val_loss: 381107456.0000 - val_rmse: 19521.9746\n",
      "Epoch 159/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 486974016.0000 - rmse: 22067.4883 - val_loss: 401165664.0000 - val_rmse: 20029.1211\n",
      "Epoch 160/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 485711904.0000 - rmse: 22038.8730 - val_loss: 394745408.0000 - val_rmse: 19868.2012\n",
      "Epoch 161/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 422912288.0000 - rmse: 20564.8320 - val_loss: 376000096.0000 - val_rmse: 19390.7227\n",
      "Epoch 162/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 496084352.0000 - rmse: 22272.9512 - val_loss: 348021472.0000 - val_rmse: 18655.3340\n",
      "Epoch 163/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 460858048.0000 - rmse: 21467.6055 - val_loss: 420955968.0000 - val_rmse: 20517.2109\n",
      "Epoch 164/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 431746336.0000 - rmse: 20778.5059 - val_loss: 417505088.0000 - val_rmse: 20432.9414\n",
      "Epoch 165/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 433703520.0000 - rmse: 20825.5488 - val_loss: 367680448.0000 - val_rmse: 19174.9961\n",
      "Epoch 166/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 416756672.0000 - rmse: 20414.6191 - val_loss: 360775264.0000 - val_rmse: 18994.0859\n",
      "Epoch 167/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 477494656.0000 - rmse: 21851.6504 - val_loss: 355131168.0000 - val_rmse: 18844.9238\n",
      "Epoch 168/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 399571424.0000 - rmse: 19989.2832 - val_loss: 400505728.0000 - val_rmse: 20012.6387\n",
      "Epoch 169/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 444313088.0000 - rmse: 21078.7363 - val_loss: 369085216.0000 - val_rmse: 19211.5898\n",
      "Epoch 170/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 437927008.0000 - rmse: 20926.7051 - val_loss: 406238720.0000 - val_rmse: 20155.3652\n",
      "Epoch 171/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 421597536.0000 - rmse: 20532.8398 - val_loss: 486430176.0000 - val_rmse: 22055.1621\n",
      "Epoch 172/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 453784256.0000 - rmse: 21302.2129 - val_loss: 405210688.0000 - val_rmse: 20129.8457\n",
      "Epoch 173/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 387406624.0000 - rmse: 19682.6484 - val_loss: 380302688.0000 - val_rmse: 19501.3516\n",
      "Epoch 174/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 422983456.0000 - rmse: 20566.5625 - val_loss: 392475072.0000 - val_rmse: 19810.9844\n",
      "Epoch 175/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 417127904.0000 - rmse: 20423.7090 - val_loss: 400212640.0000 - val_rmse: 20005.3145\n",
      "Epoch 176/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 469263584.0000 - rmse: 21662.4922 - val_loss: 412254528.0000 - val_rmse: 20304.0527\n",
      "Epoch 177/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 447732320.0000 - rmse: 21159.6855 - val_loss: 411364416.0000 - val_rmse: 20282.1211\n",
      "Epoch 178/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 451972800.0000 - rmse: 21259.6523 - val_loss: 690220928.0000 - val_rmse: 26272.0566\n",
      "Epoch 179/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 443343424.0000 - rmse: 21055.7227 - val_loss: 367796800.0000 - val_rmse: 19178.0293\n",
      "Epoch 180/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 431153344.0000 - rmse: 20764.2324 - val_loss: 360539488.0000 - val_rmse: 18987.8770\n",
      "Epoch 181/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 437149952.0000 - rmse: 20908.1309 - val_loss: 373088128.0000 - val_rmse: 19315.4902\n",
      "Epoch 182/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 399057408.0000 - rmse: 19976.4219 - val_loss: 378839488.0000 - val_rmse: 19463.7988\n",
      "Epoch 183/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 385499936.0000 - rmse: 19634.1523 - val_loss: 397966400.0000 - val_rmse: 19949.0957\n",
      "Epoch 184/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 398523936.0000 - rmse: 19963.0645 - val_loss: 359440416.0000 - val_rmse: 18958.9141\n",
      "Epoch 185/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 400939328.0000 - rmse: 20023.4688 - val_loss: 371483840.0000 - val_rmse: 19273.9160\n",
      "Epoch 186/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 461273632.0000 - rmse: 21477.2812 - val_loss: 344714656.0000 - val_rmse: 18566.4922\n",
      "Epoch 187/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 434782752.0000 - rmse: 20851.4453 - val_loss: 382610784.0000 - val_rmse: 19560.4395\n",
      "Epoch 188/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 401181440.0000 - rmse: 20029.5137 - val_loss: 392845632.0000 - val_rmse: 19820.3340\n",
      "Epoch 189/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 425309984.0000 - rmse: 20623.0449 - val_loss: 425967744.0000 - val_rmse: 20638.9863\n",
      "Epoch 190/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 467074432.0000 - rmse: 21611.9043 - val_loss: 380399264.0000 - val_rmse: 19503.8262\n",
      "Epoch 191/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 412365568.0000 - rmse: 20306.7871 - val_loss: 374904224.0000 - val_rmse: 19362.4434\n",
      "Epoch 192/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 363430144.0000 - rmse: 19063.8438 - val_loss: 365192288.0000 - val_rmse: 19110.0059\n",
      "Epoch 193/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 386535136.0000 - rmse: 19660.4961 - val_loss: 373183584.0000 - val_rmse: 19317.9609\n",
      "Epoch 194/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 397791840.0000 - rmse: 19944.7188 - val_loss: 364250304.0000 - val_rmse: 19085.3418\n",
      "Epoch 195/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 401997120.0000 - rmse: 20049.8652 - val_loss: 352131744.0000 - val_rmse: 18765.1738\n",
      "Epoch 196/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 331205600.0000 - rmse: 18199.0547 - val_loss: 431234368.0000 - val_rmse: 20766.1836\n",
      "Epoch 197/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 404773152.0000 - rmse: 20118.9746 - val_loss: 377686976.0000 - val_rmse: 19434.1699\n",
      "Epoch 198/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 381330144.0000 - rmse: 19527.6758 - val_loss: 353839840.0000 - val_rmse: 18810.6309\n",
      "Epoch 199/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 382042016.0000 - rmse: 19545.8945 - val_loss: 348918720.0000 - val_rmse: 18679.3652\n",
      "Epoch 200/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 398595200.0000 - rmse: 19964.8496 - val_loss: 349012320.0000 - val_rmse: 18681.8711\n",
      "Epoch 201/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 380101056.0000 - rmse: 19496.1797 - val_loss: 367021088.0000 - val_rmse: 19157.7949\n",
      "Epoch 202/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 405627264.0000 - rmse: 20140.1895 - val_loss: 360910464.0000 - val_rmse: 18997.6445\n",
      "Epoch 203/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 361543360.0000 - rmse: 19014.2930 - val_loss: 391617952.0000 - val_rmse: 19789.3398\n",
      "Epoch 204/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 396131392.0000 - rmse: 19903.0508 - val_loss: 392598048.0000 - val_rmse: 19814.0879\n",
      "Epoch 205/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 360463104.0000 - rmse: 18985.8652 - val_loss: 369566496.0000 - val_rmse: 19224.1133\n",
      "Epoch 206/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 399333952.0000 - rmse: 19983.3418 - val_loss: 414591296.0000 - val_rmse: 20361.5156\n",
      "Epoch 207/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 365810336.0000 - rmse: 19126.1680 - val_loss: 385605952.0000 - val_rmse: 19636.8516\n",
      "Epoch 208/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 399573568.0000 - rmse: 19989.3359 - val_loss: 467984416.0000 - val_rmse: 21632.9473\n",
      "Epoch 209/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 395792448.0000 - rmse: 19894.5332 - val_loss: 592550464.0000 - val_rmse: 24342.3594\n",
      "Epoch 210/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 383654496.0000 - rmse: 19587.0996 - val_loss: 384002656.0000 - val_rmse: 19595.9863\n",
      "Epoch 211/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 402995072.0000 - rmse: 20074.7363 - val_loss: 390785504.0000 - val_rmse: 19768.2949\n",
      "Epoch 212/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 366058080.0000 - rmse: 19132.6445 - val_loss: 440525888.0000 - val_rmse: 20988.7090\n",
      "Epoch 213/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 373691424.0000 - rmse: 19331.0996 - val_loss: 361806816.0000 - val_rmse: 19021.2207\n",
      "Epoch 214/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 341411264.0000 - rmse: 18477.3184 - val_loss: 359631712.0000 - val_rmse: 18963.9590\n",
      "Epoch 215/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 368084576.0000 - rmse: 19185.5312 - val_loss: 406515296.0000 - val_rmse: 20162.2246\n",
      "Epoch 216/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 366816448.0000 - rmse: 19152.4531 - val_loss: 484251296.0000 - val_rmse: 22005.7109\n",
      "Epoch 217/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 347593824.0000 - rmse: 18643.8691 - val_loss: 371130048.0000 - val_rmse: 19264.7363\n",
      "Epoch 218/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 415591424.0000 - rmse: 20386.0586 - val_loss: 354910976.0000 - val_rmse: 18839.0820\n",
      "Epoch 219/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 340653120.0000 - rmse: 18456.7910 - val_loss: 360713344.0000 - val_rmse: 18992.4551\n",
      "Epoch 220/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 359080320.0000 - rmse: 18949.4141 - val_loss: 359361696.0000 - val_rmse: 18956.8379\n",
      "Epoch 221/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 373905216.0000 - rmse: 19336.6289 - val_loss: 531462752.0000 - val_rmse: 23053.4766\n",
      "Epoch 222/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 386029440.0000 - rmse: 19647.6328 - val_loss: 344537056.0000 - val_rmse: 18561.7090\n",
      "Epoch 223/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 359328064.0000 - rmse: 18955.9512 - val_loss: 377019680.0000 - val_rmse: 19416.9941\n",
      "Epoch 224/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 373021568.0000 - rmse: 19313.7656 - val_loss: 422853088.0000 - val_rmse: 20563.3926\n",
      "Epoch 225/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 358637312.0000 - rmse: 18937.7227 - val_loss: 381774688.0000 - val_rmse: 19539.0547\n",
      "Epoch 226/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 313193152.0000 - rmse: 17697.2637 - val_loss: 403074624.0000 - val_rmse: 20076.7188\n",
      "Epoch 227/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 342441504.0000 - rmse: 18505.1758 - val_loss: 380104896.0000 - val_rmse: 19496.2793\n",
      "Epoch 228/400\n",
      "166/166 [==============================] - 0s 3ms/step - loss: 366807232.0000 - rmse: 19152.2129 - val_loss: 348350880.0000 - val_rmse: 18664.1602\n",
      "Epoch 229/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 371226752.0000 - rmse: 19267.2461 - val_loss: 352963520.0000 - val_rmse: 18787.3242\n",
      "Epoch 230/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 343952064.0000 - rmse: 18545.9453 - val_loss: 386053856.0000 - val_rmse: 19648.2539\n",
      "Epoch 231/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 350578400.0000 - rmse: 18723.7383 - val_loss: 359660192.0000 - val_rmse: 18964.7090\n",
      "Epoch 232/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 328691072.0000 - rmse: 18129.8398 - val_loss: 351496832.0000 - val_rmse: 18748.2480\n",
      "Epoch 233/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 398111648.0000 - rmse: 19952.7344 - val_loss: 336384928.0000 - val_rmse: 18340.7988\n",
      "Epoch 234/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 361325152.0000 - rmse: 19008.5547 - val_loss: 346730080.0000 - val_rmse: 18620.6895\n",
      "Epoch 235/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 366491520.0000 - rmse: 19143.9688 - val_loss: 355271552.0000 - val_rmse: 18848.6484\n",
      "Epoch 236/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 362985440.0000 - rmse: 19052.1777 - val_loss: 378855936.0000 - val_rmse: 19464.2227\n",
      "Epoch 237/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 340722688.0000 - rmse: 18458.6758 - val_loss: 345924960.0000 - val_rmse: 18599.0586\n",
      "Epoch 238/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 344547360.0000 - rmse: 18561.9863 - val_loss: 447225856.0000 - val_rmse: 21147.7148\n",
      "Epoch 239/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 361968032.0000 - rmse: 19025.4570 - val_loss: 343919168.0000 - val_rmse: 18545.0586\n",
      "Epoch 240/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 317478560.0000 - rmse: 17817.9277 - val_loss: 342551328.0000 - val_rmse: 18508.1426\n",
      "Epoch 241/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 355357600.0000 - rmse: 18850.9316 - val_loss: 332232064.0000 - val_rmse: 18227.2344\n",
      "Epoch 242/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 332625632.0000 - rmse: 18238.0273 - val_loss: 398189024.0000 - val_rmse: 19954.6738\n",
      "Epoch 243/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 346132192.0000 - rmse: 18604.6289 - val_loss: 338802784.0000 - val_rmse: 18406.5957\n",
      "Epoch 244/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 329542304.0000 - rmse: 18153.3008 - val_loss: 352582336.0000 - val_rmse: 18777.1758\n",
      "Epoch 245/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 335367808.0000 - rmse: 18313.0508 - val_loss: 398424896.0000 - val_rmse: 19960.5840\n",
      "Epoch 246/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 349662400.0000 - rmse: 18699.2617 - val_loss: 357810624.0000 - val_rmse: 18915.8828\n",
      "Epoch 247/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 395752192.0000 - rmse: 19893.5215 - val_loss: 482446176.0000 - val_rmse: 21964.6582\n",
      "Epoch 248/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 376216224.0000 - rmse: 19396.2949 - val_loss: 344470752.0000 - val_rmse: 18559.9238\n",
      "Epoch 249/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 321313728.0000 - rmse: 17925.2266 - val_loss: 368940096.0000 - val_rmse: 19207.8125\n",
      "Epoch 250/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 322963840.0000 - rmse: 17971.1953 - val_loss: 380923200.0000 - val_rmse: 19517.2539\n",
      "Epoch 251/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 342211648.0000 - rmse: 18498.9629 - val_loss: 448761856.0000 - val_rmse: 21184.0000\n",
      "Epoch 252/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 335251232.0000 - rmse: 18309.8672 - val_loss: 343384960.0000 - val_rmse: 18530.6484\n",
      "Epoch 253/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 304631552.0000 - rmse: 17453.6973 - val_loss: 325326688.0000 - val_rmse: 18036.8145\n",
      "Epoch 254/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 322783264.0000 - rmse: 17966.1699 - val_loss: 347563648.0000 - val_rmse: 18643.0586\n",
      "Epoch 255/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 329382272.0000 - rmse: 18148.8926 - val_loss: 345609536.0000 - val_rmse: 18590.5762\n",
      "Epoch 256/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 331592192.0000 - rmse: 18209.6738 - val_loss: 337576384.0000 - val_rmse: 18373.2520\n",
      "Epoch 257/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 319510816.0000 - rmse: 17874.8652 - val_loss: 331055744.0000 - val_rmse: 18194.9375\n",
      "Epoch 258/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 286670656.0000 - rmse: 16931.3516 - val_loss: 348631200.0000 - val_rmse: 18671.6680\n",
      "Epoch 259/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 316582144.0000 - rmse: 17792.7559 - val_loss: 329565632.0000 - val_rmse: 18153.9434\n",
      "Epoch 260/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 303546464.0000 - rmse: 17422.5840 - val_loss: 340897376.0000 - val_rmse: 18463.4062\n",
      "Epoch 261/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 351078592.0000 - rmse: 18737.0918 - val_loss: 335790912.0000 - val_rmse: 18324.5977\n",
      "Epoch 262/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 323461472.0000 - rmse: 17985.0352 - val_loss: 399413216.0000 - val_rmse: 19985.3242\n",
      "Epoch 263/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 306895328.0000 - rmse: 17518.4277 - val_loss: 465781600.0000 - val_rmse: 21581.9746\n",
      "Epoch 264/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 306782848.0000 - rmse: 17515.2168 - val_loss: 335066272.0000 - val_rmse: 18304.8164\n",
      "Epoch 265/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 290287104.0000 - rmse: 17037.8145 - val_loss: 322744768.0000 - val_rmse: 17965.0977\n",
      "Epoch 266/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 346624576.0000 - rmse: 18617.8555 - val_loss: 339254016.0000 - val_rmse: 18418.8496\n",
      "Epoch 267/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 306662624.0000 - rmse: 17511.7852 - val_loss: 331658240.0000 - val_rmse: 18211.4863\n",
      "Epoch 268/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 358793120.0000 - rmse: 18941.8359 - val_loss: 373103232.0000 - val_rmse: 19315.8809\n",
      "Epoch 269/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 286736032.0000 - rmse: 16933.2812 - val_loss: 320593856.0000 - val_rmse: 17905.1348\n",
      "Epoch 270/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 304482816.0000 - rmse: 17449.4355 - val_loss: 458815488.0000 - val_rmse: 21419.9785\n",
      "Epoch 271/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 280667808.0000 - rmse: 16753.1426 - val_loss: 342499040.0000 - val_rmse: 18506.7305\n",
      "Epoch 272/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 289855520.0000 - rmse: 17025.1445 - val_loss: 396327424.0000 - val_rmse: 19907.9746\n",
      "Epoch 273/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 319767520.0000 - rmse: 17882.0449 - val_loss: 408166688.0000 - val_rmse: 20203.1348\n",
      "Epoch 274/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 295622816.0000 - rmse: 17193.6855 - val_loss: 333324032.0000 - val_rmse: 18257.1641\n",
      "Epoch 275/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 319168512.0000 - rmse: 17865.2871 - val_loss: 397823296.0000 - val_rmse: 19945.5078\n",
      "Epoch 276/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 306814368.0000 - rmse: 17516.1172 - val_loss: 330081344.0000 - val_rmse: 18168.1406\n",
      "Epoch 277/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 286865504.0000 - rmse: 16937.1035 - val_loss: 323428512.0000 - val_rmse: 17984.1191\n",
      "Epoch 278/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 284301472.0000 - rmse: 16861.2422 - val_loss: 327331680.0000 - val_rmse: 18092.3105\n",
      "Epoch 279/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 303209696.0000 - rmse: 17412.9180 - val_loss: 340690240.0000 - val_rmse: 18457.7969\n",
      "Epoch 280/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 318341024.0000 - rmse: 17842.1133 - val_loss: 426999200.0000 - val_rmse: 20663.9590\n",
      "Epoch 281/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 288638432.0000 - rmse: 16989.3633 - val_loss: 321139744.0000 - val_rmse: 17920.3730\n",
      "Epoch 282/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 275883360.0000 - rmse: 16609.7363 - val_loss: 338945472.0000 - val_rmse: 18410.4727\n",
      "Epoch 283/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 280261792.0000 - rmse: 16741.0215 - val_loss: 417699808.0000 - val_rmse: 20437.7051\n",
      "Epoch 284/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 309892960.0000 - rmse: 17603.7773 - val_loss: 363347136.0000 - val_rmse: 19061.6660\n",
      "Epoch 285/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 308742560.0000 - rmse: 17571.0723 - val_loss: 337795488.0000 - val_rmse: 18379.2129\n",
      "Epoch 286/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 270301248.0000 - rmse: 16440.8418 - val_loss: 378570752.0000 - val_rmse: 19456.8945\n",
      "Epoch 287/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 305358880.0000 - rmse: 17474.5215 - val_loss: 358233952.0000 - val_rmse: 18927.0684\n",
      "Epoch 288/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 278726048.0000 - rmse: 16695.0898 - val_loss: 361339104.0000 - val_rmse: 19008.9219\n",
      "Epoch 289/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 322147552.0000 - rmse: 17948.4688 - val_loss: 344734560.0000 - val_rmse: 18567.0293\n",
      "Epoch 290/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 299157184.0000 - rmse: 17296.1602 - val_loss: 337545632.0000 - val_rmse: 18372.4141\n",
      "Epoch 291/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 302227744.0000 - rmse: 17384.6992 - val_loss: 341956096.0000 - val_rmse: 18492.0547\n",
      "Epoch 292/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 293995168.0000 - rmse: 17146.2871 - val_loss: 332731840.0000 - val_rmse: 18240.9395\n",
      "Epoch 293/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 313595968.0000 - rmse: 17708.6406 - val_loss: 377535360.0000 - val_rmse: 19430.2695\n",
      "Epoch 294/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 283705088.0000 - rmse: 16843.5469 - val_loss: 318425376.0000 - val_rmse: 17844.4766\n",
      "Epoch 295/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 295730912.0000 - rmse: 17196.8281 - val_loss: 330226016.0000 - val_rmse: 18172.1211\n",
      "Epoch 296/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 274742624.0000 - rmse: 16575.3613 - val_loss: 323411392.0000 - val_rmse: 17983.6426\n",
      "Epoch 297/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 282565024.0000 - rmse: 16809.6699 - val_loss: 350485280.0000 - val_rmse: 18721.2520\n",
      "Epoch 298/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 262786400.0000 - rmse: 16210.6875 - val_loss: 327152832.0000 - val_rmse: 18087.3672\n",
      "Epoch 299/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 297265248.0000 - rmse: 17241.3828 - val_loss: 411056832.0000 - val_rmse: 20274.5371\n",
      "Epoch 300/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 313309152.0000 - rmse: 17700.5410 - val_loss: 335321088.0000 - val_rmse: 18311.7754\n",
      "Epoch 301/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 283041088.0000 - rmse: 16823.8242 - val_loss: 355039520.0000 - val_rmse: 18842.4922\n",
      "Epoch 302/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 281558048.0000 - rmse: 16779.6914 - val_loss: 360815136.0000 - val_rmse: 18995.1348\n",
      "Epoch 303/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 268928128.0000 - rmse: 16399.0273 - val_loss: 346783712.0000 - val_rmse: 18622.1289\n",
      "Epoch 304/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 272953536.0000 - rmse: 16521.3047 - val_loss: 382060416.0000 - val_rmse: 19546.3652\n",
      "Epoch 305/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 261614928.0000 - rmse: 16174.5146 - val_loss: 323676864.0000 - val_rmse: 17991.0215\n",
      "Epoch 306/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 245526544.0000 - rmse: 15669.2871 - val_loss: 330869184.0000 - val_rmse: 18189.8105\n",
      "Epoch 307/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 258823344.0000 - rmse: 16087.9873 - val_loss: 337010336.0000 - val_rmse: 18357.8418\n",
      "Epoch 308/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 268108224.0000 - rmse: 16374.0107 - val_loss: 355819616.0000 - val_rmse: 18863.1816\n",
      "Epoch 309/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 261299296.0000 - rmse: 16164.7549 - val_loss: 345841504.0000 - val_rmse: 18596.8145\n",
      "Epoch 310/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 267188400.0000 - rmse: 16345.8984 - val_loss: 375380864.0000 - val_rmse: 19374.7480\n",
      "Epoch 311/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 255942720.0000 - rmse: 15998.2100 - val_loss: 352633728.0000 - val_rmse: 18778.5449\n",
      "Epoch 312/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 295554560.0000 - rmse: 17191.7012 - val_loss: 349047456.0000 - val_rmse: 18682.8125\n",
      "Epoch 313/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 265740288.0000 - rmse: 16301.5430 - val_loss: 375734368.0000 - val_rmse: 19383.8691\n",
      "Epoch 314/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 265335664.0000 - rmse: 16289.1270 - val_loss: 338913472.0000 - val_rmse: 18409.6035\n",
      "Epoch 315/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 299348864.0000 - rmse: 17301.7012 - val_loss: 426921984.0000 - val_rmse: 20662.0898\n",
      "Epoch 316/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 278442272.0000 - rmse: 16686.5898 - val_loss: 350309824.0000 - val_rmse: 18716.5664\n",
      "Epoch 317/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 286427904.0000 - rmse: 16924.1816 - val_loss: 320891264.0000 - val_rmse: 17913.4375\n",
      "Epoch 318/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 233669376.0000 - rmse: 15286.2480 - val_loss: 333207744.0000 - val_rmse: 18253.9785\n",
      "Epoch 319/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 255949296.0000 - rmse: 15998.4150 - val_loss: 339576064.0000 - val_rmse: 18427.5898\n",
      "Epoch 320/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 283958848.0000 - rmse: 16851.0781 - val_loss: 345333728.0000 - val_rmse: 18583.1562\n",
      "Epoch 321/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 263286848.0000 - rmse: 16226.1162 - val_loss: 350925024.0000 - val_rmse: 18732.9922\n",
      "Epoch 322/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 297005280.0000 - rmse: 17233.8418 - val_loss: 335663840.0000 - val_rmse: 18321.1309\n",
      "Epoch 323/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 333452832.0000 - rmse: 18260.6914 - val_loss: 472447232.0000 - val_rmse: 21735.8516\n",
      "Epoch 324/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 244622432.0000 - rmse: 15640.4102 - val_loss: 371287616.0000 - val_rmse: 19268.8242\n",
      "Epoch 325/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 272498208.0000 - rmse: 16507.5195 - val_loss: 330846208.0000 - val_rmse: 18189.1777\n",
      "Epoch 326/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 246810976.0000 - rmse: 15710.2188 - val_loss: 364122400.0000 - val_rmse: 19081.9922\n",
      "Epoch 327/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 262512864.0000 - rmse: 16202.2490 - val_loss: 342927840.0000 - val_rmse: 18518.3105\n",
      "Epoch 328/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 249663424.0000 - rmse: 15800.7412 - val_loss: 349253216.0000 - val_rmse: 18688.3184\n",
      "Epoch 329/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 264419360.0000 - rmse: 16260.9766 - val_loss: 420550400.0000 - val_rmse: 20507.3262\n",
      "Epoch 330/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 252913376.0000 - rmse: 15903.2500 - val_loss: 326023616.0000 - val_rmse: 18056.1250\n",
      "Epoch 331/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 240531856.0000 - rmse: 15509.0898 - val_loss: 341769600.0000 - val_rmse: 18487.0117\n",
      "Epoch 332/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 265990000.0000 - rmse: 16309.2002 - val_loss: 368818528.0000 - val_rmse: 19204.6484\n",
      "Epoch 333/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 263419920.0000 - rmse: 16230.2158 - val_loss: 322149920.0000 - val_rmse: 17948.5352\n",
      "Epoch 334/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 269847392.0000 - rmse: 16427.0332 - val_loss: 329576064.0000 - val_rmse: 18154.2305\n",
      "Epoch 335/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 271505792.0000 - rmse: 16477.4336 - val_loss: 415245856.0000 - val_rmse: 20377.5820\n",
      "Epoch 336/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 266835104.0000 - rmse: 16335.0879 - val_loss: 331824000.0000 - val_rmse: 18216.0371\n",
      "Epoch 337/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 263246432.0000 - rmse: 16224.8711 - val_loss: 340595040.0000 - val_rmse: 18455.2168\n",
      "Epoch 338/400\n",
      "166/166 [==============================] - 1s 4ms/step - loss: 266925040.0000 - rmse: 16337.8408 - val_loss: 340630752.0000 - val_rmse: 18456.1855\n",
      "Epoch 339/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 255497696.0000 - rmse: 15984.2949 - val_loss: 348987456.0000 - val_rmse: 18681.2051\n",
      "Epoch 340/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 267865280.0000 - rmse: 16366.5908 - val_loss: 353086944.0000 - val_rmse: 18790.6074\n",
      "Epoch 341/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 250607984.0000 - rmse: 15830.6025 - val_loss: 347820224.0000 - val_rmse: 18649.9395\n",
      "Epoch 342/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 234038272.0000 - rmse: 15298.3096 - val_loss: 331870560.0000 - val_rmse: 18217.3145\n",
      "Epoch 343/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 259104288.0000 - rmse: 16096.7168 - val_loss: 342646112.0000 - val_rmse: 18510.7031\n",
      "Epoch 344/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 241779344.0000 - rmse: 15549.2559 - val_loss: 359916512.0000 - val_rmse: 18971.4648\n",
      "Epoch 345/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 258982992.0000 - rmse: 16092.9482 - val_loss: 328452096.0000 - val_rmse: 18123.2480\n",
      "Epoch 346/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 272606656.0000 - rmse: 16510.8047 - val_loss: 328625632.0000 - val_rmse: 18128.0352\n",
      "Epoch 347/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 255865408.0000 - rmse: 15995.7930 - val_loss: 343301536.0000 - val_rmse: 18528.3984\n",
      "Epoch 348/400\n",
      "166/166 [==============================] - 1s 3ms/step - loss: 252758080.0000 - rmse: 15898.3672 - val_loss: 357539360.0000 - val_rmse: 18908.7109\n",
      "104/104 [==============================] - 0s 951us/step - loss: 436679104.0000 - rmse: 20896.8691\n",
      "[436679104.0, 20896.869140625]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABB6ElEQVR4nO3dd3xUVf7/8dcnCSV0CChIRFDBQlWQoiura0MUsaFgAV0Uv4qKigWWVdau61rWn4qCDRDBtgqCCoorriC9iVgAqYKCNOkQ8v79ce8Mk0wSAsyQgJ/n4zGPuXPuOfd+5irzyTnnFpOEc845l2gpRR2Ac865g5MnGOecc0nhCcY551xSeIJxzjmXFJ5gnHPOJYUnGOecc0nhCca5XMystpnJzNIKUfcaM/tqf8S1P5jZIjM7s6jjcAcHTzDugBb+IG43s6q5ymeGSaJ2EYUWG0tZM9toZh8VdSz7wsxeN7OHijoOd+DwBOMOBguBTpEPZtYQSC+6cOJcCmwDzjazGkUdjHP7iycYdzAYDHSO+dwFGBRbwcwqmtkgM1tlZovN7O9mlhKuSzWzf5nZb2b2E3BeHm1fMbMVZvazmT1kZql7EF8X4EVgNnBlrm3/ycwmmNk6M1tqZteE5elm9mQY63oz+8rM4pKmmVU2s5Hh91obLmfGrP/CzB40s/FmtsHMxsT29szs6nAfq82szx58p9xxXG9m881sjZmNMLPDwnIzs6fNbGX4PWabWYNwXVszmxvG9bOZ3bm3+3fFkycYdzCYCFQws+PCH/7LgTdy1fl/QEXgSODPBAnp2nDd9cD5wAlAM4IeR6yBQBZwdFjnbOC6wgRmZrWA04Ah4atzrnUfh7FVA5oAM8PV/wKaAicDVYC7gew8dpECvAYcAdQCtgDP5apzRfhdDwFKAneG+z8e6AdcDRwGZACZ7CEz+wvwKHAZUANYDAwLV58NtAbqAZUI/tusDte9AtwgqTzQAPh8T/ftijdPMO5gEenFnAV8D/wcWRGTdHpL2iBpEfAkwQ8rBD+Mz0haKmkNwY9lpO2hwLnAbZI2SVoJPA10LGRcnYHZkuYCQ4H6ZnZCuO5K4DNJQyXtkLRa0sywZ/VXoIeknyXtlDRB0rbcGw/bvCdps6QNwMMECTTWa5J+lLQFeJsgkUGQSEdK+jLc9r3kncR250rgVUnTw+30BlqF8187gPLAsYBJ+k7SirDdDuB4M6sgaa2k6Xuxb1eMeYJxB4vBBH+pX0Ou4TGgKsFf7otjyhYDNcPlw4CludZFHAGUAFaEw1jrgJcIegOF0Zmg54Kk5cA4giEzgMOBBXm0qQqUzmddDmZWxsxeCoe5fge+BCrlGsL7JWZ5M1AuXM7xvSVtYlfvYk8cRswxk7Qx3E5NSZ8T9KieB341s/5mViGsegnQFlhsZuPMrNVe7NsVY55g3EFB0mKCyf62wH9yrf6N4K/lI2LKarGrl7OC4Mc+dl3EUoIJ+qqSKoWvCpLq7y4mMzsZqAv0NrNfzOwXoAXQKTwFeilwVB5NfwO25rMut57AMUALSRUIhqMArBBtc3xvMytDMEy2p5YTc2zNrGy4nZ8BJD0rqSlQn2Co7K6wfIqk9gTJ+gOC3pU7iHiCcQeTrsBfwr/EoyTtJPjxetjMypvZEcAd7JqneRu41cwyzawy0Cum7QpgDPCkmVUwsxQzO8rMcg9D5aUL8ClwPMGwVBOCuYYyBMNuQ4AzzewyM0szswwzayIpG3gVeMrMDgtPQmhlZqXy2Ed5gnmXdWZWBehbiLgi3gXOD080KAk8wO5/E1LNrHTMqyTwJnCtmTUJY3wEmCRpkZmdZGYtzKwEsIkgce40s5JmdqWZVZS0A/gd2LkHsbsDgCcYd9CQtEDS1HxW30LwA/cT8BXBj+Kr4boBwGhgFjCd+B5QZ4IhtrnAWoIf5gJPNzaz0gRzO/9P0i8xr4UEw3ldJC0h6HH1BNYQTPA3DjdxJ/ANMCVc9zh5/3t9huCU7N8ITnb4pKC4Ykn6FuhOcCxWhN9t2W6a9SJIaJHX55LGEszfvBdu5yh2zVFVIDi+awmG0VYTnMAAwRzYonBo7/+AqwobuzswmD9wzDnnXDJ4D8Y551xSeIJxzjmXFJ5gnHPOJYUnGOecc0mx29uR/1FUrVpVtWvXLuownHPugDJt2rTfJFXLa50nmFDt2rWZOjW/M1ydc87lxcwW57fOh8icc84lhScY55xzSeEJxjnnXFL4HEwBduzYwbJly9i6dWtRh3LQKl26NJmZmZQoUaKoQ3HOJZgnmAIsW7aM8uXLU7t2bcwKc3NatycksXr1apYtW0adOnWKOhznXIL5EFkBtm7dSkZGhieXJDEzMjIyvIfo3EHKE8xueHJJLj++zh28kpZgzOxVM1tpZnPyWHenmcnMqsaU9Taz+Wb2g5mdE1Pe1My+Cdc9a+EvkpmVMrO3wvJJ4eNZI226mNm88NWFJNq5E37+GTZuTOZenHPuwJPMHszrQJvchWZ2OMFz05fElB1P8PyI+mGbF2Ie+doP6EbwZMC6MdvsCqyVdDTBM9IfD7cVeehSC6A50Dd8iFRSZGfDihWweXOy9uCccwempCUYSV8SPCgpt6eBu4HYB9G0B4ZJ2hY+kGk+0NzMagAVJH2t4ME1g4ALY9oMDJffBc4IezfnAJ9KWiNpLcETBeMSXaIl67E669at44UXXtjjdm3btmXdunWJD8g55wppv87BmNkFwM+SZuVaVZPg+eQRy8KymuR8wl6kPEcbSVnAeoLngOe3raRI9hRCfglm586Cny770UcfUalSpX3ad1ZW1j61d879se2305TNrAzQBzg7r9V5lKmA8r1tkzumbgTDb9SqVSuvKlG33QYzZ+axYQXzL6VKQcmSBW4iTpMm8MwzBdfp1asXCxYsoEmTJpQoUYJy5cpRo0YNZs6cydy5c7nwwgtZunQpW7dupUePHnTr1g3YdW+1jRs3cu655/KnP/2JCRMmULNmTYYPH056enqe+zvttNM4+eSTGT9+PBdccAG1atXi/vvvJzU1lYoVK/Lll1/y+uuv88EHH7Bz507mzJlDz5492b59O4MHD6ZUqVJ89NFHVKlShQULFtC9e3dWrVpFmTJlGDBgAMcee+yeHSTn3AFrf14HcxRQB5gVztNnAtPNrDlBL+PwmLqZwPKwPDOPcmLaLDOzNKAiwZDcMuC0XG2+yCsgSf2B/gDNmjUrls+Ofuyxx5gzZw4zZ87kiy++4LzzzmPOnDnR60ZeffVVqlSpwpYtWzjppJO45JJLyMjIyLGNefPmMXToUAYMGMBll13Ge++9x1VX5f/483Xr1jFu3DgAGjZsyOjRo6lZs2aOIbc5c+YwY8YMtm7dytFHH83jjz/OjBkzuP322xk0aBC33XYb3bp148UXX6Ru3bpMmjSJm266ic8//zzxB8k5VyzttwQj6RvgkMhnM1sENJP0m5mNAN40s6eAwwgm8ydL2mlmG8ysJTAJ6Az8v3ATI4AuwNfApcDnkmRmo4FHYib2zwZ672v8+fU0du6EGTMgMxOqV9/Xvexe8+bNc1yU+Oyzz/L+++8DsHTpUubNmxeXYOrUqUOTJk0AaNq0KYsWLSpwH5dffnl0+ZRTTuGaa67hsssu4+KLL46Wn3766ZQvX57y5ctTsWJF2rVrBwQJafbs2WzcuJEJEybQoUOHaJtt27bt1Xd2zh2YkpZgzGwoQU+iqpktA/pKeiWvupK+NbO3gblAFtBdUmSS4UaCM9LSgY/DF8ArwGAzm0/Qc+kYbmuNmT0ITAnrPSApr5MNDkhly5aNLn/xxRd89tlnfP3115QpU4bTTjstz4sWS5UqFV1OTU1ly5Ythd7Hiy++yKRJkxg1ahRNmjRhZjhOGLvNlJSU6OeUlBSysrLIzs6mUqVK0frOuT+epCUYSZ12s752rs8PAw/nUW8q0CCP8q1Ah9zl4bpXgVf3INxiq3z58mzYsCHPdevXr6dy5cqUKVOG77//nokTJyZ8/wsWLKBFixa0aNGCDz/8kKVLl+6+EVChQgXq1KnDO++8Q4cOHZDE7Nmzady4ccJjdM4VT34l/z6KnEWWrNOUMzIyOOWUU2jQoAF33XVXjnVt2rQhKyuLRo0ace+999KyZcuE7/+uu+6iYcOGNGjQgNatW+9RghgyZAivvPIKjRs3pn79+gwfPjzh8Tnnii9Tsn4ZDzDNmjVT7idafvfddxx33HEFtsvOhunToWZNqFEjmREevApznJ1zxZOZTZPULK913oNxzjmXFH67/n2U7CGyZOnevTvjx4/PUdajRw+uvfbaIorIOXew8QTzB/X8888XdQjOuYOcD5E555xLCk8w++hAHSJzzrlk8wTjnHMuKTzBFHN7e7t+gGeeeYbN/qAa51wR8QSTAGbF73kwsOcJZnePAHDOuT3hZ5EVc7G36z/rrLM45JBDePvtt9m2bRsXXXQR999/P5s2beKyyy5j2bJl7Ny5k3vvvZdff/2V5cuXc/rpp1O1alX++9//5rn9cuXKcccddzB69GiefPJJRo4cyYgRI0hLS+Pss8/mX//6F9dccw3p6el8//33LF68mNdee42BAwfy9ddf06JFC15//XUAxowZQ9++fdm2bRtHHXUUr732GuXKlduPR8s5V5x4gimk2267Ld8bN27YEDwLJub+j4XSpEkTntnNA2Fib9c/ZswY3n33XSZPnowkLrjgAr788ktWrVrFYYcdxqhRo4DgHmUVK1bkqaee4r///S9Vq1bNd/ubNm2iQYMGPPDAA6xZs4auXbvy/fffY2Y5bs+/du1aPv/8c0aMGEG7du0YP348L7/8MieddBIzZ84kMzOThx56iM8++4yyZcvy+OOP89RTT3Hfffft2UFxzh00PMEkQLKfahkxZswYxowZwwknnADAxo0bmTdvHqeeeip33nkn99xzD+effz6nnnpqobeZmprKJZdcAgQ3qCxdujTXXXcd5513Hueff360Xrt27TAzGjZsyKGHHkrDhg0BqF+/PosWLWLZsmXMnTuXU045BYDt27fTqlWrRH1159wByBNMIRXU05g+HapVg8MPz7dKQkiid+/e3HDDDXHrpk2bxkcffUTv3r05++yzC91zKF26NKmpqQCkpaUxefJkxo4dy7Bhw3juueeiDwiLvR1/7lv1Z2VlkZqayllnncXQoUP39Ws65w4SPslfzMXerv+cc87h1VdfZePGjQD8/PPPrFy5kuXLl1OmTBmuuuoq7rzzTqZPnx7XtjA2btzI+vXradu2Lc8888wePculZcuWjB8/nvnz5wOwefNmfvzxx0K3d84dfLwHkwDJHCKLvV3/ueeeyxVXXBEdeipXrhxvvPEG8+fP56677iIlJYUSJUrQr18/ALp168a5555LjRo18p3kj7Vhwwbat2/P1q1bkcTTTz9d6DirVavG66+/TqdOnaJPrnzooYeoV6/eXnxr59zBwG/XH9rb2/VD8MjkjAyoVStZ0R3c/Hb9zh24/Hb9zjnn9jsfIkuA/XUW2b5o0aJFdOgqYvDgwdGzwZxzLtE8wSRIcR9pnDRpUlGH4Jz7g/Ehst3wOark8uPr3MHLE0wBSpcuzerVq3f7I3ggDJEVR5JYvXo1pUuXLupQnHNJ4ENkBcjMzGTZsmWsWrWqwHorVwa3i/EbF++50qVLk5mZWdRhOOeSIGkJxsxeBc4HVkpqEJY9AbQDtgMLgGslrQvX9Qa6AjuBWyWNDsubAq8D6cBHQA9JMrNSwCCgKbAauFzSorBNF+DvYSgPSRq4N9+hRIkS1KlTZ7f1zjwTzj0XXn55b/binHMHp2QOkb0OtMlV9inQQFIj4EegN4CZHQ90BOqHbV4ws9SwTT+gG1A3fEW22RVYK+lo4Gng8XBbVYC+QAugOdDXzCon4ftFpaQU/0l+55zb35KWYCR9CazJVTZGUlb4cSIQGRtpDwyTtE3SQmA+0NzMagAVJH2tYCJkEHBhTJtIz+Rd4AwzM+Ac4FNJayStJUhquRNdQplBdnYy9+Cccweeopzk/yvwcbhcE1gas25ZWFYzXM5dnqNNmLTWAxkFbCuOmXUzs6lmNnV38ywFSUnxBOOcc7kVSYIxsz5AFjAkUpRHNRVQvrdtchZK/SU1k9SsWrVqBQddAB8ic865ePs9wYQT8OcDV2rX+b/LgNib3WcCy8PyzDzKc7QxszSgIsGQXH7bShofInPOuXj7NcGYWRvgHuACSbEn9Y4AOppZKTOrQzCZP1nSCmCDmbUM51c6A8Nj2nQJly8FPg8T1mjgbDOrHE7unx2WJY33YJxzLl4yT1MeCpwGVDWzZQRndvUGSgGfBvmCiZL+T9K3ZvY2MJdg6Ky7pJ3hpm5k12nKH7Nr3uYVYLCZzSfouXQEkLTGzB4EpoT1HpCU42SDxH9X78E451xufrv+UF636y+sY4+FJk1g2LDExuScc8Wd364/ybwH45xz8TzBJIDPwTjnXDxPMAng18E451w8TzAJ4ENkzjkXzxNMAvgQmXPOxfMEkwDeg3HOuXieYBLAezDOORfPE0wCeA/GOefieYJJAO/BOOdcPE8wCeA9GOeci+cJJgH8OhjnnIvnCSYBfIjMOefieYJJAB8ic865eJ5gEsB7MM45F88TTAJ4D8Y55+J5gkkA78E451w8TzAJ4D0Y55yL5wkmAbwH45xz8TzBJIBfB+Occ/E8wSSAD5E551w8TzAJ4ENkzjkXzxNMAngPxjnn4iUtwZjZq2a20szmxJRVMbNPzWxe+F45Zl1vM5tvZj+Y2Tkx5U3N7Jtw3bNmZmF5KTN7KyyfZGa1Y9p0Cfcxz8y6JOs7RngPxjnn4iWzB/M60CZXWS9grKS6wNjwM2Z2PNARqB+2ecHMUsM2/YBuQN3wFdlmV2CtpKOBp4HHw21VAfoCLYDmQN/YRJYM3oNxzrl4SUswkr4E1uQqbg8MDJcHAhfGlA+TtE3SQmA+0NzMagAVJH0tScCgXG0i23oXOCPs3ZwDfCppjaS1wKfEJ7qE8h6Mc87F299zMIdKWgEQvh8SltcElsbUWxaW1QyXc5fnaCMpC1gPZBSwrThm1s3MpprZ1FWrVu31l/LTlJ1zLl5xmeS3PMpUQPnetslZKPWX1ExSs2rVqhUq0Lz4EJlzzsXb3wnm13DYi/B9ZVi+DDg8pl4msDwsz8yjPEcbM0sDKhIMyeW3raTxITLnnIu3vxPMCCByVlcXYHhMecfwzLA6BJP5k8NhtA1m1jKcX+mcq01kW5cCn4fzNKOBs82scji5f3ZYljTeg3HOuXhpydqwmQ0FTgOqmtkygjO7HgPeNrOuwBKgA4Ckb83sbWAukAV0l7Qz3NSNBGekpQMfhy+AV4DBZjafoOfSMdzWGjN7EJgS1ntAUu6TDRLKezDOORfP5L+MADRr1kxTp07dq7YdO8KMGfDDDwkOyjnnijkzmyapWV7rissk/wHNezDOORfPE0wC+ByMc87F8wSTAH4djHPOxfMEkwA+ROacc/E8wSSAD5E551w8TzAJ4D0Y55yL5wkmAbwH45xz8TzBJID3YJxzLp4nmATwHoxzzsXzBJMA3oNxzrl4nmASwK+Dcc65eJ5gEsCHyJxzLp4nmATwITLnnIvnCSYBvAfjnHPxPMEkgPdgnHMunieYBPAejHPOxfMEkwDeg3HOuXieYBLAT1N2zrl4nmASwIfInHMunieYBPAhMueci+cJJgG8B+Occ/E8wSSA92Cccy5eoRKMmfUwswoWeMXMppvZ2Xu7UzO73cy+NbM5ZjbUzEqbWRUz+9TM5oXvlWPq9zaz+Wb2g5mdE1Pe1My+Cdc9a2YWlpcys7fC8klmVntvYy3c9/EejHPO5VbYHsxfJf0OnA1UA64FHtubHZpZTeBWoJmkBkAq0BHoBYyVVBcYG37GzI4P19cH2gAvmFlquLl+QDegbvhqE5Z3BdZKOhp4Gnh8b2ItLO/BOOdcvMImGAvf2wKvSZoVU7Y30oB0M0sDygDLgfbAwHD9QODCcLk9MEzSNkkLgflAczOrAVSQ9LUkAYNytYls613gjEjvJhnMggTjScY553YpbIKZZmZjCBLMaDMrD+zVoJCkn4F/AUuAFcB6SWOAQyWtCOusAA4Jm9QElsZsYllYVjNczl2eo42kLGA9kLE38RZGSngUPcE459wuhU0wXQmGrE6StBkoQTBMtsfCuZX2QB3gMKCsmV1VUJM8ylRAeUFtcsfSzcymmtnUVatWFRx4ATzBOOdcvMImmFbAD5LWhcng7wS9gr1xJrBQ0ipJO4D/ACcDv4bDXoTvK8P6y4DDY9pnEgypLQuXc5fnaBMOw1UE1uQORFJ/Sc0kNatWrdpefp1giAx8ot8552IVNsH0AzabWWPgbmAxwZzH3lgCtDSzMuG8yBnAd8AIoEtYpwswPFweAXQMzwyrQzCZPzkcRttgZi3D7XTO1SayrUuBz8N5mqTwHoxzzsVLK2S9LEkys/bAvyW9YmZddtsqD5Immdm7wHQgC5gB9AfKAW+bWVeCJNQhrP+tmb0NzA3rd5e0M9zcjcDrQDrwcfgCeAUYbGbzCXouHfcm1sLyHoxzzsUrbILZYGa9gauBU8PThEvs7U4l9QX65ireRtCbyav+w8DDeZRPBRrkUb6VMEHtD96Dcc65eIUdIrucIAH8VdIvBGdpPZG0qA4w3oNxzrl4hUowYVIZAlQ0s/OBrZL2dg7moOM9GOeci1fYW8VcBkwmGHa6DJhkZpcmM7ADSSTBeA/GOed2KewcTB+Ca2BWAphZNeAzgqvk//B8iMw55+IVdg4mJZJcQqv3oO1Bz4fInHMuXmF7MJ+Y2WhgaPj5cuCj5IR04PEejHPOxStUgpF0l5ldApxCcBuW/pLeT2pkBxDvwTjnXLzC9mCQ9B7wXhJjOWB5D8Y55+IVmGDMbAN53CSSoBcjSRWSEtUBxnswzjkXr8AEI6n8/grkQOY9GOeci+dngiWAXwfjnHPxPMEkgA+ROedcPE8wCeBDZM45F88TTAJ4D8Y55+J5gkkA78E451w8TzAJ4D0Y55yL5wkmAbwH45xz8TzBJICfpuycc/E8wSSAD5E551w8TzAJ4ENkzjkXzxNMAngPxjnn4nmCSQDvwTjnXDxPMAngPRjnnItXJAnGzCqZ2btm9r2ZfWdmrcysipl9ambzwvfKMfV7m9l8M/vBzM6JKW9qZt+E6541C/oSZlbKzN4KyyeZWe3kfp/g3Xswzjm3S1H1YP4NfCLpWKAx8B3QCxgrqS4wNvyMmR0PdATqA22AF8wsNdxOP6AbUDd8tQnLuwJrJR0NPA08nswv4z0Y55yLt98TjJlVAFoDrwBI2i5pHdAeGBhWGwhcGC63B4ZJ2iZpITAfaG5mNYAKkr6WJGBQrjaRbb0LnBHp3SSDXwfjnHPxiqIHcySwCnjNzGaY2ctmVhY4VNIKgPD9kLB+TWBpTPtlYVnNcDl3eY42krKA9UBG7kDMrJuZTTWzqatWrdrrL+RDZM45F68oEkwacCLQT9IJwCbC4bB85NXzUAHlBbXJWSD1l9RMUrNq1aoVHHUBfIjMOefiFUWCWQYskzQp/PwuQcL5NRz2InxfGVP/8Jj2mcDysDwzj/IcbcwsDagIrEn4Nwl5D8Y55+Lt9wQj6RdgqZkdExadAcwFRgBdwrIuwPBweQTQMTwzrA7BZP7kcBhtg5m1DOdXOudqE9nWpcDn4TxNUngPxjnn4qUV0X5vAYaYWUngJ+BagmT3tpl1BZYAHQAkfWtmbxMkoSygu6Sd4XZuBF4H0oGPwxcEJxAMNrP5BD2Xjsn8Mt6Dcc65eEWSYCTNBJrlseqMfOo/DDycR/lUoEEe5VsJE9T+4D0Y55yL51fyJ4D3YJxzLp4nmATw62Cccy6eJ5gE8CEy55yL5wkmAXyIzDnn4nmC2Udbtmzhv/99C5jnPRjnnIvhCWYfbdy4kfvv7wiM9h6Mc87F8ASzj9LT08OlLd6Dcc65GJ5g9lFsgvEejHPO7eIJZh+lpqaSllYC78E451xOnmASoFSpdGAzO3futqpzzv1heIJJgDJlygBbWJO0+zU759yBxxNMApQtmw5sYcmSoo7EOeeKD08wCVCmTDqlSm1h8eKijsQ554oPTzAJkJ6eTunSnmCccy6WJ5gESE9Pp2TJzZ5gnHMuhieYBChTpgxpacEcjJ+q7JxzAU8wCZCeno7ZFrZuhVWrijoa55wrHjzBJEBwNf8WAL79tmhjcc654sITTALEJpjx44s2FuecKy48wSRAeno627ZtoUED+Oqroo7GOeeKB08wCVCmTBk2b97MqafChAnw++9FHZFzzhU9TzAJkJ6ezpYtW7jySrF5M1x9dVFH5JxzRa/IEoyZpZrZDDMbGX6uYmafmtm88L1yTN3eZjbfzH4ws3Niypua2TfhumfNgocXm1kpM3srLJ9kZrWT+V0it+xv2nQbjzwCI0bAl18mc4/OOVf8FWUPpgfwXcznXsBYSXWBseFnzOx4oCNQH2gDvGBmqWGbfkA3oG74ahOWdwXWSjoaeBp4PJlfJJJgtmzZwi23wCGHwMMPJ3OPzjlX/BVJgjGzTOA84OWY4vbAwHB5IHBhTPkwSdskLQTmA83NrAZQQdLXkgQMytUmsq13gTMivZtkiE0w6elwxx0wZgxMmZKsPTrnXPFXVD2YZ4C7gdhnQB4qaQVA+H5IWF4TWBpTb1lYVjNczl2eo42kLGA9kJE7CDPrZmZTzWzqqn24QjK4XT9s3rwZgBtvhMqV4frr4Zdf9nqzzjl3QNvvCcbMzgdWSppW2CZ5lKmA8oLa5CyQ+ktqJqlZtWrVChlOvNgeDECFCjBkCMyfD2ecAatX7/WmnXPugFUUPZhTgAvMbBEwDPiLmb0B/BoOexG+rwzrLwMOj2mfCSwPyzPzKM/RxszSgIpA0h4HljvBAJx7LowcGSSZa66BBQuStXfnnCue9nuCkdRbUqak2gST959LugoYAXQJq3UBhofLI4CO4ZlhdQgm8yeHw2gbzKxlOL/SOVebyLYuDfeRtNtQ5pVgAE47DR59NEg0Rx8NH32UrAicc674KU7XwTwGnGVm84Czws9I+hZ4G5gLfAJ0l7QzbHMjwYkC84EFwMdh+StAhpnNB+4gPCMtWSJzMJs2bYpbd9tt8PHHcNxxQU/mjjv8hpjOuT8GS+If9geUZs2aaerUqXvV9tdff6V69eo8+eST3HHHHXnWmTkTevYMro85+miYPRtKlNiHgJ1zrhgws2mSmuW1rjj1YA5Yhx56KLVq1WLy5Mn51mnSBMaOhffeg++/h5de2n/xOedcUfAEkyDNmzcvMMFEtGsHf/5zcCFm06bw+OP+kDLn3MHJE0yCNG/enIULF/LLbi58MYN77w2uj5k+HXr1gm7dYPBgTzTOuYOLJ5gEOe+88zAznnjiid3W/ctf4M474Z13oEcPePll6NwZBg2C//53PwTrnHP7gU/yh/Zlkj+ia9euDB48mJ9//pk9uXBz1qxgjiYiOxtWroQqVfxEAOdc8eaT/PtJjx492LFjB++9994etWvcGFJTd33+6CM44gg48UT4+usEB+mcc/uJJ5gEatiwIccddxxDhgzZ47bvvANHHRUsn39+cLuZ9evhT3+Cf/wjuOVMVtau+tu3w7Bh8Mkn0LWrz98454ofHyILJWKIDOCpp56iZ8+e/Oc//+Giiy7ao7bbtkHp0sHy0KFw0knBNTMRr74aDJt99hl88w2MG7dr3XffwbHH7nP4zjm3RwoaIvMEE0pUgtmxYwfNmjVj9uzZXH/99bzwwgukpaUVun2jRsH8y4oVweeMDFi7Nmcds/geS79+0L49lCkTzOmccAKUL5+zzurVULEi7EE4zjlXIJ+D2Y9KlCjB2LFjuf322xkwYAC33HLLHrX/+mtYuDBIImbBtTIAZ50VnHn2xRfBrWYeeQT694caNYL5m8ceC3owhx4aXGdTr96uG2yuXQsvvACZmXDqqTBvXmK/s3PO5UmSvySaNm2qRLvjjjsE6JprrtHSpUtzrFu0aJE2bNiw223cfbcE0pdf5l+na9egTvXqUrt20n33SaVLB+XZ2dJJJwXrGzaUypULlps0kX79NWg/b96uZeec2xPAVOXzu1rkP+zF5ZWMBLNlyxZdffXVKlOmjI444giNGjVK2dnZ2rx5sypXrqzrrrtut9uYNk06/3xpy5aC9iN99ZW0atWushtvDP7rRl6PPSbt2CH98ov04IOSmXTssdK990qVKwfLixZJP/+cgC/unPvDKCjB+BxMKFFzMHmZNm0al19+OQsWLKBNmzZ06tSJLl26UKlSJX799VdKliyZ8H3++is8/XRwKxqADRugXLld6889NzgDLTczuPtu2LQJ7roLatWCn38O5naeeQZuvTWYF3LOOfBJ/kJJZoIB2L59O8899xx33nknkkhJSSE7O5tRo0bRtm3bpO131apgDqZevZzlixfD8OHwww9BH6dZsyCpDB4MU6YEdWrWhBkzoG5dKFUqOPngxBOD5cjp02XKBO2eeAJatYJzzknaV3HOFUMFJZgiH5oqLq9kDJHlZfDgwWrbtq1efvllHXLIIcrMzNR1112nxYsX56g3a9Ys/fLLL3rggQe0efPm/RKbJM2dK3XsKL3wQjC0dvLJOYfaIi8zqVUraeLEYAgvUv755/lve9Mm6ZtvpAcekJ57br99JedcEuFzMMUnwcR64oknBAhQrVq1NHXqVEnShAkTBKhcuXIC9Oabb+732LKypEMP3ZVkrrgiSAqNGkmzZ0tDh+ZMNo8+KlWsuEPt2v0U3cacOcEcUt260kUXBScgxCapRFm3Tvrpp93XOxhceOGF6tWrV1GH4VxUQQnGr4goQjfddBMbN26kYcOG3HbbbTRv3pxOnToxYcIEADZu3AjA6NGj6dSpU57bCP77QvDU6MRJTYWBA4M7Pl93HURurda9e/DesGEw/LZ5c7A+IwM+/XQwH374f9xww3IyMzN45JHg4lEp71Ojx46F1q2D+aEKFWD+fDjssGB5T/ToAWPGBHNFCT4Mxc6ECRPYsGFDUYfhXOHkl3n+aK+i6MHE+u2339SzZ0+lp6erRo0aeuSRRwSobNmyAtSsWTM1b95cxxxzjJ544gnt3LlTkvTUU0/p8MMP15aY08xmzpypbdu27ffvcPPNvcIe2fhoL6VkSal3b+mOO4KezuzZQa8msr5RIyklRfrzn3fVb91aGjtWWrxYOu00qVev/M+i27FDqlIlaLtgQeHi/O6775SdnZ2w772/7NixQ2amxo0bF3UozkXhQ2TFP8FErFmzRuvXr5ckjRs3Tm+//bZSU1N1+umn65xzzlFGRkZ0WK1GjRrR5f/85z+SpLFjxwpQp06dCrzOJhk/sJ07dxag118fpMcekwYPDk6dzs6Wtm+XZs0K6mVlKcdQGWwSSMcdJ/XsKR12WDCsdtNNEswTSOedJ33ySXCNz/ffS5s3S088IX388a7tvPVW5LtJGzfmHePHH38sQEOGDIlbt2PHDj388MPR41/cLF++XIBq1qy5X/bXtWtXNWrUaL/syx24PMEcQAkmL7HJYNWqVTrmmGN08cUXq0KFCtEEA6hBgwaqUKGCSpQoIUDly5fXjBkzJEkzZszQkiVLlJWVpXvuuUeHHnqoFi5cmNA4zzrrLAHq27fvbutOmiT16yfBNEGa4BuFoWrQoEjSmChAt902USClpgbltWpJ//xnsJyRIZUqtSvJXHCBVLNmsHzZZcEFpN99FyS4Ll2kE074PwHq1eshvf12kIwiPv/8KwHq339gQo9LokyfPl2ASpYsuV96YJH/rw7E3p7bfzzBHOAJJj+//PKLhgwZon/84x86/vjj1apVK9WqVUtDhgzRxx9/rBo1aqhatWq66KKLVKpUKdWsWVOnn3569IejoMniHTt26Pfff89RNmXKFG3dulVDhw7VFVdcEdemfv36AnTVVVcVKv6sLOnyy58XoIce6h8t37JFql9fqlfvRQF66aUBql9fSkuTXn89GGqL7QH16CHVrh0s160bJJlbbgk+ly8vlSgRnGQQ1D9ZgKpW7SOQXn1VWrs2OEkABglQSspDmjUrSEr5dWZWrgwuTJ0yJefFqcOGSc2bB2fXJdpHH30U/W+X+79Nom3cuDG6r99++63AuuvXr9dRRx2lcePGJTUmVzx5gjlIE8zufP3117rgggtUvnx5HXHEESpfvrwqV66sAQMG6IILLlDVqlX12muv6ZlnntFbb72lWbNmae3atZKk6667TjVr1tTWrVslSaNHjxag6tWr67jjjhOgX375Jcf+qlSpIkCtWrWKlk2ePLnAIadbbrkl7KXcFreuZ8+eAtS7d2/99JP0xRdB+YABwf+5N94onXJKcHeCb74J7mYQsX37dp1zTlCvUqXg/brrflNaWlr4w3mtatQI7mJQr14k+dwfruumnj2DHhBIf/2r1Lmz9Nln0sMPz9cVVwTlZctKqakXq0GDXedcN26saC8r8of/W28tUpMmJ6pfv8V68sngtj+9ekn9d+VUZWcHw34RY8eO1YoVK3Icj1dffTX6o/9Tkk+b++qrr6L7mj59eo5169ev15QpU6Kfx48fH/6R8FDC48jOztasyNiqK5aKVYIBDgf+C3wHfAv0CMurAJ8C88L3yjFtegPzgR+Ac2LKmwLfhOueZdeFo6WAt8LySUDt3cV1MCaYiHXr1mndunVas2ZNdPJ/xowZOuSQQ3IMsQEyM3Xt2lUpKSkConMSF154YVzdu+++W5999pmWLl2qLVu2RMszMjK0c+dOLViwQIAOO+ywaKLKLTKsdtZZZ8Wtu+CCCwTo8ssvz+M75f99I6d5v/fe/zRggLRmjbR0qdSnTx+ZmcqUKaOWLc/RmDGRxBJ5dQm/wwmC/oLsHOvNRgpQ5cqTw7JVYf3WGjFCuvDCoN6RRwbvS5ZI48dL8FxYr79AKlMm6IWlpu66x9yTTwYnK2zYEJzwkZqaqksu+at27AjWDx0qNW78aPQYT548Oe57f/aZNGFCsLxgQXBq+erVBfyPUYCnn346uq8PPvggx7pTTz1VQHSOb9CgoOdXmFsf7anhw4cL0MyZMyVJO3fuLNSQ3cqVK3XjjTfm2dN7//339+u1ZYnyr3/9S1dffXVRhxGnuCWYGsCJ4XJ54EfgeOCfQK+wvBfweLh8PDArTBp1gAVAarhuMtAKMOBj4Nyw/CbgxXC5I/DW7uI6mBNMflavXq3Jkyfrt99+04wZMzRs2LDoRH2tWrV0xBFHCFCZMmWiQ2q5k0zkdfLJwdBTmzZtBOif//ynbr/99uj6iy++WC1bttS3336rm2++Wddee63GjBmjypUri3wmriM9pczMTHXo0EEdO3aUFPzIDBo0SL/99ptWrlypDz/8UKNHj9a7776rLVu2RBPTLbfcEt3W/PnzVbZsWV166aVq3769GjZsqKysYL6mdOngAtMTTmid63u9qMGDx6hDB+m226QyZf4qQI8++ph27JAqVfowrFda8JLMVqpcOWn48OBf1iWXSOnpElwpQEcf/X/68MNdCeuQQ7J12GHZeuyxXTchPfJIqXbtN8LtVtdjj2Vr9erIdnpEYzvjjFHq3l2KdHI2bdq13fHjpbPO+lUg5e5UjB49OtpLLUi7du2i12E9/fTTuu666zRu3DhlZ2dHY4gMifXt21eAzjzzTGVnZ+c4o3F3Fi5cqOrVq8f1kiL69OkjQK+99pok6dhjj9XFF1+82+1GzsJ8//33c5TPmTNHgF544YV82+7cuVM//vhjob9DIq1bty5uZCDi5JNPVmpqaqFukrs/FasEExcADAfOCnsnNbQrCf2gXb2X3jH1R4dJpQbwfUx5J+Cl2DrhchrwW6R3k9/rj5hg8rN69Wrt2LFD06ZNU79+/XT99dfr/vvv1+bNmzVq1Cg1b948+iPTq1cv3XTTTdHPI0eOVN26dXMkntatc/9w53xVrFhRgO69917dfvvt+uijj/Tyyy+rVKlScXWbNGkSTWYXXXSR2rdvL0ClS5fOkQwhmAy/+eab1aVLF6Wnp6ty5cpavHixbrzxRpmZ3nzzTQ0cuF79+kkrVqxQpUqV8owvKytLc+fOjX4+//zztX79ep17bucc9Vq1+pM2bdquTz/9QsF8zg7BTpUtW0eAjj++kWbP/kZ162apXr1tysw8UmbVBP8Lk8O2sFfUKWa70wQbBEGPIjU1OIEjJWWgQOrQIVtr1mzUk0/uSjBlyrwrQKVKva4aNaRt24Ke0TvvLBKghg1P1aZNwSnea9ZI06dL334bzIktXrxY559/sQB16nSjSpcurczMIP4jjzxSs2fPjsZ23nlP6tFHpauuuipMoEfrscceU5UqVTRx4kTNnz8/7v+tHTuCWCIGDBggYobXtm7dGu1drF69Wuedd54A3XXXXVq4cGF037vTqFEjAXrwwQdzlA8bNkyAunbtqnHjxumNN97Qtm3bcpzWP3DgQKWkpGj+/Pnq06ePevToscf/hkaOHKk33nhjj9t16NBB9erVi+ulZWdnR////DzmdhmffPKJDj/8cK2O6apu2rRJF110kWbPnr3H+98bxTbBALWBJUAFYF2udWvD9+eAq2LKXwEuBZoBn8WUnwqMDJfnAJkx6xYAVQuKxRPMnnnrrbf01FNPSQr+53/wwQejP+CzZs3SSy+9pD59+mj8+PH6+OOPlZqaqv79+6tz58664YYb9OOPP2rQoEGqW7euhg0bluP069hXpIdz8cUX5yjPXT8lJUVnnnmmLr30Uj388MPq2LFjjvXXXnutJk2aJEl68MEHo+Vly5ZV586dowlq1xzNrteTTz4ZXa5evbogGEqMjS/yOuuss2K2UU4nnHB6XOI74oi6uu++F6KfmzU7TX37ShkZHQQVVapUKbVsebFSU0sLjhYEbevWratbb71VgB5//Endeuv3gotkVk4wX3Xr7tDdd78oCBL2oYfWEcxS69abVaHCBsGL0X3Wrr1OTZtGekbbBBcqJaWGcn73EUpJOSLmWJXTTTd1V1pamkqWLKm0tE6qWFFq2bJVtE6FChWjy+XLl9eUKVO0fPlq3XnnnVq6dKluvjn41Vm7Vpo4caIuuugiAWrXrp2k4Ae2RYsWWrRokdLT06Pbatz4PN1///3Rz8uWLcv3/81vv/02Wi/S65WkDRs2RHtEsa8TTzxRbdu2jda74oorBOj111+P1lm6dKnGjx+fZ+/hm2++0bpcY7b169dXpUqVtCMc41y/fr369esX/RyxZMkS7dixQ1lZWVq4cGH0/6fvvvsuR73IKeqxyViSLr/8ckHOYcxRo0YJUM+ePfM9RolULBMMUA6YBlwcfs4vwTyfR4K5BDgpjwTzYbj8bR4JJiOPGLoBU4GptWrVSuhB/yMqaGx8d9eWzJ07V/3799ett96qG264QT/88IM+/fRTTZw4Ua1bt9bKlSvVokULVa9eXTNmzNDatWujQ2jPPvusRowYkWN7kydPVr169TRy5EiNHz8+x7pIwmjZsqWuueYalS5dWg0aNNDf/va36DN8OnbsGP3xC36Qa2vixIn68MMPVaVKFd1xxx16/vnnNXHiRJ199tkaPny4brzxRhH2cLp2/T+1avWnMKEcobffflvVqlXTgw8+qGrVqkW3G/nRbNeuXY6kt2zZMr322n+UklJa7dpdEf0O2dnZKlGiRDTRxb5q1z5KgJo3P1UvvPCiUlNTw3WllJJSOlf9S1Wp0mxVq3aTwMKypuF7a8GZat16o2rV6iVoLrg52jY9vbPgEkFlwS1heYnoerNLVLNmY9WqVVtpaVUF5wrQscc2VnDq+U7BSzniSU8/RNOnr1OJEiUF6M9//nPc9wvqBUnn/fff16ZNm7R9+3ZJwVmPmzZtkiTdfvt9SklJUbNmzdSgQQNJ0qZNm1Wt2hF5bpPwD5RVq1YpOzs7en1ZixYtouvPOOMMQTB0fNNNN0UT3LJly1SyZEldf/31koLhtZkzZ0bb9enTR7feemv0koK3wou1/ve//+mRRx5RamqqzjvvPHXq1ClHPJE/3qQgMUZ6eqmpqWrVqpWuueYaDRs2LLrdO+64Q5dffrluuOEG3XXXXQJ0wgknSAoeG/Lll19qwoQJ2rFjh37++Wfdd9990bMDP/jgA7388ssF/vssSLFLMEAJgmGsO2LKfIjMFWjTpk3aGHMF5fr16/Xhhx/u8XUaCxYs0HnnnRcd616/fn103mD79u16//33o9ts06aNateuHb2eqCBZWVk5/vLMzs7WO++8E3cj02nTpkV/ADZt2qSbb75Zhx12mKpUqaJRo0blON03Kysrbj///ve/1aJFC917771atGiRnnvuOTVr1kwpKSmqXr169K/s5cuX66abnlGjRrfo2muvC3+8btHRR/8rx4/ZSSc114kn9tajj25X/fqv65VXftd990mR8zKys6XevTcoNfXPgrqqV+97wXeCegrmnzJVr95fw+R4hk49NTiB4fjjx8ckrwsF5QUoLe1wASpRola4rlyeP/rVq/9FkKk6dc4WoIYNr9eVV66RWZrq12+uUqVKKS2thJo1u0ANGjRUjRo11KZN8EdBRsZfdM89vZSWlqY//elJpaSUjdt+jx49VKtWrejnp59+WkOGDMlRx8zUtm3b6OdDDjlEpUqVUqVKlaJDtYQ91BEjRujqq6/OM3lFlk877TTdc8890bLcPWAILqA++uij9cQTT+gvf/lL9Lo2QJdffk9c/dj1sOsehhAMW8buo1GjRtE/Ttq3b6+uXbsKgqHsyN1B9lSxSjAEE/KDgGdylT9Bzkn+f4bL9ck5yf8Tuyb5pwAt2TXJ3zYs707OSf63dxeXJxiXl6ysrKRcaDhlyhQtWbIk+jk7OzvfM+0K65NPPslx+nBu8+f/qipVtqh//6DHOHToUH399deF3v6vv0p33hmcWLBokTR9eraefnqbBg+Wtm3bpmeeeUYbNmzQkiXBNUsg3XPP//Tpp+P1wAPZOvHEDYLbdeqpf9Gbb76p337brnPOeVNdu85T1ap3ChqrRInjVaXKQ0pL+5tKltysXr2kxYt/1vHHPy/IkpmUnn5v+ON/kuA2wWFKTa2s8uUPE1QRHCf4QBdfPE4pKZFeXGlBQwFq3Lil+vT5u/r3/121a/dVjRqnq0KF+tEf4czMBjr00A6C4JquMWM2qmnTTho2bKR++kl67LFvVa5cJzVpcopq1DhMNWuemGeCPOWU63Tuuddr3bp1GjZsmLp37x5NWh06dNCgQYP0ww8/aO7cuRo3bpxGjBih7t27a+TIkdFtNGjQQHfddZe6d79VZcpcqIyMLapXr546deqkv/3tbxo2bJjuuSdIOn/961+j7a6++mqVKFFCLVu2VJcuXTR8+HC99tprqly5spo3b64bbrghmvx69+69T7eWKm4J5k/hQZgNzAxfbYEMYCzBacpjgSoxbfoQDHP9QHimWFjejGC+ZQHBXE3kNOXSwDsEpylPBo7cXVyeYNwfwbZtOe9ekCx9+0q33ppzXzt35nzqaqwZM4L627cHbX7/PTgzLmLuXOkf/wjuUff115LZ/9S8+XpNmCANGJAlyBL8LtioCy6IfaLrHMEwVa26Q9WqZQkGCH7SlVcG1zEFdaS0tNXKyBikdu2GC7YKflaTJi9r6tQslSgR1KlVa1f9Xe0U7vvvOvLIN9SnzxuaNWutunQJbnFUvnxwVl9WlrR27Vq9+eabcdc3zZsXnHG4ZElwtmLr1lLPnm9o6NAxys4O7kr+1FO79jl37rYcf/Rs2rQpeq3Qxx9/rPfff19ZWVl59ki2b9+u7Ozs6PVFibieqqAE4w8cCyX7gWPOucRZsAAyM4OH3wFceSXMnQsvvwxHHgmVK8PChcHnqVPh7bchKwtOOCGov3QpVK8OffvC77/DmWfCWWfBmjXBtho1gnvuCepmZMAVV8D/+3+79v/QQ8G+584NnhT7/POQnR2sS0kJ3i+5BEaNCu44fsIJQbw9ewYP5hs6FHbuhO3bYcCA4K7lTZsG75Gf5IsuglNOgTvvDO5uXqdOcMfxOnWCB/zVqxdsc+pUOOOMoO2sWTBnDixZAsccEzwsMCUFKlWCkiWDJ9lu3x4sv/kmjB4dPPm2SpW9/2/hDxwrxMt7MM4duLKzpVwnaOUpKyuo99FH8TdE3bRJmj9/V6/rww+lv/89uEvE2rXS6adLI0dKH3wQ9MZiLVki/fCDdPPNwa2CqlSRfvstGEp8/nnpmGOC5yuZ7boQN/ZVv36w7tBDg+uojjpq17oqVYLbHU2duqsscl++0qWD99atpYoVd5Udc8yuYcrIy0y6666g59aixa7yjAzpmmv2/tjjPZjd8x6Mcy4RduyA9euhatWc5Zs3w9VXw//+B/37Q+3aQflnn0HXrsGzjEqVgvR0mDgx6OkALF8e9DgyMqBNG/jxRyhRIugVLV4c9L7+/e8gXXzySdD7Kl8+2N/SpUH9RYuC3sqoUcE+tm2Dbt3gqqvglVeCbT/55N5934J6MJ5gQp5gnHP7Q1YWpO3mUY8SPPpoMKQVGdaLtJWCBBPrl1+CB/fVrZv/NrdvD7bZtm3wUL969RLzgD5PMIXgCcY55/ZcQQkmZX8H45xz7o/BE4xzzrmk8ATjnHMuKTzBOOecSwpPMM4555LCE4xzzrmk8ATjnHMuKTzBOOecSwq/0DJkZquAxfuwiaoEz505UBxI8R5IsYLHm2web3LtabxHSKqW1wpPMAliZlPzu5q1ODqQ4j2QYgWPN9k83uRKZLw+ROaccy4pPME455xLCk8widO/qAPYQwdSvAdSrODxJpvHm1wJi9fnYJxzziWF92Ccc84lhScY55xzSeEJZh+ZWRsz+8HM5ptZr6KOJy9mtsjMvjGzmWY2NSyrYmafmtm88L1yEcb3qpmtNLM5MWX5xmdmvcPj/YOZnVNM4v2Hmf0cHuOZZta2OMRrZoeb2X/N7Dsz+9bMeoTlxfL4FhBvcT2+pc1sspnNCuO9Pywvrsc3v3iTc3wl+WsvX0AqsAA4EigJzAKOL+q48ohzEVA1V9k/gV7hci/g8SKMrzVwIjBnd/EBx4fHuRRQJzz+qcUg3n8Ad+ZRt0jjBWoAJ4bL5YEfw5iK5fEtIN7ienwNKBculwAmAS2L8fHNL96kHF/vweyb5sB8ST9J2g4MA9oXcUyF1R4YGC4PBC4sqkAkfQmsyVWcX3ztgWGStklaCMwn+O+w3+QTb36KNF5JKyRND5c3AN8BNSmmx7eAePNT1PFK0sbwY4nwJYrv8c0v3vzsU7yeYPZNTWBpzOdlFPyPoagIGGNm08ysW1h2qKQVEPyjBg4psujyll98xfmY32xms8MhtMiQSLGJ18xqAycQ/NVa7I9vrnihmB5fM0s1s5nASuBTScX6+OYTLyTh+HqC2TeWR1lxPO/7FEknAucC3c2sdVEHtA+K6zHvBxwFNAFWAE+G5cUiXjMrB7wH3Cbp94Kq5lFWHOIttsdX0k5JTYBMoLmZNSigenGNNynH1xPMvlkGHB7zORNYXkSx5EvS8vB9JfA+QRf3VzOrARC+ryy6CPOUX3zF8phL+jX8h5sNDGDXMEKRx2tmJQh+rIdI+k9YXGyPb17xFufjGyFpHfAF0IZifHwjYuNN1vH1BLNvpgB1zayOmZUEOgIjijimHMysrJmVjywDZwNzCOLsElbrAgwvmgjzlV98I4COZlbKzOoAdYHJRRBfDpEfk9BFBMcYijheMzPgFeA7SU/FrCqWxze/eIvx8a1mZpXC5XTgTOB7iu/xzTPepB3f/XX2wsH6AtoSnOmyAOhT1PHkEd+RBGeBzAK+jcQIZABjgXnhe5UijHEoQbd8B8FfTF0Lig/oEx7vH4Bzi0m8g4FvgNnhP8oaxSFe4E8EQxqzgZnhq21xPb4FxFtcj28jYEYY1xzgvrC8uB7f/OJNyvH1W8U455xLCh8ic845lxSeYJxzziWFJxjnnHNJ4QnGOedcUniCcc45lxSeYJw7CJjZaWY2sqjjcC6WJxjnnHNJ4QnGuf3IzK4Kn8cx08xeCm88uNHMnjSz6WY21syqhXWbmNnE8AaE70duQGhmR5vZZ+EzPaab2VHh5suZ2btm9r2ZDQmvineuyHiCcW4/MbPjgMsJbj7aBNgJXAmUBaYruCHpOKBv2GQQcI+kRgRXWUfKhwDPS2oMnExwVwEI7jx8G8EzPI4ETknyV3KuQGlFHYBzfyBnAE2BKWHnIp3gJojZwFthnTeA/5hZRaCSpHFh+UDgnfC+cjUlvQ8gaStAuL3JkpaFn2cCtYGvkv6tnMuHJxjn9h8DBkrqnaPQ7N5c9Qq6f1NBw17bYpZ34v++XRHzITLn9p+xwKVmdghEn9t+BMG/w0vDOlcAX0laD6w1s1PD8quBcQqejbLMzC4Mt1HKzMrszy/hXGH5XzjO7SeS5prZ3wmeLppCcDfm7sAmoL6ZTQPWE8zTQHCb9xfDBPITcG1YfjXwkpk9EG6jw378Gs4Vmt9N2bkiZmYbJZUr6jicSzQfInPOOZcU3oNxzjmXFN6Dcc45lxSeYJxzziWFJxjnnHNJ4QnGOedcUniCcc45lxT/H0GoPKys/4nTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"emb_model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             multiple                  860       \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             multiple                  39        \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             multiple                  4         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             multiple                  36        \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             multiple                  156       \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             multiple                  3840      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             multiple                  4128      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             multiple                  528       \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             multiple                  272       \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             multiple                  68        \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             multiple                  5         \n",
      "=================================================================\n",
      "Total params: 9,936\n",
      "Trainable params: 9,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "emb_model = train_and_get_deep_learning_model(\"emb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Result of Embedding Model and Generate the CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[157251.16  248304.06  211173.38  160709.7    70034.586  95465.78\n",
      "  44185.54   48945.723  77350.85  118380.88 ] (5000,)\n"
     ]
    }
   ],
   "source": [
    "predict(emb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for Tree-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16589, 123) (16589,)\n",
      "(13271, 123) (13271,) (3318, 123) (3318,)\n",
      "Parameters: {'max_depth': 60} (RMSE: 23615.85570644092)\n"
     ]
    }
   ],
   "source": [
    "# For Decision Tree\n",
    "parameters = {'max_depth':[20,40,60,80,100]}\n",
    "grid_search(\"tree\", parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16589, 123) (16589,)\n",
      "(13271, 123) (13271,) (3318, 123) (3318,)\n",
      "Parameters: {'max_depth': 20, 'n_estimators': 200} (RMSE: 20223.445655211148)\n"
     ]
    }
   ],
   "source": [
    "# For Random Forest\n",
    "parameters = {'n_estimators':[100,150,200],'max_depth':[15,20,25]}\n",
    "grid_search(\"forest\", parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16589, 123) (16589,)\n",
      "(13271, 123) (13271,) (3318, 123) (3318,)\n",
      "Parameters: {'learning_rate': 0.15, 'n_estimators': 400} (RMSE: 20231.49654140914)\n"
     ]
    }
   ],
   "source": [
    "# For Gradient Boosting Rrgession\n",
    "parameters = {'n_estimators':[300,350,400],'learning_rate':[0.1,0.15,0.2]}\n",
    "grid_search(\"gbr\", parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Tree-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16589, 123) (16589,)\n",
      "(13271, 123) (13271,) (3318, 123) (3318,)\n",
      "25186.063664124038\n",
      "[244400. 219800. 575300.  72400.  94600.  57800.  55000.  77300. 146100.\n",
      " 193400. 153800.  85800.  62500. 150500.  21800.]\n",
      "[241800. 212500. 600600.  71300.  94400.  59400.  55900.  77600. 151800.\n",
      " 183500. 152900.  83000.  59200. 150500.  13100.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=60)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_get_tree_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16589, 123) (16589,)\n",
      "(13271, 123) (13271,) (3318, 123) (3318,)\n",
      "20161.7246159245\n",
      "[248508.16964286 217728.5        574756.          72082.637548\n",
      "  95552.30708509  57826.65304323  55602.80717949  77382.52014386\n",
      " 149504.5        189435.78571429 163054.          83155.83299344\n",
      "  64310.         149837.14145327  28791.00813492]\n",
      "[241800. 212500. 600600.  71300.  94400.  59400.  55900.  77600. 151800.\n",
      " 183500. 152900.  83000.  59200. 150500.  13100.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=20, n_estimators=200, random_state=2021)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_get_forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16589, 123) (16589,)\n",
      "(13271, 123) (13271,) (3318, 123) (3318,)\n",
      "19888.026404773278\n",
      "[257340.14553086 220788.6212772  549174.99469605  77638.00881067\n",
      "  96634.78883606  78862.67074946  56552.33966576  81114.27313008\n",
      " 169058.31385724 195114.2208581  163862.89833371  85020.0836861\n",
      "  51740.19114359 144085.88576201  28779.06766311]\n",
      "[241800. 212500. 600600.  71300.  94400.  59400.  55900.  77600. 151800.\n",
      " 183500. 152900.  83000.  59200. 150500.  13100.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.15, max_features='sqrt',\n",
       "                          n_estimators=400, random_state=2021)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_get_gbr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the result when combine different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16589, 123) (16589,)\n",
      "(13271, 123) (13271,) (3318, 123) (3318,)\n",
      "gbr rmse: 19888.026404773278, forest rmse: 20161.7246159245, dl rmse: 21949.906520801313, ensemblee rmse: 18136.46786168507\n"
     ]
    }
   ],
   "source": [
    "get_ensemble_model(baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16589, 123) (16589,)\n",
      "(13271, 123) (13271,) (3318, 123) (3318,)\n",
      "gbr rmse: 19888.026404773278, forest rmse: 20161.7246159245, dl rmse: 20896.866858278154, ensemblee rmse: 17730.58258886812\n"
     ]
    }
   ],
   "source": [
    "get_ensemble_model(emb_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c468ecac6db731323e72de7cfd0c8c9743bf341e8fe46d77b9f6f4da3ba35254"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('media': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
